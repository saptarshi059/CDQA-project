{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26e327",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Generate tuples (Question, Token, SemType)\n",
    "%cd ~/Desktop/CDQA-project\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "s = pd.read_json('metamap_output.json')\n",
    "\n",
    "documents = s.iloc[0][0]['Document']['Utterances']\n",
    "Metamap_Tokenizations = [] #To be used for final QA\n",
    "\n",
    "def retrieve_tokens(SyntaxUnits):\n",
    "    tokens = []\n",
    "    for i in range(len(SyntaxUnits)):\n",
    "        tokens.append(SyntaxUnits[i]['InputMatch'])\n",
    "    return tokens\n",
    "\n",
    "def retrieve_mappings(Mappings):\n",
    "    mapped_semantic_types = []\n",
    "    #No mappings found\n",
    "    if len(Mappings) == 0:\n",
    "        return [] #These words will get their embeddings from BERT\n",
    "    else:\n",
    "        candidates = Mappings[0]['MappingCandidates'] #Choosing Only top mappings\n",
    "        for cnd in candidates:\n",
    "            mapped_semantic_types.append([' '.join(cnd['MatchedWords']), cnd['CandidateCUI'], \\\n",
    "                                              cnd['CandidatePreferred'], cnd['SemTypes'][0]])\n",
    "            entities.add(cnd['CandidatePreferred'])\n",
    "    return mapped_semantic_types\n",
    "\n",
    "entities = set()\n",
    "for doc in tqdm(documents):\n",
    "    Phrases = doc['Phrases']\n",
    "    Phrase_Tokenizations = []\n",
    "    Mappings = []\n",
    "    for ph in Phrases:\n",
    "        Phrase_Tokenizations.append(retrieve_tokens(ph['SyntaxUnits']))\n",
    "        Mappings.append(retrieve_mappings(ph['Mappings']))\n",
    "    #Flattening the Lists\n",
    "    Phrase_Tokenizations = [item for sublist in Phrase_Tokenizations for item in sublist]\n",
    "    Mappings = [item for sublist in Mappings for item in sublist]\n",
    "    #Creating the final list\n",
    "    Metamap_Tokenizations.append((doc['UttText'], Phrase_Tokenizations, Mappings))\n",
    "    \n",
    "#Removing extra spaces from each question\n",
    "for index, tup in enumerate(Metamap_Tokenizations):\n",
    "    temp = list(tup)\n",
    "    #The only question which was causing an issue\n",
    "    if type(temp[0]) != str:\n",
    "        temp[0] = 'What were the impact of event scaleâ€“revised scores?'\n",
    "    else:\n",
    "        temp[0] = temp[0].strip()\n",
    "    Metamap_Tokenizations[index] = tuple(temp)\n",
    "    \n",
    "print(f\"Number of entities discovered: {len(entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c162aff",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Replacing each shorthand mapping with KG concept\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "for i in range(len(Metamap_Tokenizations)):\n",
    "    for j in range(len(Metamap_Tokenizations[i][2])):\n",
    "        mycursor.execute(\"select STY_RL from SRDEF where ABR = '%s' \" % Metamap_Tokenizations[i][2][j][3])\n",
    "        Metamap_Tokenizations[i][2][j][3] = mycursor.fetchall()[0][0]\n",
    "\n",
    "mycursor.close()\n",
    "\n",
    "#Saving Metamap_Tokenizations for use during question embedding creation\n",
    "pd.DataFrame(Metamap_Tokenizations, columns=['Question','Tokenization',\\\n",
    "                                            'Mappings']).to_pickle('Metamap_Tokenizations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f268d3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating the CUI_Preferred_Concept_Semantic_Type_Lookup_Table\n",
    "cuis = [y[1] for x in Metamap_Tokenizations for y in x[2]]\n",
    "pc = [y[2] for x in Metamap_Tokenizations for y in x[2]]\n",
    "st = [y[3] for x in Metamap_Tokenizations for y in x[2]]\n",
    "CUI_Preferred_Concept_Semantic_Type_Lookup_Table = pd.DataFrame(zip(cuis, pc, st), \\\n",
    "                                                            columns=['CUI','Preferred_Concept', 'Semantic_Type']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274d62b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#New version of creating KGT (Metathesaurus + Semantic Network)\n",
    "import sqlite3\n",
    "from collections import namedtuple\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#For sqlite (Metathesaurus)\n",
    "conn = sqlite3.connect('umls.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#All Knowledge Graph Triples\n",
    "KGT = set()\n",
    "\n",
    "#To make the code more readable (pythonic)\n",
    "CUIREL = namedtuple('CUIREL', ['CUI', 'REL'])\n",
    "STREL = namedtuple('STREL', ['ST', 'REL'])\n",
    "\n",
    "for row in tqdm(CUI_Preferred_Concept_Semantic_Type_Lookup_Table.itertuples(index=False)):\n",
    "    '''--->Extracting relations from the Metathesaurus<---'''\n",
    "    #This gives the \"incoming\" relations i.e. CUI2 - RELA - CUI1\n",
    "    incoming_relations = []\n",
    "    cursor.execute('''SELECT DISTINCT CUI2, RELA\n",
    "                                  FROM MRREL\n",
    "                                  WHERE CUI1 = '%s' AND RELA <> '';''' % row.CUI)\n",
    "    results = cursor.fetchall()\n",
    "    #Checking which of the returned tuples are present in our lookup table \n",
    "    for tupA in results:\n",
    "        CUIREL2 = CUIREL(*tupA)\n",
    "        for tupB in CUI_Preferred_Concept_Semantic_Type_Lookup_Table.query('CUI==@CUIREL2.CUI').itertuples(index=False):\n",
    "            KGT.add((tupB.Preferred_Concept, CUIREL2.REL, row.Preferred_Concept))\n",
    "    \n",
    "    #This gives the \"outgoing\" relations i.e. CUI1 - RELA - CUI2\n",
    "    outgoing_relations = []\n",
    "    cursor.execute('''SELECT DISTINCT CUI1, RELA \n",
    "                                  FROM MRREL\n",
    "                                  WHERE CUI2 = '%s' AND STYPE2 = 'CUI' AND RELA <> '';''' % row.CUI)\n",
    "    results = cursor.fetchall()\n",
    "    #Checking which of the returned tuples are present in our lookup table\n",
    "    for tupA in results:\n",
    "        CUIREL2 = CUIREL(*tupA)\n",
    "        for tupB in CUI_Preferred_Concept_Semantic_Type_Lookup_Table.query('CUI==@CUIREL2.CUI').itertuples(index=False):\n",
    "            KGT.add((row.Preferred_Concept, CUIREL2.REL, tupB.Preferred_Concept))\n",
    "    \n",
    "    '''--->Extracting relations from the Semantic Network<---'''\n",
    "    #Generating possible semantic types connected to the current one\n",
    "    mycursor.execute('''SELECT STY_RL2, RL\n",
    "                        FROM SRSTR \n",
    "                        WHERE STY_RL1 = '%s';''' % row.Semantic_Type)\n",
    "    possible_semantic_types_2 = [STREL(*x) for x in mycursor.fetchall()]\n",
    "    for sem_type_2 in possible_semantic_types_2:\n",
    "        for result in CUI_Preferred_Concept_Semantic_Type_Lookup_Table.query('Semantic_Type==@sem_type_2.ST').itertuples(index=False):\n",
    "            KGT.add((row.Preferred_Concept, sem_type_2.REL, result.Preferred_Concept))\n",
    "\n",
    "conn.close()\n",
    "mycursor.close()\n",
    "\n",
    "print(f\"Number of triples extracted: {len(KGT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2ec36",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating Train/Validation/Test splits for training KGE's\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Converting set to pandas dataframe for easy splitting\n",
    "KGT = pd.DataFrame(KGT)\n",
    "\n",
    "#Giving the KGT dataframe meaningful column names\n",
    "KGT.rename(columns={0: \"E1\", 1: \"Rel\", 2: \"E2\"}, inplace=True)\n",
    "\n",
    "#80/10/10 split\n",
    "train, validation, test = np.split(KGT.sample(frac=1, random_state=42), [int(.8*len(KGT)), int(.9*len(KGT))])\n",
    "\n",
    "#Creating folder where dataset files will be saved\n",
    "try:\n",
    "    os.mkdir(os.path.join(os.path.abspath(os.getcwd()), \"UMLS_KG\"))\n",
    "    print(\"KG Directory Created\")\n",
    "except:\n",
    "    print(\"KG Directory Already Exists\")\n",
    "\n",
    "dataset_path = os.path.abspath(\"UMLS_KG\")\n",
    "\n",
    "#Saving datasets as .txt files to be used for training the KGE\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-train.txt'), train.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-valid.txt'), validation.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-test.txt'), test.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "\n",
    "print('KG dataset saved...')\n",
    "\n",
    "KGT.to_pickle(os.path.join(dataset_path, \"KGT.pkl\"))\n",
    "print('KGT saved as dataframe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
