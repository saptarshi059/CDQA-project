{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a26e327",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/CDQA-project\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4987a0d8a4345eea20f93e0cb5a9e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate tuples (Question, Token, SemType)\n",
    "%cd ~/Desktop/CDQA-project\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "s = pd.read_json('sample.json')\n",
    "\n",
    "documents = s.iloc[0][0]['Document']['Utterances']\n",
    "Metamap_Tokenizations = [] #To be used for final QA\n",
    "\n",
    "def retrieve_tokens(SyntaxUnits):\n",
    "    tokens = []\n",
    "    for i in range(len(SyntaxUnits)):\n",
    "        tokens.append(SyntaxUnits[i]['InputMatch'])\n",
    "    return tokens\n",
    "\n",
    "def retrieve_mappings(Mappings):\n",
    "    mapped_semantic_types = []\n",
    "    #No mappings found\n",
    "    if len(Mappings) == 0:\n",
    "        return [] #These words will get their embeddings from BERT\n",
    "    else:\n",
    "        candidates = Mappings[0]['MappingCandidates'] #Choosing Only top mappings\n",
    "        for cnd in candidates:\n",
    "            mapped_semantic_types.append([' '.join(cnd['MatchedWords']),cnd['SemTypes'][0]])\n",
    "    return mapped_semantic_types\n",
    "    \n",
    "for doc in tqdm(documents):\n",
    "    Phrases = doc['Phrases']\n",
    "    Phrase_Tokenizations = []\n",
    "    Mappings = []\n",
    "    for ph in Phrases:\n",
    "        Phrase_Tokenizations.append(retrieve_tokens(ph['SyntaxUnits']))\n",
    "        Mappings.append(retrieve_mappings(ph['Mappings']))\n",
    "    #Flattening the Lists\n",
    "    Phrase_Tokenizations = [item for sublist in Phrase_Tokenizations for item in sublist]\n",
    "    Mappings = [item for sublist in Mappings for item in sublist]\n",
    "    #Creating the final list\n",
    "    Metamap_Tokenizations.append((doc['UttText'], Phrase_Tokenizations, Mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c162aff",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Replacing each shorthand mapping with KG concept\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "for i in range(len(Metamap_Tokenizations)):\n",
    "    for j in range(len(Metamap_Tokenizations[i][2])):\n",
    "        mycursor.execute(\"select STY_RL from SRDEF where ABR = '%s' \" % Metamap_Tokenizations[i][2][j][1])\n",
    "        Metamap_Tokenizations[i][2][j][1] = mycursor.fetchall()[0][0]\n",
    "\n",
    "mycursor.close()\n",
    "\n",
    "#Saving Metamap_Tokenizations for use during question embedding creation\n",
    "pd.DataFrame(Metamap_Tokenizations,columns=['Question','Tokenization',\\\n",
    "                                            'Mappings']).to_pickle('Metamap_Tokenizations.pkl', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f819869d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288e8f38d3364f76872d76b30319e48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating the KG triples (KGT)\n",
    "from itertools import permutations\n",
    "\n",
    "All_Mappings = [y for x in Metamap_Tokenizations for y in x[2]]\n",
    "All_Concept_Pairs = permutations(All_Mappings, 2)\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "KGT = set() #I'm using a set to avoid repeated triples.\n",
    "\n",
    "for term_pair in tqdm(All_Concept_Pairs):\n",
    "    semantic_type1 = term_pair[0][1]\n",
    "    semantic_type2 = term_pair[1][1]\n",
    "    mycursor.execute(\"select RL from SRSTR where STY_RL1 = '%s' and STY_RL2 = '%s' \" % (semantic_type1, semantic_type2))\n",
    "    relation = mycursor.fetchall()\n",
    "    if relation != []:\n",
    "        KGT.add((semantic_type1, relation[0][0], semantic_type2))\n",
    "\n",
    "mycursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab2ec36",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG Directory Already Exists\n",
      "KG dataset saved...\n",
      "KGT saved as dataframe\n"
     ]
    }
   ],
   "source": [
    "#Creating Train/Validation/Test splits for training KGE's\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Converting set to pandas dataframe for easily split \n",
    "KGT = pd.DataFrame(KGT)\n",
    "\n",
    "#Giving the KGT dataframe meaningful column names\n",
    "KGT.rename(columns={0: \"E1\", 1: \"Rel\", 2: \"E2\"}, inplace=True)\n",
    "\n",
    "#80/10/10 split\n",
    "train, validation, test = np.split(KGT.sample(frac=1, random_state=42), [int(.8*len(KGT)), int(.9*len(KGT))])\n",
    "\n",
    "#Creating folder where dataset files will be saved\n",
    "try:\n",
    "    os.mkdir(os.path.join(os.path.abspath(os.getcwd()), \"UMLS_KG\"))\n",
    "    print(\"KG Directory Created\")\n",
    "except OSError as error:\n",
    "    print(\"KG Directory Already Exists\")\n",
    "\n",
    "dataset_path = os.path.abspath(\"UMLS_KG\")\n",
    "\n",
    "#Saving datasets as .txt files to be used for training the KGE\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-train.txt'), train.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-valid.txt'), validation.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-test.txt'), test.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "\n",
    "print('KG dataset saved...')\n",
    "\n",
    "KGT.to_pickle(os.path.join(dataset_path, \"KGT.pkl\"))\n",
    "print('KGT saved as dataframe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
