{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a463e2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_rel2desc(results, file_name):\n",
    "    RELs = []\n",
    "    Descriptions = []\n",
    "    for val in results:\n",
    "        RELs.append(val[0].strip())\n",
    "        Descriptions.append(val[0].replace('_',' ').strip())\n",
    "\n",
    "    pd.DataFrame(zip(RELs, Descriptions), columns=['REL', 'Description']).to_pickle(f'{file_name}_rel2desc.pkl')\n",
    "    print(f'Created {file_name}_rel2desc...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05884fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "It turns out that all relations mentioned in metathesaurus_rel2desc aren't present in MRRREL. The latter has 950 \n",
    "while the former has 976. So, we don't use the former & create a custom mapping (REL -> DESC) for all relations \n",
    "present in MRREL. \n",
    "'''\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(os.path.join(os.path.abspath('../..'), 'umls.db'))\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT DISTINCT RELA FROM MRREL\")\n",
    "results = cursor.fetchall()\n",
    "\n",
    "create_rel2desc(results, 'MRREL')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49eaa8d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating semantic n/w rel2desc dataframe\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "mycursor.execute(\"SELECT DISTINCT RL FROM SRSTR\")\n",
    "results = mycursor.fetchall()\n",
    "\n",
    "create_rel2desc(results, 'SEM_NW')\n",
    "mycursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55947d8e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Reading in the necessary files\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "BERT_variant = 'phiyodr/bert-base-finetuned-squad2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_variant)\n",
    "\n",
    "UMLS_KG_path = os.path.abspath('../../../UMLS_KG')\n",
    "\n",
    "with open(os.path.join(UMLS_KG_path, 'KGT.pkl'), 'rb') as f:\n",
    "    KGT = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(UMLS_KG_path, 'entity2idx.pkl'), 'rb') as f:\n",
    "    entity2id = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(UMLS_KG_path, 'relation2idx.pkl'), 'rb') as f:\n",
    "    relation2id = pickle.load(f)\n",
    "\n",
    "KGE_path = os.path.join(UMLS_KG_path, os.path.relpath('embeddings/distmult'))\n",
    "\n",
    "ent_embeddings = pd.read_csv(os.path.join(KGE_path, 'ent_embedding.tsv'), sep='\\t', header=None)\n",
    "rel_embeddings = pd.read_csv(os.path.join(KGE_path, 'rel_embedding.tsv'), sep='\\t', header=None)    \n",
    "\n",
    "MRREL_rel2desc = pd.read_pickle('MRREL_rel2desc.pkl')\n",
    "SEM_NW_rel2desc = pd.read_pickle('SEM_NW_rel2desc.pkl')\n",
    "\n",
    "total_rel2desc = pd.concat([MRREL_rel2desc, SEM_NW_rel2desc], ignore_index=True)\n",
    "\n",
    "print('Loaded all necessary files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5e5e0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating training dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "#Instead of storing the full target vector, I am storing the indices of the natural text word pieces. \n",
    "This allows us to create the target representation at runtime.\n",
    "'''\n",
    "mean_embeddings = []\n",
    "multiple_hot_targets_indices = []\n",
    "\n",
    "def gen_sample(triple):\n",
    "    \n",
    "    '''\n",
    "    We can expand using this scheme since we've taken care of the correct direction during KGT construction.\n",
    "    It will, always be E1 - REL - E2.\n",
    "    '''\n",
    "    natural_text = triple.E1 + ' ' + \\\n",
    "                    total_rel2desc.query('REL==@triple.Rel').Description.values[0] + ' ' + triple.E2\n",
    "\n",
    "    '''\n",
    "    #Creating the target multiple-hot vector.\n",
    "    target = np.zeros(vocab_size)\n",
    "\n",
    "    #Replacing those elements in the target vector with 1, which are activated for this sample.\n",
    "    np.put(target, tokenizer(natural_text)['input_ids'], 1)\n",
    "    '''\n",
    "        \n",
    "    #Creating the mean embedding for the triple\n",
    "    E1_tensor = torch.from_numpy(ent_embeddings.iloc[entity2id[triple.E1]].to_numpy()).float()\n",
    "    Rel_tensor = torch.from_numpy(rel_embeddings.iloc[relation2id[triple.Rel]].to_numpy()).float()\n",
    "    E2_tensor = torch.from_numpy(ent_embeddings.iloc[entity2id[triple.E2]].to_numpy()).float()\n",
    "\n",
    "    return torch.mean(torch.stack([E1_tensor, Rel_tensor, E2_tensor]), dim=0), tokenizer(natural_text)['input_ids']\n",
    "    \n",
    "print('Creating training samples according to the conversion scheme...')\n",
    "for trple in tqdm(KGT.itertuples()):\n",
    "    train, test = gen_sample(trple)\n",
    "    mean_embeddings.append(train)\n",
    "    multiple_hot_targets_indices.append(test)\n",
    "\n",
    "pd.DataFrame(zip(mean_embeddings, multiple_hot_targets_indices), columns=['train', 'test']).to_pickle('Homogenization_data.pkl')\n",
    "\n",
    "print('FFN training dataset created...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
