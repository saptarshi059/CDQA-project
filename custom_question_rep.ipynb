{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Extracting embeddings for non-domain terms. I'm simply using BERT's tokenizer for the nDT's.\n",
    "#Creating question representations in this block.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DTE_BERT_Lookup_Table = pd.read_pickle(os.path.join(os.path.abspath('UMLS_KG'), 'embeddings/distmult/DTE_to_BERT.pkl'))\n",
    "\n",
    "Metamap_Tokenizations = pd.read_pickle('Metamap_Tokenizations.pkl')\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "BERT_embeddings = model.get_input_embeddings()\n",
    "\n",
    "CLS_embedding = BERT_embeddings(torch.LongTensor([101]))\n",
    "SEP_embedding = BERT_embeddings(torch.LongTensor([102]))\n",
    "\n",
    "all_entities = DTE_BERT_Lookup_Table['Term'].to_list()\n",
    "\n",
    "def custom_question_rep_gen(ques):\n",
    "    \n",
    "    def clean_term(word):\n",
    "        return re.sub(r'[\\W\\s]', '', word).lower()\n",
    "\n",
    "    tup = Metamap_Tokenizations.query(\"Question==@ques\")\n",
    "\n",
    "    metamap_tokenized_question = tup['Tokenization'][0]\n",
    "\n",
    "    #Removing punctuations/spaces from domain-terms for easy comparison\n",
    "    mappings = tup['Mappings'][0]\n",
    "    for i,x in enumerate(mappings):\n",
    "        mappings[i][0] = clean_term(x[0])\n",
    "\n",
    "    domain_terms = [x[0] for x in mappings]\n",
    "\n",
    "    question_embeddings = []\n",
    "    for word in metamap_tokenized_question:\n",
    "        '''\n",
    "        This is done to easily check if the current word is a DT or not since DT form of the same word \n",
    "        are obtained bit differently.\n",
    "        '''\n",
    "        filtered_word = clean_term(word)\n",
    "\n",
    "        '''\n",
    "        This means that the filtered_word has to be a domain term which also has a KG expansion. If if does not,\n",
    "        then use its BERT embeddings.\n",
    "        '''\n",
    "\n",
    "        if filtered_word in domain_terms: #Use DTE_BERT_Matrix\n",
    "            mapped_concept = mappings[domain_terms.index(filtered_word)][1]\n",
    "            if mapped_concept in all_entities: \n",
    "                question_embeddings.append(DTE_BERT_Lookup_Table.query(\"Term==@mapped_concept\")['Embedding'].values[0])\n",
    "            \n",
    "        #The mapped_concept doesn't have an expansion in the KG or the term isn't a DT. Thus, its BERT embeddings are used.\n",
    "        else:\n",
    "            subword_indices = tokenizer(word)['input_ids'][1:-1] #Take all tokens between [CLS] & [SEP]\n",
    "            for index in subword_indices:\n",
    "                question_embeddings.append(BERT_embeddings(torch.LongTensor([index])))\n",
    "\n",
    "    #In this way, I don't have to add the CLS & SEP embeddings during fine-tuning.\n",
    "    final_representation = torch.unsqueeze(torch.cat((CLS_embedding,\\\n",
    "                                                      torch.cat([*question_embeddings]),\\\n",
    "                                                      SEP_embedding)), dim=1)\n",
    "\n",
    "    return final_representation\n",
    "\n",
    "custom_question_embeddings = custom_question_rep_gen('What is the main cause of HIV-1 infection in children? ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
