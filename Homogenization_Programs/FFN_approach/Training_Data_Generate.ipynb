{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55947d8e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Reading in the necessary files\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "BERT_variant = 'navteca/roberta-base-squad2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_variant)\n",
    "\n",
    "UMLS_KG_path = os.path.abspath('../../UMLS_KG')\n",
    "\n",
    "with open(os.path.join(UMLS_KG_path, 'KGT.pkl'), 'rb') as file:\n",
    "    KGT = pickle.load(file)\n",
    "    \n",
    "with open(os.path.join(UMLS_KG_path, 'entity2idx.pkl'), 'rb') as f:\n",
    "    entity2id = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(UMLS_KG_path, 'relation2idx.pkl'), 'rb') as f:\n",
    "    relation2id = pickle.load(f)\n",
    "\n",
    "KGE_path = os.path.join(UMLS_KG_path, os.path.relpath('embeddings/distmult'))\n",
    "\n",
    "ent_embeddings = pd.read_csv(os.path.join(KGE_path, 'ent_embedding.tsv'), sep='\\t', header=None)\n",
    "rel_embeddings = pd.read_csv(os.path.join(KGE_path, 'rel_embedding.tsv'), sep='\\t', header=None)    \n",
    "\n",
    "metamap_rel2desc = pd.read_csv(os.path.abspath('../../metamap_rel2desc.csv'))\n",
    "\n",
    "print('Loaded all necessary files...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb029d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating semantic n/w rel2desc dataframe & concatenating it with metamap rel2desc to get the entire collection\n",
    "import numpy as np\n",
    "\n",
    "file = np.loadtxt(os.path.abspath('../../sem_nw_rels.txt'), dtype=str)\n",
    "sem_nw_rel = []\n",
    "sem_new_rel_desc = []\n",
    "for line in file:\n",
    "    sem_nw_rel.append(line)\n",
    "    sem_new_rel_desc.append(line.replace('_',' '))\n",
    "sem_nw_rel_df = pd.DataFrame(zip(sem_nw_rel, sem_new_rel_desc), columns = ['REL', 'Description'])\n",
    "'''\n",
    "Even though there are certain duplicate elements such as 'isa' in both metathesaurus & sem n/w\n",
    "we have to keep both since each has a different vector.\n",
    "'''\n",
    "total_rel2desc = pd.concat([metamap_rel2desc, sem_nw_rel_df], ignore_index=True)\n",
    "\n",
    "print('Complete rel2desc created...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5e5e0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating training dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "mean_embeddings = []\n",
    "multiple_hot_targets = []\n",
    "\n",
    "def gen_sample(triples_collection):\n",
    "    \n",
    "    def gen_target_vector(triple):\n",
    "        '''\n",
    "        We can expand using this scheme since we've taken care of the correct direction during KGT construction.\n",
    "        It will, always be E1 - REL - E2.\n",
    "        '''\n",
    "        natural_text = triple.E1 + ' ' + \\\n",
    "                        total_rel2desc.query('REL==@triple.Rel').Description.values[0] + ' ' + triple.E2\n",
    "\n",
    "        #Creating the target multiple-hot vector.\n",
    "        target = np.zeros(vocab_size)\n",
    "        \n",
    "        #Replacing those elements in the target vector with 1, which are activated for this sample.\n",
    "        np.put(target, tokenizer(natural_text)['input_ids'], 1)\n",
    "        \n",
    "        return target\n",
    "        \n",
    "    def gen_mean_embeddings(triple):\n",
    "        E1_tensor = torch.from_numpy(ent_embeddings.iloc[entity2id[triple.E1]].to_numpy()).float()\n",
    "        Rel_tensor = torch.from_numpy(rel_embeddings.iloc[relation2id[triple.Rel]].to_numpy()).float()\n",
    "        E2_tensor = torch.from_numpy(ent_embeddings.iloc[entity2id[triple.E2]].to_numpy()).float()\n",
    "        \n",
    "        return torch.mean(torch.stack([E1_tensor, Rel_tensor, E2_tensor]), dim=0)\n",
    "  \n",
    "    for triple in triples_collection.itertuples():\n",
    "        mean_embeddings.append(gen_mean_embeddings(triple))\n",
    "        multiple_hot_targets.append(gen_target_vector(triple))\n",
    "\n",
    "print('Creating training samples according to the conversion scheme...')\n",
    "for current_entity in tqdm(entity2id.keys()):\n",
    "    gen_sample(KGT.query('E1==@current_entity or E2==@current_entity'))\n",
    "\n",
    "#Saving the dataset as pandas dataframe\n",
    "pd.DataFrame(zip(mean_embeddings, multiple_hot_targets), \\\n",
    "             columns = ['mean_embedding', 'vocab_mapping']).to_pickle('Homogenization_data.pkl')\n",
    "\n",
    "print('FFN training dataset created...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
