{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a26e327",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/CDQA-project\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38215e1b80774ca3afa0e38710dbb023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2007.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate tuples (Question, Token, SemType)\n",
    "%cd ~/Desktop/CDQA-project\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "s = pd.read_json('metamap_output.json')\n",
    "\n",
    "documents = s.iloc[0][0]['Document']['Utterances']\n",
    "Metamap_Tokenizations = [] #To be used for final QA\n",
    "\n",
    "def retrieve_tokens(SyntaxUnits):\n",
    "    tokens = []\n",
    "    for i in range(len(SyntaxUnits)):\n",
    "        tokens.append(SyntaxUnits[i]['InputMatch'])\n",
    "    return tokens\n",
    "\n",
    "def retrieve_mappings(Mappings):\n",
    "    mapped_semantic_types = []\n",
    "    #No mappings found\n",
    "    if len(Mappings) == 0:\n",
    "        return [] #These words will get their embeddings from BERT\n",
    "    else:\n",
    "        candidates = Mappings[0]['MappingCandidates'] #Choosing Only top mappings\n",
    "        for cnd in candidates:\n",
    "            mapped_semantic_types.append([' '.join(cnd['MatchedWords']),cnd['SemTypes'][0]])\n",
    "    return mapped_semantic_types\n",
    "    \n",
    "for doc in tqdm(documents):\n",
    "    Phrases = doc['Phrases']\n",
    "    Phrase_Tokenizations = []\n",
    "    Mappings = []\n",
    "    for ph in Phrases:\n",
    "        Phrase_Tokenizations.append(retrieve_tokens(ph['SyntaxUnits']))\n",
    "        Mappings.append(retrieve_mappings(ph['Mappings']))\n",
    "    #Flattening the Lists\n",
    "    Phrase_Tokenizations = [item for sublist in Phrase_Tokenizations for item in sublist]\n",
    "    Mappings = [item for sublist in Mappings for item in sublist]\n",
    "    #Creating the final list\n",
    "    Metamap_Tokenizations.append((doc['UttText'], Phrase_Tokenizations, Mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c162aff",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Replacing each shorthand mapping with KG concept\n",
    "import mysql.connector\n",
    "from numba import jit\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "@jit(nopython=True)\n",
    "def shrthand_to_mapped_concept(Metamap_Tokenizations):\n",
    "    for i in range(len(Metamap_Tokenizations)):\n",
    "        for j in range(len(Metamap_Tokenizations[i][2])):\n",
    "            mycursor.execute(\"select STY_RL from SRDEF where ABR = '%s' \" % Metamap_Tokenizations[i][2][j][1])\n",
    "            Metamap_Tokenizations[i][2][j][1] = mycursor.fetchall()[0][0]\n",
    "    return Metamap_Tokenizations\n",
    "\n",
    "mycursor.close()\n",
    "\n",
    "Metamap_Tokenizations = shrthand_to_mapped_concept(Metamap_Tokenizations)\n",
    "\n",
    "#Saving Metamap_Tokenizations for use during question embedding creation\n",
    "pd.DataFrame(Metamap_Tokenizations,columns=['Question','Tokenization',\\\n",
    "                                            'Mappings']).to_pickle('Metamap_Tokenizations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f819869d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3e94763cb64b8ca1b150b093dbbe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3e3910d956af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msemantic_type1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msemantic_type2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmycursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select RL from SRSTR where STY_RL1 = '%s' and STY_RL2 = '%s' \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msemantic_type1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_type2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mrelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmycursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrelation\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mysql/connector/cursor_cext.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, params, multi)\u001b[0m\n\u001b[1;32m    274\u001b[0m             result = self._cnx.cmd_query(stmt, raw=self._raw,\n\u001b[1;32m    275\u001b[0m                                          \u001b[0mbuffered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                                          raw_as_string=self._raw_as_string)\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMySQLInterfaceError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             raise errors.get_mysql_exception(msg=exc.msg, errno=exc.errno,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mysql/connector/connection_cext.py\u001b[0m in \u001b[0;36mcmd_query\u001b[0;34m(self, query, raw, buffered, raw_as_string)\u001b[0m\n\u001b[1;32m    507\u001b[0m             self._cmysql.query(query,\n\u001b[1;32m    508\u001b[0m                                \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffered\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                                raw_as_string=raw_as_string)\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMySQLInterfaceError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise errors.get_mysql_exception(exc.errno, msg=exc.msg,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Generating the KG triples (KGT)\n",
    "from itertools import permutations\n",
    "\n",
    "All_Mappings = [y for x in Metamap_Tokenizations for y in x[2]]\n",
    "All_Concept_Pairs = permutations(All_Mappings, 2)\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "KGT = set() #I'm using a set to avoid repeated triples.\n",
    "\n",
    "for term_pair in tqdm(All_Concept_Pairs):\n",
    "    semantic_type1 = term_pair[0][1]\n",
    "    semantic_type2 = term_pair[1][1]\n",
    "    mycursor.execute(\"select RL from SRSTR where STY_RL1 = '%s' and STY_RL2 = '%s' \" % (semantic_type1, semantic_type2))\n",
    "    relation = mycursor.fetchall()\n",
    "    if relation != []:\n",
    "        KGT.add((semantic_type1, relation[0][0], semantic_type2))\n",
    "\n",
    "mycursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2ec36",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Creating Train/Validation/Test splits for training KGE's\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Converting set to pandas dataframe for easily split \n",
    "KGT = pd.DataFrame(KGT)\n",
    "\n",
    "#Giving the KGT dataframe meaningful column names\n",
    "KGT.rename(columns={0: \"E1\", 1: \"Rel\", 2: \"E2\"}, inplace=True)\n",
    "\n",
    "#80/10/10 split\n",
    "train, validation, test = np.split(KGT.sample(frac=1, random_state=42), [int(.8*len(KGT)), int(.9*len(KGT))])\n",
    "\n",
    "#Creating folder where dataset files will be saved\n",
    "try:\n",
    "    os.mkdir(os.path.join(os.path.abspath(os.getcwd()), \"UMLS_KG\"))\n",
    "    print(\"KG Directory Created\")\n",
    "except:\n",
    "    print(\"KG Directory Already Exists\")\n",
    "\n",
    "dataset_path = os.path.abspath(\"UMLS_KG\")\n",
    "\n",
    "#Saving datasets as .txt files to be used for training the KGE\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-train.txt'), train.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-valid.txt'), validation.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-test.txt'), test.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "\n",
    "print('KG dataset saved...')\n",
    "\n",
    "KGT.to_pickle(os.path.join(dataset_path, \"KGT.pkl\"))\n",
    "print('KGT saved as dataframe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
