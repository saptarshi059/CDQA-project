{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Run this cell once to install pykg2vec\n",
    "!git clone https://github.com/Sujit-O/pykg2vec.git\n",
    "%cd pykg2vec/\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Generate tuples (Question, Token, SemType)\n",
    "%cd ~/Desktop/covid-project\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "s = pd.read_json('sample.json')\n",
    "\n",
    "documents = s.iloc[0][0]['Document']['Utterances']\n",
    "Metamap_Tokenizations = [] #To be used for final QA\n",
    "\n",
    "def retrieve_tokens(SyntaxUnits):\n",
    "    tokens = []\n",
    "    for i in range(len(SyntaxUnits)):\n",
    "        tokens.append(SyntaxUnits[i]['InputMatch'])\n",
    "    return tokens\n",
    "\n",
    "def retrieve_mappings(Mappings):\n",
    "    mapped_semantic_types = []\n",
    "    #No mappings found\n",
    "    if len(Mappings) == 0:\n",
    "        return [] #These words will get their embeddings from BERT\n",
    "    else:\n",
    "        candidates = Mappings[0]['MappingCandidates'] #Choosing Only top mappings\n",
    "        for cnd in candidates:\n",
    "            mapped_semantic_types.append([' '.join(cnd['MatchedWords']),cnd['SemTypes'][0]])\n",
    "    return mapped_semantic_types\n",
    "    \n",
    "for doc in tqdm(documents):\n",
    "    Phrases = doc['Phrases']\n",
    "    Phrase_Tokenizations = []\n",
    "    Mappings = []\n",
    "    for ph in Phrases:\n",
    "        Phrase_Tokenizations.append(retrieve_tokens(ph['SyntaxUnits']))\n",
    "        Mappings.append(retrieve_mappings(ph['Mappings']))\n",
    "    #Flattening the Lists\n",
    "    Phrase_Tokenizations = [item for sublist in Phrase_Tokenizations for item in sublist]\n",
    "    Mappings = [item for sublist in Mappings for item in sublist]\n",
    "    #Creating the final list\n",
    "    Metamap_Tokenizations.append((doc['UttText'], Phrase_Tokenizations, Mappings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Replacing each shorthand mapping with KG concept\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "for i in range(len(Metamap_Tokenizations)):\n",
    "    for j in range(len(Metamap_Tokenizations[i][2])):\n",
    "        mycursor.execute(\"select STY_RL from SRDEF where ABR = '%s' \" % Metamap_Tokenizations[i][2][j][1])\n",
    "        Metamap_Tokenizations[i][2][j][1] = mycursor.fetchall()[0][0]\n",
    "\n",
    "mycursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Generating the KG triples (KGT)\n",
    "from itertools import permutations\n",
    "\n",
    "All_Mappings = [y for x in Metamap_Tokenizations for y in x[2]]\n",
    "All_Concept_Pairs = permutations(All_Mappings, 2)\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi123!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "KGT = set()\n",
    "\n",
    "for term_pair in tqdm(All_Concept_Pairs):\n",
    "    semantic_type1 = term_pair[0][1]\n",
    "    semantic_type2 = term_pair[1][1]\n",
    "    mycursor.execute(\"select RL from SRSTR where STY_RL1 = '%s' and STY_RL2 = '%s' \" % (semantic_type1, semantic_type2))\n",
    "    relation = mycursor.fetchall()\n",
    "    if relation != []:\n",
    "        KGT.add((semantic_type1, relation[0][0], semantic_type2))\n",
    "\n",
    "mycursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating Train/Validation/Test splits for training KGE's\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Converting set to pandas dataframe for easily split \n",
    "KGT = pd.DataFrame(KGT)\n",
    "\n",
    "#Giving the KGT dataframe meaningful column names\n",
    "KGT.rename(columns={0: \"E1\", 1: \"Rel\", 2: \"E2\"}, inplace=True)\n",
    "\n",
    "#80/10/10 split\n",
    "train, validation, test = np.split(KGT.sample(frac=1, random_state=42), [int(.8*len(KGT)), int(.9*len(KGT))])\n",
    "\n",
    "#Saving datasets as .txt files\n",
    "dataset_path = os.path.abspath('UMLS_KG')\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-train.txt'), train.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-valid.txt'), validation.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, 'UMLS_KG-test.txt'), test.values, delimiter=\"\\t\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Run this cell to execute pykg2vec programs\n",
    "%cd ~/Desktop/covid-project/pykg2vec/scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Tune KGE model\n",
    "!python pykg2vec_tune.py -mn DistMult -ds UMLS_KG -dsp ~/Desktop/covid-project/UMLS_KG \\\n",
    "-hpf ~/Desktop/covid-project/UMLS_KG/hyperparams.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Train KGE\n",
    "!python pykg2vec_train.py -mn DistMult -ds UMLS_KG -dsp ~/Desktop/covid-project/UMLS_KG \\\n",
    "-lr 0.01 -l1 True -k 768 -b 128 -l 1000 -mg 1.00 -opt \"sgd\" -s \"bern\" -ngr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Converting KGE to BERT embeddings (Domain Term Encoding (DTE)) - part1 (generating associated triples)\n",
    "#[Entity Expansion]\n",
    "%cd ~/Desktop/covid-project/UMLS_KG/\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#Mapping b/w entity and corresponding ID\n",
    "with open('entity2idx.pkl', 'rb') as f:\n",
    "    entity2id = pickle.load(f)\n",
    "\n",
    "#Mapping b/w relation and corresponding ID\n",
    "with open('relation2idx.pkl', 'rb') as f:\n",
    "    relation2id = pickle.load(f)\n",
    "\n",
    "def triple_gen(current_entity):\n",
    "    results = KGT.query(\"E1==@current_entity\")\n",
    "    connected_entities = results.E2.to_list()\n",
    "    outgoing_relations = results.Rel.to_list()\n",
    "    a = []\n",
    "    for i in range(len(results)):\n",
    "        a.append([current_entity, outgoing_relations[i], connected_entities[i]])\n",
    "    a = [y for x in a for y in x]\n",
    "    return a\n",
    "\n",
    "triple_list = []\n",
    "for entity in tqdm(entity2id.keys()):\n",
    "    triple_list.append(triple_gen(entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Converting KGE to BERT embeddings (Domain Term Encoding (DTE)) - part2 (each KG item -> (KG item, KGE))\n",
    "#KGE located here\n",
    "%cd ~/Desktop/covid-project/UMLS_KG/embeddings/distmult\n",
    "\n",
    "ent_embeddings = pd.read_csv('ent_embedding.tsv', sep='\\t', header=None)\n",
    "rel_embeddings = pd.read_csv('rel_embedding.tsv', sep='\\t', header=None)\n",
    "\n",
    "'''\n",
    "Associating each item in the triple list with respective embeddings. \n",
    "This is done to create an easy Domain Term BERT embedding matrix.\n",
    "'''\n",
    "for TL in tqdm(triple_list):\n",
    "    if TL == []:\n",
    "        continue\n",
    "    i = 0\n",
    "    for index, item in enumerate(TL):\n",
    "        if (i%3 == 0) or (i%3 == 2): #This item is an entity\n",
    "            TL[index] = (item, ent_embeddings.iloc[entity2id[item]].to_numpy())\n",
    "        else: #This item is a relation\n",
    "            TL[index] = (item, rel_embeddings.iloc[relation2id[item]].to_numpy())\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Converting KGE to BERT embeddings (Domain Term Encoding (DTE)) - part3 (Passing KGE's through BERT) \n",
    "#[Creating DTE Lookup Table]\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "matrix = model.get_input_embeddings() #BERT embeddings\n",
    "\n",
    "CLS_embedding = matrix(torch.LongTensor([101]))\n",
    "SEP_embedding = matrix(torch.LongTensor([102]))\n",
    "\n",
    "DTE_BERT_Matrix = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq in tqdm(triple_list):\n",
    "        if seq == []: #There is no expansion of the entity\n",
    "            continue\n",
    "        \n",
    "        outputs = model(inputs_embeds = torch.unsqueeze(\\\n",
    "                                        torch.cat(\\\n",
    "                                        (CLS_embedding,\\\n",
    "                                         torch.FloatTensor([x[1] for x in seq]), SEP_embedding)), dim=1))\n",
    "        \n",
    "        #Collecting all the embeddings for the current domain term in e[]\n",
    "        e = []\n",
    "        \n",
    "        '''\n",
    "        Starting at 1 & ending at (len -1) to a/c for [CLS] & [SEP].\n",
    "        Step size is 3 since the required entity occurs in spaces of 3, according to the expansion scheme.\n",
    "        '''\n",
    "        for i in range(1, (len(seq) - 1), 3): \n",
    "            e.append(outputs[0][i])\n",
    "        \n",
    "        '''\n",
    "        The BERT embedding for each entity will be the average of all its occurrences.\n",
    "        *e provides all the elements of e (unpacking).\n",
    "        '''\n",
    "        DTE_BERT_Matrix[seq[0][0]] = torch.mean(torch.stack([*e], dim=0), dim=0)\n",
    "\n",
    "'''\n",
    "Saving DTE_BERT embeddings to a lookup table (dataframe) & clearing DTE_BERT_Matrix.\n",
    "Dataframes allow quicker lookups\n",
    "'''\n",
    "s = pd.DataFrame(list(DTE_BERT_Matrix.items()),columns = ['Term','Embedding'])\n",
    "s.to_csv('DTE_BERT_Matrix.csv')\n",
    "DTE_BERT_Matrix.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Extracting embeddings for non-domain terms. I'm simply using BERT's tokenizer for the nDT's.\n",
    "#Creating question representations in this block.\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import re\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_representations = []\n",
    "all_available_entity_embeddings = s['Term'].to_list()\n",
    "\n",
    "for tup in Metamap_Tokenizations:\n",
    "    metamap_tokenized_question = tup[1]\n",
    "    \n",
    "    #Removing punctuations/spaces from domain-terms for easy comparison\n",
    "    domain_terms = [re.sub(r'[\\W\\s]','',x[0]).lower() for x in tup[2]] \n",
    "\n",
    "    '''\n",
    "    Note: is_split_into_words is not the same as pre-tokenized. BERT uses subwords tokenization.\n",
    "    Thus, when the above is set to True, it simply tells the tokenizer to run BERT's scheme on the resp. words.\n",
    "    '''\n",
    "    encoded_input = tokenizer(metamap_tokenized_question, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    \n",
    "    question_embeddings = []\n",
    "    start_index = 1\n",
    "    for word in metamap_tokenized_question:\n",
    "        filtered_word = re.sub(r'\\W','',word).lower()\n",
    "        number_of_subwords = len(tokenizer(word)['input_ids']) - 2 #1 for CLS & 1 for SEP\n",
    "        end_index = start_index + number_of_subwords\n",
    "        \n",
    "        '''\n",
    "        This means that the filtered_word has to be a domain term which also has a KG expansion. If if does not,\n",
    "        then simply use its BERT embeddings.\n",
    "        '''\n",
    "        if filtered_word in domain_terms: #Use DTE_BERT_Matrix\n",
    "            mapped_concept = tup[2][domain_terms.index(filtered_word)][1]\n",
    "            if mapped_concept in all_available_entity_embeddings:\n",
    "                question_embeddings.append(s.query(\"Term==@mapped_concept\")['Embedding'].values[0])\n",
    "            else: #The DT doesn't have an expansion in the KG & so its BERT embeddings are used.\n",
    "                question_embeddings.append(outputs.last_hidden_state[0][start_index:end_index])\n",
    "        else: #Use Regular BERT subword embeddings\n",
    "            question_embeddings.append(outputs.last_hidden_state[0][start_index:end_index])\n",
    "\n",
    "        start_index = end_index\n",
    "        \n",
    "    #In this way, I don't have to add the CLS & SEP embeddings during fine-tuning.\n",
    "    final_representation = torch.unsqueeze(torch.cat((CLS_embedding,\\\n",
    "                                                      torch.cat([*question_embeddings]),\\\n",
    "                                                      SEP_embedding)), dim=1)\n",
    "    \n",
    "    question_representations.append(final_representation)\n",
    "\n",
    "#Saving the question vectors to disk\n",
    "with open('question_representation.data', 'wb') as filehandle:\n",
    "    pickle.dump(question_representations, filehandle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
