{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210be83",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Generate tuples (Question, Token, SemType) - old approach + old code - don't run this cell now\n",
    "%cd ~/Desktop/CDQA-project\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "s = pd.read_json('text.out')\n",
    "\n",
    "documents = s.iloc[0][0]['Document']['Utterances']\n",
    "Metamap_Tokenizations = [] #To be used for final QA\n",
    "\n",
    "def retrieve_tokens(SyntaxUnits):\n",
    "    tokens = []\n",
    "    for i in range(len(SyntaxUnits)):\n",
    "        tokens.append(SyntaxUnits[i]['InputMatch'])\n",
    "    return tokens\n",
    "\n",
    "def retrieve_mappings(Mappings):\n",
    "    mapped_semantic_types = []\n",
    "    #No mappings found\n",
    "    if len(Mappings) == 0:\n",
    "        return [] #These words will get their embeddings from BERT\n",
    "    else:\n",
    "        candidates = Mappings[0]['MappingCandidates'] #Choosing Only top mappings\n",
    "        for cnd in candidates:\n",
    "            mapped_semantic_types.append([' '.join(cnd['MatchedWords']), cnd['CandidateCUI'], \\\n",
    "                                              cnd['CandidatePreferred'], cnd['SemTypes'][0]])\n",
    "            entities.add(cnd['CandidatePreferred'])\n",
    "    return mapped_semantic_types\n",
    "\n",
    "entities = set()\n",
    "for doc in tqdm(documents):\n",
    "    Phrases = doc['Phrases']\n",
    "    Phrase_Tokenizations = []\n",
    "    Mappings = []\n",
    "    for ph in Phrases:\n",
    "        Phrase_Tokenizations.append(retrieve_tokens(ph['SyntaxUnits']))\n",
    "        Mappings.append(retrieve_mappings(ph['Mappings']))\n",
    "    #Flattening the Lists\n",
    "    Phrase_Tokenizations = [item for sublist in Phrase_Tokenizations for item in sublist]\n",
    "    Mappings = [item for sublist in Mappings for item in sublist]\n",
    "    #Creating the final list\n",
    "    Metamap_Tokenizations.append((doc['UttText'], Phrase_Tokenizations, Mappings))\n",
    "    \n",
    "#Removing extra spaces from each question\n",
    "for index, tup in enumerate(Metamap_Tokenizations):\n",
    "    temp = list(tup)\n",
    "    #The only question which was causing an issue\n",
    "    if type(temp[0]) != str:\n",
    "        temp[0] = 'What were the impact of event scaleâ€“revised scores?'\n",
    "    else:\n",
    "        temp[0] = temp[0].strip()\n",
    "    Metamap_Tokenizations[index] = tuple(temp)\n",
    "    \n",
    "print(f\"Number of entities discovered: {len(entities)}\")\n",
    "\n",
    "#Replacing each shorthand mapping with KG concept - IF USING SN\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi1234!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "for i in range(len(Metamap_Tokenizations)):\n",
    "    for j in range(len(Metamap_Tokenizations[i][2])):\n",
    "        mycursor.execute(\"select STY_RL from SRDEF where ABR = '%s' \" % Metamap_Tokenizations[i][2][j][3])\n",
    "        Metamap_Tokenizations[i][2][j][3] = mycursor.fetchall()[0][0]\n",
    "\n",
    "mycursor.close()\n",
    "\n",
    "#Saving Metamap_Tokenizations for use during question embedding creation\n",
    "pd.DataFrame(Metamap_Tokenizations, columns=['Question','Tokenization',\\\n",
    "                                            'Mappings']).to_pickle('Metamap_Tokenizations.pkl')\n",
    "\n",
    "'''--->Extracting relations from the Semantic Network<---'''\n",
    "    #Generating possible semantic types connected to the current one\n",
    "    mycursor.execute('''SELECT STY_RL2, RL\n",
    "                        FROM SRSTR \n",
    "                        WHERE STY_RL1 = '%s';''' % row.Semantic_Type)\n",
    "    possible_semantic_types_2 = [STREL(*x) for x in mycursor.fetchall()]\n",
    "    for sem_type_2 in possible_semantic_types_2:\n",
    "        if sem_type_2.ST == None:\n",
    "            continue\n",
    "        ST2 = sem_type_2.ST.decode()\n",
    "        REL = sem_type_2.REL.decode()\n",
    "        for result in CUI_Preferred_Concept_Semantic_Type_Lookup_Table.query('Semantic_Type==@ST2').itertuples(index=False):\n",
    "            KGT.add((row.Preferred_Concept, REL, result.Preferred_Concept))      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e289cc6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Just need to run this cell once to update the dataset with the cleaned questions - done\n",
    "import json\n",
    "import os\n",
    "data_path = os.path.relpath('data/COVID-QA-original.json')\n",
    "\n",
    "dataset = json.load(open(data_path, 'r'))\n",
    "with open('Cleaned_Questions_utf8_replaced.txt','r') as file:\n",
    "    cleaned_questions = file.read().split('\\n')\n",
    "    \n",
    "index = 0\n",
    "for i in range(len(dataset['data'])):\n",
    "    for question_dictionary in dataset['data'][i]['paragraphs'][0]['qas']:\n",
    "        question_dictionary['question'] = cleaned_questions[index]\n",
    "        index += 1\n",
    "        \n",
    "with open('COVID-QA_cleaned.json', 'w') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f75f276",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities discovered: 1896\n"
     ]
    }
   ],
   "source": [
    "#Generate tuples (question, (matched_text, CUI, preferred candidate)) - new approach\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "texts = json.load(open('MM_NEW.json'))['AllDocuments'][0]['Document']['Utterances']\n",
    "\n",
    "Metamap_Tokenizations = []\n",
    "for ques in texts:\n",
    "    mappings = []\n",
    "    question_text = ques['UttText']\n",
    "    ques_start_idx = int(ques['UttStartPos'])\n",
    "    for phr in ques['Phrases']:\n",
    "        if phr['Mappings'] != []:\n",
    "            for phr_dict in phr[\"Mappings\"][0]['MappingCandidates']: #Choosing the first candidate\n",
    "                start_idx = int(phr_dict['ConceptPIs'][0]['StartPos']) - ques_start_idx\n",
    "                end_idx = start_idx + int(phr_dict['ConceptPIs'][0]['Length'])\n",
    "                mappings.append((question_text[start_idx:end_idx], phr_dict['CandidateCUI'], \\\n",
    "                                 phr_dict['CandidatePreferred']))\n",
    "    Metamap_Tokenizations.append((question_text, mappings))\n",
    "\n",
    "entities = set()\n",
    "for mappings in Metamap_Tokenizations:\n",
    "    for tup in mappings[1]:\n",
    "        entities.add(tup[2])\n",
    "print(f\"Number of entities discovered: {len(entities)}\")\n",
    "\n",
    "#Saving Metamap_Tokenizations for use during question embedding creation\n",
    "pd.DataFrame(Metamap_Tokenizations, columns=['Question', 'Mappings']).to_pickle('Metamap_Tokenizations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfc2fc9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#New version of CUI-PC_Lookup table\n",
    "cuis = [y[1] for x in Metamap_Tokenizations for y in x[1]]\n",
    "pc = [y[2] for x in Metamap_Tokenizations for y in x[1]]\n",
    "CUI_Preferred_Concept_Lookup_Table = pd.DataFrame(zip(cuis, pc), columns=['CUI','Preferred_Concept']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a274d62b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1896it [21:22,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples extracted: 6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#New version of creating KGT (only Metathesaurus)\n",
    "import sqlite3\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi1234!\", database=\"umls\")\n",
    "mydb.set_charset_collation('latin1')\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#For sqlite (Metathesaurus)\n",
    "conn = sqlite3.connect('umls.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#All Knowledge Graph Triples\n",
    "KGT = set()\n",
    "\n",
    "#To make the code more readable (pythonic)\n",
    "CUIREL = namedtuple('CUIREL', ['CUI', 'REL'])\n",
    "#STREL = namedtuple('STREL', ['ST', 'REL'])\n",
    "\n",
    "for row in tqdm(CUI_Preferred_Concept_Lookup_Table.itertuples(index=False)):\n",
    "    '''--->Extracting relations from the Metathesaurus<---'''\n",
    "    #This gives the \"incoming\" relations i.e. CUI2 - RELA - CUI1\n",
    "    incoming_relations = []\n",
    "    cursor.execute('''SELECT DISTINCT CUI2, RELA\n",
    "                                  FROM MRREL\n",
    "                                  WHERE CUI1 = '%s' AND RELA <> '';''' % row.CUI)\n",
    "    results = cursor.fetchall()\n",
    "    #Checking which of the returned tuples are present in our lookup table \n",
    "    for tupA in results:\n",
    "        CUIREL2 = CUIREL(*tupA)\n",
    "        for tupB in CUI_Preferred_Concept_Lookup_Table.query('CUI==@CUIREL2.CUI').itertuples(index=False):\n",
    "            KGT.add((tupB.Preferred_Concept, CUIREL2.REL, row.Preferred_Concept))\n",
    "\n",
    "    #This gives the \"outgoing\" relations i.e. CUI1 - RELA - CUI2\n",
    "    outgoing_relations = []\n",
    "    cursor.execute('''SELECT DISTINCT CUI1, RELA \n",
    "                                  FROM MRREL\n",
    "                                  WHERE CUI2 = '%s' AND STYPE2 = 'CUI' AND RELA <> '';''' % row.CUI)\n",
    "    results = cursor.fetchall()\n",
    "    #Checking which of the returned tuples are present in our lookup table\n",
    "    for tupA in results:\n",
    "        CUIREL2 = CUIREL(*tupA)\n",
    "        for tupB in CUI_Preferred_Concept_Lookup_Table.query('CUI==@CUIREL2.CUI').itertuples(index=False):\n",
    "            KGT.add((row.Preferred_Concept, CUIREL2.REL, tupB.Preferred_Concept))\n",
    "    \n",
    "    \n",
    "conn.close()\n",
    "mycursor.close()\n",
    "\n",
    "print(f\"Number of triples extracted: {len(KGT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab2ec36",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG Directory Created\n",
      "KG dataset saved...\n",
      "KGT saved as dataframe\n"
     ]
    }
   ],
   "source": [
    "#Creating Train/Validation/Test splits for training KGE's\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Converting set to pandas dataframe for easy splitting\n",
    "KGT = pd.DataFrame(KGT)\n",
    "\n",
    "#Giving the KGT dataframe meaningful column names\n",
    "KGT.rename(columns={0: \"E1\", 1: \"Rel\", 2: \"E2\"}, inplace=True)\n",
    "\n",
    "#80/10/10 split\n",
    "train, validation, test = np.split(KGT.sample(frac=1, random_state=42), [int(.8*len(KGT)), int(.9*len(KGT))])\n",
    "\n",
    "source_of_KG = 'MT'\n",
    "\n",
    "#Creating folder where dataset files will be saved\n",
    "try:\n",
    "    os.mkdir(os.path.join(os.path.abspath(os.getcwd()), f\"UMLS_KG_{source_of_KG}\"))\n",
    "    print(\"KG Directory Created\")\n",
    "except:\n",
    "    print(\"KG Directory Already Exists\")\n",
    "\n",
    "dataset_path = os.path.abspath(f\"UMLS_KG_{source_of_KG}\")\n",
    "\n",
    "#Saving datasets as .txt files to be used for training the KGE\n",
    "np.savetxt(os.path.join(dataset_path, f'UMLS_KG_{source_of_KG}-train.txt'), train.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, f'UMLS_KG_{source_of_KG}-valid.txt'), validation.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, f'UMLS_KG_{source_of_KG}-test.txt'), test.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "\n",
    "print('KG dataset saved...')\n",
    "\n",
    "KGT.to_pickle(os.path.join(dataset_path, \"KGT.pkl\"))\n",
    "print('KGT saved as dataframe')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
