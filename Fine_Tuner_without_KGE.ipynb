{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8116cc36",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading in the dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_covidqa():\n",
    "    with open('COVID-QA.json', 'rb') as f:\n",
    "        covidqa_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in covidqa_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    \n",
    "    return contexts, questions, answers\n",
    "\n",
    "all_contexts, all_questions, all_answers = read_covidqa()\n",
    "\n",
    "#Converting to a dataframe for easy k-fold splits\n",
    "full_dataset = pd.DataFrame(list(zip(all_contexts, all_questions, all_answers)), columns =['context', 'question', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f9c605",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preprocess_input(dataset):\n",
    "    answers = dataset['answer'].to_list()\n",
    "    context = dataset['context'].to_list()\n",
    "    \n",
    "    for answer, context in zip(dataset['answer'], dataset['context']):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "    \n",
    "    encodings = tokenizer(dataset['context'].to_list(), dataset['question'].to_list(), \\\n",
    "                          truncation=True, padding=True)\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        \n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e74c7d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Code to compute F1 & EM scores\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def compute_EM(df):\n",
    "    EM = []\n",
    "    for i in range(len(df)):\n",
    "        a_gold = df['true_answer'][i]\n",
    "        a_pred = df['predicted_answer'][i]\n",
    "        EM.append(int(normalize_answer(a_gold) == normalize_answer(a_pred)))\n",
    "    return np.mean(EM)\n",
    "\n",
    "def compute_f1_main(df):\n",
    "    F1 = []\n",
    "    for i in range(len(df)):\n",
    "        a_gold = df['true_answer'][i]\n",
    "        a_pred = df['predicted_answer'][i]\n",
    "        F1.append(compute_f1(a_gold,a_pred))\n",
    "    return np.mean(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa0c80d0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Dataloader Object Creation\n",
    "import torch\n",
    "\n",
    "class CovidQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f372b32",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64bbdeb87fc4d1e82aa7392752bc3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=285.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45b1672d7944c37b5a1948d15ce7353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b46b0cf2b1448aa4277c44f48b67ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=17756393.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'prajjwal1/bert-tiny' at '/home/ubuntu/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /pytorch/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /pytorch/caffe2/serialize/inline_container.cc:132)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7f59da934193 in /home/ubuntu/.local/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: caffe2::serialize::PyTorchStreamReader::init() + 0x1f5b (0x7f59dd88f9eb in /home/ubuntu/.local/lib/python3.7/site-packages/torch/lib/libtorch.so)\nframe #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::string const&) + 0x64 (0x7f59dd890c04 in /home/ubuntu/.local/lib/python3.7/site-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x6c6536 (0x7f5a257c1536 in /home/ubuntu/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #4: <unknown function> + 0x295a74 (0x7f5a25390a74 in /home/ubuntu/.local/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #5: _PyMethodDef_RawFastCallDict + 0x24d (0x55dc2f52263d in /home/ubuntu/anaconda3/bin/python)\nframe #6: _PyCFunction_FastCallDict + 0x21 (0x55dc2f5227c1 in /home/ubuntu/anaconda3/bin/python)\nframe #7: _PyObject_Call_Prepend + 0x63 (0x55dc2f5207a3 in /home/ubuntu/anaconda3/bin/python)\nframe #8: PyObject_Call + 0x6e (0x55dc2f5133ae in /home/ubuntu/anaconda3/bin/python)\nframe #9: <unknown function> + 0x9e9dc (0x55dc2f48a9dc in /home/ubuntu/anaconda3/bin/python)\nframe #10: _PyObject_FastCallKeywords + 0x128 (0x55dc2f558938 in /home/ubuntu/anaconda3/bin/python)\nframe #11: _PyEval_EvalFrameDefault + 0x522e (0x55dc2f5be27e in /home/ubuntu/anaconda3/bin/python)\nframe #12: _PyEval_EvalCodeWithName + 0x5da (0x55dc2f500fea in /home/ubuntu/anaconda3/bin/python)\nframe #13: _PyFunction_FastCallDict + 0x1d5 (0x55dc2f501df5 in /home/ubuntu/anaconda3/bin/python)\nframe #14: _PyObject_Call_Prepend + 0x63 (0x55dc2f5207a3 in /home/ubuntu/anaconda3/bin/python)\nframe #15: <unknown function> + 0x16bd2a (0x55dc2f557d2a in /home/ubuntu/anaconda3/bin/python)\nframe #16: _PyObject_FastCallKeywords + 0x128 (0x55dc2f558938 in /home/ubuntu/anaconda3/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x49f6 (0x55dc2f5bda46 in /home/ubuntu/anaconda3/bin/python)\nframe #18: _PyEval_EvalCodeWithName + 0x2f9 (0x55dc2f500d09 in /home/ubuntu/anaconda3/bin/python)\nframe #19: _PyFunction_FastCallKeywords + 0x387 (0x55dc2f5510b7 in /home/ubuntu/anaconda3/bin/python)\nframe #20: _PyEval_EvalFrameDefault + 0x14e9 (0x55dc2f5ba539 in /home/ubuntu/anaconda3/bin/python)\nframe #21: _PyEval_EvalCodeWithName + 0xc30 (0x55dc2f501640 in /home/ubuntu/anaconda3/bin/python)\nframe #22: _PyFunction_FastCallDict + 0x3ff (0x55dc2f50201f in /home/ubuntu/anaconda3/bin/python)\nframe #23: _PyObject_Call_Prepend + 0x63 (0x55dc2f5207a3 in /home/ubuntu/anaconda3/bin/python)\nframe #24: PyObject_Call + 0x6e (0x55dc2f5133ae in /home/ubuntu/anaconda3/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x1e47 (0x55dc2f5bae97 in /home/ubuntu/anaconda3/bin/python)\nframe #26: _PyEval_EvalCodeWithName + 0x2f9 (0x55dc2f500d09 in /home/ubuntu/anaconda3/bin/python)\nframe #27: _PyFunction_FastCallKeywords + 0x387 (0x55dc2f5510b7 in /home/ubuntu/anaconda3/bin/python)\nframe #28: _PyEval_EvalFrameDefault + 0x4a99 (0x55dc2f5bdae9 in /home/ubuntu/anaconda3/bin/python)\nframe #29: _PyEval_EvalCodeWithName + 0x2f9 (0x55dc2f500d09 in /home/ubuntu/anaconda3/bin/python)\nframe #30: PyEval_EvalCodeEx + 0x44 (0x55dc2f501be4 in /home/ubuntu/anaconda3/bin/python)\nframe #31: PyEval_EvalCode + 0x1c (0x55dc2f501c0c in /home/ubuntu/anaconda3/bin/python)\nframe #32: <unknown function> + 0x1dcdbd (0x55dc2f5c8dbd in /home/ubuntu/anaconda3/bin/python)\nframe #33: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x55dc2f551739 in /home/ubuntu/anaconda3/bin/python)\nframe #34: _PyCFunction_FastCallKeywords + 0x21 (0x55dc2f5519d1 in /home/ubuntu/anaconda3/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x4705 (0x55dc2f5bd755 in /home/ubuntu/anaconda3/bin/python)\nframe #36: _PyGen_Send + 0x2a2 (0x55dc2f559952 in /home/ubuntu/anaconda3/bin/python)\nframe #37: _PyEval_EvalFrameDefault + 0x1a85 (0x55dc2f5baad5 in /home/ubuntu/anaconda3/bin/python)\nframe #38: _PyGen_Send + 0x2a2 (0x55dc2f559952 in /home/ubuntu/anaconda3/bin/python)\nframe #39: _PyEval_EvalFrameDefault + 0x1a85 (0x55dc2f5baad5 in /home/ubuntu/anaconda3/bin/python)\nframe #40: _PyGen_Send + 0x2a2 (0x55dc2f559952 in /home/ubuntu/anaconda3/bin/python)\nframe #41: _PyMethodDef_RawFastCallKeywords + 0x8d (0x55dc2f5516dd in /home/ubuntu/anaconda3/bin/python)\nframe #42: _PyMethodDescr_FastCallKeywords + 0x4f (0x55dc2f55877f in /home/ubuntu/anaconda3/bin/python)\nframe #43: _PyEval_EvalFrameDefault + 0x4bec (0x55dc2f5bdc3c in /home/ubuntu/anaconda3/bin/python)\nframe #44: _PyFunction_FastCallKeywords + 0xfb (0x55dc2f550e2b in /home/ubuntu/anaconda3/bin/python)\nframe #45: _PyEval_EvalFrameDefault + 0x416 (0x55dc2f5b9466 in /home/ubuntu/anaconda3/bin/python)\nframe #46: _PyFunction_FastCallKeywords + 0xfb (0x55dc2f550e2b in /home/ubuntu/anaconda3/bin/python)\nframe #47: _PyEval_EvalFrameDefault + 0x6a0 (0x55dc2f5b96f0 in /home/ubuntu/anaconda3/bin/python)\nframe #48: _PyEval_EvalCodeWithName + 0x2f9 (0x55dc2f500d09 in /home/ubuntu/anaconda3/bin/python)\nframe #49: _PyFunction_FastCallDict + 0x3ff (0x55dc2f50201f in /home/ubuntu/anaconda3/bin/python)\nframe #50: _PyObject_Call_Prepend + 0x63 (0x55dc2f5207a3 in /home/ubuntu/anaconda3/bin/python)\nframe #51: PyObject_Call + 0x6e (0x55dc2f5133ae in /home/ubuntu/anaconda3/bin/python)\nframe #52: _PyEval_EvalFrameDefault + 0x1e47 (0x55dc2f5bae97 in /home/ubuntu/anaconda3/bin/python)\nframe #53: _PyEval_EvalCodeWithName + 0x5da (0x55dc2f500fea in /home/ubuntu/anaconda3/bin/python)\nframe #54: _PyFunction_FastCallKeywords + 0x387 (0x55dc2f5510b7 in /home/ubuntu/anaconda3/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x14e9 (0x55dc2f5ba539 in /home/ubuntu/anaconda3/bin/python)\nframe #56: <unknown function> + 0x16d419 (0x55dc2f559419 in /home/ubuntu/anaconda3/bin/python)\nframe #57: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x55dc2f551739 in /home/ubuntu/anaconda3/bin/python)\nframe #58: _PyCFunction_FastCallKeywords + 0x21 (0x55dc2f5519d1 in /home/ubuntu/anaconda3/bin/python)\nframe #59: _PyEval_EvalFrameDefault + 0x4705 (0x55dc2f5bd755 in /home/ubuntu/anaconda3/bin/python)\nframe #60: _PyEval_EvalCodeWithName + 0x5da (0x55dc2f500fea in /home/ubuntu/anaconda3/bin/python)\nframe #61: _PyFunction_FastCallKeywords + 0x387 (0x55dc2f5510b7 in /home/ubuntu/anaconda3/bin/python)\nframe #62: _PyEval_EvalFrameDefault + 0x6a0 (0x55dc2f5b96f0 in /home/ubuntu/anaconda3/bin/python)\nframe #63: <unknown function> + 0x16d419 (0x55dc2f559419 in /home/ubuntu/anaconda3/bin/python)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ae36d6ee0b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             return cls._model_mapping[type(config)].from_pretrained(\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             )\n\u001b[1;32m    362\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 raise OSError(\n\u001b[0;32m-> 1066\u001b[0;31m                     \u001b[0;34mf\"Unable to load weights from pytorch checkpoint file for '{pretrained_model_name_or_path}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m                     \u001b[0;34mf\"at '{resolved_archive_file}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                     \u001b[0;34m\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'prajjwal1/bert-tiny' at '/home/ubuntu/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5'If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. "
     ]
    }
   ],
   "source": [
    "#Main fine-tuning loop\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForQuestionAnswering, QuestionAnsweringPipeline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "num_epochs = 3\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name = 'navteca/roberta-base-squad2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "fold_F1_score = []\n",
    "fold_EM_score = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(full_dataset)): \n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "     \n",
    "    train_dataset = CovidQADataset(preprocess_input(full_dataset.iloc[train_ids]))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    model.init_weights()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch in tqdm(range(3)):\n",
    "        for batch in tqdm(train_loader):\n",
    "            optim.zero_grad()\n",
    "            # question_texts = batch['question_text']   # not implemented yet\n",
    "            # context_texts = batch['context_text']\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            # input_embds, offset = [], []                                  # not implemented yet\n",
    "            # for q_text, c_text in zip(question_texts, context_texts):\n",
    "            #     this_input_embds, this_n_token_adj = custom_question_rep_gen(q_text, c_text)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, \\\n",
    "                            end_positions=end_positions)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "\n",
    "    # Evaluationfor this fold\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_data = full_dataset.iloc[test_ids]\n",
    "        nlp = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer, device=-1 if device == torch.device('cpu')\\\n",
    "                                else 0)\n",
    "        questions = []\n",
    "        true_answers = []\n",
    "        predicted_answers = []\n",
    "        final_df = pd.DataFrame(columns=['question', 'true_answer', 'predicted_answer'])\n",
    "\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i in range(len(test_data)):\n",
    "            context = test_data.iloc[i]['context']\n",
    "            questions.append(test_data.iloc[i]['question'])\n",
    "            true_answers.append(test_data.iloc[i]['answer']['text'])\n",
    "\n",
    "            # Generate outputs\n",
    "            QA_input = {'question': questions[i], 'context': context}\n",
    "            predicted_answers.append(nlp(QA_input)['answer'])\n",
    "        \n",
    "        final_df['question'] = questions\n",
    "        final_df['true_answer'] = true_answers\n",
    "        final_df['predicted_answer'] = predicted_answers\n",
    "            \n",
    "    #Print F1\n",
    "    fold_F1_score.append(compute_f1_main(final_df))\n",
    "    print(f'F1 for fold {fold}: {fold_F1_score[fold]}')\n",
    "    \n",
    "    #Print EM\n",
    "    fold_EM_score.append(compute_EM(final_df))\n",
    "    print(f'EM for fold {fold}: {fold_EM_score[fold]}')\n",
    "\n",
    "print(f\"Avg. F1: {np.mean(fold_F1_score)}\")\n",
    "print(f\"Avg. EM: {np.mean(fold_EM_score)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
