************************************
** MODEL TIME ID: 20220225-064540 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/5 iter: 0/15 loss: 1.1811 Acc: 40.6250% F1: 0.290 Time: 0.97s (0.00s)
Fold 0 train - epoch: 0/5 iter: 1/15 loss: 0.9262 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 2/15 loss: 1.1509 Acc: 50.0000% F1: 0.222 Time: 0.95s (9.37s)
Fold 0 train - epoch: 0/5 iter: 3/15 loss: 0.8973 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 4/15 loss: 1.0221 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.86s)
Fold 0 train - epoch: 0/5 iter: 5/15 loss: 0.9681 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 6/15 loss: 0.8792 Acc: 68.7500% F1: 0.336 Time: 0.94s (9.32s)
Fold 0 train - epoch: 0/5 iter: 7/15 loss: 0.8903 Acc: 46.8750% F1: 0.268 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 8/15 loss: 1.0451 Acc: 43.7500% F1: 0.296 Time: 0.94s (9.65s)
Fold 0 train - epoch: 0/5 iter: 9/15 loss: 1.0173 Acc: 43.7500% F1: 0.296 Time: 0.94s (0.02s)
Fold 0 train - epoch: 0/5 iter: 10/15 loss: 1.0220 Acc: 43.7500% F1: 0.297 Time: 0.96s (9.01s)
Fold 0 train - epoch: 0/5 iter: 11/15 loss: 0.9173 Acc: 43.7500% F1: 0.241 Time: 0.94s (0.02s)
Fold 0 train - epoch: 0/5 iter: 12/15 loss: 0.9815 Acc: 65.6250% F1: 0.422 Time: 0.94s (8.47s)
Fold 0 train - epoch: 0/5 iter: 13/15 loss: 0.8989 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.38s)
Fold 0 train - epoch: 0/5 iter: 14/15 loss: 0.6523 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 52.4444% F1: 0.3151 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5495 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 0/5 iter: 1/2 loss: 1.7652 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 0 train - epoch: 1/5 iter: 0/15 loss: 0.9238 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 0 train - epoch: 1/5 iter: 1/15 loss: 0.8806 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/5 iter: 2/15 loss: 1.0046 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.92s)
Fold 0 train - epoch: 1/5 iter: 3/15 loss: 0.8339 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/5 iter: 4/15 loss: 1.0113 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.53s)
Fold 0 train - epoch: 1/5 iter: 5/15 loss: 0.9528 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 0 train - epoch: 1/5 iter: 6/15 loss: 0.7932 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.88s)
Fold 0 train - epoch: 1/5 iter: 7/15 loss: 0.9539 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 8/15 loss: 0.9698 Acc: 56.2500% F1: 0.240 Time: 0.94s (9.44s)
Fold 0 train - epoch: 1/5 iter: 9/15 loss: 0.9445 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 10/15 loss: 0.9766 Acc: 43.7500% F1: 0.203 Time: 0.96s (9.36s)
Fold 0 train - epoch: 1/5 iter: 11/15 loss: 0.8754 Acc: 65.6250% F1: 0.327 Time: 0.95s (0.31s)
Fold 0 train - epoch: 1/5 iter: 12/15 loss: 1.0255 Acc: 46.8750% F1: 0.305 Time: 0.94s (8.66s)
Fold 0 train - epoch: 1/5 iter: 13/15 loss: 0.9375 Acc: 46.8750% F1: 0.277 Time: 0.94s (1.13s)
Fold 0 train - epoch: 1/5 iter: 14/15 loss: 0.5706 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 54.0000% F1: 0.2594 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7684 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3480 Acc: 5.5556% F1: 0.048 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 46.0000% F1: 0.2572 *
*********************************************************
Performing epoch 2 of 5
Fold 0 train - epoch: 2/5 iter: 0/15 loss: 0.8995 Acc: 65.6250% F1: 0.430 Time: 0.96s (0.00s)
Fold 0 train - epoch: 2/5 iter: 1/15 loss: 0.8842 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/5 iter: 2/15 loss: 0.9100 Acc: 40.6250% F1: 0.193 Time: 0.96s (9.65s)
Fold 0 train - epoch: 2/5 iter: 3/15 loss: 0.7840 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/5 iter: 4/15 loss: 0.9233 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.24s)
Fold 0 train - epoch: 2/5 iter: 5/15 loss: 0.9106 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 0 train - epoch: 2/5 iter: 6/15 loss: 0.7574 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.22s)
Fold 0 train - epoch: 2/5 iter: 7/15 loss: 0.8871 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 8/15 loss: 0.9490 Acc: 56.2500% F1: 0.240 Time: 0.94s (9.49s)
Fold 0 train - epoch: 2/5 iter: 9/15 loss: 0.9471 Acc: 53.1250% F1: 0.311 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/5 iter: 10/15 loss: 0.9959 Acc: 46.8750% F1: 0.213 Time: 0.96s (9.18s)
Fold 0 train - epoch: 2/5 iter: 11/15 loss: 0.7885 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.03s)
Fold 0 train - epoch: 2/5 iter: 12/15 loss: 0.9839 Acc: 50.0000% F1: 0.227 Time: 0.94s (8.85s)
Fold 0 train - epoch: 2/5 iter: 13/15 loss: 0.8563 Acc: 62.5000% F1: 0.314 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 14/15 loss: 0.3481 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 55.7778% F1: 0.2723 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8114 Acc: 65.6250% F1: 0.469 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4064 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 42.0000% F1: 0.2217 *
*********************************************************
Performing epoch 3 of 5
Fold 0 train - epoch: 3/5 iter: 0/15 loss: 0.8590 Acc: 59.3750% F1: 0.378 Time: 0.96s (0.00s)
Fold 0 train - epoch: 3/5 iter: 1/15 loss: 0.7836 Acc: 68.7500% F1: 0.548 Time: 0.94s (0.03s)
Fold 0 train - epoch: 3/5 iter: 2/15 loss: 0.8786 Acc: 50.0000% F1: 0.384 Time: 0.96s (10.12s)
Fold 0 train - epoch: 3/5 iter: 3/15 loss: 0.6843 Acc: 68.7500% F1: 0.407 Time: 0.94s (0.03s)
Fold 0 train - epoch: 3/5 iter: 4/15 loss: 0.8542 Acc: 50.0000% F1: 0.227 Time: 0.94s (9.32s)
Fold 0 train - epoch: 3/5 iter: 5/15 loss: 0.8059 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 6/15 loss: 0.7118 Acc: 75.0000% F1: 0.409 Time: 0.94s (9.24s)
Fold 0 train - epoch: 3/5 iter: 7/15 loss: 0.8048 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.03s)
Fold 0 train - epoch: 3/5 iter: 8/15 loss: 0.8890 Acc: 59.3750% F1: 0.316 Time: 0.95s (9.44s)
Fold 0 train - epoch: 3/5 iter: 9/15 loss: 0.8411 Acc: 65.6250% F1: 0.450 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 10/15 loss: 0.8357 Acc: 68.7500% F1: 0.484 Time: 0.96s (9.06s)
Fold 0 train - epoch: 3/5 iter: 11/15 loss: 0.7395 Acc: 59.3750% F1: 0.335 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 12/15 loss: 0.9445 Acc: 65.6250% F1: 0.551 Time: 0.94s (9.12s)
Fold 0 train - epoch: 3/5 iter: 13/15 loss: 0.7894 Acc: 59.3750% F1: 0.366 Time: 0.94s (0.02s)
Fold 0 train - epoch: 3/5 iter: 14/15 loss: 0.1072 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 61.7778% F1: 0.4135 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/5 iter: 0/2 loss: 1.1607 Acc: 37.5000% F1: 0.238 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3341 Acc: 16.6667% F1: 0.153 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 30.0000% F1: 0.2557 *
*********************************************************
Performing epoch 4 of 5
Fold 0 train - epoch: 4/5 iter: 0/15 loss: 0.7503 Acc: 62.5000% F1: 0.559 Time: 0.96s (0.00s)
Fold 0 train - epoch: 4/5 iter: 1/15 loss: 0.7155 Acc: 68.7500% F1: 0.660 Time: 0.94s (0.03s)
Fold 0 train - epoch: 4/5 iter: 2/15 loss: 0.7623 Acc: 53.1250% F1: 0.442 Time: 0.96s (10.06s)
Fold 0 train - epoch: 4/5 iter: 3/15 loss: 0.5424 Acc: 78.1250% F1: 0.808 Time: 0.94s (0.03s)
Fold 0 train - epoch: 4/5 iter: 4/15 loss: 0.6895 Acc: 65.6250% F1: 0.629 Time: 0.94s (9.16s)
Fold 0 train - epoch: 4/5 iter: 5/15 loss: 0.7321 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 6/15 loss: 0.5894 Acc: 75.0000% F1: 0.440 Time: 0.94s (9.34s)
Fold 0 train - epoch: 4/5 iter: 7/15 loss: 0.6936 Acc: 62.5000% F1: 0.395 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 8/15 loss: 0.6891 Acc: 71.8750% F1: 0.614 Time: 0.94s (9.73s)
Fold 0 train - epoch: 4/5 iter: 9/15 loss: 0.6979 Acc: 71.8750% F1: 0.629 Time: 0.94s (0.03s)
Fold 0 train - epoch: 4/5 iter: 10/15 loss: 0.7583 Acc: 75.0000% F1: 0.534 Time: 0.96s (9.50s)
Fold 0 train - epoch: 4/5 iter: 11/15 loss: 0.5658 Acc: 84.3750% F1: 0.824 Time: 0.95s (0.06s)
Fold 0 train - epoch: 4/5 iter: 12/15 loss: 0.8354 Acc: 59.3750% F1: 0.564 Time: 0.94s (9.48s)
Fold 0 train - epoch: 4/5 iter: 13/15 loss: 0.5684 Acc: 75.0000% F1: 0.728 Time: 0.94s (1.03s)
Fold 0 train - epoch: 4/5 iter: 14/15 loss: 0.0204 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 68.8889% F1: 0.6227 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0427 Acc: 50.0000% F1: 0.278 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7368 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 38.0000% F1: 0.2644 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/5 iter: 0/15 loss: 1.1848 Acc: 28.1250% F1: 0.193 Time: 0.97s (0.00s)
Fold 1 train - epoch: 0/5 iter: 1/15 loss: 1.0798 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 1 train - epoch: 0/5 iter: 2/15 loss: 1.0953 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.25s)
Fold 1 train - epoch: 0/5 iter: 3/15 loss: 0.9562 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/5 iter: 4/15 loss: 1.0170 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.57s)
Fold 1 train - epoch: 0/5 iter: 5/15 loss: 1.0030 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 1 train - epoch: 0/5 iter: 6/15 loss: 0.8571 Acc: 59.3750% F1: 0.296 Time: 0.94s (9.07s)
Fold 1 train - epoch: 0/5 iter: 7/15 loss: 0.9086 Acc: 43.7500% F1: 0.256 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/5 iter: 8/15 loss: 1.0706 Acc: 43.7500% F1: 0.295 Time: 0.94s (9.50s)
Fold 1 train - epoch: 0/5 iter: 9/15 loss: 0.9464 Acc: 56.2500% F1: 0.400 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 10/15 loss: 0.9424 Acc: 71.8750% F1: 0.510 Time: 0.96s (9.03s)
Fold 1 train - epoch: 0/5 iter: 11/15 loss: 0.9424 Acc: 43.7500% F1: 0.279 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/5 iter: 12/15 loss: 0.9997 Acc: 59.3750% F1: 0.399 Time: 0.94s (9.18s)
Fold 1 train - epoch: 0/5 iter: 13/15 loss: 0.9634 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/5 iter: 14/15 loss: 0.5390 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 51.5556% F1: 0.3178 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6415 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5967 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 1 train - epoch: 1/5 iter: 0/15 loss: 0.9329 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 1 train - epoch: 1/5 iter: 1/15 loss: 0.8815 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/5 iter: 2/15 loss: 1.0089 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.49s)
Fold 1 train - epoch: 1/5 iter: 3/15 loss: 0.8440 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/5 iter: 4/15 loss: 1.0008 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.88s)
Fold 1 train - epoch: 1/5 iter: 5/15 loss: 0.9208 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/5 iter: 6/15 loss: 0.7513 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.40s)
Fold 1 train - epoch: 1/5 iter: 7/15 loss: 0.9095 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 8/15 loss: 1.0052 Acc: 56.2500% F1: 0.240 Time: 0.94s (10.31s)
Fold 1 train - epoch: 1/5 iter: 9/15 loss: 0.9879 Acc: 50.0000% F1: 0.267 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 10/15 loss: 0.9820 Acc: 50.0000% F1: 0.265 Time: 0.95s (9.99s)
Fold 1 train - epoch: 1/5 iter: 11/15 loss: 0.8961 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.04s)
Fold 1 train - epoch: 1/5 iter: 12/15 loss: 1.0173 Acc: 50.0000% F1: 0.301 Time: 0.94s (9.19s)
Fold 1 train - epoch: 1/5 iter: 13/15 loss: 0.8880 Acc: 56.2500% F1: 0.320 Time: 0.94s (0.02s)
Fold 1 train - epoch: 1/5 iter: 14/15 loss: 0.7217 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 54.6667% F1: 0.2616 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7831 Acc: 62.5000% F1: 0.385 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3425 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 42.0000% F1: 0.2199 *
*********************************************************
Performing epoch 2 of 5
Fold 1 train - epoch: 2/5 iter: 0/15 loss: 0.9103 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.00s)
Fold 1 train - epoch: 2/5 iter: 1/15 loss: 0.9018 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.04s)
Fold 1 train - epoch: 2/5 iter: 2/15 loss: 0.9091 Acc: 50.0000% F1: 0.262 Time: 0.96s (9.91s)
Fold 1 train - epoch: 2/5 iter: 3/15 loss: 0.8191 Acc: 62.5000% F1: 0.309 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 4/15 loss: 0.9116 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.07s)
Fold 1 train - epoch: 2/5 iter: 5/15 loss: 0.8898 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 6/15 loss: 0.7738 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.39s)
Fold 1 train - epoch: 2/5 iter: 7/15 loss: 0.9140 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 1 train - epoch: 2/5 iter: 8/15 loss: 0.9734 Acc: 56.2500% F1: 0.240 Time: 0.94s (9.83s)
Fold 1 train - epoch: 2/5 iter: 9/15 loss: 0.9500 Acc: 50.0000% F1: 0.267 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 10/15 loss: 0.9694 Acc: 46.8750% F1: 0.213 Time: 0.96s (9.43s)
Fold 1 train - epoch: 2/5 iter: 11/15 loss: 0.8580 Acc: 62.5000% F1: 0.256 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 12/15 loss: 0.9601 Acc: 59.3750% F1: 0.312 Time: 0.94s (9.36s)
Fold 1 train - epoch: 2/5 iter: 13/15 loss: 0.8716 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 14/15 loss: 0.3598 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 56.6667% F1: 0.2688 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7341 Acc: 68.7500% F1: 0.407 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4533 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 48.0000% F1: 0.2651 *
*********************************************************
Performing epoch 3 of 5
Fold 1 train - epoch: 3/5 iter: 0/15 loss: 0.8416 Acc: 62.5000% F1: 0.361 Time: 0.96s (0.00s)
Fold 1 train - epoch: 3/5 iter: 1/15 loss: 0.8763 Acc: 62.5000% F1: 0.389 Time: 0.94s (0.02s)
Fold 1 train - epoch: 3/5 iter: 2/15 loss: 0.8953 Acc: 53.1250% F1: 0.306 Time: 0.95s (8.97s)
Fold 1 train - epoch: 3/5 iter: 3/15 loss: 0.7579 Acc: 68.7500% F1: 0.428 Time: 0.94s (0.03s)
Fold 1 train - epoch: 3/5 iter: 4/15 loss: 0.8399 Acc: 53.1250% F1: 0.333 Time: 0.94s (8.42s)
Fold 1 train - epoch: 3/5 iter: 5/15 loss: 0.8537 Acc: 62.5000% F1: 0.389 Time: 0.94s (0.03s)
Fold 1 train - epoch: 3/5 iter: 6/15 loss: 0.7251 Acc: 71.8750% F1: 0.426 Time: 0.94s (8.83s)
Fold 1 train - epoch: 3/5 iter: 7/15 loss: 0.8607 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 8/15 loss: 0.8647 Acc: 56.2500% F1: 0.303 Time: 0.94s (9.42s)
Fold 1 train - epoch: 3/5 iter: 9/15 loss: 0.8718 Acc: 62.5000% F1: 0.442 Time: 0.94s (0.02s)
Fold 1 train - epoch: 3/5 iter: 10/15 loss: 0.8438 Acc: 50.0000% F1: 0.332 Time: 0.96s (9.16s)
Fold 1 train - epoch: 3/5 iter: 11/15 loss: 0.7945 Acc: 59.3750% F1: 0.360 Time: 0.94s (0.02s)
Fold 1 train - epoch: 3/5 iter: 12/15 loss: 0.9315 Acc: 56.2500% F1: 0.376 Time: 0.94s (8.88s)
Fold 1 train - epoch: 3/5 iter: 13/15 loss: 0.8540 Acc: 53.1250% F1: 0.241 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 14/15 loss: 0.1202 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 59.3333% F1: 0.3667 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8701 Acc: 62.5000% F1: 0.320 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4386 Acc: 11.1111% F1: 0.083 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 44.0000% F1: 0.2846 *
*********************************************************
Performing epoch 4 of 5
Fold 1 train - epoch: 4/5 iter: 0/15 loss: 0.7485 Acc: 68.7500% F1: 0.594 Time: 0.95s (0.00s)
Fold 1 train - epoch: 4/5 iter: 1/15 loss: 0.7026 Acc: 75.0000% F1: 0.695 Time: 0.94s (0.03s)
Fold 1 train - epoch: 4/5 iter: 2/15 loss: 0.8341 Acc: 56.2500% F1: 0.477 Time: 0.96s (9.38s)
Fold 1 train - epoch: 4/5 iter: 3/15 loss: 0.5821 Acc: 78.1250% F1: 0.808 Time: 0.94s (0.03s)
Fold 1 train - epoch: 4/5 iter: 4/15 loss: 0.7164 Acc: 68.7500% F1: 0.637 Time: 0.94s (8.73s)
Fold 1 train - epoch: 4/5 iter: 5/15 loss: 0.7451 Acc: 62.5000% F1: 0.528 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/5 iter: 6/15 loss: 0.6316 Acc: 68.7500% F1: 0.424 Time: 0.94s (9.09s)
Fold 1 train - epoch: 4/5 iter: 7/15 loss: 0.7467 Acc: 53.1250% F1: 0.333 Time: 0.94s (0.03s)
Fold 1 train - epoch: 4/5 iter: 8/15 loss: 0.8202 Acc: 50.0000% F1: 0.343 Time: 0.95s (9.79s)
Fold 1 train - epoch: 4/5 iter: 9/15 loss: 0.7380 Acc: 65.6250% F1: 0.465 Time: 0.94s (0.03s)
Fold 1 train - epoch: 4/5 iter: 10/15 loss: 0.7533 Acc: 75.0000% F1: 0.640 Time: 0.97s (9.36s)
Fold 1 train - epoch: 4/5 iter: 11/15 loss: 0.5011 Acc: 78.1250% F1: 0.683 Time: 0.95s (0.22s)
Fold 1 train - epoch: 4/5 iter: 12/15 loss: 0.7763 Acc: 65.6250% F1: 0.537 Time: 0.94s (9.10s)
Fold 1 train - epoch: 4/5 iter: 13/15 loss: 0.7061 Acc: 65.6250% F1: 0.539 Time: 0.94s (1.05s)
Fold 1 train - epoch: 4/5 iter: 14/15 loss: 0.0277 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 66.6667% F1: 0.5751 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0771 Acc: 62.5000% F1: 0.342 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5810 Acc: 22.2222% F1: 0.205 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3782 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/5 iter: 0/15 loss: 1.1255 Acc: 34.3750% F1: 0.328 Time: 1.00s (0.00s)
Fold 2 train - epoch: 0/5 iter: 1/15 loss: 0.9952 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.04s)
Fold 2 train - epoch: 0/5 iter: 2/15 loss: 1.1172 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.93s)
Fold 2 train - epoch: 0/5 iter: 3/15 loss: 0.9388 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/5 iter: 4/15 loss: 1.0202 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.35s)
Fold 2 train - epoch: 0/5 iter: 5/15 loss: 0.9416 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/5 iter: 6/15 loss: 0.8888 Acc: 68.7500% F1: 0.336 Time: 0.94s (9.30s)
Fold 2 train - epoch: 0/5 iter: 7/15 loss: 0.8787 Acc: 56.2500% F1: 0.362 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/5 iter: 8/15 loss: 1.1918 Acc: 37.5000% F1: 0.247 Time: 0.94s (9.84s)
Fold 2 train - epoch: 0/5 iter: 9/15 loss: 1.0180 Acc: 40.6250% F1: 0.278 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 10/15 loss: 0.9951 Acc: 37.5000% F1: 0.259 Time: 0.96s (9.51s)
Fold 2 train - epoch: 0/5 iter: 11/15 loss: 0.9233 Acc: 56.2500% F1: 0.373 Time: 0.95s (0.25s)
Fold 2 train - epoch: 0/5 iter: 12/15 loss: 1.0687 Acc: 37.5000% F1: 0.216 Time: 0.94s (10.00s)
Fold 2 train - epoch: 0/5 iter: 13/15 loss: 0.9393 Acc: 56.2500% F1: 0.240 Time: 0.94s (1.47s)
Fold 2 train - epoch: 0/5 iter: 14/15 loss: 0.5210 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 49.7778% F1: 0.3158 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5597 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5878 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 2 train - epoch: 1/5 iter: 0/15 loss: 0.8456 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.00s)
Fold 2 train - epoch: 1/5 iter: 1/15 loss: 0.8851 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 2/15 loss: 0.9700 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.31s)
Fold 2 train - epoch: 1/5 iter: 3/15 loss: 0.8531 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 2 train - epoch: 1/5 iter: 4/15 loss: 1.0138 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.87s)
Fold 2 train - epoch: 1/5 iter: 5/15 loss: 0.9640 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 6/15 loss: 0.7917 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.37s)
Fold 2 train - epoch: 1/5 iter: 7/15 loss: 0.9426 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 8/15 loss: 1.0762 Acc: 53.1250% F1: 0.231 Time: 0.94s (10.33s)
Fold 2 train - epoch: 1/5 iter: 9/15 loss: 1.0173 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 10/15 loss: 0.9584 Acc: 46.8750% F1: 0.213 Time: 0.95s (8.91s)
Fold 2 train - epoch: 1/5 iter: 11/15 loss: 0.8599 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 12/15 loss: 0.9915 Acc: 59.3750% F1: 0.352 Time: 0.94s (9.64s)
Fold 2 train - epoch: 1/5 iter: 13/15 loss: 0.8877 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 14/15 loss: 0.6709 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 55.3333% F1: 0.2455 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7305 Acc: 81.2500% F1: 0.571 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2513 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.2605 *
*********************************************************
Performing epoch 2 of 5
Fold 2 train - epoch: 2/5 iter: 0/15 loss: 0.8973 Acc: 53.1250% F1: 0.278 Time: 0.96s (0.00s)
Fold 2 train - epoch: 2/5 iter: 1/15 loss: 0.9109 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 2/15 loss: 0.8996 Acc: 50.0000% F1: 0.262 Time: 0.96s (9.92s)
Fold 2 train - epoch: 2/5 iter: 3/15 loss: 0.8349 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 4/15 loss: 0.8849 Acc: 53.1250% F1: 0.278 Time: 0.95s (9.49s)
Fold 2 train - epoch: 2/5 iter: 5/15 loss: 0.8661 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 6/15 loss: 0.7867 Acc: 68.7500% F1: 0.272 Time: 0.95s (9.37s)
Fold 2 train - epoch: 2/5 iter: 7/15 loss: 0.8444 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 8/15 loss: 1.0734 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.75s)
Fold 2 train - epoch: 2/5 iter: 9/15 loss: 0.9960 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 10/15 loss: 0.9720 Acc: 46.8750% F1: 0.213 Time: 0.96s (9.19s)
Fold 2 train - epoch: 2/5 iter: 11/15 loss: 0.8053 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 12/15 loss: 0.9458 Acc: 59.3750% F1: 0.312 Time: 0.94s (9.44s)
Fold 2 train - epoch: 2/5 iter: 13/15 loss: 0.8879 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 14/15 loss: 0.3636 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 56.0000% F1: 0.2664 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6114 Acc: 81.2500% F1: 0.571 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4579 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2586 *
*********************************************************
Performing epoch 3 of 5
Fold 2 train - epoch: 3/5 iter: 0/15 loss: 0.8365 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.00s)
Fold 2 train - epoch: 3/5 iter: 1/15 loss: 0.7879 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/5 iter: 2/15 loss: 0.9238 Acc: 46.8750% F1: 0.249 Time: 0.96s (10.12s)
Fold 2 train - epoch: 3/5 iter: 3/15 loss: 0.7806 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/5 iter: 4/15 loss: 0.8560 Acc: 53.1250% F1: 0.278 Time: 0.94s (9.84s)
Fold 2 train - epoch: 3/5 iter: 5/15 loss: 0.8240 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/5 iter: 6/15 loss: 0.7335 Acc: 71.8750% F1: 0.351 Time: 0.95s (9.58s)
Fold 2 train - epoch: 3/5 iter: 7/15 loss: 0.8875 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 8/15 loss: 0.9894 Acc: 50.0000% F1: 0.222 Time: 0.95s (10.38s)
Fold 2 train - epoch: 3/5 iter: 9/15 loss: 0.9090 Acc: 53.1250% F1: 0.350 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 10/15 loss: 0.9028 Acc: 59.3750% F1: 0.417 Time: 0.96s (9.30s)
Fold 2 train - epoch: 3/5 iter: 11/15 loss: 0.7614 Acc: 62.5000% F1: 0.396 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 12/15 loss: 0.9566 Acc: 62.5000% F1: 0.429 Time: 0.94s (9.94s)
Fold 2 train - epoch: 3/5 iter: 13/15 loss: 0.8545 Acc: 62.5000% F1: 0.351 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 14/15 loss: 0.2167 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 58.4444% F1: 0.3407 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6815 Acc: 71.8750% F1: 0.566 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3795 Acc: 11.1111% F1: 0.078 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3076 *
*********************************************************
Performing epoch 4 of 5
Fold 2 train - epoch: 4/5 iter: 0/15 loss: 0.7568 Acc: 62.5000% F1: 0.482 Time: 0.96s (0.00s)
Fold 2 train - epoch: 4/5 iter: 1/15 loss: 0.7051 Acc: 71.8750% F1: 0.590 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/5 iter: 2/15 loss: 0.8399 Acc: 56.2500% F1: 0.538 Time: 0.96s (9.78s)
Fold 2 train - epoch: 4/5 iter: 3/15 loss: 0.6790 Acc: 68.7500% F1: 0.412 Time: 0.95s (0.04s)
Fold 2 train - epoch: 4/5 iter: 4/15 loss: 0.7359 Acc: 62.5000% F1: 0.560 Time: 0.94s (9.18s)
Fold 2 train - epoch: 4/5 iter: 5/15 loss: 0.7432 Acc: 71.8750% F1: 0.590 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/5 iter: 6/15 loss: 0.6590 Acc: 78.1250% F1: 0.469 Time: 0.94s (9.50s)
Fold 2 train - epoch: 4/5 iter: 7/15 loss: 0.7996 Acc: 53.1250% F1: 0.333 Time: 0.94s (0.02s)
Fold 2 train - epoch: 4/5 iter: 8/15 loss: 0.8527 Acc: 53.1250% F1: 0.429 Time: 0.94s (9.95s)
Fold 2 train - epoch: 4/5 iter: 9/15 loss: 0.8591 Acc: 65.6250% F1: 0.467 Time: 0.95s (0.03s)
Fold 2 train - epoch: 4/5 iter: 10/15 loss: 0.8236 Acc: 59.3750% F1: 0.422 Time: 0.96s (9.27s)
Fold 2 train - epoch: 4/5 iter: 11/15 loss: 0.6556 Acc: 75.0000% F1: 0.513 Time: 0.95s (0.03s)
Fold 2 train - epoch: 4/5 iter: 12/15 loss: 0.8330 Acc: 65.6250% F1: 0.568 Time: 0.94s (9.62s)
Fold 2 train - epoch: 4/5 iter: 13/15 loss: 0.7445 Acc: 62.5000% F1: 0.464 Time: 0.94s (0.02s)
Fold 2 train - epoch: 4/5 iter: 14/15 loss: 0.0419 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 64.8889% F1: 0.5262 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/5 iter: 0/2 loss: 0.7007 Acc: 68.7500% F1: 0.358 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5492 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3022 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/5 iter: 0/15 loss: 1.1754 Acc: 34.3750% F1: 0.244 Time: 0.97s (0.00s)
Fold 3 train - epoch: 0/5 iter: 1/15 loss: 0.9561 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.04s)
Fold 3 train - epoch: 0/5 iter: 2/15 loss: 1.1329 Acc: 46.8750% F1: 0.213 Time: 0.96s (9.57s)
Fold 3 train - epoch: 0/5 iter: 3/15 loss: 0.9103 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 4/15 loss: 0.9580 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.97s)
Fold 3 train - epoch: 0/5 iter: 5/15 loss: 0.8591 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 6/15 loss: 0.8606 Acc: 62.5000% F1: 0.310 Time: 0.94s (8.98s)
Fold 3 train - epoch: 0/5 iter: 7/15 loss: 0.9101 Acc: 46.8750% F1: 0.244 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 8/15 loss: 1.1759 Acc: 37.5000% F1: 0.221 Time: 0.95s (9.32s)
Fold 3 train - epoch: 0/5 iter: 9/15 loss: 1.0624 Acc: 40.6250% F1: 0.286 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 10/15 loss: 1.0006 Acc: 50.0000% F1: 0.333 Time: 0.96s (8.83s)
Fold 3 train - epoch: 0/5 iter: 11/15 loss: 0.9264 Acc: 43.7500% F1: 0.280 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 12/15 loss: 0.9758 Acc: 56.2500% F1: 0.352 Time: 0.94s (10.15s)
Fold 3 train - epoch: 0/5 iter: 13/15 loss: 0.9239 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 14/15 loss: 0.7316 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 50.0000% F1: 0.2915 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6273 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5924 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 3 train - epoch: 1/5 iter: 0/15 loss: 0.8785 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.00s)
Fold 3 train - epoch: 1/5 iter: 1/15 loss: 0.8975 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/5 iter: 2/15 loss: 0.9384 Acc: 50.0000% F1: 0.222 Time: 0.96s (10.09s)
Fold 3 train - epoch: 1/5 iter: 3/15 loss: 0.8384 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/5 iter: 4/15 loss: 1.0083 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.48s)
Fold 3 train - epoch: 1/5 iter: 5/15 loss: 0.9407 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 6/15 loss: 0.7855 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.41s)
Fold 3 train - epoch: 1/5 iter: 7/15 loss: 0.9460 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/5 iter: 8/15 loss: 1.1162 Acc: 53.1250% F1: 0.231 Time: 0.94s (10.33s)
Fold 3 train - epoch: 1/5 iter: 9/15 loss: 1.0420 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 10/15 loss: 0.9762 Acc: 50.0000% F1: 0.265 Time: 0.96s (9.57s)
Fold 3 train - epoch: 1/5 iter: 11/15 loss: 0.8388 Acc: 62.5000% F1: 0.261 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/5 iter: 12/15 loss: 0.9298 Acc: 56.2500% F1: 0.292 Time: 0.95s (10.18s)
Fold 3 train - epoch: 1/5 iter: 13/15 loss: 0.8803 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/5 iter: 14/15 loss: 0.7023 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 55.3333% F1: 0.2458 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7683 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3324 Acc: 22.2222% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3424 *
*********************************************************
Performing epoch 2 of 5
Fold 3 train - epoch: 2/5 iter: 0/15 loss: 0.8592 Acc: 62.5000% F1: 0.389 Time: 0.95s (0.00s)
Fold 3 train - epoch: 2/5 iter: 1/15 loss: 0.8999 Acc: 65.6250% F1: 0.404 Time: 0.95s (0.04s)
Fold 3 train - epoch: 2/5 iter: 2/15 loss: 0.8941 Acc: 53.1250% F1: 0.306 Time: 0.96s (9.61s)
Fold 3 train - epoch: 2/5 iter: 3/15 loss: 0.8099 Acc: 65.6250% F1: 0.361 Time: 0.95s (0.04s)
Fold 3 train - epoch: 2/5 iter: 4/15 loss: 0.8976 Acc: 53.1250% F1: 0.278 Time: 0.94s (9.47s)
Fold 3 train - epoch: 2/5 iter: 5/15 loss: 0.8813 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.04s)
Fold 3 train - epoch: 2/5 iter: 6/15 loss: 0.7887 Acc: 71.8750% F1: 0.351 Time: 0.94s (9.82s)
Fold 3 train - epoch: 2/5 iter: 7/15 loss: 0.8609 Acc: 50.0000% F1: 0.257 Time: 0.94s (0.03s)
Fold 3 train - epoch: 2/5 iter: 8/15 loss: 1.1021 Acc: 53.1250% F1: 0.231 Time: 0.95s (10.67s)
Fold 3 train - epoch: 2/5 iter: 9/15 loss: 1.0124 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 3 train - epoch: 2/5 iter: 10/15 loss: 0.9444 Acc: 50.0000% F1: 0.265 Time: 0.96s (9.45s)
Fold 3 train - epoch: 2/5 iter: 11/15 loss: 0.7776 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.04s)
Fold 3 train - epoch: 2/5 iter: 12/15 loss: 0.9017 Acc: 56.2500% F1: 0.240 Time: 0.94s (10.58s)
Fold 3 train - epoch: 2/5 iter: 13/15 loss: 0.8497 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.03s)
Fold 3 train - epoch: 2/5 iter: 14/15 loss: 0.3812 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 58.2222% F1: 0.3007 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6801 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/5 iter: 1/2 loss: 1.5215 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.2739 *
*********************************************************
Performing epoch 3 of 5
Fold 3 train - epoch: 3/5 iter: 0/15 loss: 0.7815 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.00s)
Fold 3 train - epoch: 3/5 iter: 1/15 loss: 0.8153 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 3 train - epoch: 3/5 iter: 2/15 loss: 0.8722 Acc: 50.0000% F1: 0.262 Time: 0.96s (9.92s)
Fold 3 train - epoch: 3/5 iter: 3/15 loss: 0.7198 Acc: 68.7500% F1: 0.403 Time: 0.94s (0.03s)
Fold 3 train - epoch: 3/5 iter: 4/15 loss: 0.8167 Acc: 59.3750% F1: 0.466 Time: 0.94s (10.01s)
Fold 3 train - epoch: 3/5 iter: 5/15 loss: 0.7727 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.04s)
Fold 3 train - epoch: 3/5 iter: 6/15 loss: 0.7221 Acc: 75.0000% F1: 0.447 Time: 0.94s (10.26s)
Fold 3 train - epoch: 3/5 iter: 7/15 loss: 0.8544 Acc: 53.1250% F1: 0.296 Time: 0.94s (0.03s)
Fold 3 train - epoch: 3/5 iter: 8/15 loss: 0.9583 Acc: 43.7500% F1: 0.222 Time: 0.94s (10.47s)
Fold 3 train - epoch: 3/5 iter: 9/15 loss: 0.9985 Acc: 46.8750% F1: 0.332 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 10/15 loss: 0.9037 Acc: 56.2500% F1: 0.510 Time: 0.96s (9.63s)
Fold 3 train - epoch: 3/5 iter: 11/15 loss: 0.7200 Acc: 68.7500% F1: 0.612 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 12/15 loss: 0.7999 Acc: 65.6250% F1: 0.548 Time: 0.94s (10.23s)
Fold 3 train - epoch: 3/5 iter: 13/15 loss: 0.8487 Acc: 65.6250% F1: 0.366 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 14/15 loss: 0.1478 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 59.5556% F1: 0.4015 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7966 Acc: 65.6250% F1: 0.317 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6026 Acc: 27.7778% F1: 0.185 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 52.0000% F1: 0.3484 *
*********************************************************
Performing epoch 4 of 5
Fold 3 train - epoch: 4/5 iter: 0/15 loss: 0.6597 Acc: 71.8750% F1: 0.630 Time: 0.95s (0.00s)
Fold 3 train - epoch: 4/5 iter: 1/15 loss: 0.6970 Acc: 78.1250% F1: 0.654 Time: 0.95s (0.03s)
Fold 3 train - epoch: 4/5 iter: 2/15 loss: 0.7519 Acc: 59.3750% F1: 0.465 Time: 0.96s (9.37s)
Fold 3 train - epoch: 4/5 iter: 3/15 loss: 0.5884 Acc: 78.1250% F1: 0.510 Time: 0.94s (0.03s)
Fold 3 train - epoch: 4/5 iter: 4/15 loss: 0.7200 Acc: 65.6250% F1: 0.687 Time: 0.94s (9.20s)
Fold 3 train - epoch: 4/5 iter: 5/15 loss: 0.6352 Acc: 68.7500% F1: 0.543 Time: 0.95s (0.04s)
Fold 3 train - epoch: 4/5 iter: 6/15 loss: 0.5607 Acc: 81.2500% F1: 0.520 Time: 0.94s (9.05s)
Fold 3 train - epoch: 4/5 iter: 7/15 loss: 0.8564 Acc: 56.2500% F1: 0.333 Time: 0.94s (0.03s)
Fold 3 train - epoch: 4/5 iter: 8/15 loss: 0.8586 Acc: 56.2500% F1: 0.368 Time: 0.94s (9.80s)
Fold 3 train - epoch: 4/5 iter: 9/15 loss: 0.8562 Acc: 59.3750% F1: 0.422 Time: 0.95s (0.03s)
Fold 3 train - epoch: 4/5 iter: 10/15 loss: 0.8349 Acc: 65.6250% F1: 0.564 Time: 0.96s (9.25s)
Fold 3 train - epoch: 4/5 iter: 11/15 loss: 0.5686 Acc: 71.8750% F1: 0.729 Time: 0.95s (0.03s)
Fold 3 train - epoch: 4/5 iter: 12/15 loss: 0.6892 Acc: 68.7500% F1: 0.569 Time: 0.94s (9.61s)
Fold 3 train - epoch: 4/5 iter: 13/15 loss: 0.7172 Acc: 62.5000% F1: 0.506 Time: 0.94s (0.03s)
Fold 3 train - epoch: 4/5 iter: 14/15 loss: 0.0791 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 67.5556% F1: 0.5754 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0224 Acc: 50.0000% F1: 0.265 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7610 Acc: 27.7778% F1: 0.185 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 42.0000% F1: 0.3068 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/5 iter: 0/15 loss: 1.1872 Acc: 34.3750% F1: 0.239 Time: 0.99s (0.00s)
Fold 4 train - epoch: 0/5 iter: 1/15 loss: 1.0169 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 4 train - epoch: 0/5 iter: 2/15 loss: 1.0522 Acc: 50.0000% F1: 0.262 Time: 0.96s (8.61s)
Fold 4 train - epoch: 0/5 iter: 3/15 loss: 0.9519 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 4/15 loss: 1.0007 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.27s)
Fold 4 train - epoch: 0/5 iter: 5/15 loss: 0.9249 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.35s)
Fold 4 train - epoch: 0/5 iter: 6/15 loss: 0.8515 Acc: 68.7500% F1: 0.375 Time: 0.94s (7.84s)
Fold 4 train - epoch: 0/5 iter: 7/15 loss: 0.8431 Acc: 59.3750% F1: 0.379 Time: 0.94s (1.89s)
Fold 4 train - epoch: 0/5 iter: 8/15 loss: 1.2027 Acc: 43.7500% F1: 0.279 Time: 0.94s (6.98s)
Fold 4 train - epoch: 0/5 iter: 9/15 loss: 1.0418 Acc: 40.6250% F1: 0.266 Time: 0.94s (2.46s)
Fold 4 train - epoch: 0/5 iter: 10/15 loss: 0.9906 Acc: 43.7500% F1: 0.305 Time: 0.95s (5.60s)
Fold 4 train - epoch: 0/5 iter: 11/15 loss: 0.9358 Acc: 53.1250% F1: 0.344 Time: 0.94s (3.46s)
Fold 4 train - epoch: 0/5 iter: 12/15 loss: 0.9940 Acc: 53.1250% F1: 0.280 Time: 0.95s (5.52s)
Fold 4 train - epoch: 0/5 iter: 13/15 loss: 0.9117 Acc: 56.2500% F1: 0.240 Time: 0.94s (5.22s)
Fold 4 train - epoch: 0/5 iter: 14/15 loss: 0.5014 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 52.2222% F1: 0.3128 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5433 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6412 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 4 train - epoch: 1/5 iter: 0/15 loss: 0.9007 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 4 train - epoch: 1/5 iter: 1/15 loss: 0.9291 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 2/15 loss: 0.9970 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.75s)
Fold 4 train - epoch: 1/5 iter: 3/15 loss: 0.8614 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 4/15 loss: 1.0253 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.22s)
Fold 4 train - epoch: 1/5 iter: 5/15 loss: 0.8870 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 6/15 loss: 0.7504 Acc: 68.7500% F1: 0.272 Time: 0.95s (9.26s)
Fold 4 train - epoch: 1/5 iter: 7/15 loss: 0.8892 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 4 train - epoch: 1/5 iter: 8/15 loss: 1.1097 Acc: 53.1250% F1: 0.231 Time: 0.95s (10.01s)
Fold 4 train - epoch: 1/5 iter: 9/15 loss: 0.9881 Acc: 50.0000% F1: 0.265 Time: 0.94s (0.04s)
Fold 4 train - epoch: 1/5 iter: 10/15 loss: 0.9125 Acc: 53.1250% F1: 0.311 Time: 0.96s (9.52s)
Fold 4 train - epoch: 1/5 iter: 11/15 loss: 0.8756 Acc: 59.3750% F1: 0.303 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 12/15 loss: 0.9525 Acc: 56.2500% F1: 0.370 Time: 0.94s (10.29s)
Fold 4 train - epoch: 1/5 iter: 13/15 loss: 0.9162 Acc: 59.3750% F1: 0.381 Time: 0.94s (0.02s)
Fold 4 train - epoch: 1/5 iter: 14/15 loss: 0.7712 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 55.7778% F1: 0.2815 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7216 Acc: 78.1250% F1: 0.547 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3842 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.3652 *
*********************************************************
Performing epoch 2 of 5
Fold 4 train - epoch: 2/5 iter: 0/15 loss: 0.8422 Acc: 59.3750% F1: 0.389 Time: 0.95s (0.00s)
Fold 4 train - epoch: 2/5 iter: 1/15 loss: 0.8486 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.03s)
Fold 4 train - epoch: 2/5 iter: 2/15 loss: 0.8778 Acc: 59.3750% F1: 0.362 Time: 0.96s (9.32s)
Fold 4 train - epoch: 2/5 iter: 3/15 loss: 0.7803 Acc: 62.5000% F1: 0.310 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/5 iter: 4/15 loss: 0.9329 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.79s)
Fold 4 train - epoch: 2/5 iter: 5/15 loss: 0.8300 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/5 iter: 6/15 loss: 0.7524 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.92s)
Fold 4 train - epoch: 2/5 iter: 7/15 loss: 0.8393 Acc: 50.0000% F1: 0.257 Time: 0.94s (0.03s)
Fold 4 train - epoch: 2/5 iter: 8/15 loss: 1.1066 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.89s)
Fold 4 train - epoch: 2/5 iter: 9/15 loss: 0.9936 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 10/15 loss: 0.9884 Acc: 50.0000% F1: 0.265 Time: 0.95s (9.45s)
Fold 4 train - epoch: 2/5 iter: 11/15 loss: 0.7708 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 12/15 loss: 0.8852 Acc: 56.2500% F1: 0.245 Time: 0.94s (9.98s)
Fold 4 train - epoch: 2/5 iter: 13/15 loss: 0.8821 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 14/15 loss: 0.3300 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 56.8889% F1: 0.2895 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7003 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4488 Acc: 27.7778% F1: 0.185 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.3407 *
*********************************************************
Performing epoch 3 of 5
Fold 4 train - epoch: 3/5 iter: 0/15 loss: 0.7535 Acc: 65.6250% F1: 0.500 Time: 0.97s (0.00s)
Fold 4 train - epoch: 3/5 iter: 1/15 loss: 0.8127 Acc: 71.8750% F1: 0.605 Time: 0.95s (0.04s)
Fold 4 train - epoch: 3/5 iter: 2/15 loss: 0.8360 Acc: 56.2500% F1: 0.345 Time: 0.96s (9.34s)
Fold 4 train - epoch: 3/5 iter: 3/15 loss: 0.7549 Acc: 68.7500% F1: 0.424 Time: 0.95s (0.04s)
Fold 4 train - epoch: 3/5 iter: 4/15 loss: 0.8120 Acc: 59.3750% F1: 0.403 Time: 0.94s (9.15s)
Fold 4 train - epoch: 3/5 iter: 5/15 loss: 0.8140 Acc: 62.5000% F1: 0.358 Time: 0.95s (0.05s)
Fold 4 train - epoch: 3/5 iter: 6/15 loss: 0.7424 Acc: 71.8750% F1: 0.396 Time: 0.95s (9.36s)
Fold 4 train - epoch: 3/5 iter: 7/15 loss: 0.8261 Acc: 50.0000% F1: 0.282 Time: 0.95s (0.05s)
Fold 4 train - epoch: 3/5 iter: 8/15 loss: 0.9851 Acc: 50.0000% F1: 0.311 Time: 0.94s (10.12s)
Fold 4 train - epoch: 3/5 iter: 9/15 loss: 0.8977 Acc: 53.1250% F1: 0.311 Time: 0.94s (0.45s)
Fold 4 train - epoch: 3/5 iter: 10/15 loss: 0.9215 Acc: 50.0000% F1: 0.300 Time: 0.96s (8.17s)
Fold 4 train - epoch: 3/5 iter: 11/15 loss: 0.7316 Acc: 75.0000% F1: 0.642 Time: 0.94s (1.84s)
Fold 4 train - epoch: 3/5 iter: 12/15 loss: 0.8663 Acc: 62.5000% F1: 0.389 Time: 0.94s (7.85s)
Fold 4 train - epoch: 3/5 iter: 13/15 loss: 0.9024 Acc: 53.1250% F1: 0.394 Time: 0.94s (2.77s)
Fold 4 train - epoch: 3/5 iter: 14/15 loss: 0.0931 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 60.8889% F1: 0.4145 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8291 Acc: 53.1250% F1: 0.296 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4292 Acc: 33.3333% F1: 0.259 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.3854 *
*********************************************************
Performing epoch 4 of 5
Fold 4 train - epoch: 4/5 iter: 0/15 loss: 0.6844 Acc: 68.7500% F1: 0.643 Time: 0.96s (0.00s)
Fold 4 train - epoch: 4/5 iter: 1/15 loss: 0.6884 Acc: 71.8750% F1: 0.678 Time: 0.94s (0.03s)
Fold 4 train - epoch: 4/5 iter: 2/15 loss: 0.6828 Acc: 78.1250% F1: 0.672 Time: 0.96s (9.99s)
Fold 4 train - epoch: 4/5 iter: 3/15 loss: 0.6462 Acc: 71.8750% F1: 0.682 Time: 0.94s (0.03s)
Fold 4 train - epoch: 4/5 iter: 4/15 loss: 0.7124 Acc: 78.1250% F1: 0.673 Time: 0.94s (9.14s)
Fold 4 train - epoch: 4/5 iter: 5/15 loss: 0.6977 Acc: 75.0000% F1: 0.621 Time: 0.94s (0.03s)
Fold 4 train - epoch: 4/5 iter: 6/15 loss: 0.5969 Acc: 81.2500% F1: 0.516 Time: 0.94s (9.04s)
Fold 4 train - epoch: 4/5 iter: 7/15 loss: 0.7880 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.03s)
Fold 4 train - epoch: 4/5 iter: 8/15 loss: 0.9363 Acc: 59.3750% F1: 0.366 Time: 0.94s (9.53s)
Fold 4 train - epoch: 4/5 iter: 9/15 loss: 0.7347 Acc: 68.7500% F1: 0.483 Time: 0.95s (0.03s)
Fold 4 train - epoch: 4/5 iter: 10/15 loss: 0.7187 Acc: 68.7500% F1: 0.591 Time: 0.96s (8.54s)
Fold 4 train - epoch: 4/5 iter: 11/15 loss: 0.5998 Acc: 75.0000% F1: 0.695 Time: 0.95s (0.36s)
Fold 4 train - epoch: 4/5 iter: 12/15 loss: 0.6763 Acc: 68.7500% F1: 0.583 Time: 0.94s (8.64s)
Fold 4 train - epoch: 4/5 iter: 13/15 loss: 0.7570 Acc: 68.7500% F1: 0.623 Time: 0.94s (1.30s)
Fold 4 train - epoch: 4/5 iter: 14/15 loss: 0.0427 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 71.1111% F1: 0.6073 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/5 iter: 0/2 loss: 1.1075 Acc: 46.8750% F1: 0.273 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6161 Acc: 50.0000% F1: 0.360 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.4485 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/5 iter: 0/15 loss: 1.0685 Acc: 34.3750% F1: 0.244 Time: 0.98s (0.00s)
Fold 5 train - epoch: 0/5 iter: 1/15 loss: 1.0215 Acc: 53.1250% F1: 0.278 Time: 0.94s (0.05s)
Fold 5 train - epoch: 0/5 iter: 2/15 loss: 1.0961 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.53s)
Fold 5 train - epoch: 0/5 iter: 3/15 loss: 0.9757 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.04s)
Fold 5 train - epoch: 0/5 iter: 4/15 loss: 1.0544 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.12s)
Fold 5 train - epoch: 0/5 iter: 5/15 loss: 0.8729 Acc: 62.5000% F1: 0.319 Time: 0.94s (0.02s)
Fold 5 train - epoch: 0/5 iter: 6/15 loss: 0.8756 Acc: 59.3750% F1: 0.296 Time: 0.94s (8.87s)
Fold 5 train - epoch: 0/5 iter: 7/15 loss: 0.8490 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 8/15 loss: 1.1839 Acc: 46.8750% F1: 0.316 Time: 0.94s (9.42s)
Fold 5 train - epoch: 0/5 iter: 9/15 loss: 1.0105 Acc: 40.6250% F1: 0.286 Time: 0.94s (0.02s)
Fold 5 train - epoch: 0/5 iter: 10/15 loss: 1.0075 Acc: 37.5000% F1: 0.212 Time: 0.96s (8.59s)
Fold 5 train - epoch: 0/5 iter: 11/15 loss: 0.9329 Acc: 50.0000% F1: 0.292 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/5 iter: 12/15 loss: 1.0751 Acc: 37.5000% F1: 0.182 Time: 0.94s (10.24s)
Fold 5 train - epoch: 0/5 iter: 13/15 loss: 0.9023 Acc: 62.5000% F1: 0.353 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/5 iter: 14/15 loss: 0.4958 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 50.0000% F1: 0.2878 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5788 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6747 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 5 train - epoch: 1/5 iter: 0/15 loss: 0.8498 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.00s)
Fold 5 train - epoch: 1/5 iter: 1/15 loss: 0.9073 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/5 iter: 2/15 loss: 0.9403 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.66s)
Fold 5 train - epoch: 1/5 iter: 3/15 loss: 0.8139 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/5 iter: 4/15 loss: 1.0031 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.02s)
Fold 5 train - epoch: 1/5 iter: 5/15 loss: 0.9066 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/5 iter: 6/15 loss: 0.7669 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.06s)
Fold 5 train - epoch: 1/5 iter: 7/15 loss: 0.9112 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 8/15 loss: 1.1364 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.26s)
Fold 5 train - epoch: 1/5 iter: 9/15 loss: 0.9808 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 10/15 loss: 0.9739 Acc: 53.1250% F1: 0.311 Time: 0.96s (9.00s)
Fold 5 train - epoch: 1/5 iter: 11/15 loss: 0.8527 Acc: 53.1250% F1: 0.307 Time: 0.95s (0.03s)
Fold 5 train - epoch: 1/5 iter: 12/15 loss: 0.9330 Acc: 62.5000% F1: 0.402 Time: 0.94s (10.62s)
Fold 5 train - epoch: 1/5 iter: 13/15 loss: 0.8929 Acc: 53.1250% F1: 0.279 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/5 iter: 14/15 loss: 0.6576 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 55.3333% F1: 0.2742 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7634 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3939 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.2833 *
*********************************************************
Performing epoch 2 of 5
Fold 5 train - epoch: 2/5 iter: 0/15 loss: 0.8465 Acc: 65.6250% F1: 0.424 Time: 0.96s (0.00s)
Fold 5 train - epoch: 2/5 iter: 1/15 loss: 0.8569 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 5 train - epoch: 2/5 iter: 2/15 loss: 0.9126 Acc: 53.1250% F1: 0.275 Time: 0.96s (9.39s)
Fold 5 train - epoch: 2/5 iter: 3/15 loss: 0.7577 Acc: 62.5000% F1: 0.310 Time: 0.94s (0.02s)
Fold 5 train - epoch: 2/5 iter: 4/15 loss: 0.9131 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.61s)
Fold 5 train - epoch: 2/5 iter: 5/15 loss: 0.8522 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.02s)
Fold 5 train - epoch: 2/5 iter: 6/15 loss: 0.7724 Acc: 71.8750% F1: 0.351 Time: 0.94s (8.88s)
Fold 5 train - epoch: 2/5 iter: 7/15 loss: 0.8733 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.32s)
Fold 5 train - epoch: 2/5 iter: 8/15 loss: 1.1082 Acc: 53.1250% F1: 0.231 Time: 0.95s (8.91s)
Fold 5 train - epoch: 2/5 iter: 9/15 loss: 1.0168 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.81s)
Fold 5 train - epoch: 2/5 iter: 10/15 loss: 0.9201 Acc: 46.8750% F1: 0.213 Time: 0.96s (8.17s)
Fold 5 train - epoch: 2/5 iter: 11/15 loss: 0.7838 Acc: 62.5000% F1: 0.314 Time: 0.95s (1.07s)
Fold 5 train - epoch: 2/5 iter: 12/15 loss: 0.9350 Acc: 53.1250% F1: 0.236 Time: 0.94s (9.17s)
Fold 5 train - epoch: 2/5 iter: 13/15 loss: 0.9147 Acc: 56.2500% F1: 0.403 Time: 0.94s (0.64s)
Fold 5 train - epoch: 2/5 iter: 14/15 loss: 0.3907 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 56.6667% F1: 0.2918 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8116 Acc: 65.6250% F1: 0.396 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4047 Acc: 11.1111% F1: 0.089 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.2572 *
*********************************************************
Performing epoch 3 of 5
Fold 5 train - epoch: 3/5 iter: 0/15 loss: 0.7916 Acc: 65.6250% F1: 0.524 Time: 0.95s (0.00s)
Fold 5 train - epoch: 3/5 iter: 1/15 loss: 0.8123 Acc: 68.7500% F1: 0.594 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 2/15 loss: 0.8653 Acc: 56.2500% F1: 0.326 Time: 0.95s (9.71s)
Fold 5 train - epoch: 3/5 iter: 3/15 loss: 0.7120 Acc: 62.5000% F1: 0.310 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 4/15 loss: 0.8087 Acc: 62.5000% F1: 0.528 Time: 0.94s (9.05s)
Fold 5 train - epoch: 3/5 iter: 5/15 loss: 0.7845 Acc: 65.6250% F1: 0.409 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 6/15 loss: 0.7243 Acc: 75.0000% F1: 0.415 Time: 0.95s (9.38s)
Fold 5 train - epoch: 3/5 iter: 7/15 loss: 0.7998 Acc: 56.2500% F1: 0.333 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 8/15 loss: 1.1064 Acc: 53.1250% F1: 0.294 Time: 0.94s (9.89s)
Fold 5 train - epoch: 3/5 iter: 9/15 loss: 1.0432 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 5 train - epoch: 3/5 iter: 10/15 loss: 0.8604 Acc: 53.1250% F1: 0.362 Time: 0.95s (9.86s)
Fold 5 train - epoch: 3/5 iter: 11/15 loss: 0.7510 Acc: 65.6250% F1: 0.392 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 12/15 loss: 0.8502 Acc: 62.5000% F1: 0.405 Time: 0.94s (9.81s)
Fold 5 train - epoch: 3/5 iter: 13/15 loss: 0.8567 Acc: 68.7500% F1: 0.591 Time: 0.94s (0.02s)
Fold 5 train - epoch: 3/5 iter: 14/15 loss: 0.1426 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 61.7778% F1: 0.4227 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9354 Acc: 46.8750% F1: 0.268 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3973 Acc: 33.3333% F1: 0.190 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 42.0000% F1: 0.3000 *
*********************************************************
Performing epoch 4 of 5
Fold 5 train - epoch: 4/5 iter: 0/15 loss: 0.6728 Acc: 62.5000% F1: 0.576 Time: 0.95s (0.00s)
Fold 5 train - epoch: 4/5 iter: 1/15 loss: 0.6787 Acc: 84.3750% F1: 0.724 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/5 iter: 2/15 loss: 0.7663 Acc: 71.8750% F1: 0.604 Time: 0.95s (10.00s)
Fold 5 train - epoch: 4/5 iter: 3/15 loss: 0.6846 Acc: 75.0000% F1: 0.484 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/5 iter: 4/15 loss: 0.7305 Acc: 59.3750% F1: 0.519 Time: 0.94s (9.49s)
Fold 5 train - epoch: 4/5 iter: 5/15 loss: 0.6984 Acc: 68.7500% F1: 0.480 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/5 iter: 6/15 loss: 0.6789 Acc: 71.8750% F1: 0.351 Time: 0.94s (9.40s)
Fold 5 train - epoch: 4/5 iter: 7/15 loss: 0.7572 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 8/15 loss: 0.9479 Acc: 56.2500% F1: 0.348 Time: 0.94s (9.77s)
Fold 5 train - epoch: 4/5 iter: 9/15 loss: 0.8595 Acc: 53.1250% F1: 0.361 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/5 iter: 10/15 loss: 0.7464 Acc: 65.6250% F1: 0.571 Time: 0.95s (9.38s)
Fold 5 train - epoch: 4/5 iter: 11/15 loss: 0.7090 Acc: 68.7500% F1: 0.481 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/5 iter: 12/15 loss: 0.7104 Acc: 71.8750% F1: 0.702 Time: 0.94s (9.97s)
Fold 5 train - epoch: 4/5 iter: 13/15 loss: 0.8336 Acc: 62.5000% F1: 0.448 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/5 iter: 14/15 loss: 0.0549 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 66.2222% F1: 0.5285 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/5 iter: 0/2 loss: 0.9983 Acc: 50.0000% F1: 0.259 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5357 Acc: 33.3333% F1: 0.190 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 44.0000% F1: 0.3113 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/5 iter: 0/15 loss: 1.0719 Acc: 34.3750% F1: 0.253 Time: 0.97s (0.00s)
Fold 6 train - epoch: 0/5 iter: 1/15 loss: 0.9981 Acc: 53.1250% F1: 0.278 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/5 iter: 2/15 loss: 1.0490 Acc: 53.1250% F1: 0.275 Time: 0.96s (8.71s)
Fold 6 train - epoch: 0/5 iter: 3/15 loss: 0.9295 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 4/15 loss: 1.0727 Acc: 50.0000% F1: 0.227 Time: 0.94s (8.55s)
Fold 6 train - epoch: 0/5 iter: 5/15 loss: 0.8895 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.06s)
Fold 6 train - epoch: 0/5 iter: 6/15 loss: 0.8238 Acc: 75.0000% F1: 0.447 Time: 0.94s (9.04s)
Fold 6 train - epoch: 0/5 iter: 7/15 loss: 0.8746 Acc: 53.1250% F1: 0.317 Time: 0.95s (0.78s)
Fold 6 train - epoch: 0/5 iter: 8/15 loss: 1.2062 Acc: 34.3750% F1: 0.183 Time: 0.94s (8.32s)
Fold 6 train - epoch: 0/5 iter: 9/15 loss: 1.0412 Acc: 43.7500% F1: 0.265 Time: 0.94s (1.59s)
Fold 6 train - epoch: 0/5 iter: 10/15 loss: 1.0207 Acc: 40.6250% F1: 0.225 Time: 0.96s (7.44s)
Fold 6 train - epoch: 0/5 iter: 11/15 loss: 0.8925 Acc: 59.3750% F1: 0.360 Time: 0.94s (1.79s)
Fold 6 train - epoch: 0/5 iter: 12/15 loss: 0.9967 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.46s)
Fold 6 train - epoch: 0/5 iter: 13/15 loss: 0.9557 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.93s)
Fold 6 train - epoch: 0/5 iter: 14/15 loss: 0.5818 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 51.7778% F1: 0.2886 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5642 Acc: 87.5000% F1: 0.467 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6588 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 6 train - epoch: 1/5 iter: 0/15 loss: 0.8625 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 6 train - epoch: 1/5 iter: 1/15 loss: 0.9285 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/5 iter: 2/15 loss: 0.9561 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.90s)
Fold 6 train - epoch: 1/5 iter: 3/15 loss: 0.8324 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/5 iter: 4/15 loss: 0.9972 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.29s)
Fold 6 train - epoch: 1/5 iter: 5/15 loss: 0.9423 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/5 iter: 6/15 loss: 0.7735 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.61s)
Fold 6 train - epoch: 1/5 iter: 7/15 loss: 0.8850 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 8/15 loss: 1.1102 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.32s)
Fold 6 train - epoch: 1/5 iter: 9/15 loss: 1.0262 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 10/15 loss: 0.9542 Acc: 46.8750% F1: 0.213 Time: 0.96s (9.15s)
Fold 6 train - epoch: 1/5 iter: 11/15 loss: 0.8994 Acc: 59.3750% F1: 0.335 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 12/15 loss: 0.9606 Acc: 50.0000% F1: 0.295 Time: 0.95s (10.22s)
Fold 6 train - epoch: 1/5 iter: 13/15 loss: 0.9440 Acc: 43.7500% F1: 0.262 Time: 0.94s (0.02s)
Fold 6 train - epoch: 1/5 iter: 14/15 loss: 0.5720 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 53.3333% F1: 0.2536 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7628 Acc: 65.6250% F1: 0.396 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3216 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 50.0000% F1: 0.3077 *
*********************************************************
Performing epoch 2 of 5
Fold 6 train - epoch: 2/5 iter: 0/15 loss: 0.8256 Acc: 62.5000% F1: 0.525 Time: 0.96s (0.00s)
Fold 6 train - epoch: 2/5 iter: 1/15 loss: 0.8607 Acc: 75.0000% F1: 0.628 Time: 0.94s (0.03s)
Fold 6 train - epoch: 2/5 iter: 2/15 loss: 0.8715 Acc: 56.2500% F1: 0.321 Time: 0.96s (8.91s)
Fold 6 train - epoch: 2/5 iter: 3/15 loss: 0.8277 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.03s)
Fold 6 train - epoch: 2/5 iter: 4/15 loss: 0.9151 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.79s)
Fold 6 train - epoch: 2/5 iter: 5/15 loss: 0.8791 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 6 train - epoch: 2/5 iter: 6/15 loss: 0.7355 Acc: 71.8750% F1: 0.351 Time: 0.94s (9.59s)
Fold 6 train - epoch: 2/5 iter: 7/15 loss: 0.8286 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.15s)
Fold 6 train - epoch: 2/5 iter: 8/15 loss: 1.1309 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.27s)
Fold 6 train - epoch: 2/5 iter: 9/15 loss: 1.0266 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.52s)
Fold 6 train - epoch: 2/5 iter: 10/15 loss: 0.9261 Acc: 50.0000% F1: 0.265 Time: 0.96s (8.67s)
Fold 6 train - epoch: 2/5 iter: 11/15 loss: 0.7900 Acc: 56.2500% F1: 0.289 Time: 0.94s (0.41s)
Fold 6 train - epoch: 2/5 iter: 12/15 loss: 0.8790 Acc: 56.2500% F1: 0.240 Time: 0.94s (10.21s)
Fold 6 train - epoch: 2/5 iter: 13/15 loss: 0.8907 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.04s)
Fold 6 train - epoch: 2/5 iter: 14/15 loss: 0.3549 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 57.5556% F1: 0.3256 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8139 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3280 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.2868 *
*********************************************************
Performing epoch 3 of 5
Fold 6 train - epoch: 3/5 iter: 0/15 loss: 0.7365 Acc: 71.8750% F1: 0.664 Time: 0.96s (0.00s)
Fold 6 train - epoch: 3/5 iter: 1/15 loss: 0.7791 Acc: 68.7500% F1: 0.583 Time: 0.94s (0.03s)
Fold 6 train - epoch: 3/5 iter: 2/15 loss: 0.8194 Acc: 50.0000% F1: 0.291 Time: 0.95s (8.83s)
Fold 6 train - epoch: 3/5 iter: 3/15 loss: 0.7982 Acc: 71.8750% F1: 0.458 Time: 0.94s (0.02s)
Fold 6 train - epoch: 3/5 iter: 4/15 loss: 0.7879 Acc: 71.8750% F1: 0.685 Time: 0.94s (8.40s)
Fold 6 train - epoch: 3/5 iter: 5/15 loss: 0.7932 Acc: 62.5000% F1: 0.319 Time: 0.94s (0.03s)
Fold 6 train - epoch: 3/5 iter: 6/15 loss: 0.6706 Acc: 75.0000% F1: 0.415 Time: 0.94s (8.66s)
Fold 6 train - epoch: 3/5 iter: 7/15 loss: 0.7561 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.04s)
Fold 6 train - epoch: 3/5 iter: 8/15 loss: 1.0488 Acc: 53.1250% F1: 0.292 Time: 0.94s (9.41s)
Fold 6 train - epoch: 3/5 iter: 9/15 loss: 0.9806 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.61s)
Fold 6 train - epoch: 3/5 iter: 10/15 loss: 0.8455 Acc: 56.2500% F1: 0.383 Time: 0.96s (8.59s)
Fold 6 train - epoch: 3/5 iter: 11/15 loss: 0.7391 Acc: 62.5000% F1: 0.377 Time: 0.94s (0.57s)
Fold 6 train - epoch: 3/5 iter: 12/15 loss: 0.8302 Acc: 62.5000% F1: 0.409 Time: 0.94s (9.12s)
Fold 6 train - epoch: 3/5 iter: 13/15 loss: 0.9011 Acc: 53.1250% F1: 0.236 Time: 0.94s (0.21s)
Fold 6 train - epoch: 3/5 iter: 14/15 loss: 0.1074 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 62.0000% F1: 0.4382 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/5 iter: 0/2 loss: 1.0368 Acc: 50.0000% F1: 0.232 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4378 Acc: 38.8889% F1: 0.283 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.3707 *
*********************************************************
Performing epoch 4 of 5
Fold 6 train - epoch: 4/5 iter: 0/15 loss: 0.7182 Acc: 62.5000% F1: 0.574 Time: 0.96s (0.00s)
Fold 6 train - epoch: 4/5 iter: 1/15 loss: 0.6851 Acc: 71.8750% F1: 0.681 Time: 0.94s (0.03s)
Fold 6 train - epoch: 4/5 iter: 2/15 loss: 0.6916 Acc: 75.0000% F1: 0.697 Time: 0.95s (9.09s)
Fold 6 train - epoch: 4/5 iter: 3/15 loss: 0.6578 Acc: 65.6250% F1: 0.449 Time: 0.94s (0.03s)
Fold 6 train - epoch: 4/5 iter: 4/15 loss: 0.7089 Acc: 71.8750% F1: 0.616 Time: 0.94s (9.21s)
Fold 6 train - epoch: 4/5 iter: 5/15 loss: 0.6201 Acc: 75.0000% F1: 0.614 Time: 0.94s (0.03s)
Fold 6 train - epoch: 4/5 iter: 6/15 loss: 0.5774 Acc: 81.2500% F1: 0.516 Time: 0.94s (10.12s)
Fold 6 train - epoch: 4/5 iter: 7/15 loss: 0.7630 Acc: 59.3750% F1: 0.379 Time: 0.95s (0.26s)
Fold 6 train - epoch: 4/5 iter: 8/15 loss: 0.9741 Acc: 56.2500% F1: 0.307 Time: 0.94s (9.45s)
Fold 6 train - epoch: 4/5 iter: 9/15 loss: 0.9222 Acc: 53.1250% F1: 0.335 Time: 0.95s (0.72s)
Fold 6 train - epoch: 4/5 iter: 10/15 loss: 0.6632 Acc: 71.8750% F1: 0.512 Time: 0.95s (8.56s)
Fold 6 train - epoch: 4/5 iter: 11/15 loss: 0.6413 Acc: 71.8750% F1: 0.619 Time: 0.94s (0.95s)
Fold 6 train - epoch: 4/5 iter: 12/15 loss: 0.6712 Acc: 71.8750% F1: 0.619 Time: 0.94s (8.80s)
Fold 6 train - epoch: 4/5 iter: 13/15 loss: 0.7398 Acc: 71.8750% F1: 0.622 Time: 0.94s (0.51s)
Fold 6 train - epoch: 4/5 iter: 14/15 loss: 0.0502 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 68.6667% F1: 0.5668 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/5 iter: 0/2 loss: 1.2113 Acc: 46.8750% F1: 0.222 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6196 Acc: 33.3333% F1: 0.241 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 42.0000% F1: 0.3381 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/5 iter: 0/15 loss: 1.1380 Acc: 31.2500% F1: 0.228 Time: 0.96s (0.00s)
Fold 7 train - epoch: 0/5 iter: 1/15 loss: 0.9359 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.04s)
Fold 7 train - epoch: 0/5 iter: 2/15 loss: 1.0724 Acc: 50.0000% F1: 0.222 Time: 0.95s (9.13s)
Fold 7 train - epoch: 0/5 iter: 3/15 loss: 0.9820 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.03s)
Fold 7 train - epoch: 0/5 iter: 4/15 loss: 1.0436 Acc: 46.8750% F1: 0.213 Time: 0.94s (8.97s)
Fold 7 train - epoch: 0/5 iter: 5/15 loss: 0.9534 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.25s)
Fold 7 train - epoch: 0/5 iter: 6/15 loss: 0.8152 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.76s)
Fold 7 train - epoch: 0/5 iter: 7/15 loss: 0.9002 Acc: 43.7500% F1: 0.254 Time: 0.95s (0.69s)
Fold 7 train - epoch: 0/5 iter: 8/15 loss: 1.1963 Acc: 46.8750% F1: 0.265 Time: 0.94s (8.38s)
Fold 7 train - epoch: 0/5 iter: 9/15 loss: 1.0197 Acc: 56.2500% F1: 0.398 Time: 0.94s (1.54s)
Fold 7 train - epoch: 0/5 iter: 10/15 loss: 0.9949 Acc: 37.5000% F1: 0.259 Time: 0.96s (8.26s)
Fold 7 train - epoch: 0/5 iter: 11/15 loss: 0.9586 Acc: 50.0000% F1: 0.266 Time: 0.95s (1.19s)
Fold 7 train - epoch: 0/5 iter: 12/15 loss: 0.9747 Acc: 65.6250% F1: 0.424 Time: 0.94s (8.43s)
Fold 7 train - epoch: 0/5 iter: 13/15 loss: 0.9202 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.55s)
Fold 7 train - epoch: 0/5 iter: 14/15 loss: 0.6620 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 52.0000% F1: 0.2993 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5990 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5375 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 7 train - epoch: 1/5 iter: 0/15 loss: 0.8886 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 7 train - epoch: 1/5 iter: 1/15 loss: 0.9255 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 7 train - epoch: 1/5 iter: 2/15 loss: 0.9676 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.22s)
Fold 7 train - epoch: 1/5 iter: 3/15 loss: 0.8308 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 7 train - epoch: 1/5 iter: 4/15 loss: 1.0164 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.96s)
Fold 7 train - epoch: 1/5 iter: 5/15 loss: 0.9385 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.09s)
Fold 7 train - epoch: 1/5 iter: 6/15 loss: 0.7764 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.94s)
Fold 7 train - epoch: 1/5 iter: 7/15 loss: 0.9168 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.39s)
Fold 7 train - epoch: 1/5 iter: 8/15 loss: 1.1005 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.74s)
Fold 7 train - epoch: 1/5 iter: 9/15 loss: 0.9991 Acc: 46.8750% F1: 0.213 Time: 0.95s (1.12s)
Fold 7 train - epoch: 1/5 iter: 10/15 loss: 0.9755 Acc: 43.7500% F1: 0.203 Time: 0.96s (8.78s)
Fold 7 train - epoch: 1/5 iter: 11/15 loss: 0.8703 Acc: 62.5000% F1: 0.314 Time: 0.94s (0.64s)
Fold 7 train - epoch: 1/5 iter: 12/15 loss: 0.9262 Acc: 62.5000% F1: 0.358 Time: 0.95s (8.80s)
Fold 7 train - epoch: 1/5 iter: 13/15 loss: 0.8955 Acc: 59.3750% F1: 0.336 Time: 0.95s (0.29s)
Fold 7 train - epoch: 1/5 iter: 14/15 loss: 0.4273 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 55.5556% F1: 0.2576 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7309 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3056 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 46.0000% F1: 0.2130 *
*********************************************************
Performing epoch 2 of 5
Fold 7 train - epoch: 2/5 iter: 0/15 loss: 0.8457 Acc: 62.5000% F1: 0.385 Time: 0.95s (0.00s)
Fold 7 train - epoch: 2/5 iter: 1/15 loss: 0.8834 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.03s)
Fold 7 train - epoch: 2/5 iter: 2/15 loss: 0.8900 Acc: 56.2500% F1: 0.320 Time: 0.96s (9.02s)
Fold 7 train - epoch: 2/5 iter: 3/15 loss: 0.7881 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.03s)
Fold 7 train - epoch: 2/5 iter: 4/15 loss: 0.9662 Acc: 53.1250% F1: 0.278 Time: 0.94s (9.15s)
Fold 7 train - epoch: 2/5 iter: 5/15 loss: 0.8742 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 7 train - epoch: 2/5 iter: 6/15 loss: 0.7400 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.37s)
Fold 7 train - epoch: 2/5 iter: 7/15 loss: 0.8332 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 8/15 loss: 1.0909 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.50s)
Fold 7 train - epoch: 2/5 iter: 9/15 loss: 1.0356 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.47s)
Fold 7 train - epoch: 2/5 iter: 10/15 loss: 0.9640 Acc: 43.7500% F1: 0.203 Time: 0.96s (9.48s)
Fold 7 train - epoch: 2/5 iter: 11/15 loss: 0.7858 Acc: 68.7500% F1: 0.343 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 12/15 loss: 0.9113 Acc: 59.3750% F1: 0.306 Time: 0.94s (9.78s)
Fold 7 train - epoch: 2/5 iter: 13/15 loss: 0.8776 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 14/15 loss: 0.2734 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 57.5556% F1: 0.2919 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6939 Acc: 75.0000% F1: 0.429 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3946 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 48.0000% F1: 0.2192 *
*********************************************************
Performing epoch 3 of 5
Fold 7 train - epoch: 3/5 iter: 0/15 loss: 0.8018 Acc: 59.3750% F1: 0.306 Time: 0.96s (0.00s)
Fold 7 train - epoch: 3/5 iter: 1/15 loss: 0.8067 Acc: 62.5000% F1: 0.389 Time: 0.94s (0.03s)
Fold 7 train - epoch: 3/5 iter: 2/15 loss: 0.8387 Acc: 56.2500% F1: 0.321 Time: 0.96s (9.33s)
Fold 7 train - epoch: 3/5 iter: 3/15 loss: 0.7898 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.03s)
Fold 7 train - epoch: 3/5 iter: 4/15 loss: 0.8521 Acc: 65.6250% F1: 0.444 Time: 0.94s (9.31s)
Fold 7 train - epoch: 3/5 iter: 5/15 loss: 0.8174 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.03s)
Fold 7 train - epoch: 3/5 iter: 6/15 loss: 0.7156 Acc: 75.0000% F1: 0.415 Time: 0.94s (9.37s)
Fold 7 train - epoch: 3/5 iter: 7/15 loss: 0.7979 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 8/15 loss: 1.0667 Acc: 59.3750% F1: 0.368 Time: 0.94s (9.61s)
Fold 7 train - epoch: 3/5 iter: 9/15 loss: 0.9325 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 10/15 loss: 0.8809 Acc: 53.1250% F1: 0.364 Time: 0.96s (10.22s)
Fold 7 train - epoch: 3/5 iter: 11/15 loss: 0.7804 Acc: 59.3750% F1: 0.360 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 12/15 loss: 0.8477 Acc: 71.8750% F1: 0.584 Time: 0.94s (9.99s)
Fold 7 train - epoch: 3/5 iter: 13/15 loss: 0.9163 Acc: 59.3750% F1: 0.517 Time: 0.94s (0.03s)
Fold 7 train - epoch: 3/5 iter: 14/15 loss: 0.1230 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 61.7778% F1: 0.3989 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7715 Acc: 56.2500% F1: 0.417 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4254 Acc: 11.1111% F1: 0.128 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 40.0000% F1: 0.3121 *
*********************************************************
Performing epoch 4 of 5
Fold 7 train - epoch: 4/5 iter: 0/15 loss: 0.6497 Acc: 71.8750% F1: 0.661 Time: 0.96s (0.00s)
Fold 7 train - epoch: 4/5 iter: 1/15 loss: 0.6596 Acc: 78.1250% F1: 0.662 Time: 0.94s (0.03s)
Fold 7 train - epoch: 4/5 iter: 2/15 loss: 0.7293 Acc: 68.7500% F1: 0.639 Time: 0.96s (8.65s)
Fold 7 train - epoch: 4/5 iter: 3/15 loss: 0.6808 Acc: 75.0000% F1: 0.648 Time: 0.94s (0.03s)
Fold 7 train - epoch: 4/5 iter: 4/15 loss: 0.7060 Acc: 62.5000% F1: 0.437 Time: 0.94s (8.72s)
Fold 7 train - epoch: 4/5 iter: 5/15 loss: 0.7165 Acc: 65.6250% F1: 0.409 Time: 0.94s (0.03s)
Fold 7 train - epoch: 4/5 iter: 6/15 loss: 0.6010 Acc: 81.2500% F1: 0.516 Time: 0.95s (9.48s)
Fold 7 train - epoch: 4/5 iter: 7/15 loss: 0.6528 Acc: 62.5000% F1: 0.415 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 8/15 loss: 0.9247 Acc: 59.3750% F1: 0.411 Time: 0.94s (9.92s)
Fold 7 train - epoch: 4/5 iter: 9/15 loss: 0.7518 Acc: 59.3750% F1: 0.398 Time: 0.95s (0.31s)
Fold 7 train - epoch: 4/5 iter: 10/15 loss: 0.7581 Acc: 62.5000% F1: 0.564 Time: 0.95s (9.47s)
Fold 7 train - epoch: 4/5 iter: 11/15 loss: 0.7341 Acc: 68.7500% F1: 0.655 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 12/15 loss: 0.7253 Acc: 65.6250% F1: 0.602 Time: 0.94s (9.41s)
Fold 7 train - epoch: 4/5 iter: 13/15 loss: 0.7602 Acc: 68.7500% F1: 0.564 Time: 0.94s (0.03s)
Fold 7 train - epoch: 4/5 iter: 14/15 loss: 0.0240 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 68.0000% F1: 0.5708 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/5 iter: 0/2 loss: 0.6227 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 4/5 iter: 1/2 loss: 1.9320 Acc: 5.5556% F1: 0.074 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3129 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/5 iter: 0/15 loss: 1.1149 Acc: 37.5000% F1: 0.264 Time: 0.97s (0.00s)
Fold 8 train - epoch: 0/5 iter: 1/15 loss: 0.9564 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.04s)
Fold 8 train - epoch: 0/5 iter: 2/15 loss: 1.0169 Acc: 53.1250% F1: 0.275 Time: 0.97s (9.45s)
Fold 8 train - epoch: 0/5 iter: 3/15 loss: 1.0121 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.04s)
Fold 8 train - epoch: 0/5 iter: 4/15 loss: 1.0628 Acc: 50.0000% F1: 0.222 Time: 0.95s (9.18s)
Fold 8 train - epoch: 0/5 iter: 5/15 loss: 0.9199 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.04s)
Fold 8 train - epoch: 0/5 iter: 6/15 loss: 0.9111 Acc: 56.2500% F1: 0.240 Time: 0.94s (9.02s)
Fold 8 train - epoch: 0/5 iter: 7/15 loss: 0.8764 Acc: 53.1250% F1: 0.317 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 8/15 loss: 1.2335 Acc: 50.0000% F1: 0.278 Time: 0.94s (8.83s)
Fold 8 train - epoch: 0/5 iter: 9/15 loss: 1.0227 Acc: 40.6250% F1: 0.228 Time: 0.94s (0.50s)
Fold 8 train - epoch: 0/5 iter: 10/15 loss: 0.9572 Acc: 46.8750% F1: 0.314 Time: 0.95s (9.57s)
Fold 8 train - epoch: 0/5 iter: 11/15 loss: 0.9421 Acc: 56.2500% F1: 0.289 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 12/15 loss: 0.9670 Acc: 59.3750% F1: 0.370 Time: 0.94s (9.45s)
Fold 8 train - epoch: 0/5 iter: 13/15 loss: 0.9231 Acc: 56.2500% F1: 0.287 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 14/15 loss: 0.7206 Acc: 50.0000% F1: 0.333 Time: 0.10s (0.04s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 52.0000% F1: 0.2951 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5819 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6694 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 8 train - epoch: 1/5 iter: 0/15 loss: 0.8454 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 8 train - epoch: 1/5 iter: 1/15 loss: 0.8978 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 8 train - epoch: 1/5 iter: 2/15 loss: 0.9813 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.79s)
Fold 8 train - epoch: 1/5 iter: 3/15 loss: 0.8471 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.08s)
Fold 8 train - epoch: 1/5 iter: 4/15 loss: 1.0489 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.66s)
Fold 8 train - epoch: 1/5 iter: 5/15 loss: 0.9082 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.74s)
Fold 8 train - epoch: 1/5 iter: 6/15 loss: 0.8022 Acc: 68.7500% F1: 0.272 Time: 0.94s (7.86s)
Fold 8 train - epoch: 1/5 iter: 7/15 loss: 0.9692 Acc: 46.8750% F1: 0.213 Time: 0.94s (1.28s)
Fold 8 train - epoch: 1/5 iter: 8/15 loss: 1.1338 Acc: 53.1250% F1: 0.231 Time: 0.94s (7.43s)
Fold 8 train - epoch: 1/5 iter: 9/15 loss: 1.0154 Acc: 46.8750% F1: 0.213 Time: 0.94s (2.56s)
Fold 8 train - epoch: 1/5 iter: 10/15 loss: 0.9733 Acc: 46.8750% F1: 0.213 Time: 0.96s (7.39s)
Fold 8 train - epoch: 1/5 iter: 11/15 loss: 0.8883 Acc: 62.5000% F1: 0.256 Time: 0.94s (2.06s)
Fold 8 train - epoch: 1/5 iter: 12/15 loss: 0.9622 Acc: 59.3750% F1: 0.344 Time: 0.94s (7.16s)
Fold 8 train - epoch: 1/5 iter: 13/15 loss: 0.9195 Acc: 56.2500% F1: 0.287 Time: 0.94s (0.96s)
Fold 8 train - epoch: 1/5 iter: 14/15 loss: 0.5964 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 55.5556% F1: 0.2538 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7211 Acc: 84.3750% F1: 0.599 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3909 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 56.0000% F1: 0.3009 *
*********************************************************
Performing epoch 2 of 5
Fold 8 train - epoch: 2/5 iter: 0/15 loss: 0.8508 Acc: 62.5000% F1: 0.358 Time: 0.96s (0.00s)
Fold 8 train - epoch: 2/5 iter: 1/15 loss: 0.8862 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.04s)
Fold 8 train - epoch: 2/5 iter: 2/15 loss: 0.9119 Acc: 56.2500% F1: 0.321 Time: 0.96s (8.89s)
Fold 8 train - epoch: 2/5 iter: 3/15 loss: 0.8309 Acc: 62.5000% F1: 0.309 Time: 0.95s (0.26s)
Fold 8 train - epoch: 2/5 iter: 4/15 loss: 0.9352 Acc: 53.1250% F1: 0.278 Time: 0.94s (8.68s)
Fold 8 train - epoch: 2/5 iter: 5/15 loss: 0.9004 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.94s)
Fold 8 train - epoch: 2/5 iter: 6/15 loss: 0.7639 Acc: 68.7500% F1: 0.272 Time: 0.94s (7.78s)
Fold 8 train - epoch: 2/5 iter: 7/15 loss: 0.8513 Acc: 46.8750% F1: 0.213 Time: 0.95s (1.60s)
Fold 8 train - epoch: 2/5 iter: 8/15 loss: 1.1236 Acc: 53.1250% F1: 0.231 Time: 0.94s (7.42s)
Fold 8 train - epoch: 2/5 iter: 9/15 loss: 1.0378 Acc: 46.8750% F1: 0.213 Time: 0.95s (2.71s)
Fold 8 train - epoch: 2/5 iter: 10/15 loss: 0.9464 Acc: 46.8750% F1: 0.213 Time: 0.96s (7.50s)
Fold 8 train - epoch: 2/5 iter: 11/15 loss: 0.8060 Acc: 68.7500% F1: 0.341 Time: 0.95s (2.07s)
Fold 8 train - epoch: 2/5 iter: 12/15 loss: 0.9081 Acc: 56.2500% F1: 0.240 Time: 0.94s (7.69s)
Fold 8 train - epoch: 2/5 iter: 13/15 loss: 0.8953 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.81s)
Fold 8 train - epoch: 2/5 iter: 14/15 loss: 0.3957 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 56.8889% F1: 0.2730 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6511 Acc: 84.3750% F1: 0.599 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 2/5 iter: 1/2 loss: 1.5002 Acc: 5.5556% F1: 0.051 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 56.0000% F1: 0.3009 *
*********************************************************
Performing epoch 3 of 5
Fold 8 train - epoch: 3/5 iter: 0/15 loss: 0.8076 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.00s)
Fold 8 train - epoch: 3/5 iter: 1/15 loss: 0.8387 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 8 train - epoch: 3/5 iter: 2/15 loss: 0.9059 Acc: 50.0000% F1: 0.222 Time: 0.95s (8.95s)
Fold 8 train - epoch: 3/5 iter: 3/15 loss: 0.8376 Acc: 62.5000% F1: 0.310 Time: 0.94s (0.69s)
Fold 8 train - epoch: 3/5 iter: 4/15 loss: 0.8870 Acc: 53.1250% F1: 0.278 Time: 0.94s (8.66s)
Fold 8 train - epoch: 3/5 iter: 5/15 loss: 0.8634 Acc: 59.3750% F1: 0.248 Time: 0.94s (1.37s)
Fold 8 train - epoch: 3/5 iter: 6/15 loss: 0.7336 Acc: 71.8750% F1: 0.351 Time: 0.94s (7.85s)
Fold 8 train - epoch: 3/5 iter: 7/15 loss: 0.8180 Acc: 50.0000% F1: 0.257 Time: 0.95s (1.82s)
Fold 8 train - epoch: 3/5 iter: 8/15 loss: 1.0549 Acc: 56.2500% F1: 0.310 Time: 0.94s (7.42s)
Fold 8 train - epoch: 3/5 iter: 9/15 loss: 0.9661 Acc: 50.0000% F1: 0.265 Time: 0.94s (3.51s)
Fold 8 train - epoch: 3/5 iter: 10/15 loss: 0.9134 Acc: 53.1250% F1: 0.335 Time: 0.97s (6.48s)
Fold 8 train - epoch: 3/5 iter: 11/15 loss: 0.8252 Acc: 56.2500% F1: 0.289 Time: 0.94s (3.86s)
Fold 8 train - epoch: 3/5 iter: 12/15 loss: 0.8655 Acc: 59.3750% F1: 0.342 Time: 0.94s (4.80s)
Fold 8 train - epoch: 3/5 iter: 13/15 loss: 0.8374 Acc: 50.0000% F1: 0.291 Time: 0.94s (4.30s)
Fold 8 train - epoch: 3/5 iter: 14/15 loss: 0.1964 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 56.6667% F1: 0.2968 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7130 Acc: 75.0000% F1: 0.403 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4952 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.3704 *
*********************************************************
Performing epoch 4 of 5
Fold 8 train - epoch: 4/5 iter: 0/15 loss: 0.7155 Acc: 71.8750% F1: 0.709 Time: 0.95s (0.00s)
Fold 8 train - epoch: 4/5 iter: 1/15 loss: 0.7810 Acc: 75.0000% F1: 0.628 Time: 0.94s (0.03s)
Fold 8 train - epoch: 4/5 iter: 2/15 loss: 0.7912 Acc: 53.1250% F1: 0.311 Time: 0.96s (8.94s)
Fold 8 train - epoch: 4/5 iter: 3/15 loss: 0.7560 Acc: 68.7500% F1: 0.403 Time: 0.95s (0.04s)
Fold 8 train - epoch: 4/5 iter: 4/15 loss: 0.7684 Acc: 65.6250% F1: 0.544 Time: 0.95s (8.66s)
Fold 8 train - epoch: 4/5 iter: 5/15 loss: 0.7409 Acc: 62.5000% F1: 0.427 Time: 0.95s (0.04s)
Fold 8 train - epoch: 4/5 iter: 6/15 loss: 0.6632 Acc: 75.0000% F1: 0.415 Time: 0.95s (8.70s)
Fold 8 train - epoch: 4/5 iter: 7/15 loss: 0.7853 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.04s)
Fold 8 train - epoch: 4/5 iter: 8/15 loss: 0.9597 Acc: 59.3750% F1: 0.368 Time: 0.94s (8.80s)
Fold 8 train - epoch: 4/5 iter: 9/15 loss: 0.8782 Acc: 56.2500% F1: 0.373 Time: 0.94s (0.03s)
Fold 8 train - epoch: 4/5 iter: 10/15 loss: 0.7737 Acc: 65.6250% F1: 0.569 Time: 0.96s (10.41s)
Fold 8 train - epoch: 4/5 iter: 11/15 loss: 0.7301 Acc: 75.0000% F1: 0.740 Time: 0.95s (0.04s)
Fold 8 train - epoch: 4/5 iter: 12/15 loss: 0.7680 Acc: 75.0000% F1: 0.704 Time: 0.94s (9.03s)
Fold 8 train - epoch: 4/5 iter: 13/15 loss: 0.8104 Acc: 59.3750% F1: 0.544 Time: 0.94s (0.03s)
Fold 8 train - epoch: 4/5 iter: 14/15 loss: 0.0620 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 65.7778% F1: 0.5373 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/5 iter: 0/2 loss: 0.6288 Acc: 78.1250% F1: 0.423 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6321 Acc: 33.3333% F1: 0.211 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 62.0000% F1: 0.4235 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/5 iter: 0/15 loss: 1.0681 Acc: 43.7500% F1: 0.302 Time: 0.96s (0.00s)
Fold 9 train - epoch: 0/5 iter: 1/15 loss: 0.9854 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.04s)
Fold 9 train - epoch: 0/5 iter: 2/15 loss: 1.1297 Acc: 50.0000% F1: 0.265 Time: 0.96s (9.10s)
Fold 9 train - epoch: 0/5 iter: 3/15 loss: 0.9163 Acc: 62.5000% F1: 0.256 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/5 iter: 4/15 loss: 1.0762 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.91s)
Fold 9 train - epoch: 0/5 iter: 5/15 loss: 0.9000 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.02s)
Fold 9 train - epoch: 0/5 iter: 6/15 loss: 0.8013 Acc: 65.6250% F1: 0.264 Time: 0.94s (8.84s)
Fold 9 train - epoch: 0/5 iter: 7/15 loss: 0.9370 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.28s)
Fold 9 train - epoch: 0/5 iter: 8/15 loss: 1.1830 Acc: 50.0000% F1: 0.227 Time: 0.94s (9.04s)
Fold 9 train - epoch: 0/5 iter: 9/15 loss: 1.0779 Acc: 50.0000% F1: 0.338 Time: 0.94s (1.73s)
Fold 9 train - epoch: 0/5 iter: 10/15 loss: 1.1147 Acc: 28.1250% F1: 0.172 Time: 0.96s (8.08s)
Fold 9 train - epoch: 0/5 iter: 11/15 loss: 0.8786 Acc: 50.0000% F1: 0.289 Time: 0.95s (1.58s)
Fold 9 train - epoch: 0/5 iter: 12/15 loss: 0.9628 Acc: 56.2500% F1: 0.366 Time: 0.96s (7.76s)
Fold 9 train - epoch: 0/5 iter: 13/15 loss: 0.9260 Acc: 50.0000% F1: 0.227 Time: 0.94s (1.38s)
Fold 9 train - epoch: 0/5 iter: 14/15 loss: 0.6289 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 50.6667% F1: 0.2885 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6960 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 0/5 iter: 1/2 loss: 1.2093 Acc: 27.7778% F1: 0.145 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 58.0000% F1: 0.2447 *
*********************************************************
Performing epoch 1 of 5
Fold 9 train - epoch: 1/5 iter: 0/15 loss: 0.8992 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.00s)
Fold 9 train - epoch: 1/5 iter: 1/15 loss: 0.9481 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 2/15 loss: 1.0078 Acc: 46.8750% F1: 0.213 Time: 0.95s (9.51s)
Fold 9 train - epoch: 1/5 iter: 3/15 loss: 0.8361 Acc: 62.5000% F1: 0.256 Time: 0.94s (0.03s)
Fold 9 train - epoch: 1/5 iter: 4/15 loss: 1.0660 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.38s)
Fold 9 train - epoch: 1/5 iter: 5/15 loss: 0.8841 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 9 train - epoch: 1/5 iter: 6/15 loss: 0.7257 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.53s)
Fold 9 train - epoch: 1/5 iter: 7/15 loss: 1.0107 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 8/15 loss: 1.0793 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.62s)
Fold 9 train - epoch: 1/5 iter: 9/15 loss: 1.0787 Acc: 46.8750% F1: 0.213 Time: 0.94s (1.23s)
Fold 9 train - epoch: 1/5 iter: 10/15 loss: 0.9754 Acc: 46.8750% F1: 0.213 Time: 0.96s (8.44s)
Fold 9 train - epoch: 1/5 iter: 11/15 loss: 0.8664 Acc: 56.2500% F1: 0.284 Time: 0.94s (0.78s)
Fold 9 train - epoch: 1/5 iter: 12/15 loss: 0.9233 Acc: 65.6250% F1: 0.420 Time: 0.94s (8.35s)
Fold 9 train - epoch: 1/5 iter: 13/15 loss: 0.9064 Acc: 43.7500% F1: 0.261 Time: 0.94s (0.27s)
Fold 9 train - epoch: 1/5 iter: 14/15 loss: 0.5182 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 53.7778% F1: 0.2590 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7824 Acc: 62.5000% F1: 0.385 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 1/5 iter: 1/2 loss: 1.0231 Acc: 38.8889% F1: 0.262 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.2828 *
*********************************************************
Performing epoch 2 of 5
Fold 9 train - epoch: 2/5 iter: 0/15 loss: 0.8591 Acc: 65.6250% F1: 0.435 Time: 0.95s (0.00s)
Fold 9 train - epoch: 2/5 iter: 1/15 loss: 0.9067 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/5 iter: 2/15 loss: 0.9350 Acc: 50.0000% F1: 0.295 Time: 0.96s (9.07s)
Fold 9 train - epoch: 2/5 iter: 3/15 loss: 0.7908 Acc: 62.5000% F1: 0.256 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/5 iter: 4/15 loss: 0.9241 Acc: 53.1250% F1: 0.283 Time: 0.94s (8.79s)
Fold 9 train - epoch: 2/5 iter: 5/15 loss: 0.8537 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/5 iter: 6/15 loss: 0.7026 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.06s)
Fold 9 train - epoch: 2/5 iter: 7/15 loss: 0.9377 Acc: 46.8750% F1: 0.247 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/5 iter: 8/15 loss: 1.0607 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.62s)
Fold 9 train - epoch: 2/5 iter: 9/15 loss: 1.0259 Acc: 46.8750% F1: 0.217 Time: 0.94s (1.29s)
Fold 9 train - epoch: 2/5 iter: 10/15 loss: 0.9840 Acc: 46.8750% F1: 0.255 Time: 0.96s (8.34s)
Fold 9 train - epoch: 2/5 iter: 11/15 loss: 0.7654 Acc: 71.8750% F1: 0.351 Time: 0.95s (0.77s)
Fold 9 train - epoch: 2/5 iter: 12/15 loss: 0.8343 Acc: 59.3750% F1: 0.306 Time: 0.94s (8.41s)
Fold 9 train - epoch: 2/5 iter: 13/15 loss: 0.8734 Acc: 56.2500% F1: 0.287 Time: 0.94s (0.21s)
Fold 9 train - epoch: 2/5 iter: 14/15 loss: 0.3519 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 57.3333% F1: 0.2981 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7765 Acc: 65.6250% F1: 0.396 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 2/5 iter: 1/2 loss: 1.1309 Acc: 27.7778% F1: 0.152 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2311 *
*********************************************************
Performing epoch 3 of 5
Fold 9 train - epoch: 3/5 iter: 0/15 loss: 0.7690 Acc: 62.5000% F1: 0.469 Time: 0.96s (0.00s)
Fold 9 train - epoch: 3/5 iter: 1/15 loss: 0.8475 Acc: 65.6250% F1: 0.528 Time: 0.94s (0.03s)
Fold 9 train - epoch: 3/5 iter: 2/15 loss: 0.8769 Acc: 53.1250% F1: 0.335 Time: 0.95s (8.84s)
Fold 9 train - epoch: 3/5 iter: 3/15 loss: 0.7683 Acc: 68.7500% F1: 0.375 Time: 0.94s (0.03s)
Fold 9 train - epoch: 3/5 iter: 4/15 loss: 0.9147 Acc: 50.0000% F1: 0.329 Time: 0.94s (8.59s)
Fold 9 train - epoch: 3/5 iter: 5/15 loss: 0.7879 Acc: 65.6250% F1: 0.409 Time: 0.94s (0.48s)
Fold 9 train - epoch: 3/5 iter: 6/15 loss: 0.6294 Acc: 71.8750% F1: 0.386 Time: 0.94s (7.72s)
Fold 9 train - epoch: 3/5 iter: 7/15 loss: 0.8076 Acc: 59.3750% F1: 0.386 Time: 0.94s (1.18s)
Fold 9 train - epoch: 3/5 iter: 8/15 loss: 0.9878 Acc: 56.2500% F1: 0.307 Time: 0.94s (7.13s)
Fold 9 train - epoch: 3/5 iter: 9/15 loss: 0.9834 Acc: 50.0000% F1: 0.270 Time: 0.94s (2.75s)
Fold 9 train - epoch: 3/5 iter: 10/15 loss: 0.8464 Acc: 50.0000% F1: 0.324 Time: 0.96s (6.41s)
Fold 9 train - epoch: 3/5 iter: 11/15 loss: 0.7318 Acc: 71.8750% F1: 0.676 Time: 0.95s (2.58s)
Fold 9 train - epoch: 3/5 iter: 12/15 loss: 0.8154 Acc: 59.3750% F1: 0.367 Time: 0.94s (6.06s)
Fold 9 train - epoch: 3/5 iter: 13/15 loss: 0.8781 Acc: 53.1250% F1: 0.417 Time: 0.94s (2.57s)
Fold 9 train - epoch: 3/5 iter: 14/15 loss: 0.0882 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 60.0000% F1: 0.4113 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8653 Acc: 50.0000% F1: 0.306 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 3/5 iter: 1/2 loss: 1.2025 Acc: 27.7778% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 42.0000% F1: 0.2730 *
*********************************************************
Performing epoch 4 of 5
Fold 9 train - epoch: 4/5 iter: 0/15 loss: 0.5708 Acc: 81.2500% F1: 0.764 Time: 0.95s (0.00s)
Fold 9 train - epoch: 4/5 iter: 1/15 loss: 0.6995 Acc: 71.8750% F1: 0.628 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 2/15 loss: 0.6833 Acc: 62.5000% F1: 0.626 Time: 0.96s (8.76s)
Fold 9 train - epoch: 4/5 iter: 3/15 loss: 0.6785 Acc: 71.8750% F1: 0.635 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 4/15 loss: 0.7197 Acc: 71.8750% F1: 0.669 Time: 0.94s (8.51s)
Fold 9 train - epoch: 4/5 iter: 5/15 loss: 0.7319 Acc: 65.6250% F1: 0.528 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 6/15 loss: 0.5605 Acc: 78.1250% F1: 0.454 Time: 0.94s (8.45s)
Fold 9 train - epoch: 4/5 iter: 7/15 loss: 0.8290 Acc: 56.2500% F1: 0.566 Time: 0.95s (0.05s)
Fold 9 train - epoch: 4/5 iter: 8/15 loss: 0.8068 Acc: 62.5000% F1: 0.497 Time: 0.94s (8.41s)
Fold 9 train - epoch: 4/5 iter: 9/15 loss: 0.8199 Acc: 53.1250% F1: 0.407 Time: 0.95s (1.29s)
Fold 9 train - epoch: 4/5 iter: 10/15 loss: 0.7450 Acc: 68.7500% F1: 0.679 Time: 0.96s (8.23s)
Fold 9 train - epoch: 4/5 iter: 11/15 loss: 0.6376 Acc: 75.0000% F1: 0.595 Time: 0.95s (1.03s)
Fold 9 train - epoch: 4/5 iter: 12/15 loss: 0.6924 Acc: 75.0000% F1: 0.621 Time: 0.94s (8.47s)
Fold 9 train - epoch: 4/5 iter: 13/15 loss: 0.7053 Acc: 68.7500% F1: 0.602 Time: 0.95s (0.62s)
Fold 9 train - epoch: 4/5 iter: 14/15 loss: 0.0453 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 68.8889% F1: 0.6132 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/5 iter: 0/2 loss: 0.9999 Acc: 46.8750% F1: 0.325 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 4/5 iter: 1/2 loss: 1.4894 Acc: 27.7778% F1: 0.188 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 40.0000% F1: 0.2842 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 55.2000% F1: 0.2371
	Epoch 1 Accuracy: 51.0000% F1: 0.2833
	Epoch 2 Accuracy: 49.2000% F1: 0.2655
	Epoch 3 Accuracy: 44.8000% F1: 0.3208
	Epoch 4 Accuracy: 45.8000% F1: 0.3370
all done :)
************************************
** MODEL TIME ID: 20220225-102300 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/5 iter: 0/15 loss: 1.2828 Acc: 34.3750% F1: 0.231 Time: 0.96s (0.00s)
Fold 0 train - epoch: 0/5 iter: 1/15 loss: 0.9700 Acc: 53.1250% F1: 0.342 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 2/15 loss: 1.0987 Acc: 46.8750% F1: 0.213 Time: 0.96s (8.77s)
Fold 0 train - epoch: 0/5 iter: 3/15 loss: 0.9043 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 4/15 loss: 0.9883 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.74s)
Fold 0 train - epoch: 0/5 iter: 5/15 loss: 0.9181 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 0 train - epoch: 0/5 iter: 6/15 loss: 0.8574 Acc: 65.6250% F1: 0.361 Time: 0.94s (9.37s)
Fold 0 train - epoch: 0/5 iter: 7/15 loss: 0.8617 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.03s)
Fold 0 train - epoch: 0/5 iter: 8/15 loss: 1.0145 Acc: 53.1250% F1: 0.321 Time: 0.94s (9.90s)
Fold 0 train - epoch: 0/5 iter: 9/15 loss: 0.9387 Acc: 65.6250% F1: 0.466 Time: 0.95s (0.03s)
Fold 0 train - epoch: 0/5 iter: 10/15 loss: 0.9732 Acc: 56.2500% F1: 0.400 Time: 0.95s (8.91s)
Fold 0 train - epoch: 0/5 iter: 11/15 loss: 0.9447 Acc: 43.7500% F1: 0.263 Time: 0.95s (0.06s)
Fold 0 train - epoch: 0/5 iter: 12/15 loss: 1.0948 Acc: 53.1250% F1: 0.317 Time: 0.95s (8.85s)
Fold 0 train - epoch: 0/5 iter: 13/15 loss: 0.9535 Acc: 46.8750% F1: 0.251 Time: 0.94s (0.84s)
Fold 0 train - epoch: 0/5 iter: 14/15 loss: 0.7126 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 52.8889% F1: 0.3292 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6292 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5571 Acc: 5.5556% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.3321 *
*********************************************************
Performing epoch 1 of 5
Fold 0 train - epoch: 1/5 iter: 0/15 loss: 0.8938 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.00s)
Fold 0 train - epoch: 1/5 iter: 1/15 loss: 0.8790 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/5 iter: 2/15 loss: 0.9292 Acc: 50.0000% F1: 0.222 Time: 0.95s (9.46s)
Fold 0 train - epoch: 1/5 iter: 3/15 loss: 0.8142 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/5 iter: 4/15 loss: 1.0229 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.46s)
Fold 0 train - epoch: 1/5 iter: 5/15 loss: 0.9319 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 6/15 loss: 0.7923 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.79s)
Fold 0 train - epoch: 1/5 iter: 7/15 loss: 0.8983 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 8/15 loss: 0.9692 Acc: 56.2500% F1: 0.240 Time: 0.95s (9.08s)
Fold 0 train - epoch: 1/5 iter: 9/15 loss: 0.9392 Acc: 50.0000% F1: 0.267 Time: 0.95s (0.04s)
Fold 0 train - epoch: 1/5 iter: 10/15 loss: 0.9503 Acc: 50.0000% F1: 0.265 Time: 0.96s (8.82s)
Fold 0 train - epoch: 1/5 iter: 11/15 loss: 0.8511 Acc: 65.6250% F1: 0.328 Time: 0.95s (0.04s)
Fold 0 train - epoch: 1/5 iter: 12/15 loss: 1.0074 Acc: 65.6250% F1: 0.435 Time: 0.95s (8.78s)
Fold 0 train - epoch: 1/5 iter: 13/15 loss: 0.8995 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.75s)
Fold 0 train - epoch: 1/5 iter: 14/15 loss: 0.5375 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 56.2222% F1: 0.2673 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7970 Acc: 68.7500% F1: 0.583 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3225 Acc: 5.5556% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 46.0000% F1: 0.3615 *
*********************************************************
Performing epoch 2 of 5
Fold 0 train - epoch: 2/5 iter: 0/15 loss: 0.8777 Acc: 56.2500% F1: 0.294 Time: 0.96s (0.00s)
Fold 0 train - epoch: 2/5 iter: 1/15 loss: 0.8290 Acc: 71.8750% F1: 0.499 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 2/15 loss: 0.8283 Acc: 53.1250% F1: 0.276 Time: 0.95s (8.56s)
Fold 0 train - epoch: 2/5 iter: 3/15 loss: 0.7530 Acc: 65.6250% F1: 0.361 Time: 0.94s (0.02s)
Fold 0 train - epoch: 2/5 iter: 4/15 loss: 0.9009 Acc: 50.0000% F1: 0.227 Time: 0.94s (8.25s)
Fold 0 train - epoch: 2/5 iter: 5/15 loss: 0.8910 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/5 iter: 6/15 loss: 0.6913 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.57s)
Fold 0 train - epoch: 2/5 iter: 7/15 loss: 0.8292 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.69s)
Fold 0 train - epoch: 2/5 iter: 8/15 loss: 0.9746 Acc: 56.2500% F1: 0.240 Time: 0.95s (8.20s)
Fold 0 train - epoch: 2/5 iter: 9/15 loss: 0.9029 Acc: 59.3750% F1: 0.386 Time: 0.95s (1.39s)
Fold 0 train - epoch: 2/5 iter: 10/15 loss: 0.9542 Acc: 46.8750% F1: 0.252 Time: 0.96s (7.57s)
Fold 0 train - epoch: 2/5 iter: 11/15 loss: 0.7931 Acc: 62.5000% F1: 0.314 Time: 0.94s (1.59s)
Fold 0 train - epoch: 2/5 iter: 12/15 loss: 0.8861 Acc: 59.3750% F1: 0.376 Time: 0.94s (7.25s)
Fold 0 train - epoch: 2/5 iter: 13/15 loss: 0.8317 Acc: 56.2500% F1: 0.291 Time: 0.94s (2.39s)
Fold 0 train - epoch: 2/5 iter: 14/15 loss: 0.2701 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 58.4444% F1: 0.3202 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/5 iter: 0/2 loss: 0.9134 Acc: 50.0000% F1: 0.327 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3171 Acc: 22.2222% F1: 0.201 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 40.0000% F1: 0.3403 *
*********************************************************
Performing epoch 3 of 5
Fold 0 train - epoch: 3/5 iter: 0/15 loss: 0.8061 Acc: 65.6250% F1: 0.528 Time: 0.95s (0.00s)
Fold 0 train - epoch: 3/5 iter: 1/15 loss: 0.7669 Acc: 81.2500% F1: 0.571 Time: 0.94s (0.03s)
Fold 0 train - epoch: 3/5 iter: 2/15 loss: 0.7663 Acc: 65.6250% F1: 0.640 Time: 0.96s (8.71s)
Fold 0 train - epoch: 3/5 iter: 3/15 loss: 0.6269 Acc: 75.0000% F1: 0.691 Time: 0.94s (0.03s)
Fold 0 train - epoch: 3/5 iter: 4/15 loss: 0.6908 Acc: 65.6250% F1: 0.601 Time: 0.94s (8.20s)
Fold 0 train - epoch: 3/5 iter: 5/15 loss: 0.7947 Acc: 71.8750% F1: 0.481 Time: 0.95s (0.03s)
Fold 0 train - epoch: 3/5 iter: 6/15 loss: 0.5958 Acc: 81.2500% F1: 0.697 Time: 0.94s (8.70s)
Fold 0 train - epoch: 3/5 iter: 7/15 loss: 0.7924 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 8/15 loss: 0.9523 Acc: 56.2500% F1: 0.337 Time: 0.94s (8.75s)
Fold 0 train - epoch: 3/5 iter: 9/15 loss: 0.7694 Acc: 71.8750% F1: 0.517 Time: 0.94s (0.14s)
Fold 0 train - epoch: 3/5 iter: 10/15 loss: 0.8827 Acc: 65.6250% F1: 0.467 Time: 0.96s (8.47s)
Fold 0 train - epoch: 3/5 iter: 11/15 loss: 0.6938 Acc: 65.6250% F1: 0.446 Time: 0.94s (0.71s)
Fold 0 train - epoch: 3/5 iter: 12/15 loss: 0.8329 Acc: 65.6250% F1: 0.606 Time: 0.94s (7.69s)
Fold 0 train - epoch: 3/5 iter: 13/15 loss: 0.7359 Acc: 65.6250% F1: 0.576 Time: 0.94s (1.82s)
Fold 0 train - epoch: 3/5 iter: 14/15 loss: 0.0742 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 68.2222% F1: 0.5566 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9452 Acc: 50.0000% F1: 0.324 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6394 Acc: 16.6667% F1: 0.179 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 38.0000% F1: 0.3136 *
*********************************************************
Performing epoch 4 of 5
Fold 0 train - epoch: 4/5 iter: 0/15 loss: 0.6783 Acc: 68.7500% F1: 0.591 Time: 0.96s (0.00s)
Fold 0 train - epoch: 4/5 iter: 1/15 loss: 0.6010 Acc: 87.5000% F1: 0.817 Time: 0.95s (0.03s)
Fold 0 train - epoch: 4/5 iter: 2/15 loss: 0.7280 Acc: 65.6250% F1: 0.578 Time: 0.96s (8.88s)
Fold 0 train - epoch: 4/5 iter: 3/15 loss: 0.4793 Acc: 78.1250% F1: 0.718 Time: 0.94s (0.02s)
Fold 0 train - epoch: 4/5 iter: 4/15 loss: 0.5726 Acc: 84.3750% F1: 0.841 Time: 0.94s (8.38s)
Fold 0 train - epoch: 4/5 iter: 5/15 loss: 0.5837 Acc: 75.0000% F1: 0.632 Time: 0.95s (0.03s)
Fold 0 train - epoch: 4/5 iter: 6/15 loss: 0.5272 Acc: 78.1250% F1: 0.696 Time: 0.94s (8.77s)
Fold 0 train - epoch: 4/5 iter: 7/15 loss: 0.6229 Acc: 71.8750% F1: 0.489 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 8/15 loss: 0.8032 Acc: 68.7500% F1: 0.554 Time: 0.94s (8.83s)
Fold 0 train - epoch: 4/5 iter: 9/15 loss: 0.6128 Acc: 81.2500% F1: 0.577 Time: 0.94s (0.02s)
Fold 0 train - epoch: 4/5 iter: 10/15 loss: 0.7351 Acc: 71.8750% F1: 0.629 Time: 0.96s (8.64s)
Fold 0 train - epoch: 4/5 iter: 11/15 loss: 0.5141 Acc: 84.3750% F1: 0.819 Time: 0.95s (0.03s)
Fold 0 train - epoch: 4/5 iter: 12/15 loss: 0.6348 Acc: 75.0000% F1: 0.731 Time: 0.94s (8.67s)
Fold 0 train - epoch: 4/5 iter: 13/15 loss: 0.5016 Acc: 78.1250% F1: 0.790 Time: 0.94s (0.03s)
Fold 0 train - epoch: 4/5 iter: 14/15 loss: 0.0325 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 76.4444% F1: 0.6974 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/5 iter: 0/2 loss: 1.1840 Acc: 37.5000% F1: 0.253 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 4/5 iter: 1/2 loss: 1.8345 Acc: 33.3333% F1: 0.257 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 36.0000% F1: 0.3125 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/5 iter: 0/15 loss: 1.1887 Acc: 34.3750% F1: 0.217 Time: 0.97s (0.00s)
Fold 1 train - epoch: 0/5 iter: 1/15 loss: 0.9978 Acc: 53.1250% F1: 0.278 Time: 0.94s (0.03s)
Fold 1 train - epoch: 0/5 iter: 2/15 loss: 1.0675 Acc: 50.0000% F1: 0.222 Time: 0.95s (9.03s)
Fold 1 train - epoch: 0/5 iter: 3/15 loss: 0.9272 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 1 train - epoch: 0/5 iter: 4/15 loss: 0.9900 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.30s)
Fold 1 train - epoch: 0/5 iter: 5/15 loss: 0.9674 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 6/15 loss: 0.8427 Acc: 71.8750% F1: 0.349 Time: 0.94s (8.63s)
Fold 1 train - epoch: 0/5 iter: 7/15 loss: 0.9387 Acc: 46.8750% F1: 0.244 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 8/15 loss: 0.9950 Acc: 59.3750% F1: 0.381 Time: 0.94s (9.53s)
Fold 1 train - epoch: 0/5 iter: 9/15 loss: 0.9341 Acc: 62.5000% F1: 0.427 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 10/15 loss: 0.9979 Acc: 50.0000% F1: 0.343 Time: 0.96s (9.28s)
Fold 1 train - epoch: 0/5 iter: 11/15 loss: 0.9316 Acc: 40.6250% F1: 0.228 Time: 0.94s (0.23s)
Fold 1 train - epoch: 0/5 iter: 12/15 loss: 1.1002 Acc: 50.0000% F1: 0.324 Time: 0.94s (8.78s)
Fold 1 train - epoch: 0/5 iter: 13/15 loss: 0.9250 Acc: 59.3750% F1: 0.360 Time: 0.94s (0.89s)
Fold 1 train - epoch: 0/5 iter: 14/15 loss: 0.6625 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 53.3333% F1: 0.3175 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6439 Acc: 81.2500% F1: 0.448 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6546 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 52.0000% F1: 0.2281 *
*********************************************************
Performing epoch 1 of 5
Fold 1 train - epoch: 1/5 iter: 0/15 loss: 0.8243 Acc: 59.3750% F1: 0.378 Time: 0.96s (0.00s)
Fold 1 train - epoch: 1/5 iter: 1/15 loss: 0.9266 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/5 iter: 2/15 loss: 0.9550 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.98s)
Fold 1 train - epoch: 1/5 iter: 3/15 loss: 0.8802 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/5 iter: 4/15 loss: 0.9594 Acc: 50.0000% F1: 0.222 Time: 0.95s (8.52s)
Fold 1 train - epoch: 1/5 iter: 5/15 loss: 0.9527 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.04s)
Fold 1 train - epoch: 1/5 iter: 6/15 loss: 0.7572 Acc: 68.7500% F1: 0.272 Time: 0.95s (9.01s)
Fold 1 train - epoch: 1/5 iter: 7/15 loss: 0.9079 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 1 train - epoch: 1/5 iter: 8/15 loss: 0.9598 Acc: 56.2500% F1: 0.240 Time: 0.95s (10.11s)
Fold 1 train - epoch: 1/5 iter: 9/15 loss: 0.9025 Acc: 50.0000% F1: 0.267 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 10/15 loss: 0.9859 Acc: 46.8750% F1: 0.213 Time: 0.96s (8.70s)
Fold 1 train - epoch: 1/5 iter: 11/15 loss: 0.8811 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 1 train - epoch: 1/5 iter: 12/15 loss: 1.0181 Acc: 53.1250% F1: 0.317 Time: 0.94s (8.69s)
Fold 1 train - epoch: 1/5 iter: 13/15 loss: 0.8961 Acc: 50.0000% F1: 0.264 Time: 0.94s (0.08s)
Fold 1 train - epoch: 1/5 iter: 14/15 loss: 0.6410 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 54.6667% F1: 0.2645 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7984 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2934 Acc: 22.2222% F1: 0.157 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 56.0000% F1: 0.3386 *
*********************************************************
Performing epoch 2 of 5
Fold 1 train - epoch: 2/5 iter: 0/15 loss: 0.8453 Acc: 59.3750% F1: 0.430 Time: 0.96s (0.00s)
Fold 1 train - epoch: 2/5 iter: 1/15 loss: 0.8879 Acc: 56.2500% F1: 0.326 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 2/15 loss: 0.8650 Acc: 53.1250% F1: 0.275 Time: 0.96s (9.52s)
Fold 1 train - epoch: 2/5 iter: 3/15 loss: 0.7850 Acc: 65.6250% F1: 0.361 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 4/15 loss: 0.8678 Acc: 50.0000% F1: 0.227 Time: 0.94s (8.25s)
Fold 1 train - epoch: 2/5 iter: 5/15 loss: 0.9210 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 6/15 loss: 0.7480 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.85s)
Fold 1 train - epoch: 2/5 iter: 7/15 loss: 0.8287 Acc: 53.1250% F1: 0.296 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 8/15 loss: 0.9772 Acc: 56.2500% F1: 0.240 Time: 0.95s (9.10s)
Fold 1 train - epoch: 2/5 iter: 9/15 loss: 0.8656 Acc: 62.5000% F1: 0.419 Time: 0.94s (0.03s)
Fold 1 train - epoch: 2/5 iter: 10/15 loss: 0.9506 Acc: 50.0000% F1: 0.265 Time: 0.96s (8.64s)
Fold 1 train - epoch: 2/5 iter: 11/15 loss: 0.7806 Acc: 68.7500% F1: 0.383 Time: 0.94s (0.17s)
Fold 1 train - epoch: 2/5 iter: 12/15 loss: 0.9299 Acc: 53.1250% F1: 0.317 Time: 0.95s (8.49s)
Fold 1 train - epoch: 2/5 iter: 13/15 loss: 0.8229 Acc: 59.3750% F1: 0.335 Time: 0.94s (0.95s)
Fold 1 train - epoch: 2/5 iter: 14/15 loss: 0.3509 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 58.4444% F1: 0.3305 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8363 Acc: 62.5000% F1: 0.309 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3071 Acc: 22.2222% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 48.0000% F1: 0.3189 *
*********************************************************
Performing epoch 3 of 5
Fold 1 train - epoch: 3/5 iter: 0/15 loss: 0.7790 Acc: 62.5000% F1: 0.480 Time: 0.96s (0.00s)
Fold 1 train - epoch: 3/5 iter: 1/15 loss: 0.7480 Acc: 78.1250% F1: 0.657 Time: 0.94s (0.03s)
Fold 1 train - epoch: 3/5 iter: 2/15 loss: 0.8092 Acc: 56.2500% F1: 0.326 Time: 0.96s (8.88s)
Fold 1 train - epoch: 3/5 iter: 3/15 loss: 0.6919 Acc: 71.8750% F1: 0.447 Time: 0.94s (0.03s)
Fold 1 train - epoch: 3/5 iter: 4/15 loss: 0.7817 Acc: 53.1250% F1: 0.284 Time: 0.94s (7.88s)
Fold 1 train - epoch: 3/5 iter: 5/15 loss: 0.8218 Acc: 59.3750% F1: 0.344 Time: 0.95s (0.20s)
Fold 1 train - epoch: 3/5 iter: 6/15 loss: 0.6766 Acc: 78.1250% F1: 0.482 Time: 0.94s (8.15s)
Fold 1 train - epoch: 3/5 iter: 7/15 loss: 0.8033 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.76s)
Fold 1 train - epoch: 3/5 iter: 8/15 loss: 0.9106 Acc: 62.5000% F1: 0.483 Time: 0.94s (8.27s)
Fold 1 train - epoch: 3/5 iter: 9/15 loss: 0.6875 Acc: 71.8750% F1: 0.506 Time: 0.95s (0.85s)
Fold 1 train - epoch: 3/5 iter: 10/15 loss: 0.8473 Acc: 59.3750% F1: 0.415 Time: 0.96s (7.54s)
Fold 1 train - epoch: 3/5 iter: 11/15 loss: 0.7101 Acc: 65.6250% F1: 0.437 Time: 0.94s (1.27s)
Fold 1 train - epoch: 3/5 iter: 12/15 loss: 0.8843 Acc: 62.5000% F1: 0.450 Time: 0.94s (7.03s)
Fold 1 train - epoch: 3/5 iter: 13/15 loss: 0.7223 Acc: 56.2500% F1: 0.326 Time: 0.94s (1.93s)
Fold 1 train - epoch: 3/5 iter: 14/15 loss: 0.1469 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 64.2222% F1: 0.4469 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9218 Acc: 56.2500% F1: 0.297 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3303 Acc: 33.3333% F1: 0.182 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.3471 *
*********************************************************
Performing epoch 4 of 5
Fold 1 train - epoch: 4/5 iter: 0/15 loss: 0.6586 Acc: 71.8750% F1: 0.659 Time: 0.96s (0.00s)
Fold 1 train - epoch: 4/5 iter: 1/15 loss: 0.6295 Acc: 81.2500% F1: 0.754 Time: 0.94s (0.03s)
Fold 1 train - epoch: 4/5 iter: 2/15 loss: 0.7241 Acc: 71.8750% F1: 0.735 Time: 0.96s (10.34s)
Fold 1 train - epoch: 4/5 iter: 3/15 loss: 0.5678 Acc: 68.7500% F1: 0.460 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/5 iter: 4/15 loss: 0.6055 Acc: 71.8750% F1: 0.727 Time: 0.94s (8.11s)
Fold 1 train - epoch: 4/5 iter: 5/15 loss: 0.6448 Acc: 78.1250% F1: 0.734 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/5 iter: 6/15 loss: 0.5983 Acc: 78.1250% F1: 0.625 Time: 0.94s (8.58s)
Fold 1 train - epoch: 4/5 iter: 7/15 loss: 0.7323 Acc: 62.5000% F1: 0.718 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 8/15 loss: 0.7662 Acc: 62.5000% F1: 0.480 Time: 0.94s (9.30s)
Fold 1 train - epoch: 4/5 iter: 9/15 loss: 0.6010 Acc: 75.0000% F1: 0.643 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 10/15 loss: 0.8123 Acc: 59.3750% F1: 0.540 Time: 0.95s (8.84s)
Fold 1 train - epoch: 4/5 iter: 11/15 loss: 0.4950 Acc: 81.2500% F1: 0.700 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 12/15 loss: 0.5723 Acc: 81.2500% F1: 0.745 Time: 0.94s (8.95s)
Fold 1 train - epoch: 4/5 iter: 13/15 loss: 0.6675 Acc: 68.7500% F1: 0.560 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/5 iter: 14/15 loss: 0.0373 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 72.4444% F1: 0.6548 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0265 Acc: 59.3750% F1: 0.311 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 4/5 iter: 1/2 loss: 1.4382 Acc: 38.8889% F1: 0.274 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 52.0000% F1: 0.4163 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/5 iter: 0/15 loss: 1.1290 Acc: 40.6250% F1: 0.266 Time: 0.98s (0.00s)
Fold 2 train - epoch: 0/5 iter: 1/15 loss: 0.9854 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/5 iter: 2/15 loss: 1.1264 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.62s)
Fold 2 train - epoch: 0/5 iter: 3/15 loss: 0.9801 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/5 iter: 4/15 loss: 0.9845 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.45s)
Fold 2 train - epoch: 0/5 iter: 5/15 loss: 0.9530 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/5 iter: 6/15 loss: 0.8702 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.46s)
Fold 2 train - epoch: 0/5 iter: 7/15 loss: 0.8904 Acc: 46.8750% F1: 0.270 Time: 0.94s (0.21s)
Fold 2 train - epoch: 0/5 iter: 8/15 loss: 1.0799 Acc: 43.7500% F1: 0.256 Time: 0.94s (9.12s)
Fold 2 train - epoch: 0/5 iter: 9/15 loss: 0.9919 Acc: 50.0000% F1: 0.350 Time: 0.94s (0.83s)
Fold 2 train - epoch: 0/5 iter: 10/15 loss: 1.0065 Acc: 50.0000% F1: 0.351 Time: 0.96s (7.58s)
Fold 2 train - epoch: 0/5 iter: 11/15 loss: 0.9171 Acc: 56.2500% F1: 0.344 Time: 0.95s (1.76s)
Fold 2 train - epoch: 0/5 iter: 12/15 loss: 1.0950 Acc: 40.6250% F1: 0.253 Time: 0.95s (7.00s)
Fold 2 train - epoch: 0/5 iter: 13/15 loss: 0.9259 Acc: 56.2500% F1: 0.287 Time: 0.94s (3.14s)
Fold 2 train - epoch: 0/5 iter: 14/15 loss: 0.7433 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.03s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 51.3333% F1: 0.3064 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5866 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 0/5 iter: 1/2 loss: 1.4551 Acc: 5.5556% F1: 0.044 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 58.0000% F1: 0.2807 *
*********************************************************
Performing epoch 1 of 5
Fold 2 train - epoch: 1/5 iter: 0/15 loss: 0.8495 Acc: 59.3750% F1: 0.378 Time: 0.96s (0.00s)
Fold 2 train - epoch: 1/5 iter: 1/15 loss: 0.9247 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.04s)
Fold 2 train - epoch: 1/5 iter: 2/15 loss: 0.9354 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.72s)
Fold 2 train - epoch: 1/5 iter: 3/15 loss: 0.8102 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 4/15 loss: 0.9684 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.15s)
Fold 2 train - epoch: 1/5 iter: 5/15 loss: 0.9513 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 2 train - epoch: 1/5 iter: 6/15 loss: 0.7427 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.41s)
Fold 2 train - epoch: 1/5 iter: 7/15 loss: 0.9142 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 2 train - epoch: 1/5 iter: 8/15 loss: 1.0558 Acc: 53.1250% F1: 0.231 Time: 0.95s (8.78s)
Fold 2 train - epoch: 1/5 iter: 9/15 loss: 0.9683 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.32s)
Fold 2 train - epoch: 1/5 iter: 10/15 loss: 0.9661 Acc: 46.8750% F1: 0.213 Time: 0.96s (8.61s)
Fold 2 train - epoch: 1/5 iter: 11/15 loss: 0.8614 Acc: 62.5000% F1: 0.256 Time: 0.94s (1.14s)
Fold 2 train - epoch: 1/5 iter: 12/15 loss: 0.9961 Acc: 50.0000% F1: 0.222 Time: 0.95s (8.69s)
Fold 2 train - epoch: 1/5 iter: 13/15 loss: 0.8777 Acc: 56.2500% F1: 0.240 Time: 0.95s (2.19s)
Fold 2 train - epoch: 1/5 iter: 14/15 loss: 0.6352 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 54.6667% F1: 0.2484 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7317 Acc: 84.3750% F1: 0.599 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2286 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.3275 *
*********************************************************
Performing epoch 2 of 5
Fold 2 train - epoch: 2/5 iter: 0/15 loss: 0.8442 Acc: 59.3750% F1: 0.306 Time: 0.96s (0.00s)
Fold 2 train - epoch: 2/5 iter: 1/15 loss: 0.9089 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 2 train - epoch: 2/5 iter: 2/15 loss: 0.8848 Acc: 53.1250% F1: 0.275 Time: 0.96s (8.90s)
Fold 2 train - epoch: 2/5 iter: 3/15 loss: 0.7916 Acc: 68.7500% F1: 0.407 Time: 0.94s (0.02s)
Fold 2 train - epoch: 2/5 iter: 4/15 loss: 0.8613 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.27s)
Fold 2 train - epoch: 2/5 iter: 5/15 loss: 0.9224 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 6/15 loss: 0.7498 Acc: 68.7500% F1: 0.277 Time: 0.94s (8.85s)
Fold 2 train - epoch: 2/5 iter: 7/15 loss: 0.8599 Acc: 50.0000% F1: 0.257 Time: 0.94s (0.03s)
Fold 2 train - epoch: 2/5 iter: 8/15 loss: 1.0643 Acc: 53.1250% F1: 0.231 Time: 0.95s (9.63s)
Fold 2 train - epoch: 2/5 iter: 9/15 loss: 0.9417 Acc: 50.0000% F1: 0.265 Time: 0.94s (0.03s)
Fold 2 train - epoch: 2/5 iter: 10/15 loss: 0.9597 Acc: 50.0000% F1: 0.265 Time: 0.96s (9.20s)
Fold 2 train - epoch: 2/5 iter: 11/15 loss: 0.7980 Acc: 65.6250% F1: 0.327 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 12/15 loss: 0.9922 Acc: 53.1250% F1: 0.283 Time: 0.94s (8.95s)
Fold 2 train - epoch: 2/5 iter: 13/15 loss: 0.8099 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.72s)
Fold 2 train - epoch: 2/5 iter: 14/15 loss: 0.3567 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 57.1111% F1: 0.2872 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6440 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3408 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2996 *
*********************************************************
Performing epoch 3 of 5
Fold 2 train - epoch: 3/5 iter: 0/15 loss: 0.8099 Acc: 59.3750% F1: 0.383 Time: 0.96s (0.00s)
Fold 2 train - epoch: 3/5 iter: 1/15 loss: 0.7744 Acc: 65.6250% F1: 0.409 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 2/15 loss: 0.8423 Acc: 53.1250% F1: 0.275 Time: 0.96s (9.12s)
Fold 2 train - epoch: 3/5 iter: 3/15 loss: 0.7421 Acc: 65.6250% F1: 0.361 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/5 iter: 4/15 loss: 0.7623 Acc: 56.2500% F1: 0.401 Time: 0.94s (8.81s)
Fold 2 train - epoch: 3/5 iter: 5/15 loss: 0.8175 Acc: 62.5000% F1: 0.358 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/5 iter: 6/15 loss: 0.6720 Acc: 78.1250% F1: 0.460 Time: 0.95s (9.60s)
Fold 2 train - epoch: 3/5 iter: 7/15 loss: 0.8350 Acc: 56.2500% F1: 0.349 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/5 iter: 8/15 loss: 0.9888 Acc: 46.8750% F1: 0.267 Time: 0.95s (9.13s)
Fold 2 train - epoch: 3/5 iter: 9/15 loss: 0.8048 Acc: 68.7500% F1: 0.484 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 10/15 loss: 0.8800 Acc: 68.7500% F1: 0.491 Time: 0.96s (8.36s)
Fold 2 train - epoch: 3/5 iter: 11/15 loss: 0.7071 Acc: 59.3750% F1: 0.391 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 12/15 loss: 0.9432 Acc: 59.3750% F1: 0.518 Time: 0.94s (8.67s)
Fold 2 train - epoch: 3/5 iter: 13/15 loss: 0.7453 Acc: 62.5000% F1: 0.375 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 14/15 loss: 0.1760 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 61.7778% F1: 0.4240 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6469 Acc: 62.5000% F1: 0.305 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3880 Acc: 11.1111% F1: 0.074 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 44.0000% F1: 0.2688 *
*********************************************************
Performing epoch 4 of 5
Fold 2 train - epoch: 4/5 iter: 0/15 loss: 0.7172 Acc: 65.6250% F1: 0.500 Time: 0.95s (0.00s)
Fold 2 train - epoch: 4/5 iter: 1/15 loss: 0.6561 Acc: 78.1250% F1: 0.552 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/5 iter: 2/15 loss: 0.7286 Acc: 71.8750% F1: 0.633 Time: 0.95s (9.17s)
Fold 2 train - epoch: 4/5 iter: 3/15 loss: 0.5995 Acc: 75.0000% F1: 0.675 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/5 iter: 4/15 loss: 0.6297 Acc: 71.8750% F1: 0.698 Time: 0.94s (9.01s)
Fold 2 train - epoch: 4/5 iter: 5/15 loss: 0.6551 Acc: 81.2500% F1: 0.694 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/5 iter: 6/15 loss: 0.5366 Acc: 87.5000% F1: 0.735 Time: 0.95s (9.73s)
Fold 2 train - epoch: 4/5 iter: 7/15 loss: 0.7984 Acc: 65.6250% F1: 0.434 Time: 0.95s (0.04s)
Fold 2 train - epoch: 4/5 iter: 8/15 loss: 0.8870 Acc: 59.3750% F1: 0.457 Time: 0.94s (9.22s)
Fold 2 train - epoch: 4/5 iter: 9/15 loss: 0.6540 Acc: 75.0000% F1: 0.650 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/5 iter: 10/15 loss: 0.6849 Acc: 75.0000% F1: 0.640 Time: 0.96s (8.66s)
Fold 2 train - epoch: 4/5 iter: 11/15 loss: 0.5155 Acc: 78.1250% F1: 0.679 Time: 0.95s (0.03s)
Fold 2 train - epoch: 4/5 iter: 12/15 loss: 0.8576 Acc: 68.7500% F1: 0.573 Time: 0.94s (8.95s)
Fold 2 train - epoch: 4/5 iter: 13/15 loss: 0.6534 Acc: 71.8750% F1: 0.617 Time: 0.94s (0.25s)
Fold 2 train - epoch: 4/5 iter: 14/15 loss: 0.0314 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 73.3333% F1: 0.6255 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8700 Acc: 68.7500% F1: 0.378 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 4/5 iter: 1/2 loss: 1.3505 Acc: 27.7778% F1: 0.244 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.4521 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/5 iter: 0/15 loss: 1.2367 Acc: 34.3750% F1: 0.221 Time: 0.98s (0.00s)
Fold 3 train - epoch: 0/5 iter: 1/15 loss: 0.9533 Acc: 53.1250% F1: 0.311 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 2/15 loss: 1.1435 Acc: 50.0000% F1: 0.262 Time: 0.96s (9.90s)
Fold 3 train - epoch: 0/5 iter: 3/15 loss: 0.9006 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 3 train - epoch: 0/5 iter: 4/15 loss: 1.0364 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.47s)
Fold 3 train - epoch: 0/5 iter: 5/15 loss: 0.8977 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.02s)
Fold 3 train - epoch: 0/5 iter: 6/15 loss: 0.8521 Acc: 71.8750% F1: 0.351 Time: 0.94s (8.48s)
Fold 3 train - epoch: 0/5 iter: 7/15 loss: 0.9454 Acc: 46.8750% F1: 0.268 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 8/15 loss: 1.0667 Acc: 46.8750% F1: 0.265 Time: 0.94s (9.10s)
Fold 3 train - epoch: 0/5 iter: 9/15 loss: 0.9625 Acc: 56.2500% F1: 0.390 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 10/15 loss: 0.9892 Acc: 40.6250% F1: 0.286 Time: 0.96s (8.52s)
Fold 3 train - epoch: 0/5 iter: 11/15 loss: 0.9303 Acc: 50.0000% F1: 0.312 Time: 0.94s (0.02s)
Fold 3 train - epoch: 0/5 iter: 12/15 loss: 0.9937 Acc: 53.1250% F1: 0.352 Time: 0.94s (9.19s)
Fold 3 train - epoch: 0/5 iter: 13/15 loss: 0.9302 Acc: 56.2500% F1: 0.287 Time: 0.94s (0.41s)
Fold 3 train - epoch: 0/5 iter: 14/15 loss: 0.9317 Acc: 0.0000% F1: 0.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 52.0000% F1: 0.3160 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6417 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6440 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 50.0000% F1: 0.2222 *
*********************************************************
Performing epoch 1 of 5
Fold 3 train - epoch: 1/5 iter: 0/15 loss: 0.8648 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.00s)
Fold 3 train - epoch: 1/5 iter: 1/15 loss: 0.8711 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 3 train - epoch: 1/5 iter: 2/15 loss: 0.9426 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.14s)
Fold 3 train - epoch: 1/5 iter: 3/15 loss: 0.8248 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 3 train - epoch: 1/5 iter: 4/15 loss: 0.9843 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.84s)
Fold 3 train - epoch: 1/5 iter: 5/15 loss: 0.9052 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 6/15 loss: 0.7627 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.36s)
Fold 3 train - epoch: 1/5 iter: 7/15 loss: 0.9161 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 8/15 loss: 1.1080 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.86s)
Fold 3 train - epoch: 1/5 iter: 9/15 loss: 1.0019 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 10/15 loss: 1.0219 Acc: 50.0000% F1: 0.265 Time: 0.96s (8.49s)
Fold 3 train - epoch: 1/5 iter: 11/15 loss: 0.8407 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 12/15 loss: 0.9160 Acc: 56.2500% F1: 0.240 Time: 0.94s (9.02s)
Fold 3 train - epoch: 1/5 iter: 13/15 loss: 0.9183 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 14/15 loss: 0.7348 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 55.3333% F1: 0.2418 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7374 Acc: 71.8750% F1: 0.506 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3954 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.2651 *
*********************************************************
Performing epoch 2 of 5
Fold 3 train - epoch: 2/5 iter: 0/15 loss: 0.8519 Acc: 62.5000% F1: 0.444 Time: 0.95s (0.00s)
Fold 3 train - epoch: 2/5 iter: 1/15 loss: 0.8648 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 3 train - epoch: 2/5 iter: 2/15 loss: 0.8791 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.20s)
Fold 3 train - epoch: 2/5 iter: 3/15 loss: 0.7660 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.02s)
Fold 3 train - epoch: 2/5 iter: 4/15 loss: 0.8732 Acc: 53.1250% F1: 0.278 Time: 0.94s (8.67s)
Fold 3 train - epoch: 2/5 iter: 5/15 loss: 0.8819 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.04s)
Fold 3 train - epoch: 2/5 iter: 6/15 loss: 0.7625 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.75s)
Fold 3 train - epoch: 2/5 iter: 7/15 loss: 0.8763 Acc: 50.0000% F1: 0.257 Time: 0.94s (0.03s)
Fold 3 train - epoch: 2/5 iter: 8/15 loss: 1.0734 Acc: 46.8750% F1: 0.217 Time: 0.94s (9.29s)
Fold 3 train - epoch: 2/5 iter: 9/15 loss: 0.9934 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 10/15 loss: 0.9836 Acc: 59.3750% F1: 0.389 Time: 0.99s (8.71s)
Fold 3 train - epoch: 2/5 iter: 11/15 loss: 0.7541 Acc: 65.6250% F1: 0.327 Time: 0.95s (0.05s)
Fold 3 train - epoch: 2/5 iter: 12/15 loss: 0.8949 Acc: 62.5000% F1: 0.361 Time: 0.94s (9.16s)
Fold 3 train - epoch: 2/5 iter: 13/15 loss: 0.8507 Acc: 65.6250% F1: 0.419 Time: 0.94s (0.03s)
Fold 3 train - epoch: 2/5 iter: 14/15 loss: 0.3435 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 58.4444% F1: 0.3228 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7370 Acc: 75.0000% F1: 0.590 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4703 Acc: 27.7778% F1: 0.185 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.3847 *
*********************************************************
Performing epoch 3 of 5
Fold 3 train - epoch: 3/5 iter: 0/15 loss: 0.7561 Acc: 62.5000% F1: 0.444 Time: 0.96s (0.00s)
Fold 3 train - epoch: 3/5 iter: 1/15 loss: 0.7681 Acc: 68.7500% F1: 0.444 Time: 0.94s (0.04s)
Fold 3 train - epoch: 3/5 iter: 2/15 loss: 0.7842 Acc: 62.5000% F1: 0.534 Time: 0.96s (9.30s)
Fold 3 train - epoch: 3/5 iter: 3/15 loss: 0.6893 Acc: 75.0000% F1: 0.691 Time: 0.94s (0.04s)
Fold 3 train - epoch: 3/5 iter: 4/15 loss: 0.7886 Acc: 62.5000% F1: 0.506 Time: 0.94s (8.83s)
Fold 3 train - epoch: 3/5 iter: 5/15 loss: 0.7482 Acc: 71.8750% F1: 0.475 Time: 0.94s (0.03s)
Fold 3 train - epoch: 3/5 iter: 6/15 loss: 0.6289 Acc: 81.2500% F1: 0.520 Time: 0.94s (8.99s)
Fold 3 train - epoch: 3/5 iter: 7/15 loss: 0.8674 Acc: 50.0000% F1: 0.282 Time: 0.95s (0.05s)
Fold 3 train - epoch: 3/5 iter: 8/15 loss: 0.9764 Acc: 50.0000% F1: 0.353 Time: 0.95s (9.30s)
Fold 3 train - epoch: 3/5 iter: 9/15 loss: 0.8841 Acc: 62.5000% F1: 0.419 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 10/15 loss: 0.8797 Acc: 68.7500% F1: 0.495 Time: 0.96s (8.69s)
Fold 3 train - epoch: 3/5 iter: 11/15 loss: 0.6873 Acc: 62.5000% F1: 0.394 Time: 0.94s (0.02s)
Fold 3 train - epoch: 3/5 iter: 12/15 loss: 0.8299 Acc: 59.3750% F1: 0.370 Time: 0.94s (9.17s)
Fold 3 train - epoch: 3/5 iter: 13/15 loss: 0.7927 Acc: 59.3750% F1: 0.464 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 14/15 loss: 0.1983 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 64.2222% F1: 0.4692 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9119 Acc: 59.3750% F1: 0.324 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4735 Acc: 22.2222% F1: 0.157 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.3236 *
*********************************************************
Performing epoch 4 of 5
Fold 3 train - epoch: 4/5 iter: 0/15 loss: 0.7137 Acc: 62.5000% F1: 0.480 Time: 0.95s (0.00s)
Fold 3 train - epoch: 4/5 iter: 1/15 loss: 0.6544 Acc: 84.3750% F1: 0.717 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/5 iter: 2/15 loss: 0.6458 Acc: 71.8750% F1: 0.674 Time: 0.96s (9.09s)
Fold 3 train - epoch: 4/5 iter: 3/15 loss: 0.5784 Acc: 78.1250% F1: 0.712 Time: 0.94s (0.03s)
Fold 3 train - epoch: 4/5 iter: 4/15 loss: 0.6565 Acc: 71.8750% F1: 0.607 Time: 0.94s (8.73s)
Fold 3 train - epoch: 4/5 iter: 5/15 loss: 0.6275 Acc: 68.7500% F1: 0.543 Time: 0.94s (0.03s)
Fold 3 train - epoch: 4/5 iter: 6/15 loss: 0.5169 Acc: 81.2500% F1: 0.734 Time: 0.94s (8.76s)
Fold 3 train - epoch: 4/5 iter: 7/15 loss: 0.8193 Acc: 50.0000% F1: 0.302 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/5 iter: 8/15 loss: 0.8165 Acc: 59.3750% F1: 0.501 Time: 0.94s (9.08s)
Fold 3 train - epoch: 4/5 iter: 9/15 loss: 0.6815 Acc: 78.1250% F1: 0.554 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 10/15 loss: 0.7978 Acc: 53.1250% F1: 0.378 Time: 0.95s (8.54s)
Fold 3 train - epoch: 4/5 iter: 11/15 loss: 0.5835 Acc: 78.1250% F1: 0.783 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/5 iter: 12/15 loss: 0.7068 Acc: 75.0000% F1: 0.650 Time: 0.95s (9.32s)
Fold 3 train - epoch: 4/5 iter: 13/15 loss: 0.5716 Acc: 78.1250% F1: 0.790 Time: 0.95s (0.04s)
Fold 3 train - epoch: 4/5 iter: 14/15 loss: 0.0484 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 70.8889% F1: 0.6297 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8976 Acc: 59.3750% F1: 0.298 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7518 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 44.0000% F1: 0.2994 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/5 iter: 0/15 loss: 1.1357 Acc: 34.3750% F1: 0.235 Time: 0.97s (0.00s)
Fold 4 train - epoch: 0/5 iter: 1/15 loss: 1.0327 Acc: 50.0000% F1: 0.267 Time: 0.94s (0.03s)
Fold 4 train - epoch: 0/5 iter: 2/15 loss: 1.0738 Acc: 46.8750% F1: 0.213 Time: 0.96s (8.91s)
Fold 4 train - epoch: 0/5 iter: 3/15 loss: 0.9250 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 4/15 loss: 0.9843 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.55s)
Fold 4 train - epoch: 0/5 iter: 5/15 loss: 0.9182 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 6/15 loss: 0.8899 Acc: 65.6250% F1: 0.327 Time: 0.94s (9.08s)
Fold 4 train - epoch: 0/5 iter: 7/15 loss: 0.9316 Acc: 53.1250% F1: 0.296 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 8/15 loss: 1.1260 Acc: 40.6250% F1: 0.234 Time: 0.94s (9.93s)
Fold 4 train - epoch: 0/5 iter: 9/15 loss: 0.9752 Acc: 46.8750% F1: 0.325 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 10/15 loss: 0.9920 Acc: 53.1250% F1: 0.376 Time: 0.95s (8.45s)
Fold 4 train - epoch: 0/5 iter: 11/15 loss: 0.9066 Acc: 50.0000% F1: 0.312 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 12/15 loss: 1.0417 Acc: 50.0000% F1: 0.317 Time: 0.94s (9.28s)
Fold 4 train - epoch: 0/5 iter: 13/15 loss: 0.9278 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 14/15 loss: 0.6792 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 51.1111% F1: 0.3072 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5793 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6332 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 4 train - epoch: 1/5 iter: 0/15 loss: 0.8175 Acc: 59.3750% F1: 0.378 Time: 0.96s (0.00s)
Fold 4 train - epoch: 1/5 iter: 1/15 loss: 0.9204 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 4 train - epoch: 1/5 iter: 2/15 loss: 0.9255 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.29s)
Fold 4 train - epoch: 1/5 iter: 3/15 loss: 0.7860 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 4/15 loss: 0.9802 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.21s)
Fold 4 train - epoch: 1/5 iter: 5/15 loss: 0.9032 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 6/15 loss: 0.7382 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.44s)
Fold 4 train - epoch: 1/5 iter: 7/15 loss: 0.9246 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 8/15 loss: 1.0561 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.18s)
Fold 4 train - epoch: 1/5 iter: 9/15 loss: 1.0041 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 4 train - epoch: 1/5 iter: 10/15 loss: 0.9772 Acc: 46.8750% F1: 0.213 Time: 0.96s (8.37s)
Fold 4 train - epoch: 1/5 iter: 11/15 loss: 0.8746 Acc: 59.3750% F1: 0.301 Time: 0.94s (0.02s)
Fold 4 train - epoch: 1/5 iter: 12/15 loss: 0.9562 Acc: 56.2500% F1: 0.292 Time: 0.94s (9.49s)
Fold 4 train - epoch: 1/5 iter: 13/15 loss: 0.9020 Acc: 62.5000% F1: 0.380 Time: 0.94s (0.02s)
Fold 4 train - epoch: 1/5 iter: 14/15 loss: 0.5874 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 55.5556% F1: 0.2708 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7683 Acc: 68.7500% F1: 0.407 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2857 Acc: 33.3333% F1: 0.211 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 56.0000% F1: 0.3636 *
*********************************************************
Performing epoch 2 of 5
Fold 4 train - epoch: 2/5 iter: 0/15 loss: 0.8206 Acc: 65.6250% F1: 0.495 Time: 0.96s (0.00s)
Fold 4 train - epoch: 2/5 iter: 1/15 loss: 0.8892 Acc: 62.5000% F1: 0.389 Time: 0.94s (0.03s)
Fold 4 train - epoch: 2/5 iter: 2/15 loss: 0.8175 Acc: 56.2500% F1: 0.321 Time: 0.96s (10.14s)
Fold 4 train - epoch: 2/5 iter: 3/15 loss: 0.7638 Acc: 68.7500% F1: 0.403 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/5 iter: 4/15 loss: 0.8420 Acc: 56.2500% F1: 0.326 Time: 0.94s (8.61s)
Fold 4 train - epoch: 2/5 iter: 5/15 loss: 0.8447 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/5 iter: 6/15 loss: 0.7652 Acc: 71.8750% F1: 0.351 Time: 0.94s (8.53s)
Fold 4 train - epoch: 2/5 iter: 7/15 loss: 0.8163 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 8/15 loss: 1.0240 Acc: 53.1250% F1: 0.231 Time: 0.95s (9.40s)
Fold 4 train - epoch: 2/5 iter: 9/15 loss: 0.9569 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 10/15 loss: 0.9761 Acc: 46.8750% F1: 0.213 Time: 0.95s (8.10s)
Fold 4 train - epoch: 2/5 iter: 11/15 loss: 0.7745 Acc: 68.7500% F1: 0.383 Time: 0.94s (0.46s)
Fold 4 train - epoch: 2/5 iter: 12/15 loss: 0.9145 Acc: 53.1250% F1: 0.236 Time: 0.94s (8.98s)
Fold 4 train - epoch: 2/5 iter: 13/15 loss: 0.8766 Acc: 53.1250% F1: 0.306 Time: 0.94s (0.62s)
Fold 4 train - epoch: 2/5 iter: 14/15 loss: 0.2264 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 58.2222% F1: 0.3218 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8232 Acc: 62.5000% F1: 0.305 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 2/5 iter: 1/2 loss: 1.2567 Acc: 27.7778% F1: 0.152 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.3405 *
*********************************************************
Performing epoch 3 of 5
Fold 4 train - epoch: 3/5 iter: 0/15 loss: 0.7788 Acc: 68.7500% F1: 0.614 Time: 0.96s (0.00s)
Fold 4 train - epoch: 3/5 iter: 1/15 loss: 0.8035 Acc: 62.5000% F1: 0.526 Time: 0.94s (0.04s)
Fold 4 train - epoch: 3/5 iter: 2/15 loss: 0.7570 Acc: 71.8750% F1: 0.619 Time: 0.96s (10.15s)
Fold 4 train - epoch: 3/5 iter: 3/15 loss: 0.6675 Acc: 71.8750% F1: 0.442 Time: 0.94s (0.02s)
Fold 4 train - epoch: 3/5 iter: 4/15 loss: 0.6984 Acc: 78.1250% F1: 0.781 Time: 0.94s (8.86s)
Fold 4 train - epoch: 3/5 iter: 5/15 loss: 0.7946 Acc: 62.5000% F1: 0.444 Time: 0.94s (0.02s)
Fold 4 train - epoch: 3/5 iter: 6/15 loss: 0.6455 Acc: 78.1250% F1: 0.460 Time: 0.94s (8.81s)
Fold 4 train - epoch: 3/5 iter: 7/15 loss: 0.7761 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 8/15 loss: 0.9498 Acc: 50.0000% F1: 0.232 Time: 0.94s (9.25s)
Fold 4 train - epoch: 3/5 iter: 9/15 loss: 0.8503 Acc: 56.2500% F1: 0.370 Time: 0.94s (0.03s)
Fold 4 train - epoch: 3/5 iter: 10/15 loss: 0.8756 Acc: 56.2500% F1: 0.400 Time: 0.96s (8.19s)
Fold 4 train - epoch: 3/5 iter: 11/15 loss: 0.6621 Acc: 71.8750% F1: 0.633 Time: 0.94s (0.02s)
Fold 4 train - epoch: 3/5 iter: 12/15 loss: 0.8525 Acc: 62.5000% F1: 0.419 Time: 0.94s (9.55s)
Fold 4 train - epoch: 3/5 iter: 13/15 loss: 0.7433 Acc: 68.7500% F1: 0.701 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 14/15 loss: 0.0718 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 65.3333% F1: 0.5312 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/5 iter: 0/2 loss: 1.0038 Acc: 37.5000% F1: 0.232 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 3/5 iter: 1/2 loss: 1.2770 Acc: 38.8889% F1: 0.300 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 38.0000% F1: 0.3618 *
*********************************************************
Performing epoch 4 of 5
Fold 4 train - epoch: 4/5 iter: 0/15 loss: 0.6710 Acc: 75.0000% F1: 0.715 Time: 0.96s (0.00s)
Fold 4 train - epoch: 4/5 iter: 1/15 loss: 0.6895 Acc: 81.2500% F1: 0.797 Time: 0.94s (0.03s)
Fold 4 train - epoch: 4/5 iter: 2/15 loss: 0.6580 Acc: 68.7500% F1: 0.634 Time: 0.97s (9.17s)
Fold 4 train - epoch: 4/5 iter: 3/15 loss: 0.5210 Acc: 81.2500% F1: 0.788 Time: 0.95s (0.04s)
Fold 4 train - epoch: 4/5 iter: 4/15 loss: 0.5609 Acc: 75.0000% F1: 0.720 Time: 0.94s (8.78s)
Fold 4 train - epoch: 4/5 iter: 5/15 loss: 0.4928 Acc: 78.1250% F1: 0.709 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 6/15 loss: 0.4647 Acc: 78.1250% F1: 0.598 Time: 0.94s (8.49s)
Fold 4 train - epoch: 4/5 iter: 7/15 loss: 0.7195 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 8/15 loss: 0.9385 Acc: 50.0000% F1: 0.279 Time: 0.94s (9.10s)
Fold 4 train - epoch: 4/5 iter: 9/15 loss: 0.6672 Acc: 81.2500% F1: 0.575 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 10/15 loss: 0.6878 Acc: 71.8750% F1: 0.632 Time: 0.97s (8.11s)
Fold 4 train - epoch: 4/5 iter: 11/15 loss: 0.5599 Acc: 81.2500% F1: 0.713 Time: 0.94s (0.70s)
Fold 4 train - epoch: 4/5 iter: 12/15 loss: 0.7799 Acc: 62.5000% F1: 0.419 Time: 0.95s (8.55s)
Fold 4 train - epoch: 4/5 iter: 13/15 loss: 0.6701 Acc: 65.6250% F1: 0.526 Time: 0.94s (1.07s)
Fold 4 train - epoch: 4/5 iter: 14/15 loss: 0.0392 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 72.0000% F1: 0.6331 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0780 Acc: 53.1250% F1: 0.311 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5373 Acc: 38.8889% F1: 0.288 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.4226 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/5 iter: 0/15 loss: 1.1349 Acc: 43.7500% F1: 0.416 Time: 0.98s (0.00s)
Fold 5 train - epoch: 0/5 iter: 1/15 loss: 1.0414 Acc: 53.1250% F1: 0.278 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/5 iter: 2/15 loss: 1.1104 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.03s)
Fold 5 train - epoch: 0/5 iter: 3/15 loss: 0.8998 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 4/15 loss: 1.0675 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.53s)
Fold 5 train - epoch: 0/5 iter: 5/15 loss: 0.9150 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/5 iter: 6/15 loss: 0.8751 Acc: 65.6250% F1: 0.269 Time: 0.94s (8.60s)
Fold 5 train - epoch: 0/5 iter: 7/15 loss: 0.9020 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 8/15 loss: 1.1529 Acc: 43.7500% F1: 0.253 Time: 0.94s (8.94s)
Fold 5 train - epoch: 0/5 iter: 9/15 loss: 1.0372 Acc: 43.7500% F1: 0.296 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 10/15 loss: 0.9832 Acc: 46.8750% F1: 0.333 Time: 0.96s (8.70s)
Fold 5 train - epoch: 0/5 iter: 11/15 loss: 0.9126 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.15s)
Fold 5 train - epoch: 0/5 iter: 12/15 loss: 1.0464 Acc: 56.2500% F1: 0.350 Time: 0.95s (9.58s)
Fold 5 train - epoch: 0/5 iter: 13/15 loss: 0.9231 Acc: 59.3750% F1: 0.362 Time: 0.94s (0.25s)
Fold 5 train - epoch: 0/5 iter: 14/15 loss: 0.7221 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 53.5556% F1: 0.3361 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6536 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 0/5 iter: 1/2 loss: 1.4435 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 50.0000% F1: 0.2222 *
*********************************************************
Performing epoch 1 of 5
Fold 5 train - epoch: 1/5 iter: 0/15 loss: 0.8394 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.00s)
Fold 5 train - epoch: 1/5 iter: 1/15 loss: 0.9370 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/5 iter: 2/15 loss: 0.9412 Acc: 50.0000% F1: 0.222 Time: 0.96s (9.20s)
Fold 5 train - epoch: 1/5 iter: 3/15 loss: 0.7925 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 4/15 loss: 1.0056 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.61s)
Fold 5 train - epoch: 1/5 iter: 5/15 loss: 0.8861 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 6/15 loss: 0.7594 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.78s)
Fold 5 train - epoch: 1/5 iter: 7/15 loss: 0.9121 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 5 train - epoch: 1/5 iter: 8/15 loss: 1.0655 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.53s)
Fold 5 train - epoch: 1/5 iter: 9/15 loss: 0.9916 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/5 iter: 10/15 loss: 0.9617 Acc: 46.8750% F1: 0.213 Time: 0.96s (9.64s)
Fold 5 train - epoch: 1/5 iter: 11/15 loss: 0.8539 Acc: 65.6250% F1: 0.366 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 12/15 loss: 0.9534 Acc: 59.3750% F1: 0.306 Time: 0.94s (9.44s)
Fold 5 train - epoch: 1/5 iter: 13/15 loss: 0.8700 Acc: 56.2500% F1: 0.345 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 14/15 loss: 0.6523 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 55.7778% F1: 0.2748 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/5 iter: 0/2 loss: 0.8067 Acc: 68.7500% F1: 0.407 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2131 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.2683 *
*********************************************************
Performing epoch 2 of 5
Fold 5 train - epoch: 2/5 iter: 0/15 loss: 0.8280 Acc: 65.6250% F1: 0.548 Time: 0.95s (0.00s)
Fold 5 train - epoch: 2/5 iter: 1/15 loss: 0.8623 Acc: 65.6250% F1: 0.430 Time: 0.94s (0.03s)
Fold 5 train - epoch: 2/5 iter: 2/15 loss: 0.8393 Acc: 53.1250% F1: 0.306 Time: 0.96s (9.15s)
Fold 5 train - epoch: 2/5 iter: 3/15 loss: 0.7646 Acc: 68.7500% F1: 0.403 Time: 0.94s (0.02s)
Fold 5 train - epoch: 2/5 iter: 4/15 loss: 0.8673 Acc: 56.2500% F1: 0.327 Time: 0.94s (8.71s)
Fold 5 train - epoch: 2/5 iter: 5/15 loss: 0.8300 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.03s)
Fold 5 train - epoch: 2/5 iter: 6/15 loss: 0.7537 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.73s)
Fold 5 train - epoch: 2/5 iter: 7/15 loss: 0.8813 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.03s)
Fold 5 train - epoch: 2/5 iter: 8/15 loss: 1.0384 Acc: 53.1250% F1: 0.231 Time: 0.95s (9.90s)
Fold 5 train - epoch: 2/5 iter: 9/15 loss: 0.9899 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 5 train - epoch: 2/5 iter: 10/15 loss: 0.9350 Acc: 43.7500% F1: 0.203 Time: 0.96s (8.60s)
Fold 5 train - epoch: 2/5 iter: 11/15 loss: 0.7863 Acc: 62.5000% F1: 0.314 Time: 0.95s (0.04s)
Fold 5 train - epoch: 2/5 iter: 12/15 loss: 0.9029 Acc: 56.2500% F1: 0.240 Time: 0.94s (9.51s)
Fold 5 train - epoch: 2/5 iter: 13/15 loss: 0.8462 Acc: 59.3750% F1: 0.336 Time: 0.94s (0.02s)
Fold 5 train - epoch: 2/5 iter: 14/15 loss: 0.2864 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 58.2222% F1: 0.3272 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8142 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 2/5 iter: 1/2 loss: 1.2024 Acc: 16.6667% F1: 0.111 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 44.0000% F1: 0.2688 *
*********************************************************
Performing epoch 3 of 5
Fold 5 train - epoch: 3/5 iter: 0/15 loss: 0.7646 Acc: 68.7500% F1: 0.623 Time: 0.96s (0.00s)
Fold 5 train - epoch: 3/5 iter: 1/15 loss: 0.7825 Acc: 71.8750% F1: 0.616 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 2/15 loss: 0.8247 Acc: 68.7500% F1: 0.479 Time: 0.96s (9.32s)
Fold 5 train - epoch: 3/5 iter: 3/15 loss: 0.7220 Acc: 68.7500% F1: 0.400 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 4/15 loss: 0.7202 Acc: 75.0000% F1: 0.706 Time: 0.94s (9.04s)
Fold 5 train - epoch: 3/5 iter: 5/15 loss: 0.7510 Acc: 68.7500% F1: 0.431 Time: 0.96s (0.04s)
Fold 5 train - epoch: 3/5 iter: 6/15 loss: 0.6551 Acc: 81.2500% F1: 0.516 Time: 0.94s (9.35s)
Fold 5 train - epoch: 3/5 iter: 7/15 loss: 0.7765 Acc: 62.5000% F1: 0.397 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 8/15 loss: 0.9274 Acc: 56.2500% F1: 0.307 Time: 0.94s (10.06s)
Fold 5 train - epoch: 3/5 iter: 9/15 loss: 0.8586 Acc: 56.2500% F1: 0.368 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 10/15 loss: 0.9006 Acc: 53.1250% F1: 0.378 Time: 0.95s (9.01s)
Fold 5 train - epoch: 3/5 iter: 11/15 loss: 0.7194 Acc: 75.0000% F1: 0.484 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 12/15 loss: 0.8435 Acc: 65.6250% F1: 0.404 Time: 0.95s (9.81s)
Fold 5 train - epoch: 3/5 iter: 13/15 loss: 0.7548 Acc: 62.5000% F1: 0.528 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 14/15 loss: 0.0615 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 66.8889% F1: 0.4973 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/5 iter: 0/2 loss: 1.0748 Acc: 40.6250% F1: 0.254 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 3/5 iter: 1/2 loss: 1.0084 Acc: 50.0000% F1: 0.382 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 44.0000% F1: 0.4545 *
*********************************************************
Performing epoch 4 of 5
Fold 5 train - epoch: 4/5 iter: 0/15 loss: 0.7032 Acc: 71.8750% F1: 0.661 Time: 0.96s (0.00s)
Fold 5 train - epoch: 4/5 iter: 1/15 loss: 0.6855 Acc: 75.0000% F1: 0.694 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/5 iter: 2/15 loss: 0.8115 Acc: 71.8750% F1: 0.622 Time: 0.95s (9.70s)
Fold 5 train - epoch: 4/5 iter: 3/15 loss: 0.6358 Acc: 71.8750% F1: 0.645 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/5 iter: 4/15 loss: 0.6345 Acc: 75.0000% F1: 0.719 Time: 0.94s (9.75s)
Fold 5 train - epoch: 4/5 iter: 5/15 loss: 0.5248 Acc: 81.2500% F1: 0.779 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/5 iter: 6/15 loss: 0.4547 Acc: 78.1250% F1: 0.482 Time: 0.95s (8.87s)
Fold 5 train - epoch: 4/5 iter: 7/15 loss: 0.7180 Acc: 65.6250% F1: 0.426 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 8/15 loss: 0.8083 Acc: 65.6250% F1: 0.501 Time: 0.94s (9.28s)
Fold 5 train - epoch: 4/5 iter: 9/15 loss: 0.6953 Acc: 78.1250% F1: 0.672 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/5 iter: 10/15 loss: 0.7818 Acc: 59.3750% F1: 0.421 Time: 0.96s (8.71s)
Fold 5 train - epoch: 4/5 iter: 11/15 loss: 0.6006 Acc: 71.8750% F1: 0.486 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 12/15 loss: 0.6842 Acc: 68.7500% F1: 0.668 Time: 0.95s (9.36s)
Fold 5 train - epoch: 4/5 iter: 13/15 loss: 0.5265 Acc: 81.2500% F1: 0.754 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/5 iter: 14/15 loss: 0.0278 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 72.6667% F1: 0.6417 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0312 Acc: 59.3750% F1: 0.330 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 4/5 iter: 1/2 loss: 1.1257 Acc: 44.4444% F1: 0.357 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.5049 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/5 iter: 0/15 loss: 1.1608 Acc: 40.6250% F1: 0.266 Time: 0.98s (0.00s)
Fold 6 train - epoch: 0/5 iter: 1/15 loss: 1.0820 Acc: 53.1250% F1: 0.283 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/5 iter: 2/15 loss: 1.0921 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.80s)
Fold 6 train - epoch: 0/5 iter: 3/15 loss: 0.8891 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 4/15 loss: 1.0647 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.72s)
Fold 6 train - epoch: 0/5 iter: 5/15 loss: 0.9345 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 6/15 loss: 0.8283 Acc: 62.5000% F1: 0.261 Time: 0.94s (8.97s)
Fold 6 train - epoch: 0/5 iter: 7/15 loss: 0.9108 Acc: 46.8750% F1: 0.244 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 8/15 loss: 1.1308 Acc: 53.1250% F1: 0.292 Time: 0.94s (9.29s)
Fold 6 train - epoch: 0/5 iter: 9/15 loss: 1.0203 Acc: 46.8750% F1: 0.280 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 10/15 loss: 0.9924 Acc: 50.0000% F1: 0.352 Time: 0.96s (9.13s)
Fold 6 train - epoch: 0/5 iter: 11/15 loss: 0.9438 Acc: 59.3750% F1: 0.378 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 12/15 loss: 1.0104 Acc: 46.8750% F1: 0.254 Time: 0.94s (9.71s)
Fold 6 train - epoch: 0/5 iter: 13/15 loss: 0.9280 Acc: 53.1250% F1: 0.306 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 14/15 loss: 0.6775 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 52.4444% F1: 0.3055 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6341 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5332 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 6 train - epoch: 1/5 iter: 0/15 loss: 0.8387 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.00s)
Fold 6 train - epoch: 1/5 iter: 1/15 loss: 0.9124 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 6 train - epoch: 1/5 iter: 2/15 loss: 0.9073 Acc: 50.0000% F1: 0.222 Time: 0.95s (8.62s)
Fold 6 train - epoch: 1/5 iter: 3/15 loss: 0.7954 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/5 iter: 4/15 loss: 0.9816 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.61s)
Fold 6 train - epoch: 1/5 iter: 5/15 loss: 0.9143 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 6/15 loss: 0.7748 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.02s)
Fold 6 train - epoch: 1/5 iter: 7/15 loss: 0.9071 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 8/15 loss: 1.1089 Acc: 53.1250% F1: 0.231 Time: 0.94s (9.28s)
Fold 6 train - epoch: 1/5 iter: 9/15 loss: 1.0016 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.26s)
Fold 6 train - epoch: 1/5 iter: 10/15 loss: 0.9877 Acc: 43.7500% F1: 0.203 Time: 0.96s (8.89s)
Fold 6 train - epoch: 1/5 iter: 11/15 loss: 0.8221 Acc: 65.6250% F1: 0.327 Time: 0.94s (0.31s)
Fold 6 train - epoch: 1/5 iter: 12/15 loss: 0.9568 Acc: 62.5000% F1: 0.361 Time: 0.95s (9.56s)
Fold 6 train - epoch: 1/5 iter: 13/15 loss: 0.9163 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.08s)
Fold 6 train - epoch: 1/5 iter: 14/15 loss: 0.6538 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 55.3333% F1: 0.2625 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7659 Acc: 71.8750% F1: 0.418 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3333 Acc: 5.5556% F1: 0.044 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.2468 *
*********************************************************
Performing epoch 2 of 5
Fold 6 train - epoch: 2/5 iter: 0/15 loss: 0.8558 Acc: 65.6250% F1: 0.500 Time: 0.96s (0.00s)
Fold 6 train - epoch: 2/5 iter: 1/15 loss: 0.8919 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/5 iter: 2/15 loss: 0.8706 Acc: 59.3750% F1: 0.362 Time: 0.95s (8.88s)
Fold 6 train - epoch: 2/5 iter: 3/15 loss: 0.7949 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/5 iter: 4/15 loss: 0.9240 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.72s)
Fold 6 train - epoch: 2/5 iter: 5/15 loss: 0.8366 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/5 iter: 6/15 loss: 0.7380 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.82s)
Fold 6 train - epoch: 2/5 iter: 7/15 loss: 0.8448 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 8/15 loss: 1.0705 Acc: 53.1250% F1: 0.241 Time: 0.94s (8.99s)
Fold 6 train - epoch: 2/5 iter: 9/15 loss: 1.0143 Acc: 43.7500% F1: 0.203 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/5 iter: 10/15 loss: 0.9360 Acc: 50.0000% F1: 0.295 Time: 0.95s (8.90s)
Fold 6 train - epoch: 2/5 iter: 11/15 loss: 0.7970 Acc: 65.6250% F1: 0.327 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 12/15 loss: 0.9074 Acc: 59.3750% F1: 0.306 Time: 0.94s (9.59s)
Fold 6 train - epoch: 2/5 iter: 13/15 loss: 0.8847 Acc: 56.2500% F1: 0.321 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/5 iter: 14/15 loss: 0.3267 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 57.7778% F1: 0.3115 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7087 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4624 Acc: 5.5556% F1: 0.044 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2618 *
*********************************************************
Performing epoch 3 of 5
Fold 6 train - epoch: 3/5 iter: 0/15 loss: 0.7811 Acc: 59.3750% F1: 0.430 Time: 0.95s (0.00s)
Fold 6 train - epoch: 3/5 iter: 1/15 loss: 0.8137 Acc: 68.7500% F1: 0.451 Time: 0.94s (0.02s)
Fold 6 train - epoch: 3/5 iter: 2/15 loss: 0.8187 Acc: 56.2500% F1: 0.320 Time: 0.96s (8.40s)
Fold 6 train - epoch: 3/5 iter: 3/15 loss: 0.7362 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.02s)
Fold 6 train - epoch: 3/5 iter: 4/15 loss: 0.8428 Acc: 53.1250% F1: 0.280 Time: 0.94s (8.27s)
Fold 6 train - epoch: 3/5 iter: 5/15 loss: 0.7264 Acc: 75.0000% F1: 0.504 Time: 0.94s (0.02s)
Fold 6 train - epoch: 3/5 iter: 6/15 loss: 0.6779 Acc: 71.8750% F1: 0.396 Time: 0.94s (8.53s)
Fold 6 train - epoch: 3/5 iter: 7/15 loss: 0.8296 Acc: 59.3750% F1: 0.379 Time: 0.94s (0.47s)
Fold 6 train - epoch: 3/5 iter: 8/15 loss: 0.9772 Acc: 56.2500% F1: 0.307 Time: 0.94s (8.25s)
Fold 6 train - epoch: 3/5 iter: 9/15 loss: 0.9017 Acc: 62.5000% F1: 0.427 Time: 0.94s (1.12s)
Fold 6 train - epoch: 3/5 iter: 10/15 loss: 0.8633 Acc: 62.5000% F1: 0.444 Time: 0.96s (7.61s)
Fold 6 train - epoch: 3/5 iter: 11/15 loss: 0.7637 Acc: 78.1250% F1: 0.533 Time: 0.94s (1.25s)
Fold 6 train - epoch: 3/5 iter: 12/15 loss: 0.8507 Acc: 59.3750% F1: 0.367 Time: 0.94s (8.89s)
Fold 6 train - epoch: 3/5 iter: 13/15 loss: 0.8276 Acc: 65.6250% F1: 0.429 Time: 0.94s (0.96s)
Fold 6 train - epoch: 3/5 iter: 14/15 loss: 0.0851 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 64.0000% F1: 0.4191 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7288 Acc: 68.7500% F1: 0.407 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 3/5 iter: 1/2 loss: 1.5392 Acc: 5.5556% F1: 0.044 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.2392 *
*********************************************************
Performing epoch 4 of 5
Fold 6 train - epoch: 4/5 iter: 0/15 loss: 0.6806 Acc: 71.8750% F1: 0.617 Time: 0.96s (0.00s)
Fold 6 train - epoch: 4/5 iter: 1/15 loss: 0.6784 Acc: 75.0000% F1: 0.628 Time: 0.95s (0.04s)
Fold 6 train - epoch: 4/5 iter: 2/15 loss: 0.7418 Acc: 56.2500% F1: 0.477 Time: 0.96s (8.69s)
Fold 6 train - epoch: 4/5 iter: 3/15 loss: 0.6076 Acc: 71.8750% F1: 0.635 Time: 0.94s (0.04s)
Fold 6 train - epoch: 4/5 iter: 4/15 loss: 0.6885 Acc: 75.0000% F1: 0.719 Time: 0.94s (8.58s)
Fold 6 train - epoch: 4/5 iter: 5/15 loss: 0.5973 Acc: 84.3750% F1: 0.794 Time: 0.94s (0.02s)
Fold 6 train - epoch: 4/5 iter: 6/15 loss: 0.5691 Acc: 75.0000% F1: 0.499 Time: 0.94s (9.02s)
Fold 6 train - epoch: 4/5 iter: 7/15 loss: 0.6633 Acc: 75.0000% F1: 0.506 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 8/15 loss: 0.8406 Acc: 62.5000% F1: 0.403 Time: 0.94s (10.01s)
Fold 6 train - epoch: 4/5 iter: 9/15 loss: 0.7803 Acc: 65.6250% F1: 0.562 Time: 0.94s (0.02s)
Fold 6 train - epoch: 4/5 iter: 10/15 loss: 0.7788 Acc: 71.8750% F1: 0.512 Time: 0.96s (9.17s)
Fold 6 train - epoch: 4/5 iter: 11/15 loss: 0.5168 Acc: 81.2500% F1: 0.705 Time: 0.94s (0.02s)
Fold 6 train - epoch: 4/5 iter: 12/15 loss: 0.7239 Acc: 75.0000% F1: 0.521 Time: 0.94s (9.53s)
Fold 6 train - epoch: 4/5 iter: 13/15 loss: 0.7456 Acc: 68.7500% F1: 0.575 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 14/15 loss: 0.0256 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 72.2222% F1: 0.6019 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8366 Acc: 68.7500% F1: 0.367 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7267 Acc: 22.2222% F1: 0.213 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 52.0000% F1: 0.4094 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/5 iter: 0/15 loss: 1.1517 Acc: 37.5000% F1: 0.249 Time: 0.97s (0.00s)
Fold 7 train - epoch: 0/5 iter: 1/15 loss: 0.9784 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.03s)
Fold 7 train - epoch: 0/5 iter: 2/15 loss: 1.1101 Acc: 46.8750% F1: 0.213 Time: 0.95s (8.88s)
Fold 7 train - epoch: 0/5 iter: 3/15 loss: 0.9053 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 7 train - epoch: 0/5 iter: 4/15 loss: 1.1041 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.36s)
Fold 7 train - epoch: 0/5 iter: 5/15 loss: 0.9513 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 7 train - epoch: 0/5 iter: 6/15 loss: 0.7927 Acc: 71.8750% F1: 0.351 Time: 0.94s (8.60s)
Fold 7 train - epoch: 0/5 iter: 7/15 loss: 0.9149 Acc: 50.0000% F1: 0.282 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 8/15 loss: 1.0775 Acc: 46.8750% F1: 0.263 Time: 0.94s (8.57s)
Fold 7 train - epoch: 0/5 iter: 9/15 loss: 1.0211 Acc: 50.0000% F1: 0.332 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 10/15 loss: 1.0025 Acc: 43.7500% F1: 0.310 Time: 0.96s (9.26s)
Fold 7 train - epoch: 0/5 iter: 11/15 loss: 0.9265 Acc: 56.2500% F1: 0.343 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 12/15 loss: 1.0215 Acc: 53.1250% F1: 0.333 Time: 0.95s (8.97s)
Fold 7 train - epoch: 0/5 iter: 13/15 loss: 0.9292 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 14/15 loss: 0.6687 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 53.1111% F1: 0.3168 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6279 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 0/5 iter: 1/2 loss: 1.4799 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 7 train - epoch: 1/5 iter: 0/15 loss: 0.8637 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 7 train - epoch: 1/5 iter: 1/15 loss: 0.9048 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 7 train - epoch: 1/5 iter: 2/15 loss: 0.9608 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.60s)
Fold 7 train - epoch: 1/5 iter: 3/15 loss: 0.8246 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 7 train - epoch: 1/5 iter: 4/15 loss: 0.9839 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.46s)
Fold 7 train - epoch: 1/5 iter: 5/15 loss: 0.9259 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 7 train - epoch: 1/5 iter: 6/15 loss: 0.7158 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.72s)
Fold 7 train - epoch: 1/5 iter: 7/15 loss: 0.9253 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 7 train - epoch: 1/5 iter: 8/15 loss: 1.0440 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.71s)
Fold 7 train - epoch: 1/5 iter: 9/15 loss: 1.0112 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 10/15 loss: 1.0010 Acc: 46.8750% F1: 0.213 Time: 0.96s (9.30s)
Fold 7 train - epoch: 1/5 iter: 11/15 loss: 0.8210 Acc: 65.6250% F1: 0.269 Time: 0.94s (0.02s)
Fold 7 train - epoch: 1/5 iter: 12/15 loss: 0.9696 Acc: 59.3750% F1: 0.306 Time: 0.95s (9.04s)
Fold 7 train - epoch: 1/5 iter: 13/15 loss: 0.9160 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 14/15 loss: 0.5261 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 55.3333% F1: 0.2418 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7402 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2981 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 50.0000% F1: 0.2252 *
*********************************************************
Performing epoch 2 of 5
Fold 7 train - epoch: 2/5 iter: 0/15 loss: 0.8680 Acc: 59.3750% F1: 0.378 Time: 0.96s (0.00s)
Fold 7 train - epoch: 2/5 iter: 1/15 loss: 0.8970 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 2/15 loss: 0.8931 Acc: 53.1250% F1: 0.275 Time: 0.96s (8.73s)
Fold 7 train - epoch: 2/5 iter: 3/15 loss: 0.7796 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 4/15 loss: 0.8938 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.55s)
Fold 7 train - epoch: 2/5 iter: 5/15 loss: 0.9122 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 6/15 loss: 0.7676 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.88s)
Fold 7 train - epoch: 2/5 iter: 7/15 loss: 0.8111 Acc: 50.0000% F1: 0.257 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 8/15 loss: 1.0543 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.69s)
Fold 7 train - epoch: 2/5 iter: 9/15 loss: 0.9650 Acc: 50.0000% F1: 0.265 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 10/15 loss: 0.9830 Acc: 46.8750% F1: 0.252 Time: 0.96s (9.47s)
Fold 7 train - epoch: 2/5 iter: 11/15 loss: 0.7975 Acc: 71.8750% F1: 0.401 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 12/15 loss: 0.9623 Acc: 59.3750% F1: 0.306 Time: 0.94s (9.05s)
Fold 7 train - epoch: 2/5 iter: 13/15 loss: 0.8707 Acc: 50.0000% F1: 0.262 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 14/15 loss: 0.2100 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 57.3333% F1: 0.3006 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7385 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3156 Acc: 27.7778% F1: 0.175 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3513 *
*********************************************************
Performing epoch 3 of 5
Fold 7 train - epoch: 3/5 iter: 0/15 loss: 0.7752 Acc: 65.6250% F1: 0.500 Time: 0.95s (0.00s)
Fold 7 train - epoch: 3/5 iter: 1/15 loss: 0.8092 Acc: 62.5000% F1: 0.482 Time: 0.94s (0.02s)
Fold 7 train - epoch: 3/5 iter: 2/15 loss: 0.7825 Acc: 62.5000% F1: 0.534 Time: 0.96s (9.04s)
Fold 7 train - epoch: 3/5 iter: 3/15 loss: 0.7514 Acc: 71.8750% F1: 0.635 Time: 0.94s (0.02s)
Fold 7 train - epoch: 3/5 iter: 4/15 loss: 0.8050 Acc: 56.2500% F1: 0.350 Time: 0.94s (8.72s)
Fold 7 train - epoch: 3/5 iter: 5/15 loss: 0.8001 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.02s)
Fold 7 train - epoch: 3/5 iter: 6/15 loss: 0.6670 Acc: 71.8750% F1: 0.351 Time: 0.94s (9.11s)
Fold 7 train - epoch: 3/5 iter: 7/15 loss: 0.7759 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 8/15 loss: 0.9799 Acc: 56.2500% F1: 0.307 Time: 0.94s (8.98s)
Fold 7 train - epoch: 3/5 iter: 9/15 loss: 0.8482 Acc: 56.2500% F1: 0.352 Time: 0.94s (0.02s)
Fold 7 train - epoch: 3/5 iter: 10/15 loss: 0.8969 Acc: 53.1250% F1: 0.362 Time: 0.95s (9.72s)
Fold 7 train - epoch: 3/5 iter: 11/15 loss: 0.7122 Acc: 75.0000% F1: 0.484 Time: 0.95s (0.04s)
Fold 7 train - epoch: 3/5 iter: 12/15 loss: 0.8678 Acc: 53.1250% F1: 0.362 Time: 0.94s (9.70s)
Fold 7 train - epoch: 3/5 iter: 13/15 loss: 0.8142 Acc: 62.5000% F1: 0.420 Time: 0.94s (0.02s)
Fold 7 train - epoch: 3/5 iter: 14/15 loss: 0.0820 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 62.2222% F1: 0.4314 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9143 Acc: 53.1250% F1: 0.301 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 3/5 iter: 1/2 loss: 1.2183 Acc: 44.4444% F1: 0.317 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.4202 *
*********************************************************
Performing epoch 4 of 5
Fold 7 train - epoch: 4/5 iter: 0/15 loss: 0.7337 Acc: 65.6250% F1: 0.624 Time: 0.96s (0.00s)
Fold 7 train - epoch: 4/5 iter: 1/15 loss: 0.7118 Acc: 68.7500% F1: 0.635 Time: 0.94s (0.02s)
Fold 7 train - epoch: 4/5 iter: 2/15 loss: 0.7078 Acc: 71.8750% F1: 0.685 Time: 0.95s (8.64s)
Fold 7 train - epoch: 4/5 iter: 3/15 loss: 0.5661 Acc: 81.2500% F1: 0.830 Time: 0.94s (0.02s)
Fold 7 train - epoch: 4/5 iter: 4/15 loss: 0.6230 Acc: 71.8750% F1: 0.713 Time: 0.94s (8.57s)
Fold 7 train - epoch: 4/5 iter: 5/15 loss: 0.7579 Acc: 75.0000% F1: 0.513 Time: 0.95s (0.57s)
Fold 7 train - epoch: 4/5 iter: 6/15 loss: 0.5040 Acc: 78.1250% F1: 0.469 Time: 0.94s (8.34s)
Fold 7 train - epoch: 4/5 iter: 7/15 loss: 0.6598 Acc: 71.8750% F1: 0.476 Time: 0.94s (1.14s)
Fold 7 train - epoch: 4/5 iter: 8/15 loss: 0.8330 Acc: 65.6250% F1: 0.443 Time: 0.94s (8.12s)
Fold 7 train - epoch: 4/5 iter: 9/15 loss: 0.7498 Acc: 59.3750% F1: 0.420 Time: 0.94s (2.16s)
Fold 7 train - epoch: 4/5 iter: 10/15 loss: 0.7537 Acc: 68.7500% F1: 0.488 Time: 0.96s (8.11s)
Fold 7 train - epoch: 4/5 iter: 11/15 loss: 0.6529 Acc: 75.0000% F1: 0.669 Time: 0.94s (2.02s)
Fold 7 train - epoch: 4/5 iter: 12/15 loss: 0.7656 Acc: 65.6250% F1: 0.581 Time: 0.94s (7.59s)
Fold 7 train - epoch: 4/5 iter: 13/15 loss: 0.5991 Acc: 81.2500% F1: 0.791 Time: 0.94s (1.51s)
Fold 7 train - epoch: 4/5 iter: 14/15 loss: 0.0169 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 71.5556% F1: 0.6260 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/5 iter: 0/2 loss: 0.7967 Acc: 65.6250% F1: 0.359 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 4/5 iter: 1/2 loss: 1.4874 Acc: 33.3333% F1: 0.211 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.3763 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/5 iter: 0/15 loss: 1.1586 Acc: 43.7500% F1: 0.406 Time: 0.97s (0.00s)
Fold 8 train - epoch: 0/5 iter: 1/15 loss: 0.9858 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 8 train - epoch: 0/5 iter: 2/15 loss: 1.1406 Acc: 50.0000% F1: 0.222 Time: 0.95s (9.87s)
Fold 8 train - epoch: 0/5 iter: 3/15 loss: 0.9278 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 4/15 loss: 1.0094 Acc: 50.0000% F1: 0.222 Time: 0.94s (10.13s)
Fold 8 train - epoch: 0/5 iter: 5/15 loss: 0.9619 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.04s)
Fold 8 train - epoch: 0/5 iter: 6/15 loss: 0.8667 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.25s)
Fold 8 train - epoch: 0/5 iter: 7/15 loss: 0.9261 Acc: 50.0000% F1: 0.284 Time: 0.94s (0.02s)
Fold 8 train - epoch: 0/5 iter: 8/15 loss: 1.1243 Acc: 46.8750% F1: 0.263 Time: 0.94s (9.39s)
Fold 8 train - epoch: 0/5 iter: 9/15 loss: 1.0093 Acc: 40.6250% F1: 0.202 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 10/15 loss: 0.9984 Acc: 43.7500% F1: 0.283 Time: 0.96s (10.17s)
Fold 8 train - epoch: 0/5 iter: 11/15 loss: 0.8848 Acc: 59.3750% F1: 0.360 Time: 0.94s (0.02s)
Fold 8 train - epoch: 0/5 iter: 12/15 loss: 0.9840 Acc: 50.0000% F1: 0.295 Time: 0.95s (9.48s)
Fold 8 train - epoch: 0/5 iter: 13/15 loss: 0.9285 Acc: 53.1250% F1: 0.275 Time: 0.94s (0.02s)
Fold 8 train - epoch: 0/5 iter: 14/15 loss: 0.5923 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 52.4444% F1: 0.3034 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6183 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5577 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 8 train - epoch: 1/5 iter: 0/15 loss: 0.8876 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.00s)
Fold 8 train - epoch: 1/5 iter: 1/15 loss: 0.8982 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.28s)
Fold 8 train - epoch: 1/5 iter: 2/15 loss: 0.9272 Acc: 50.0000% F1: 0.222 Time: 0.96s (8.05s)
Fold 8 train - epoch: 1/5 iter: 3/15 loss: 0.8403 Acc: 59.3750% F1: 0.248 Time: 0.94s (1.28s)
Fold 8 train - epoch: 1/5 iter: 4/15 loss: 0.9718 Acc: 50.0000% F1: 0.222 Time: 0.94s (6.83s)
Fold 8 train - epoch: 1/5 iter: 5/15 loss: 0.9307 Acc: 59.3750% F1: 0.248 Time: 0.94s (2.17s)
Fold 8 train - epoch: 1/5 iter: 6/15 loss: 0.7569 Acc: 68.7500% F1: 0.272 Time: 0.94s (5.98s)
Fold 8 train - epoch: 1/5 iter: 7/15 loss: 0.9295 Acc: 46.8750% F1: 0.213 Time: 0.95s (2.88s)
Fold 8 train - epoch: 1/5 iter: 8/15 loss: 1.0207 Acc: 53.1250% F1: 0.231 Time: 0.94s (5.21s)
Fold 8 train - epoch: 1/5 iter: 9/15 loss: 0.9974 Acc: 46.8750% F1: 0.213 Time: 0.94s (4.23s)
Fold 8 train - epoch: 1/5 iter: 10/15 loss: 0.9944 Acc: 46.8750% F1: 0.217 Time: 0.95s (4.99s)
Fold 8 train - epoch: 1/5 iter: 11/15 loss: 0.8300 Acc: 65.6250% F1: 0.264 Time: 0.95s (3.74s)
Fold 8 train - epoch: 1/5 iter: 12/15 loss: 0.9506 Acc: 59.3750% F1: 0.306 Time: 0.94s (4.80s)
Fold 8 train - epoch: 1/5 iter: 13/15 loss: 0.9211 Acc: 56.2500% F1: 0.287 Time: 0.94s (2.75s)
Fold 8 train - epoch: 1/5 iter: 14/15 loss: 0.4919 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 55.7778% F1: 0.2510 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7109 Acc: 87.5000% F1: 0.632 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3598 Acc: 5.5556% F1: 0.051 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.3102 *
*********************************************************
Performing epoch 2 of 5
Fold 8 train - epoch: 2/5 iter: 0/15 loss: 0.8319 Acc: 59.3750% F1: 0.378 Time: 0.96s (0.00s)
Fold 8 train - epoch: 2/5 iter: 1/15 loss: 0.8838 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.06s)
Fold 8 train - epoch: 2/5 iter: 2/15 loss: 0.8721 Acc: 53.1250% F1: 0.276 Time: 0.96s (8.50s)
Fold 8 train - epoch: 2/5 iter: 3/15 loss: 0.8049 Acc: 59.3750% F1: 0.248 Time: 0.95s (1.61s)
Fold 8 train - epoch: 2/5 iter: 4/15 loss: 0.9195 Acc: 50.0000% F1: 0.222 Time: 0.94s (7.07s)
Fold 8 train - epoch: 2/5 iter: 5/15 loss: 0.8380 Acc: 56.2500% F1: 0.240 Time: 0.95s (2.85s)
Fold 8 train - epoch: 2/5 iter: 6/15 loss: 0.7018 Acc: 68.7500% F1: 0.272 Time: 0.95s (5.57s)
Fold 8 train - epoch: 2/5 iter: 7/15 loss: 0.8860 Acc: 46.8750% F1: 0.213 Time: 0.95s (3.66s)
Fold 8 train - epoch: 2/5 iter: 8/15 loss: 1.0381 Acc: 53.1250% F1: 0.236 Time: 0.94s (4.78s)
Fold 8 train - epoch: 2/5 iter: 9/15 loss: 0.9805 Acc: 50.0000% F1: 0.265 Time: 0.95s (5.16s)
Fold 8 train - epoch: 2/5 iter: 10/15 loss: 0.9270 Acc: 53.1250% F1: 0.311 Time: 0.96s (4.45s)
Fold 8 train - epoch: 2/5 iter: 11/15 loss: 0.7848 Acc: 71.8750% F1: 0.401 Time: 0.95s (4.82s)
Fold 8 train - epoch: 2/5 iter: 12/15 loss: 0.8497 Acc: 65.6250% F1: 0.430 Time: 0.94s (4.04s)
Fold 8 train - epoch: 2/5 iter: 13/15 loss: 0.8818 Acc: 50.0000% F1: 0.262 Time: 0.94s (4.23s)
Fold 8 train - epoch: 2/5 iter: 14/15 loss: 0.2003 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 57.1111% F1: 0.2999 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7355 Acc: 68.7500% F1: 0.397 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 2/5 iter: 1/2 loss: 1.2413 Acc: 50.0000% F1: 0.261 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 62.0000% F1: 0.4408 *
*********************************************************
Performing epoch 3 of 5
Fold 8 train - epoch: 3/5 iter: 0/15 loss: 0.7564 Acc: 68.7500% F1: 0.544 Time: 0.96s (0.00s)
Fold 8 train - epoch: 3/5 iter: 1/15 loss: 0.8279 Acc: 71.8750% F1: 0.607 Time: 0.95s (0.04s)
Fold 8 train - epoch: 3/5 iter: 2/15 loss: 0.8346 Acc: 59.3750% F1: 0.634 Time: 0.96s (8.53s)
Fold 8 train - epoch: 3/5 iter: 3/15 loss: 0.7207 Acc: 75.0000% F1: 0.675 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 4/15 loss: 0.7704 Acc: 56.2500% F1: 0.349 Time: 0.95s (8.54s)
Fold 8 train - epoch: 3/5 iter: 5/15 loss: 0.7581 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.04s)
Fold 8 train - epoch: 3/5 iter: 6/15 loss: 0.6646 Acc: 71.8750% F1: 0.351 Time: 0.94s (8.42s)
Fold 8 train - epoch: 3/5 iter: 7/15 loss: 0.8685 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 8/15 loss: 0.9794 Acc: 62.5000% F1: 0.419 Time: 0.94s (8.45s)
Fold 8 train - epoch: 3/5 iter: 9/15 loss: 0.9052 Acc: 53.1250% F1: 0.349 Time: 0.95s (0.30s)
Fold 8 train - epoch: 3/5 iter: 10/15 loss: 0.8099 Acc: 62.5000% F1: 0.443 Time: 0.96s (9.18s)
Fold 8 train - epoch: 3/5 iter: 11/15 loss: 0.7674 Acc: 68.7500% F1: 0.703 Time: 0.95s (0.04s)
Fold 8 train - epoch: 3/5 iter: 12/15 loss: 0.8110 Acc: 65.6250% F1: 0.455 Time: 0.95s (8.83s)
Fold 8 train - epoch: 3/5 iter: 13/15 loss: 0.8321 Acc: 46.8750% F1: 0.455 Time: 0.94s (0.03s)
Fold 8 train - epoch: 3/5 iter: 14/15 loss: 0.0557 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 62.6667% F1: 0.4931 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7075 Acc: 68.7500% F1: 0.433 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 3/5 iter: 1/2 loss: 1.2569 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.4116 *
*********************************************************
Performing epoch 4 of 5
Fold 8 train - epoch: 4/5 iter: 0/15 loss: 0.6065 Acc: 81.2500% F1: 0.804 Time: 0.96s (0.00s)
Fold 8 train - epoch: 4/5 iter: 1/15 loss: 0.6486 Acc: 81.2500% F1: 0.765 Time: 0.94s (0.02s)
Fold 8 train - epoch: 4/5 iter: 2/15 loss: 0.7222 Acc: 65.6250% F1: 0.637 Time: 0.96s (8.21s)
Fold 8 train - epoch: 4/5 iter: 3/15 loss: 0.6428 Acc: 62.5000% F1: 0.508 Time: 0.94s (0.31s)
Fold 8 train - epoch: 4/5 iter: 4/15 loss: 0.5978 Acc: 75.0000% F1: 0.744 Time: 0.94s (7.86s)
Fold 8 train - epoch: 4/5 iter: 5/15 loss: 0.6418 Acc: 68.7500% F1: 0.543 Time: 0.94s (1.03s)
Fold 8 train - epoch: 4/5 iter: 6/15 loss: 0.5569 Acc: 71.8750% F1: 0.426 Time: 0.95s (7.00s)
Fold 8 train - epoch: 4/5 iter: 7/15 loss: 0.7087 Acc: 65.6250% F1: 0.438 Time: 0.94s (1.77s)
Fold 8 train - epoch: 4/5 iter: 8/15 loss: 0.7958 Acc: 65.6250% F1: 0.571 Time: 0.94s (6.25s)
Fold 8 train - epoch: 4/5 iter: 9/15 loss: 0.7016 Acc: 75.0000% F1: 0.776 Time: 0.95s (3.09s)
Fold 8 train - epoch: 4/5 iter: 10/15 loss: 0.6776 Acc: 65.6250% F1: 0.469 Time: 0.96s (6.11s)
Fold 8 train - epoch: 4/5 iter: 11/15 loss: 0.6228 Acc: 78.1250% F1: 0.690 Time: 0.94s (2.68s)
Fold 8 train - epoch: 4/5 iter: 12/15 loss: 0.6399 Acc: 75.0000% F1: 0.690 Time: 0.94s (5.70s)
Fold 8 train - epoch: 4/5 iter: 13/15 loss: 0.5729 Acc: 75.0000% F1: 0.682 Time: 0.94s (2.03s)
Fold 8 train - epoch: 4/5 iter: 14/15 loss: 0.0206 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 72.0000% F1: 0.6611 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/5 iter: 0/2 loss: 0.5720 Acc: 75.0000% F1: 0.436 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5492 Acc: 27.7778% F1: 0.185 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 58.0000% F1: 0.4133 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.9576e-02,  1.4462e-02,  2.1089e-02, -2.4213e-02, -3.1370e-02,
         -1.4199e-02, -2.5904e-02, -5.0341e-02, -3.6889e-02, -3.2728e-02,
         -6.8087e-04, -3.5233e-02, -3.9792e-02, -2.0962e-02, -7.1844e-03,
         -9.9852e-03, -2.8043e-02, -2.2781e-02, -4.4245e-03, -4.3463e-02,
         -4.1087e-02, -4.5657e-02, -1.9222e-02, -7.8390e-03, -2.0070e-02,
         -3.9880e-02,  1.8200e-02, -2.9268e-02, -1.6676e-02, -2.1610e-02,
         -2.6654e-02, -2.5254e-02,  9.2145e-03, -4.5557e-02, -3.3439e-02,
         -7.2401e-03, -2.6082e-02,  1.0227e-02, -1.4157e-02, -1.4822e-02,
         -4.3798e-02, -4.1658e-02, -1.4709e-02, -2.5902e-02, -8.7081e-03,
         -3.9880e-02, -1.5114e-02, -1.3367e-02, -3.1035e-02, -3.6392e-02,
          1.2227e-02, -8.7005e-03, -1.9950e-02, -1.3439e-02, -3.7631e-02,
         -3.0060e-02, -1.7583e-03, -4.8765e-02,  1.0541e-03, -1.3689e-02,
         -2.3786e-02,  8.0220e-03, -3.0173e-02, -2.5209e-02, -2.2526e-02,
         -1.9942e-02,  8.4238e-03, -1.9661e-02, -1.2119e-02, -3.4659e-03,
         -3.1803e-02, -3.2865e-03, -1.0539e-02, -3.9146e-03, -4.3804e-02,
          2.2994e-02,  1.2197e-02, -4.6647e-02, -3.5887e-02, -1.4468e-02,
         -3.3255e-03, -1.7044e-02,  2.0406e-02,  1.0396e-02, -2.0522e-02,
         -3.4509e-02, -4.8150e-02, -5.4708e-03,  3.8065e-02, -5.5523e-02,
          3.8816e-03, -6.3457e-02, -2.4241e-02, -9.3510e-03, -1.5897e-02,
         -8.8885e-03, -1.4690e-02, -5.4748e-03, -9.9003e-03, -7.0308e-03,
          3.4854e-03, -6.2360e-04,  1.0930e-02,  2.8643e-02, -1.1186e-02,
         -2.6669e-02, -1.9786e-02,  1.7164e-02, -1.3477e-02, -4.4371e-02,
         -1.4301e-02, -1.3759e-02, -4.0720e-02, -2.1774e-02,  1.6330e-02,
         -2.6441e-02, -4.0575e-02, -3.9297e-02, -9.2287e-03, -2.3262e-02,
         -5.1135e-03,  1.7898e-02,  1.0295e-02, -4.4888e-02, -1.1444e-02,
         -6.7636e-05,  3.0896e-02, -2.1793e-02, -2.7886e-02,  7.3077e-04,
         -7.0966e-03, -2.8983e-02, -2.4230e-02, -4.3455e-03, -3.6372e-02,
         -1.4622e-02, -1.9455e-02, -6.2994e-03, -4.8066e-02,  1.2134e-03,
         -2.6100e-02, -2.2397e-02, -4.3888e-02, -4.5778e-03, -4.2798e-02,
         -4.1344e-03, -4.0403e-02, -1.4517e-02, -7.4589e-03, -2.9797e-02,
         -4.2728e-02, -5.6681e-02, -7.0879e-03,  7.4267e-03,  2.2647e-02,
         -2.2768e-02,  7.6049e-04, -3.0197e-02, -5.4283e-03,  9.8245e-03,
         -1.2252e-03, -2.3723e-02,  3.4096e-02, -1.4257e-02, -3.2253e-02,
         -1.8886e-02, -5.4247e-03, -1.1375e-02, -9.0462e-03, -1.1843e-02,
          1.3237e-02, -2.4774e-02, -2.8040e-02, -3.8893e-02, -2.9173e-02,
         -3.3035e-02, -5.6146e-03, -2.8491e-02, -1.4185e-02, -1.6383e-02,
          1.3736e-03, -2.7760e-02, -4.3374e-03,  1.7261e-02, -2.0601e-02,
         -1.9357e-03, -1.5073e-02, -9.6997e-03, -1.1300e-02, -4.2982e-02,
          8.4574e-03, -3.8737e-02, -1.5231e-02, -2.3515e-02, -1.3568e-02,
          3.3063e-03, -3.3534e-02, -8.0107e-03, -3.6351e-02, -3.9080e-02,
         -8.1655e-03, -8.9546e-03,  4.9051e-03, -6.2526e-03, -4.5883e-02,
         -2.0797e-02, -3.6205e-02, -8.1598e-03, -2.8076e-02, -8.8447e-03,
         -1.9260e-02,  1.1396e-02,  1.5052e-02, -1.6075e-02, -1.5518e-02,
         -1.7724e-02,  2.0844e-02, -2.6688e-02, -3.4171e-02, -2.8748e-02,
          7.2581e-03, -1.6509e-02, -2.8037e-02, -2.2494e-02, -6.6422e-03,
         -3.8462e-02, -4.8222e-02, -4.5790e-02, -1.1145e-02,  4.5953e-02,
         -1.1314e-04,  1.1308e-03, -4.2939e-02, -2.3548e-02,  1.3748e-02,
          4.0744e-03,  8.6413e-03, -1.0417e-03, -5.3433e-02, -4.8686e-03,
          2.5878e-02, -1.2791e-02, -1.8810e-02, -1.7619e-02, -9.7876e-03,
         -2.5285e-02, -2.1384e-02,  1.4755e-02, -3.9157e-02,  1.8993e-02,
          5.0377e-03, -2.9328e-02, -1.6048e-02, -3.0006e-02, -2.8478e-02,
         -8.5162e-03, -1.2241e-02, -1.8590e-02, -2.9219e-03, -2.3340e-02,
         -2.1185e-02, -1.4722e-02, -1.4868e-02, -1.5969e-02, -2.2238e-02,
         -2.0746e-02,  9.0726e-03,  1.1734e-02, -3.8070e-02,  9.3740e-04,
         -3.4404e-02, -5.3068e-02,  7.4766e-03, -3.3690e-02, -2.3907e-02,
         -1.1922e-02, -3.2821e-02, -4.6094e-02,  4.7032e-04, -4.1886e-02,
         -5.7862e-03, -2.4299e-02, -2.8858e-02,  1.1723e-02, -3.0093e-02,
         -4.6471e-02, -4.9816e-03, -2.3448e-03, -2.2391e-02, -3.6809e-02,
         -3.7831e-02,  8.4370e-03, -4.2447e-02, -1.9412e-02, -3.9165e-02,
         -2.3906e-02,  7.2951e-03, -4.2961e-02,  8.2211e-03, -2.2970e-02,
         -1.4029e-02,  1.2471e-03,  3.8190e-02, -5.0079e-03, -4.4626e-02,
         -1.3463e-02, -1.6834e-02,  3.9899e-04, -1.4055e-02, -3.9695e-02,
         -3.6972e-02, -1.7081e-02, -3.0984e-02, -1.9453e-02, -1.2163e-05,
         -1.2716e-02, -9.5281e-03, -2.4193e-02, -3.0648e-02,  8.4034e-03,
         -9.2663e-03, -4.4070e-02, -2.4568e-02, -1.5319e-02, -5.6182e-03,
          6.3644e-03, -1.5009e-02,  1.2869e-02, -7.8058e-03,  5.4362e-03,
          2.2388e-03, -2.6862e-02, -1.5712e-03, -1.8940e-02,  1.3159e-02,
         -1.3461e-02, -3.9902e-02, -5.0591e-02, -1.7753e-02, -1.3713e-02,
          8.0693e-03, -3.5620e-02, -3.3676e-02, -1.3030e-02, -1.6888e-02,
         -2.5140e-02, -3.4833e-02, -7.2791e-03, -3.2956e-02, -2.4693e-02,
         -1.1791e-02, -1.2542e-02, -1.6742e-02, -4.3013e-02, -1.3429e-02,
          1.2613e-03, -4.0043e-02, -1.1774e-02, -3.2534e-02, -3.4204e-02,
          8.6550e-03, -8.3946e-03, -3.6755e-02, -3.3793e-02, -2.7884e-02,
         -7.4445e-03, -1.8833e-03,  1.2610e-02,  9.8851e-03,  1.2191e-02,
         -3.6666e-02, -8.5410e-02, -2.8082e-02, -3.0407e-02, -1.9044e-02,
          2.4342e-03, -1.4180e-03, -2.6470e-02, -2.5302e-02, -2.5253e-02,
         -1.1512e-02, -2.8345e-02, -1.5937e-02,  9.6349e-03, -1.0218e-02,
          1.3560e-02,  1.5391e-02, -1.9179e-02,  1.5864e-02, -1.0996e-02,
         -1.0817e-02,  2.6805e-03, -3.1092e-02, -3.8091e-02, -2.3368e-02,
         -5.9125e-02, -1.0858e-02,  4.5047e-03, -1.8238e-02, -2.1426e-02,
         -1.7773e-02, -2.9463e-02, -3.7479e-03, -4.9064e-03, -1.5818e-02,
         -2.5526e-02, -1.4419e-03, -1.1299e-02,  8.5140e-03, -1.0698e-02,
         -1.1708e-02, -2.0608e-02, -1.4593e-02, -7.7355e-03, -7.3931e-03,
         -4.0843e-02, -2.1167e-02, -1.2983e-02, -2.9919e-03,  3.0490e-03,
         -4.7598e-03, -7.2291e-02, -9.2503e-03, -3.9466e-02, -4.7297e-02,
         -2.1130e-02, -3.5272e-02,  2.0385e-02,  5.3947e-03, -2.0301e-02,
         -9.7881e-03, -1.9998e-02, -6.5725e-02, -4.3420e-02, -3.1609e-02,
         -1.8267e-04, -3.4371e-03, -9.8789e-04,  9.6274e-03, -5.9154e-03,
         -3.4675e-02, -6.5226e-02, -2.0677e-02, -4.1714e-02,  2.1622e-02,
          2.0240e-02, -9.2820e-03, -1.7129e-02, -3.2057e-02, -3.1468e-02,
         -3.4941e-02,  2.3813e-02, -1.6644e-02, -5.0642e-02, -1.6989e-02,
         -2.9996e-02, -2.5404e-02, -2.8092e-02, -4.2349e-02, -3.9435e-02,
         -4.4279e-02, -2.2080e-02,  3.3196e-03, -1.1349e-02,  6.9159e-03,
         -3.3969e-02, -5.3818e-02,  4.0196e-02, -3.8763e-02,  1.4048e-02,
          5.9220e-04, -2.6749e-02, -1.3595e-03, -4.5813e-02, -6.8420e-03,
         -3.4372e-02, -1.8214e-02,  4.0495e-03,  3.8787e-06, -1.8598e-02,
          9.7004e-03, -2.1482e-02, -1.9200e-02, -4.7962e-02, -3.4414e-02,
         -3.2698e-02, -2.5277e-02, -3.4527e-02,  1.4325e-02, -3.6852e-02,
          4.9736e-03, -4.6518e-02, -1.2696e-02, -4.6521e-02, -3.6610e-02,
         -2.4398e-02, -3.2662e-02, -2.5911e-03, -5.8092e-02,  2.9025e-02,
         -2.1087e-02, -1.7487e-02, -3.1389e-02, -4.2529e-02, -1.0128e-02,
         -1.7335e-02,  2.8430e-02,  3.9904e-02,  8.0061e-04, -3.1003e-02,
         -7.1200e-02, -2.8507e-03, -5.9049e-02, -1.5139e-02,  5.0690e-03,
         -1.6833e-02, -9.0752e-03, -2.3263e-02, -2.1165e-02, -4.3393e-02,
         -8.7630e-03, -1.3716e-02, -2.7933e-02, -4.8012e-02, -1.4200e-02,
         -5.1839e-02, -6.0081e-03, -1.8939e-02, -1.7114e-02, -4.1764e-02,
         -5.2397e-03, -5.4814e-03, -2.0793e-02, -4.0475e-02, -4.4624e-02,
         -7.9084e-03, -3.5071e-03,  2.0474e-02, -2.6987e-02, -1.4414e-02,
         -1.9035e-02, -4.9324e-02, -2.8345e-02,  1.4301e-02, -3.8973e-02,
         -1.2840e-02, -7.5258e-03, -4.6916e-02, -4.7070e-02, -1.3649e-02,
         -5.4320e-02, -1.8288e-03, -1.8652e-03, -2.7375e-02,  1.1466e-02,
         -4.0584e-02, -1.2032e-02, -3.9389e-02, -2.1145e-02,  2.4070e-03,
          9.3934e-03, -3.9291e-02, -5.9169e-02, -4.5224e-03, -1.4565e-02,
         -2.3315e-02, -3.0413e-02, -1.4424e-02, -1.6966e-02,  1.0072e-02,
          3.6617e-02, -3.9734e-02, -3.2399e-02, -1.3331e-02, -2.1236e-02,
          2.2823e-02, -1.9419e-02, -4.0604e-03, -1.3728e-02, -1.8695e-02,
         -2.6998e-02, -1.7203e-02, -6.4163e-03, -1.3199e-02, -5.5872e-02,
         -3.5325e-02, -3.1695e-02, -4.9119e-02,  4.0982e-02,  1.8657e-02,
         -3.3573e-02, -8.9048e-03,  1.6459e-02, -3.6141e-02, -3.5255e-02,
         -2.0351e-02,  3.6519e-02, -1.1686e-02,  2.1606e-02,  1.8557e-02,
         -4.5092e-03, -9.0398e-03,  2.1982e-02, -3.4125e-02,  1.8775e-02,
         -5.1349e-02, -2.0664e-03, -1.0744e-02, -1.6671e-02,  2.6162e-02,
         -6.4523e-03, -5.7046e-02, -3.6390e-03, -1.9814e-02, -1.6118e-02,
          6.7051e-03, -5.2483e-03,  8.4542e-04, -2.6400e-02, -6.1003e-03,
         -1.2574e-02, -3.9921e-02, -2.9327e-02, -2.4630e-02,  8.4810e-03,
         -7.4100e-04, -1.8916e-02,  5.1565e-03,  2.2242e-02, -9.2054e-03,
         -5.8127e-02, -1.7812e-03, -1.1566e-02, -1.5581e-02, -2.4738e-02,
         -8.5652e-03, -1.4644e-02, -1.7377e-02, -7.8154e-03,  8.7701e-03,
         -1.5758e-02, -2.9299e-02, -5.7990e-02,  5.2301e-03, -1.7015e-03,
         -1.0382e-02, -6.0044e-03, -2.6569e-02, -1.6470e-03,  1.8650e-03,
          1.0926e-02, -2.2354e-02,  1.7114e-02, -7.1626e-03, -4.9205e-02,
         -1.3529e-02,  2.1432e-02, -1.5802e-02, -2.3511e-02, -1.6134e-02,
         -3.9555e-02,  2.8150e-02, -9.2762e-03, -8.4562e-03, -2.7197e-02,
         -7.0721e-03, -4.0385e-03, -4.7109e-02, -5.2178e-03, -4.8849e-02,
          1.6391e-02, -3.0436e-02, -2.0354e-02, -1.7472e-02, -1.0822e-02,
         -3.9952e-02, -3.1226e-02, -5.1212e-02,  1.2849e-02, -4.3737e-02,
         -1.6600e-02, -1.7369e-02, -3.5132e-02, -7.9695e-03, -1.9029e-02,
         -2.9353e-02, -2.5525e-02, -1.2254e-02,  1.0975e-03, -4.0945e-02,
         -1.6960e-02, -4.7521e-03, -1.3975e-02, -1.2778e-02, -2.7528e-02,
         -3.3573e-02, -4.6857e-03, -4.8495e-03, -1.5761e-02, -1.7572e-02,
         -1.5772e-02, -7.3530e-03, -3.1005e-02, -1.7780e-02, -1.4125e-02,
         -2.0564e-02, -1.9258e-02, -1.5787e-02, -2.5870e-02,  8.4195e-03,
          1.2669e-02, -1.9583e-02, -1.6337e-02, -1.2081e-02,  7.1074e-03,
         -4.5319e-02, -2.1659e-02, -1.2411e-02, -2.7327e-02, -1.0128e-02,
         -3.7986e-02, -1.6327e-02, -5.8103e-03, -1.7206e-02, -1.4065e-02,
          2.7339e-02, -2.2147e-02, -3.9057e-02, -8.3084e-03, -2.7950e-02,
         -1.3844e-02, -1.8129e-02, -4.8987e-02,  1.5473e-02, -2.5392e-02,
         -3.8005e-02, -4.3275e-02, -1.0993e-02, -5.9326e-03, -6.6074e-02,
         -4.9379e-03, -2.7258e-02, -2.5302e-02, -3.8126e-02, -3.0043e-02,
         -8.6250e-03, -4.3281e-03, -5.0636e-02, -5.5711e-02, -2.7074e-02,
         -3.6164e-02, -1.2752e-02, -1.9970e-02, -2.1273e-02, -5.8114e-03,
         -6.4127e-02, -2.3056e-02,  2.3241e-03, -3.5551e-02, -1.8637e-02,
         -2.2455e-02, -3.8589e-02,  3.4994e-03, -5.7992e-03,  9.8726e-03,
         -4.2325e-02, -3.3603e-02,  9.3079e-03]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/5 iter: 0/15 loss: 1.1010 Acc: 34.3750% F1: 0.239 Time: 0.99s (0.00s)
Fold 9 train - epoch: 0/5 iter: 1/15 loss: 1.0553 Acc: 53.1250% F1: 0.278 Time: 0.94s (0.04s)
Fold 9 train - epoch: 0/5 iter: 2/15 loss: 1.1256 Acc: 46.8750% F1: 0.255 Time: 0.96s (9.31s)
Fold 9 train - epoch: 0/5 iter: 3/15 loss: 0.9071 Acc: 62.5000% F1: 0.261 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/5 iter: 4/15 loss: 1.0634 Acc: 50.0000% F1: 0.222 Time: 0.94s (9.11s)
Fold 9 train - epoch: 0/5 iter: 5/15 loss: 0.9161 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/5 iter: 6/15 loss: 0.8078 Acc: 68.7500% F1: 0.336 Time: 0.94s (8.77s)
Fold 9 train - epoch: 0/5 iter: 7/15 loss: 1.0142 Acc: 40.6250% F1: 0.224 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/5 iter: 8/15 loss: 1.0824 Acc: 50.0000% F1: 0.227 Time: 0.94s (9.24s)
Fold 9 train - epoch: 0/5 iter: 9/15 loss: 1.0966 Acc: 43.7500% F1: 0.243 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 10/15 loss: 1.0764 Acc: 37.5000% F1: 0.238 Time: 0.96s (10.78s)
Fold 9 train - epoch: 0/5 iter: 11/15 loss: 0.8530 Acc: 62.5000% F1: 0.345 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/5 iter: 12/15 loss: 0.9319 Acc: 56.2500% F1: 0.370 Time: 0.94s (9.03s)
Fold 9 train - epoch: 0/5 iter: 13/15 loss: 0.9234 Acc: 56.2500% F1: 0.287 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/5 iter: 14/15 loss: 0.6333 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 51.5556% F1: 0.2909 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6908 Acc: 75.0000% F1: 0.429 Time: 0.35s (0.00s)
Fold 9 train-dev - epoch: 0/5 iter: 1/2 loss: 1.1183 Acc: 27.7778% F1: 0.145 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 58.0000% F1: 0.2447 *
*********************************************************
Performing epoch 1 of 5
Fold 9 train - epoch: 1/5 iter: 0/15 loss: 0.9417 Acc: 50.0000% F1: 0.222 Time: 0.96s (0.00s)
Fold 9 train - epoch: 1/5 iter: 1/15 loss: 0.9239 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.03s)
Fold 9 train - epoch: 1/5 iter: 2/15 loss: 0.9750 Acc: 46.8750% F1: 0.213 Time: 0.95s (9.11s)
Fold 9 train - epoch: 1/5 iter: 3/15 loss: 0.7617 Acc: 62.5000% F1: 0.256 Time: 0.94s (0.03s)
Fold 9 train - epoch: 1/5 iter: 4/15 loss: 0.9765 Acc: 50.0000% F1: 0.222 Time: 0.94s (8.54s)
Fold 9 train - epoch: 1/5 iter: 5/15 loss: 0.8825 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 9 train - epoch: 1/5 iter: 6/15 loss: 0.7016 Acc: 68.7500% F1: 0.272 Time: 0.94s (8.19s)
Fold 9 train - epoch: 1/5 iter: 7/15 loss: 0.9697 Acc: 43.7500% F1: 0.203 Time: 0.94s (0.03s)
Fold 9 train - epoch: 1/5 iter: 8/15 loss: 1.0467 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.87s)
Fold 9 train - epoch: 1/5 iter: 9/15 loss: 1.0086 Acc: 46.8750% F1: 0.213 Time: 0.94s (1.26s)
Fold 9 train - epoch: 1/5 iter: 10/15 loss: 1.0174 Acc: 40.6250% F1: 0.193 Time: 0.96s (9.10s)
Fold 9 train - epoch: 1/5 iter: 11/15 loss: 0.8085 Acc: 71.8750% F1: 0.396 Time: 0.95s (0.85s)
Fold 9 train - epoch: 1/5 iter: 12/15 loss: 0.9292 Acc: 59.3750% F1: 0.370 Time: 0.94s (8.12s)
Fold 9 train - epoch: 1/5 iter: 13/15 loss: 0.8996 Acc: 53.1250% F1: 0.275 Time: 0.94s (0.31s)
Fold 9 train - epoch: 1/5 iter: 14/15 loss: 0.5962 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 54.4444% F1: 0.2577 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7561 Acc: 68.7500% F1: 0.407 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 1/5 iter: 1/2 loss: 1.0356 Acc: 27.7778% F1: 0.152 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.2368 *
*********************************************************
Performing epoch 2 of 5
Fold 9 train - epoch: 2/5 iter: 0/15 loss: 0.8914 Acc: 56.2500% F1: 0.399 Time: 0.96s (0.00s)
Fold 9 train - epoch: 2/5 iter: 1/15 loss: 0.9030 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/5 iter: 2/15 loss: 0.8989 Acc: 53.1250% F1: 0.311 Time: 0.96s (9.15s)
Fold 9 train - epoch: 2/5 iter: 3/15 loss: 0.7583 Acc: 62.5000% F1: 0.261 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/5 iter: 4/15 loss: 0.9096 Acc: 56.2500% F1: 0.334 Time: 0.94s (9.30s)
Fold 9 train - epoch: 2/5 iter: 5/15 loss: 0.8507 Acc: 62.5000% F1: 0.320 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/5 iter: 6/15 loss: 0.7162 Acc: 68.7500% F1: 0.272 Time: 0.94s (9.12s)
Fold 9 train - epoch: 2/5 iter: 7/15 loss: 0.8925 Acc: 43.7500% F1: 0.203 Time: 0.94s (0.84s)
Fold 9 train - epoch: 2/5 iter: 8/15 loss: 1.0218 Acc: 53.1250% F1: 0.231 Time: 0.94s (8.43s)
Fold 9 train - epoch: 2/5 iter: 9/15 loss: 1.0180 Acc: 46.8750% F1: 0.257 Time: 0.94s (2.25s)
Fold 9 train - epoch: 2/5 iter: 10/15 loss: 1.0201 Acc: 56.2500% F1: 0.361 Time: 0.95s (7.71s)
Fold 9 train - epoch: 2/5 iter: 11/15 loss: 0.7132 Acc: 75.0000% F1: 0.461 Time: 0.95s (1.99s)
Fold 9 train - epoch: 2/5 iter: 12/15 loss: 0.8681 Acc: 62.5000% F1: 0.385 Time: 0.94s (7.35s)
Fold 9 train - epoch: 2/5 iter: 13/15 loss: 0.8622 Acc: 46.8750% F1: 0.452 Time: 0.94s (1.64s)
Fold 9 train - epoch: 2/5 iter: 14/15 loss: 0.2145 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 57.3333% F1: 0.3371 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8347 Acc: 59.3750% F1: 0.366 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 2/5 iter: 1/2 loss: 1.0578 Acc: 33.3333% F1: 0.235 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.3261 *
*********************************************************
Performing epoch 3 of 5
Fold 9 train - epoch: 3/5 iter: 0/15 loss: 0.7623 Acc: 68.7500% F1: 0.559 Time: 0.95s (0.00s)
Fold 9 train - epoch: 3/5 iter: 1/15 loss: 0.8211 Acc: 68.7500% F1: 0.583 Time: 0.94s (0.03s)
Fold 9 train - epoch: 3/5 iter: 2/15 loss: 0.8195 Acc: 65.6250% F1: 0.615 Time: 0.96s (9.22s)
Fold 9 train - epoch: 3/5 iter: 3/15 loss: 0.6512 Acc: 75.0000% F1: 0.654 Time: 0.94s (0.03s)
Fold 9 train - epoch: 3/5 iter: 4/15 loss: 0.7661 Acc: 59.3750% F1: 0.472 Time: 0.94s (9.49s)
Fold 9 train - epoch: 3/5 iter: 5/15 loss: 0.7122 Acc: 68.7500% F1: 0.569 Time: 0.94s (0.02s)
Fold 9 train - epoch: 3/5 iter: 6/15 loss: 0.5619 Acc: 75.0000% F1: 0.403 Time: 0.94s (8.20s)
Fold 9 train - epoch: 3/5 iter: 7/15 loss: 0.8805 Acc: 46.8750% F1: 0.273 Time: 0.94s (0.02s)
Fold 9 train - epoch: 3/5 iter: 8/15 loss: 0.9140 Acc: 59.3750% F1: 0.391 Time: 0.94s (8.22s)
Fold 9 train - epoch: 3/5 iter: 9/15 loss: 0.8237 Acc: 59.3750% F1: 0.406 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 10/15 loss: 0.9091 Acc: 59.3750% F1: 0.419 Time: 0.96s (9.19s)
Fold 9 train - epoch: 3/5 iter: 11/15 loss: 0.6717 Acc: 68.7500% F1: 0.614 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 12/15 loss: 0.7977 Acc: 68.7500% F1: 0.657 Time: 0.94s (8.68s)
Fold 9 train - epoch: 3/5 iter: 13/15 loss: 0.8097 Acc: 56.2500% F1: 0.485 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 14/15 loss: 0.0387 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 64.4444% F1: 0.5173 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9240 Acc: 50.0000% F1: 0.344 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 3/5 iter: 1/2 loss: 1.1832 Acc: 33.3333% F1: 0.258 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 44.0000% F1: 0.3185 *
*********************************************************
Performing epoch 4 of 5
Fold 9 train - epoch: 4/5 iter: 0/15 loss: 0.6120 Acc: 71.8750% F1: 0.662 Time: 0.96s (0.00s)
Fold 9 train - epoch: 4/5 iter: 1/15 loss: 0.6722 Acc: 75.0000% F1: 0.683 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 2/15 loss: 0.7191 Acc: 68.7500% F1: 0.668 Time: 0.96s (9.43s)
Fold 9 train - epoch: 4/5 iter: 3/15 loss: 0.5454 Acc: 84.3750% F1: 0.852 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 4/15 loss: 0.7037 Acc: 65.6250% F1: 0.604 Time: 0.95s (9.19s)
Fold 9 train - epoch: 4/5 iter: 5/15 loss: 0.6110 Acc: 71.8750% F1: 0.664 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 6/15 loss: 0.4334 Acc: 78.1250% F1: 0.491 Time: 0.94s (8.83s)
Fold 9 train - epoch: 4/5 iter: 7/15 loss: 0.7007 Acc: 62.5000% F1: 0.420 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 8/15 loss: 0.6907 Acc: 68.7500% F1: 0.553 Time: 0.94s (8.90s)
Fold 9 train - epoch: 4/5 iter: 9/15 loss: 0.8051 Acc: 56.2500% F1: 0.494 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 10/15 loss: 0.6521 Acc: 59.3750% F1: 0.429 Time: 0.96s (9.84s)
Fold 9 train - epoch: 4/5 iter: 11/15 loss: 0.5237 Acc: 84.3750% F1: 0.556 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 12/15 loss: 0.6637 Acc: 75.0000% F1: 0.690 Time: 0.94s (9.17s)
Fold 9 train - epoch: 4/5 iter: 13/15 loss: 0.5432 Acc: 84.3750% F1: 0.844 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/5 iter: 14/15 loss: 0.0113 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 72.0000% F1: 0.6457 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8914 Acc: 59.3750% F1: 0.386 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 4/5 iter: 1/2 loss: 1.2877 Acc: 27.7778% F1: 0.213 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3287 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 54.2000% F1: 0.2471
	Epoch 1 Accuracy: 52.2000% F1: 0.2944
	Epoch 2 Accuracy: 51.0000% F1: 0.3333
	Epoch 3 Accuracy: 45.4000% F1: 0.3459
	Epoch 4 Accuracy: 50.0000% F1: 0.3936
all done :)
************************************
** MODEL TIME ID: 20220225-135033 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/5 iter: 0/15 loss: 1.1194 Acc: 34.3750% F1: 0.240 Time: 0.97s (0.00s)
Fold 0 train - epoch: 0/5 iter: 1/15 loss: 0.9795 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 2/15 loss: 1.1970 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 0 train - epoch: 0/5 iter: 3/15 loss: 0.9058 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 0 train - epoch: 0/5 iter: 4/15 loss: 0.9357 Acc: 56.2500% F1: 0.327 Time: 0.94s (0.02s)
Fold 0 train - epoch: 0/5 iter: 5/15 loss: 0.9469 Acc: 46.8750% F1: 0.256 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 6/15 loss: 0.9261 Acc: 46.8750% F1: 0.305 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 7/15 loss: 0.8334 Acc: 46.8750% F1: 0.287 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 8/15 loss: 1.0737 Acc: 46.8750% F1: 0.261 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 9/15 loss: 0.9490 Acc: 62.5000% F1: 0.430 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 10/15 loss: 1.0163 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 11/15 loss: 0.9046 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 12/15 loss: 1.0789 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 13/15 loss: 0.9127 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 14/15 loss: 0.5472 Acc: 100.0000% F1: 1.000 Time: 0.10s (0.02s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 51.7778% F1: 0.2898 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6138 Acc: 84.3750% F1: 0.458 Time: 0.36s (0.00s)
Fold 0 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6619 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 0 train - epoch: 1/5 iter: 0/15 loss: 0.8723 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.00s)
Fold 0 train - epoch: 1/5 iter: 1/15 loss: 0.8499 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 0 train - epoch: 1/5 iter: 2/15 loss: 0.9852 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 0 train - epoch: 1/5 iter: 3/15 loss: 0.8655 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 0 train - epoch: 1/5 iter: 4/15 loss: 0.9858 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 5/15 loss: 0.9170 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 6/15 loss: 0.7802 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 7/15 loss: 0.8972 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 8/15 loss: 0.9736 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 9/15 loss: 0.9304 Acc: 50.0000% F1: 0.267 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 10/15 loss: 0.9247 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 11/15 loss: 0.8534 Acc: 59.3750% F1: 0.301 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 12/15 loss: 0.9583 Acc: 59.3750% F1: 0.395 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 13/15 loss: 0.9040 Acc: 50.0000% F1: 0.312 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 14/15 loss: 0.6055 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 55.3333% F1: 0.2771 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7215 Acc: 68.7500% F1: 0.407 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4521 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 44.0000% F1: 0.2066 *
*********************************************************
Performing epoch 2 of 5
Fold 0 train - epoch: 2/5 iter: 0/15 loss: 0.8540 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.00s)
Fold 0 train - epoch: 2/5 iter: 1/15 loss: 0.8224 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.02s)
Fold 0 train - epoch: 2/5 iter: 2/15 loss: 0.9117 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 3/15 loss: 0.7944 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 0 train - epoch: 2/5 iter: 4/15 loss: 0.9238 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 5/15 loss: 0.8860 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 6/15 loss: 0.7478 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 7/15 loss: 0.8816 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 8/15 loss: 0.9092 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 9/15 loss: 0.9123 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 10/15 loss: 0.9010 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 11/15 loss: 0.8233 Acc: 62.5000% F1: 0.314 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 12/15 loss: 0.9038 Acc: 56.2500% F1: 0.332 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 13/15 loss: 0.9140 Acc: 53.1250% F1: 0.333 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 14/15 loss: 0.4222 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 55.5556% F1: 0.2753 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7942 Acc: 75.0000% F1: 0.590 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4572 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.3215 *
*********************************************************
Performing epoch 3 of 5
Fold 0 train - epoch: 3/5 iter: 0/15 loss: 0.8001 Acc: 65.6250% F1: 0.409 Time: 0.95s (0.00s)
Fold 0 train - epoch: 3/5 iter: 1/15 loss: 0.7304 Acc: 68.7500% F1: 0.569 Time: 0.94s (0.02s)
Fold 0 train - epoch: 3/5 iter: 2/15 loss: 0.8589 Acc: 50.0000% F1: 0.262 Time: 0.94s (0.02s)
Fold 0 train - epoch: 3/5 iter: 3/15 loss: 0.7051 Acc: 62.5000% F1: 0.309 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 4/15 loss: 0.8242 Acc: 59.3750% F1: 0.466 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 5/15 loss: 0.7649 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 6/15 loss: 0.6613 Acc: 71.8750% F1: 0.396 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 7/15 loss: 0.8388 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 8/15 loss: 0.7973 Acc: 68.7500% F1: 0.584 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 9/15 loss: 0.7538 Acc: 81.2500% F1: 0.694 Time: 0.95s (0.03s)
Fold 0 train - epoch: 3/5 iter: 10/15 loss: 0.7421 Acc: 71.8750% F1: 0.617 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 11/15 loss: 0.7365 Acc: 84.3750% F1: 0.714 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 12/15 loss: 0.7804 Acc: 59.3750% F1: 0.559 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 13/15 loss: 0.7955 Acc: 56.2500% F1: 0.420 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 14/15 loss: 0.1221 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 65.1111% F1: 0.5123 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7352 Acc: 71.8750% F1: 0.343 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 3/5 iter: 1/2 loss: 2.0639 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.2705 *
*********************************************************
Performing epoch 4 of 5
Fold 0 train - epoch: 4/5 iter: 0/15 loss: 0.6982 Acc: 65.6250% F1: 0.500 Time: 0.95s (0.00s)
Fold 0 train - epoch: 4/5 iter: 1/15 loss: 0.5487 Acc: 84.3750% F1: 0.859 Time: 0.95s (0.04s)
Fold 0 train - epoch: 4/5 iter: 2/15 loss: 0.8271 Acc: 53.1250% F1: 0.365 Time: 0.95s (0.03s)
Fold 0 train - epoch: 4/5 iter: 3/15 loss: 0.5568 Acc: 81.2500% F1: 0.830 Time: 0.95s (0.03s)
Fold 0 train - epoch: 4/5 iter: 4/15 loss: 0.5700 Acc: 78.1250% F1: 0.716 Time: 0.94s (0.02s)
Fold 0 train - epoch: 4/5 iter: 5/15 loss: 0.6009 Acc: 78.1250% F1: 0.814 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 6/15 loss: 0.5994 Acc: 71.8750% F1: 0.576 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 7/15 loss: 0.6441 Acc: 71.8750% F1: 0.485 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 8/15 loss: 0.7254 Acc: 68.7500% F1: 0.615 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 9/15 loss: 0.6452 Acc: 81.2500% F1: 0.699 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 10/15 loss: 0.7726 Acc: 59.3750% F1: 0.417 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 11/15 loss: 0.4517 Acc: 84.3750% F1: 0.801 Time: 0.96s (0.02s)
Fold 0 train - epoch: 4/5 iter: 12/15 loss: 0.5996 Acc: 71.8750% F1: 0.647 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 13/15 loss: 0.5027 Acc: 81.2500% F1: 0.782 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 14/15 loss: 0.0282 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 73.7778% F1: 0.6780 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/5 iter: 0/2 loss: 1.4996 Acc: 31.2500% F1: 0.229 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 4/5 iter: 1/2 loss: 1.8028 Acc: 22.2222% F1: 0.178 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 28.0000% F1: 0.2489 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/5 iter: 0/15 loss: 1.0313 Acc: 40.6250% F1: 0.293 Time: 0.96s (0.00s)
Fold 1 train - epoch: 0/5 iter: 1/15 loss: 1.0488 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 2/15 loss: 1.1231 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 3/15 loss: 0.9659 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 4/15 loss: 0.9293 Acc: 43.7500% F1: 0.239 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 5/15 loss: 0.9661 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 6/15 loss: 0.9364 Acc: 59.3750% F1: 0.386 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 7/15 loss: 0.8540 Acc: 56.2500% F1: 0.367 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 8/15 loss: 1.0924 Acc: 53.1250% F1: 0.344 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 9/15 loss: 0.9945 Acc: 56.2500% F1: 0.368 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 10/15 loss: 0.9687 Acc: 50.0000% F1: 0.295 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 11/15 loss: 0.9372 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 12/15 loss: 1.0127 Acc: 50.0000% F1: 0.269 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 13/15 loss: 0.9259 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 14/15 loss: 0.5900 Acc: 100.0000% F1: 1.000 Time: 0.10s (0.03s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 52.0000% F1: 0.3052 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6030 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/5 iter: 1/2 loss: 1.7314 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 1 train - epoch: 1/5 iter: 0/15 loss: 0.8336 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.00s)
Fold 1 train - epoch: 1/5 iter: 1/15 loss: 0.8754 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 2/15 loss: 0.9824 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 3/15 loss: 0.8628 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.04s)
Fold 1 train - epoch: 1/5 iter: 4/15 loss: 0.9712 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 5/15 loss: 0.9228 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 6/15 loss: 0.7893 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 7/15 loss: 0.9196 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 8/15 loss: 0.9228 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 9/15 loss: 0.9681 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 10/15 loss: 0.9481 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 11/15 loss: 0.9031 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 12/15 loss: 0.9790 Acc: 46.8750% F1: 0.255 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 13/15 loss: 0.8830 Acc: 53.1250% F1: 0.343 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 14/15 loss: 0.7174 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 53.7778% F1: 0.2619 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7488 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4297 Acc: 16.6667% F1: 0.118 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 50.0000% F1: 0.3073 *
*********************************************************
Performing epoch 2 of 5
Fold 1 train - epoch: 2/5 iter: 0/15 loss: 0.8470 Acc: 56.2500% F1: 0.350 Time: 0.94s (0.00s)
Fold 1 train - epoch: 2/5 iter: 1/15 loss: 0.8314 Acc: 62.5000% F1: 0.389 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 2/15 loss: 0.9190 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 3/15 loss: 0.7841 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 4/15 loss: 0.9077 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 5/15 loss: 0.9369 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 6/15 loss: 0.7599 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 7/15 loss: 0.9033 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 8/15 loss: 0.9229 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 9/15 loss: 0.9954 Acc: 50.0000% F1: 0.267 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 10/15 loss: 0.9439 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 11/15 loss: 0.8296 Acc: 62.5000% F1: 0.261 Time: 0.96s (0.03s)
Fold 1 train - epoch: 2/5 iter: 12/15 loss: 0.8679 Acc: 59.3750% F1: 0.352 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 13/15 loss: 0.8827 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 14/15 loss: 0.4185 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 55.5556% F1: 0.2717 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7628 Acc: 59.3750% F1: 0.300 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4537 Acc: 11.1111% F1: 0.083 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 42.0000% F1: 0.2619 *
*********************************************************
Performing epoch 3 of 5
Fold 1 train - epoch: 3/5 iter: 0/15 loss: 0.7607 Acc: 75.0000% F1: 0.733 Time: 0.94s (0.00s)
Fold 1 train - epoch: 3/5 iter: 1/15 loss: 0.7285 Acc: 71.8750% F1: 0.664 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 2/15 loss: 0.8802 Acc: 59.3750% F1: 0.362 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 3/15 loss: 0.7612 Acc: 59.3750% F1: 0.296 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 4/15 loss: 0.8123 Acc: 59.3750% F1: 0.389 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 5/15 loss: 0.8842 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 6/15 loss: 0.6424 Acc: 75.0000% F1: 0.415 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 7/15 loss: 0.8084 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 8/15 loss: 0.8893 Acc: 65.6250% F1: 0.502 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 9/15 loss: 0.8487 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 10/15 loss: 0.8662 Acc: 53.1250% F1: 0.457 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 11/15 loss: 0.7563 Acc: 62.5000% F1: 0.377 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 12/15 loss: 0.8509 Acc: 62.5000% F1: 0.563 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 13/15 loss: 0.8511 Acc: 59.3750% F1: 0.465 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 14/15 loss: 0.2457 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 62.2222% F1: 0.4760 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7669 Acc: 59.3750% F1: 0.311 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 3/5 iter: 1/2 loss: 1.7275 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 38.0000% F1: 0.2234 *
*********************************************************
Performing epoch 4 of 5
Fold 1 train - epoch: 4/5 iter: 0/15 loss: 0.6263 Acc: 81.2500% F1: 0.765 Time: 0.95s (0.00s)
Fold 1 train - epoch: 4/5 iter: 1/15 loss: 0.6157 Acc: 71.8750% F1: 0.727 Time: 0.95s (0.04s)
Fold 1 train - epoch: 4/5 iter: 2/15 loss: 0.7622 Acc: 59.3750% F1: 0.493 Time: 0.95s (0.04s)
Fold 1 train - epoch: 4/5 iter: 3/15 loss: 0.6449 Acc: 65.6250% F1: 0.573 Time: 0.95s (0.04s)
Fold 1 train - epoch: 4/5 iter: 4/15 loss: 0.5900 Acc: 75.0000% F1: 0.715 Time: 0.95s (0.04s)
Fold 1 train - epoch: 4/5 iter: 5/15 loss: 0.7242 Acc: 71.8750% F1: 0.599 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 6/15 loss: 0.6191 Acc: 71.8750% F1: 0.652 Time: 0.95s (0.04s)
Fold 1 train - epoch: 4/5 iter: 7/15 loss: 0.6616 Acc: 59.3750% F1: 0.710 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 8/15 loss: 0.7588 Acc: 68.7500% F1: 0.657 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 9/15 loss: 0.6927 Acc: 65.6250% F1: 0.465 Time: 0.95s (0.04s)
Fold 1 train - epoch: 4/5 iter: 10/15 loss: 0.6733 Acc: 75.0000% F1: 0.730 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 11/15 loss: 0.6881 Acc: 71.8750% F1: 0.744 Time: 0.96s (0.03s)
Fold 1 train - epoch: 4/5 iter: 12/15 loss: 0.7563 Acc: 62.5000% F1: 0.564 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 13/15 loss: 0.7702 Acc: 62.5000% F1: 0.498 Time: 0.96s (0.02s)
Fold 1 train - epoch: 4/5 iter: 14/15 loss: 0.0312 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 68.8889% F1: 0.6469 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8076 Acc: 65.6250% F1: 0.346 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 4/5 iter: 1/2 loss: 2.0132 Acc: 11.1111% F1: 0.114 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3182 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/5 iter: 0/15 loss: 0.9701 Acc: 37.5000% F1: 0.264 Time: 0.96s (0.00s)
Fold 2 train - epoch: 0/5 iter: 1/15 loss: 1.0886 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 2/15 loss: 1.0724 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 3/15 loss: 0.9374 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 4/15 loss: 0.8593 Acc: 65.6250% F1: 0.559 Time: 0.94s (0.02s)
Fold 2 train - epoch: 0/5 iter: 5/15 loss: 0.9844 Acc: 46.8750% F1: 0.325 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 6/15 loss: 0.8979 Acc: 50.0000% F1: 0.322 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 7/15 loss: 0.8479 Acc: 50.0000% F1: 0.316 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 8/15 loss: 1.2010 Acc: 50.0000% F1: 0.280 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 9/15 loss: 0.9852 Acc: 56.2500% F1: 0.352 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 10/15 loss: 1.0030 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 11/15 loss: 0.9215 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 12/15 loss: 0.9910 Acc: 56.2500% F1: 0.297 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 13/15 loss: 0.9719 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.02s)
Fold 2 train - epoch: 0/5 iter: 14/15 loss: 0.6999 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 52.2222% F1: 0.3146 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6110 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 0/5 iter: 1/2 loss: 1.4890 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2424 *
*********************************************************
Performing epoch 1 of 5
Fold 2 train - epoch: 1/5 iter: 0/15 loss: 0.8248 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.00s)
Fold 2 train - epoch: 1/5 iter: 1/15 loss: 0.8466 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.04s)
Fold 2 train - epoch: 1/5 iter: 2/15 loss: 0.9692 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 3/15 loss: 0.8715 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 2 train - epoch: 1/5 iter: 4/15 loss: 0.9242 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 5/15 loss: 0.9642 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 6/15 loss: 0.7790 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 7/15 loss: 0.8810 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 8/15 loss: 1.0385 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 9/15 loss: 1.0122 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 10/15 loss: 0.9519 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 11/15 loss: 0.8850 Acc: 59.3750% F1: 0.301 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 12/15 loss: 0.9858 Acc: 59.3750% F1: 0.413 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 13/15 loss: 0.9173 Acc: 46.8750% F1: 0.249 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 14/15 loss: 0.6544 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 54.8889% F1: 0.2751 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7423 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2578 Acc: 11.1111% F1: 0.078 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 50.0000% F1: 0.2903 *
*********************************************************
Performing epoch 2 of 5
Fold 2 train - epoch: 2/5 iter: 0/15 loss: 0.8350 Acc: 65.6250% F1: 0.500 Time: 0.95s (0.00s)
Fold 2 train - epoch: 2/5 iter: 1/15 loss: 0.8069 Acc: 59.3750% F1: 0.344 Time: 0.95s (0.05s)
Fold 2 train - epoch: 2/5 iter: 2/15 loss: 0.9002 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 3/15 loss: 0.7764 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 4/15 loss: 0.8889 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 5/15 loss: 0.8940 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 6/15 loss: 0.7043 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 7/15 loss: 0.8926 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 8/15 loss: 1.0118 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 9/15 loss: 1.0121 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 10/15 loss: 0.8855 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 11/15 loss: 0.8088 Acc: 62.5000% F1: 0.314 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 12/15 loss: 0.9016 Acc: 56.2500% F1: 0.358 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 13/15 loss: 0.8783 Acc: 56.2500% F1: 0.289 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 14/15 loss: 0.4315 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 55.7778% F1: 0.2851 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7526 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 2/5 iter: 1/2 loss: 1.2847 Acc: 11.1111% F1: 0.078 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.2718 *
*********************************************************
Performing epoch 3 of 5
Fold 2 train - epoch: 3/5 iter: 0/15 loss: 0.7475 Acc: 71.8750% F1: 0.584 Time: 0.95s (0.00s)
Fold 2 train - epoch: 3/5 iter: 1/15 loss: 0.7228 Acc: 75.0000% F1: 0.695 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 2/15 loss: 0.8414 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 3/15 loss: 0.7429 Acc: 59.3750% F1: 0.296 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 4/15 loss: 0.7288 Acc: 62.5000% F1: 0.409 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 5/15 loss: 0.8101 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 6/15 loss: 0.6672 Acc: 71.8750% F1: 0.351 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 7/15 loss: 0.7889 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 8/15 loss: 0.9805 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 9/15 loss: 0.9191 Acc: 62.5000% F1: 0.433 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 10/15 loss: 0.8010 Acc: 71.8750% F1: 0.616 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 11/15 loss: 0.7751 Acc: 75.0000% F1: 0.654 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 12/15 loss: 0.8549 Acc: 56.2500% F1: 0.513 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 13/15 loss: 0.8369 Acc: 62.5000% F1: 0.464 Time: 0.96s (0.03s)
Fold 2 train - epoch: 3/5 iter: 14/15 loss: 0.2173 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 64.2222% F1: 0.4849 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6275 Acc: 75.0000% F1: 0.358 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6615 Acc: 16.6667% F1: 0.179 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.3922 *
*********************************************************
Performing epoch 4 of 5
Fold 2 train - epoch: 4/5 iter: 0/15 loss: 0.6645 Acc: 68.7500% F1: 0.607 Time: 0.94s (0.00s)
Fold 2 train - epoch: 4/5 iter: 1/15 loss: 0.5877 Acc: 78.1250% F1: 0.788 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 2/15 loss: 0.7570 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 3/15 loss: 0.6299 Acc: 59.3750% F1: 0.543 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 4/15 loss: 0.5710 Acc: 78.1250% F1: 0.742 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 5/15 loss: 0.6891 Acc: 68.7500% F1: 0.439 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 6/15 loss: 0.6054 Acc: 81.2500% F1: 0.727 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 7/15 loss: 0.7054 Acc: 62.5000% F1: 0.423 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 8/15 loss: 0.8464 Acc: 59.3750% F1: 0.569 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 9/15 loss: 0.7317 Acc: 75.0000% F1: 0.652 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 10/15 loss: 0.7029 Acc: 68.7500% F1: 0.667 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 11/15 loss: 0.5315 Acc: 81.2500% F1: 0.674 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 12/15 loss: 0.7213 Acc: 65.6250% F1: 0.576 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 13/15 loss: 0.6913 Acc: 65.6250% F1: 0.525 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 14/15 loss: 0.0661 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 69.3333% F1: 0.6308 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0307 Acc: 50.0000% F1: 0.323 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5447 Acc: 16.6667% F1: 0.162 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 38.0000% F1: 0.3033 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/5 iter: 0/15 loss: 1.0168 Acc: 43.7500% F1: 0.296 Time: 0.95s (0.00s)
Fold 3 train - epoch: 0/5 iter: 1/15 loss: 1.0475 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 2/15 loss: 1.0845 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 3/15 loss: 0.9164 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.02s)
Fold 3 train - epoch: 0/5 iter: 4/15 loss: 0.9169 Acc: 59.3750% F1: 0.400 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 5/15 loss: 0.9909 Acc: 43.7500% F1: 0.310 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 6/15 loss: 0.9015 Acc: 46.8750% F1: 0.319 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 7/15 loss: 0.9087 Acc: 43.7500% F1: 0.254 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 8/15 loss: 1.1381 Acc: 53.1250% F1: 0.294 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 9/15 loss: 1.0465 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 10/15 loss: 0.9964 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 11/15 loss: 0.9146 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 12/15 loss: 0.9620 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 13/15 loss: 0.8990 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 14/15 loss: 0.8406 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 51.3333% F1: 0.2902 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5707 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6389 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 3 train - epoch: 1/5 iter: 0/15 loss: 0.8360 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.00s)
Fold 3 train - epoch: 1/5 iter: 1/15 loss: 0.8725 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 2/15 loss: 0.9933 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 3/15 loss: 0.8118 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 4/15 loss: 0.9634 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 5/15 loss: 0.9158 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 6/15 loss: 0.7429 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.04s)
Fold 3 train - epoch: 1/5 iter: 7/15 loss: 0.8976 Acc: 46.8750% F1: 0.213 Time: 0.96s (0.04s)
Fold 3 train - epoch: 1/5 iter: 8/15 loss: 1.0688 Acc: 53.1250% F1: 0.231 Time: 0.96s (0.04s)
Fold 3 train - epoch: 1/5 iter: 9/15 loss: 0.9828 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 3 train - epoch: 1/5 iter: 10/15 loss: 0.9414 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 11/15 loss: 0.8776 Acc: 62.5000% F1: 0.314 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 12/15 loss: 0.9333 Acc: 56.2500% F1: 0.327 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 13/15 loss: 0.9258 Acc: 59.3750% F1: 0.336 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 14/15 loss: 0.7666 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 55.3333% F1: 0.2569 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7013 Acc: 84.3750% F1: 0.677 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3684 Acc: 5.5556% F1: 0.048 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 56.0000% F1: 0.3217 *
*********************************************************
Performing epoch 2 of 5
Fold 3 train - epoch: 2/5 iter: 0/15 loss: 0.8222 Acc: 65.6250% F1: 0.500 Time: 0.94s (0.00s)
Fold 3 train - epoch: 2/5 iter: 1/15 loss: 0.8362 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.02s)
Fold 3 train - epoch: 2/5 iter: 2/15 loss: 0.8885 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 3/15 loss: 0.8022 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 4/15 loss: 0.8689 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 5/15 loss: 0.8929 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 6/15 loss: 0.7419 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 7/15 loss: 0.8680 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 8/15 loss: 1.0626 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 9/15 loss: 1.0056 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 10/15 loss: 0.9507 Acc: 46.8750% F1: 0.213 Time: 0.96s (0.02s)
Fold 3 train - epoch: 2/5 iter: 11/15 loss: 0.8193 Acc: 65.6250% F1: 0.328 Time: 0.95s (0.05s)
Fold 3 train - epoch: 2/5 iter: 12/15 loss: 0.8504 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 13/15 loss: 0.8919 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 14/15 loss: 0.3584 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 56.4444% F1: 0.2772 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6927 Acc: 87.5000% F1: 0.763 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3543 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 62.0000% F1: 0.3989 *
*********************************************************
Performing epoch 3 of 5
Fold 3 train - epoch: 3/5 iter: 0/15 loss: 0.7801 Acc: 62.5000% F1: 0.444 Time: 0.94s (0.00s)
Fold 3 train - epoch: 3/5 iter: 1/15 loss: 0.7831 Acc: 65.6250% F1: 0.575 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 2/15 loss: 0.8755 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 3/15 loss: 0.7532 Acc: 65.6250% F1: 0.359 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 4/15 loss: 0.8172 Acc: 59.3750% F1: 0.466 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 5/15 loss: 0.8478 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 6/15 loss: 0.6896 Acc: 75.0000% F1: 0.415 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 7/15 loss: 0.8248 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 8/15 loss: 1.0152 Acc: 56.2500% F1: 0.345 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 9/15 loss: 0.9135 Acc: 56.2500% F1: 0.490 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 10/15 loss: 0.8765 Acc: 53.1250% F1: 0.335 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 11/15 loss: 0.7764 Acc: 62.5000% F1: 0.383 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 12/15 loss: 0.8302 Acc: 68.7500% F1: 0.569 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 13/15 loss: 0.8721 Acc: 59.3750% F1: 0.431 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 14/15 loss: 0.2196 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 60.4444% F1: 0.4180 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6187 Acc: 87.5000% F1: 0.763 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4967 Acc: 11.1111% F1: 0.089 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 60.0000% F1: 0.3821 *
*********************************************************
Performing epoch 4 of 5
Fold 3 train - epoch: 4/5 iter: 0/15 loss: 0.6926 Acc: 71.8750% F1: 0.634 Time: 0.94s (0.00s)
Fold 3 train - epoch: 4/5 iter: 1/15 loss: 0.6624 Acc: 65.6250% F1: 0.601 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 2/15 loss: 0.7789 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 3/15 loss: 0.6693 Acc: 71.8750% F1: 0.656 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/5 iter: 4/15 loss: 0.6652 Acc: 71.8750% F1: 0.616 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 5/15 loss: 0.6613 Acc: 68.7500% F1: 0.451 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 6/15 loss: 0.6040 Acc: 81.2500% F1: 0.504 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 7/15 loss: 0.6789 Acc: 68.7500% F1: 0.773 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 8/15 loss: 1.0442 Acc: 40.6250% F1: 0.357 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 9/15 loss: 0.7778 Acc: 65.6250% F1: 0.657 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 10/15 loss: 0.7341 Acc: 68.7500% F1: 0.643 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 11/15 loss: 0.6582 Acc: 68.7500% F1: 0.617 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 12/15 loss: 0.7359 Acc: 65.6250% F1: 0.546 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 13/15 loss: 0.7820 Acc: 59.3750% F1: 0.413 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 14/15 loss: 0.0468 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 66.0000% F1: 0.5733 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/5 iter: 0/2 loss: 0.4953 Acc: 84.3750% F1: 0.502 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 4/5 iter: 1/2 loss: 1.8301 Acc: 11.1111% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 58.0000% F1: 0.4214 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/5 iter: 0/15 loss: 1.0543 Acc: 37.5000% F1: 0.264 Time: 0.97s (0.00s)
Fold 4 train - epoch: 0/5 iter: 1/15 loss: 1.0421 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 4 train - epoch: 0/5 iter: 2/15 loss: 1.0586 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 3/15 loss: 0.9305 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 4/15 loss: 0.9329 Acc: 56.2500% F1: 0.368 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 5/15 loss: 1.0210 Acc: 40.6250% F1: 0.267 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 6/15 loss: 0.9406 Acc: 50.0000% F1: 0.332 Time: 0.95s (0.04s)
Fold 4 train - epoch: 0/5 iter: 7/15 loss: 0.8563 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.04s)
Fold 4 train - epoch: 0/5 iter: 8/15 loss: 1.1567 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.04s)
Fold 4 train - epoch: 0/5 iter: 9/15 loss: 1.0443 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 10/15 loss: 1.0755 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 11/15 loss: 0.9037 Acc: 59.3750% F1: 0.248 Time: 0.96s (0.04s)
Fold 4 train - epoch: 0/5 iter: 12/15 loss: 0.9805 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 13/15 loss: 0.9038 Acc: 53.1250% F1: 0.231 Time: 0.96s (0.04s)
Fold 4 train - epoch: 0/5 iter: 14/15 loss: 0.6302 Acc: 100.0000% F1: 1.000 Time: 0.11s (0.04s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 51.3333% F1: 0.2821 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5925 Acc: 87.5000% F1: 0.467 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5343 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 4 train - epoch: 1/5 iter: 0/15 loss: 0.8268 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 4 train - epoch: 1/5 iter: 1/15 loss: 0.8725 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.06s)
Fold 4 train - epoch: 1/5 iter: 2/15 loss: 0.9165 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.04s)
Fold 4 train - epoch: 1/5 iter: 3/15 loss: 0.8383 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.04s)
Fold 4 train - epoch: 1/5 iter: 4/15 loss: 0.9293 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.04s)
Fold 4 train - epoch: 1/5 iter: 5/15 loss: 0.9110 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.04s)
Fold 4 train - epoch: 1/5 iter: 6/15 loss: 0.7900 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.04s)
Fold 4 train - epoch: 1/5 iter: 7/15 loss: 0.8917 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 8/15 loss: 1.0415 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 9/15 loss: 0.9579 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.03s)
Fold 4 train - epoch: 1/5 iter: 10/15 loss: 0.9487 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.03s)
Fold 4 train - epoch: 1/5 iter: 11/15 loss: 0.8544 Acc: 62.5000% F1: 0.351 Time: 0.96s (0.04s)
Fold 4 train - epoch: 1/5 iter: 12/15 loss: 0.9605 Acc: 59.3750% F1: 0.367 Time: 0.96s (0.04s)
Fold 4 train - epoch: 1/5 iter: 13/15 loss: 0.9194 Acc: 53.1250% F1: 0.231 Time: 0.96s (0.04s)
Fold 4 train - epoch: 1/5 iter: 14/15 loss: 0.5459 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 55.7778% F1: 0.2691 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/5 iter: 0/2 loss: 0.6877 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3937 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 50.0000% F1: 0.2739 *
*********************************************************
Performing epoch 2 of 5
Fold 4 train - epoch: 2/5 iter: 0/15 loss: 0.8243 Acc: 65.6250% F1: 0.400 Time: 0.95s (0.00s)
Fold 4 train - epoch: 2/5 iter: 1/15 loss: 0.8386 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 2/15 loss: 0.8793 Acc: 53.1250% F1: 0.275 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/5 iter: 3/15 loss: 0.7909 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/5 iter: 4/15 loss: 0.9054 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 5/15 loss: 0.8319 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 6/15 loss: 0.7316 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 7/15 loss: 0.8593 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 8/15 loss: 1.0615 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 9/15 loss: 0.9482 Acc: 56.2500% F1: 0.352 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 10/15 loss: 0.8944 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 11/15 loss: 0.8623 Acc: 65.6250% F1: 0.394 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 12/15 loss: 0.8917 Acc: 59.3750% F1: 0.342 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 13/15 loss: 0.8618 Acc: 59.3750% F1: 0.492 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 14/15 loss: 0.3271 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 58.0000% F1: 0.3181 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7422 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3267 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.3175 *
*********************************************************
Performing epoch 3 of 5
Fold 4 train - epoch: 3/5 iter: 0/15 loss: 0.6921 Acc: 78.1250% F1: 0.755 Time: 0.95s (0.00s)
Fold 4 train - epoch: 3/5 iter: 1/15 loss: 0.7537 Acc: 65.6250% F1: 0.577 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 2/15 loss: 0.8129 Acc: 59.3750% F1: 0.362 Time: 0.96s (0.03s)
Fold 4 train - epoch: 3/5 iter: 3/15 loss: 0.6881 Acc: 65.6250% F1: 0.359 Time: 0.94s (0.03s)
Fold 4 train - epoch: 3/5 iter: 4/15 loss: 0.7589 Acc: 56.2500% F1: 0.416 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 5/15 loss: 0.7709 Acc: 65.6250% F1: 0.380 Time: 0.96s (0.04s)
Fold 4 train - epoch: 3/5 iter: 6/15 loss: 0.6553 Acc: 78.1250% F1: 0.469 Time: 0.96s (0.04s)
Fold 4 train - epoch: 3/5 iter: 7/15 loss: 0.7064 Acc: 62.5000% F1: 0.397 Time: 0.95s (0.04s)
Fold 4 train - epoch: 3/5 iter: 8/15 loss: 0.9518 Acc: 50.0000% F1: 0.384 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 9/15 loss: 0.8237 Acc: 62.5000% F1: 0.442 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 10/15 loss: 0.7999 Acc: 56.2500% F1: 0.402 Time: 0.95s (0.04s)
Fold 4 train - epoch: 3/5 iter: 11/15 loss: 0.7687 Acc: 65.6250% F1: 0.438 Time: 0.96s (0.03s)
Fold 4 train - epoch: 3/5 iter: 12/15 loss: 0.8718 Acc: 59.3750% F1: 0.491 Time: 0.96s (0.03s)
Fold 4 train - epoch: 3/5 iter: 13/15 loss: 0.8417 Acc: 59.3750% F1: 0.465 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 14/15 loss: 0.0834 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 63.3333% F1: 0.4913 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7229 Acc: 71.8750% F1: 0.284 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 3/5 iter: 1/2 loss: 1.5529 Acc: 11.1111% F1: 0.118 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3162 *
*********************************************************
Performing epoch 4 of 5
Fold 4 train - epoch: 4/5 iter: 0/15 loss: 0.5649 Acc: 75.0000% F1: 0.697 Time: 0.94s (0.00s)
Fold 4 train - epoch: 4/5 iter: 1/15 loss: 0.5996 Acc: 75.0000% F1: 0.712 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 2/15 loss: 0.6710 Acc: 71.8750% F1: 0.712 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 3/15 loss: 0.5198 Acc: 78.1250% F1: 0.718 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 4/15 loss: 0.5069 Acc: 90.6250% F1: 0.894 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 5/15 loss: 0.5506 Acc: 84.3750% F1: 0.709 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 6/15 loss: 0.5819 Acc: 75.0000% F1: 0.461 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 7/15 loss: 0.5042 Acc: 81.2500% F1: 0.871 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 8/15 loss: 0.7487 Acc: 59.3750% F1: 0.529 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 9/15 loss: 0.7694 Acc: 68.7500% F1: 0.675 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 10/15 loss: 0.6500 Acc: 75.0000% F1: 0.631 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 11/15 loss: 0.5489 Acc: 75.0000% F1: 0.506 Time: 0.96s (0.02s)
Fold 4 train - epoch: 4/5 iter: 12/15 loss: 0.6766 Acc: 62.5000% F1: 0.586 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 13/15 loss: 0.7104 Acc: 65.6250% F1: 0.528 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 14/15 loss: 0.0225 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 74.2222% F1: 0.6822 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8982 Acc: 68.7500% F1: 0.336 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5925 Acc: 33.3333% F1: 0.242 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 56.0000% F1: 0.4308 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/5 iter: 0/15 loss: 0.9657 Acc: 46.8750% F1: 0.324 Time: 0.97s (0.00s)
Fold 5 train - epoch: 0/5 iter: 1/15 loss: 1.1188 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/5 iter: 2/15 loss: 1.0432 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 3/15 loss: 0.8718 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.02s)
Fold 5 train - epoch: 0/5 iter: 4/15 loss: 0.9385 Acc: 56.2500% F1: 0.390 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 5/15 loss: 0.9469 Acc: 50.0000% F1: 0.343 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 6/15 loss: 0.8649 Acc: 53.1250% F1: 0.328 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 7/15 loss: 0.8861 Acc: 53.1250% F1: 0.317 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 8/15 loss: 1.1457 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 9/15 loss: 1.0297 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 10/15 loss: 1.0300 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 11/15 loss: 0.9269 Acc: 62.5000% F1: 0.314 Time: 0.96s (0.02s)
Fold 5 train - epoch: 0/5 iter: 12/15 loss: 0.9713 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 13/15 loss: 0.9514 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.02s)
Fold 5 train - epoch: 0/5 iter: 14/15 loss: 0.8080 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.02s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 52.8889% F1: 0.3012 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6391 Acc: 87.5000% F1: 0.467 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5908 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 5 train - epoch: 1/5 iter: 0/15 loss: 0.8312 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.00s)
Fold 5 train - epoch: 1/5 iter: 1/15 loss: 0.8761 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 2/15 loss: 0.9359 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 3/15 loss: 0.8071 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 4/15 loss: 0.9181 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/5 iter: 5/15 loss: 0.8788 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 6/15 loss: 0.7222 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 7/15 loss: 0.8823 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 8/15 loss: 1.0783 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 9/15 loss: 1.0065 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 10/15 loss: 0.9939 Acc: 43.7500% F1: 0.239 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 11/15 loss: 0.8639 Acc: 56.2500% F1: 0.320 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 12/15 loss: 0.9545 Acc: 56.2500% F1: 0.327 Time: 0.96s (0.02s)
Fold 5 train - epoch: 1/5 iter: 13/15 loss: 0.8986 Acc: 53.1250% F1: 0.306 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 14/15 loss: 0.6721 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 54.0000% F1: 0.2622 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7783 Acc: 75.0000% F1: 0.429 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3855 Acc: 11.1111% F1: 0.089 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.2833 *
*********************************************************
Performing epoch 2 of 5
Fold 5 train - epoch: 2/5 iter: 0/15 loss: 0.7502 Acc: 65.6250% F1: 0.528 Time: 0.95s (0.00s)
Fold 5 train - epoch: 2/5 iter: 1/15 loss: 0.8274 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.02s)
Fold 5 train - epoch: 2/5 iter: 2/15 loss: 0.8832 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 3/15 loss: 0.7932 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 4/15 loss: 0.8322 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 5/15 loss: 0.8619 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 6/15 loss: 0.7071 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 7/15 loss: 0.8370 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 8/15 loss: 1.0190 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 9/15 loss: 1.0006 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 10/15 loss: 0.9596 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 11/15 loss: 0.8749 Acc: 56.2500% F1: 0.289 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 12/15 loss: 0.8882 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 13/15 loss: 0.8678 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 14/15 loss: 0.4086 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 54.8889% F1: 0.2720 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8380 Acc: 59.3750% F1: 0.248 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4345 Acc: 22.2222% F1: 0.208 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.3418 *
*********************************************************
Performing epoch 3 of 5
Fold 5 train - epoch: 3/5 iter: 0/15 loss: 0.6750 Acc: 78.1250% F1: 0.788 Time: 0.94s (0.00s)
Fold 5 train - epoch: 3/5 iter: 1/15 loss: 0.7784 Acc: 68.7500% F1: 0.673 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/5 iter: 2/15 loss: 0.7782 Acc: 62.5000% F1: 0.553 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 3/15 loss: 0.6992 Acc: 65.6250% F1: 0.597 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 4/15 loss: 0.7272 Acc: 68.7500% F1: 0.583 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 5/15 loss: 0.7508 Acc: 65.6250% F1: 0.380 Time: 0.95s (0.04s)
Fold 5 train - epoch: 3/5 iter: 6/15 loss: 0.6562 Acc: 71.8750% F1: 0.351 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 7/15 loss: 0.7642 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 8/15 loss: 0.8589 Acc: 62.5000% F1: 0.485 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 9/15 loss: 0.8590 Acc: 65.6250% F1: 0.637 Time: 0.95s (0.04s)
Fold 5 train - epoch: 3/5 iter: 10/15 loss: 0.8813 Acc: 56.2500% F1: 0.398 Time: 0.96s (0.04s)
Fold 5 train - epoch: 3/5 iter: 11/15 loss: 0.8018 Acc: 71.8750% F1: 0.469 Time: 0.96s (0.03s)
Fold 5 train - epoch: 3/5 iter: 12/15 loss: 0.8255 Acc: 56.2500% F1: 0.456 Time: 0.96s (0.04s)
Fold 5 train - epoch: 3/5 iter: 13/15 loss: 0.9377 Acc: 50.0000% F1: 0.312 Time: 0.96s (0.03s)
Fold 5 train - epoch: 3/5 iter: 14/15 loss: 0.1683 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 64.6667% F1: 0.5352 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8788 Acc: 50.0000% F1: 0.227 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6994 Acc: 16.6667% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 38.0000% F1: 0.2823 *
*********************************************************
Performing epoch 4 of 5
Fold 5 train - epoch: 4/5 iter: 0/15 loss: 0.5190 Acc: 78.1250% F1: 0.749 Time: 0.94s (0.00s)
Fold 5 train - epoch: 4/5 iter: 1/15 loss: 0.5977 Acc: 71.8750% F1: 0.638 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 2/15 loss: 0.7170 Acc: 65.6250% F1: 0.586 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 3/15 loss: 0.6441 Acc: 71.8750% F1: 0.697 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 4/15 loss: 0.5474 Acc: 84.3750% F1: 0.788 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 5/15 loss: 0.5768 Acc: 68.7500% F1: 0.548 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 6/15 loss: 0.5876 Acc: 78.1250% F1: 0.634 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 7/15 loss: 0.6746 Acc: 68.7500% F1: 0.459 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 8/15 loss: 0.6971 Acc: 68.7500% F1: 0.634 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 9/15 loss: 0.7560 Acc: 59.3750% F1: 0.589 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 10/15 loss: 0.7566 Acc: 62.5000% F1: 0.459 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 11/15 loss: 0.5811 Acc: 78.1250% F1: 0.662 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 12/15 loss: 0.6162 Acc: 71.8750% F1: 0.713 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 13/15 loss: 0.6669 Acc: 62.5000% F1: 0.559 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 14/15 loss: 0.0310 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 70.8889% F1: 0.6482 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/5 iter: 0/2 loss: 0.9803 Acc: 50.0000% F1: 0.274 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 4/5 iter: 1/2 loss: 2.0777 Acc: 22.2222% F1: 0.231 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 40.0000% F1: 0.3364 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/5 iter: 0/15 loss: 0.9502 Acc: 53.1250% F1: 0.376 Time: 0.96s (0.00s)
Fold 6 train - epoch: 0/5 iter: 1/15 loss: 1.0659 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/5 iter: 2/15 loss: 1.0939 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 3/15 loss: 0.8879 Acc: 56.2500% F1: 0.286 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 4/15 loss: 0.9887 Acc: 56.2500% F1: 0.396 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 5/15 loss: 1.0117 Acc: 31.2500% F1: 0.186 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 6/15 loss: 0.8725 Acc: 65.6250% F1: 0.398 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 7/15 loss: 0.8415 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 8/15 loss: 1.2329 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 9/15 loss: 1.0918 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 10/15 loss: 1.0436 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 11/15 loss: 0.8971 Acc: 65.6250% F1: 0.370 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 12/15 loss: 0.9948 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 13/15 loss: 0.9224 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 14/15 loss: 0.6620 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 53.3333% F1: 0.3015 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6264 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 0/5 iter: 1/2 loss: 1.4526 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 6 train - epoch: 1/5 iter: 0/15 loss: 0.7943 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 6 train - epoch: 1/5 iter: 1/15 loss: 0.8743 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 2/15 loss: 0.8969 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 3/15 loss: 0.8277 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 4/15 loss: 0.9637 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 5/15 loss: 0.8674 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 6/15 loss: 0.8007 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 7/15 loss: 0.8729 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 8/15 loss: 1.0713 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 9/15 loss: 1.0310 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 10/15 loss: 0.9832 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 11/15 loss: 0.8794 Acc: 65.6250% F1: 0.365 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 12/15 loss: 0.9481 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 13/15 loss: 0.9118 Acc: 59.3750% F1: 0.336 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 14/15 loss: 0.5468 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 54.8889% F1: 0.2523 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/5 iter: 0/2 loss: 0.6839 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3967 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.2368 *
*********************************************************
Performing epoch 2 of 5
Fold 6 train - epoch: 2/5 iter: 0/15 loss: 0.8179 Acc: 59.3750% F1: 0.367 Time: 0.94s (0.00s)
Fold 6 train - epoch: 2/5 iter: 1/15 loss: 0.8556 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 6 train - epoch: 2/5 iter: 2/15 loss: 0.8622 Acc: 50.0000% F1: 0.262 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/5 iter: 3/15 loss: 0.7655 Acc: 62.5000% F1: 0.309 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 4/15 loss: 0.9020 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 5/15 loss: 0.8765 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 6/15 loss: 0.7453 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 6 train - epoch: 2/5 iter: 7/15 loss: 0.8182 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 8/15 loss: 0.9950 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 9/15 loss: 1.0098 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 10/15 loss: 0.9241 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 11/15 loss: 0.8405 Acc: 62.5000% F1: 0.375 Time: 0.95s (0.03s)
Fold 6 train - epoch: 2/5 iter: 12/15 loss: 0.9028 Acc: 46.8750% F1: 0.254 Time: 0.96s (0.02s)
Fold 6 train - epoch: 2/5 iter: 13/15 loss: 0.8902 Acc: 50.0000% F1: 0.291 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 14/15 loss: 0.4147 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 54.6667% F1: 0.2802 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7155 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3933 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.3022 *
*********************************************************
Performing epoch 3 of 5
Fold 6 train - epoch: 3/5 iter: 0/15 loss: 0.6860 Acc: 78.1250% F1: 0.716 Time: 0.94s (0.00s)
Fold 6 train - epoch: 3/5 iter: 1/15 loss: 0.7563 Acc: 62.5000% F1: 0.444 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 2/15 loss: 0.8067 Acc: 53.1250% F1: 0.306 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 3/15 loss: 0.7480 Acc: 62.5000% F1: 0.310 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 4/15 loss: 0.8032 Acc: 62.5000% F1: 0.509 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 5/15 loss: 0.7208 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 6/15 loss: 0.6650 Acc: 71.8750% F1: 0.499 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 7/15 loss: 0.7602 Acc: 62.5000% F1: 0.397 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 8/15 loss: 0.8463 Acc: 59.3750% F1: 0.396 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 9/15 loss: 0.9734 Acc: 50.0000% F1: 0.411 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 10/15 loss: 0.7871 Acc: 62.5000% F1: 0.440 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 11/15 loss: 0.7938 Acc: 59.3750% F1: 0.408 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 12/15 loss: 0.8390 Acc: 56.2500% F1: 0.368 Time: 0.96s (0.02s)
Fold 6 train - epoch: 3/5 iter: 13/15 loss: 0.8284 Acc: 62.5000% F1: 0.505 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 14/15 loss: 0.0917 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 62.0000% F1: 0.4617 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6691 Acc: 75.0000% F1: 0.373 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 3/5 iter: 1/2 loss: 1.7711 Acc: 5.5556% F1: 0.048 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.2861 *
*********************************************************
Performing epoch 4 of 5
Fold 6 train - epoch: 4/5 iter: 0/15 loss: 0.5522 Acc: 75.0000% F1: 0.708 Time: 0.94s (0.00s)
Fold 6 train - epoch: 4/5 iter: 1/15 loss: 0.6273 Acc: 71.8750% F1: 0.643 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 2/15 loss: 0.6787 Acc: 68.7500% F1: 0.654 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 3/15 loss: 0.5448 Acc: 78.1250% F1: 0.793 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 4/15 loss: 0.6227 Acc: 78.1250% F1: 0.672 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 5/15 loss: 0.5684 Acc: 81.2500% F1: 0.685 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 6/15 loss: 0.5971 Acc: 78.1250% F1: 0.690 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 7/15 loss: 0.5025 Acc: 81.2500% F1: 0.868 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 8/15 loss: 0.6798 Acc: 75.0000% F1: 0.674 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 9/15 loss: 0.8912 Acc: 62.5000% F1: 0.562 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 10/15 loss: 0.6832 Acc: 65.6250% F1: 0.571 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 11/15 loss: 0.6240 Acc: 71.8750% F1: 0.648 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 12/15 loss: 0.6938 Acc: 71.8750% F1: 0.652 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 13/15 loss: 0.7573 Acc: 65.6250% F1: 0.575 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 14/15 loss: 0.0268 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 73.3333% F1: 0.6751 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/5 iter: 0/2 loss: 1.1854 Acc: 53.1250% F1: 0.298 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5971 Acc: 38.8889% F1: 0.309 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.4125 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/5 iter: 0/15 loss: 0.9880 Acc: 46.8750% F1: 0.314 Time: 0.95s (0.00s)
Fold 7 train - epoch: 0/5 iter: 1/15 loss: 1.0659 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 7 train - epoch: 0/5 iter: 2/15 loss: 1.0495 Acc: 46.8750% F1: 0.249 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 3/15 loss: 0.8861 Acc: 59.3750% F1: 0.298 Time: 0.94s (0.02s)
Fold 7 train - epoch: 0/5 iter: 4/15 loss: 1.0119 Acc: 46.8750% F1: 0.282 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 5/15 loss: 1.0504 Acc: 43.7500% F1: 0.284 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 6/15 loss: 0.8629 Acc: 59.3750% F1: 0.296 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 7/15 loss: 0.8537 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 8/15 loss: 1.1205 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 9/15 loss: 1.1065 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 10/15 loss: 1.0199 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 11/15 loss: 0.8893 Acc: 71.8750% F1: 0.401 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 12/15 loss: 0.9889 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 13/15 loss: 0.8851 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 14/15 loss: 0.7648 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 52.8889% F1: 0.2785 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6435 Acc: 84.3750% F1: 0.458 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 0/5 iter: 1/2 loss: 1.4458 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 7 train - epoch: 1/5 iter: 0/15 loss: 0.8217 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.00s)
Fold 7 train - epoch: 1/5 iter: 1/15 loss: 0.8779 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 7 train - epoch: 1/5 iter: 2/15 loss: 0.9267 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 3/15 loss: 0.8047 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 4/15 loss: 0.9434 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 5/15 loss: 0.9072 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 6/15 loss: 0.7675 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 7/15 loss: 0.8476 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 8/15 loss: 1.0258 Acc: 56.2500% F1: 0.310 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 9/15 loss: 0.9815 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 10/15 loss: 0.9834 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 11/15 loss: 0.9201 Acc: 62.5000% F1: 0.351 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 12/15 loss: 0.9229 Acc: 46.8750% F1: 0.254 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 13/15 loss: 0.9098 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 14/15 loss: 0.5841 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 55.3333% F1: 0.2737 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7041 Acc: 81.2500% F1: 0.448 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3818 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.3316 *
*********************************************************
Performing epoch 2 of 5
Fold 7 train - epoch: 2/5 iter: 0/15 loss: 0.7779 Acc: 65.6250% F1: 0.500 Time: 0.95s (0.00s)
Fold 7 train - epoch: 2/5 iter: 1/15 loss: 0.8591 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 7 train - epoch: 2/5 iter: 2/15 loss: 0.8448 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 3/15 loss: 0.7557 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 4/15 loss: 0.9239 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 5/15 loss: 0.8449 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 6/15 loss: 0.7386 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 7/15 loss: 0.8397 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 8/15 loss: 0.9628 Acc: 53.1250% F1: 0.231 Time: 0.96s (0.02s)
Fold 7 train - epoch: 2/5 iter: 9/15 loss: 1.0207 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 10/15 loss: 0.9639 Acc: 43.7500% F1: 0.239 Time: 0.96s (0.03s)
Fold 7 train - epoch: 2/5 iter: 11/15 loss: 0.8661 Acc: 68.7500% F1: 0.412 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 12/15 loss: 0.8853 Acc: 50.0000% F1: 0.332 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 13/15 loss: 0.8545 Acc: 59.3750% F1: 0.362 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 14/15 loss: 0.4539 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 56.0000% F1: 0.3039 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7584 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4065 Acc: 27.7778% F1: 0.185 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3556 *
*********************************************************
Performing epoch 3 of 5
Fold 7 train - epoch: 3/5 iter: 0/15 loss: 0.7023 Acc: 71.8750% F1: 0.713 Time: 0.94s (0.00s)
Fold 7 train - epoch: 3/5 iter: 1/15 loss: 0.7366 Acc: 71.8750% F1: 0.663 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 2/15 loss: 0.7806 Acc: 56.2500% F1: 0.477 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 3/15 loss: 0.6800 Acc: 65.6250% F1: 0.537 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 4/15 loss: 0.8248 Acc: 65.6250% F1: 0.444 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 5/15 loss: 0.7639 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 6/15 loss: 0.6707 Acc: 75.0000% F1: 0.415 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 7/15 loss: 0.7103 Acc: 59.3750% F1: 0.379 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 8/15 loss: 0.7244 Acc: 71.8750% F1: 0.637 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 9/15 loss: 0.8679 Acc: 65.6250% F1: 0.643 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 10/15 loss: 0.8166 Acc: 59.3750% F1: 0.514 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 11/15 loss: 0.8012 Acc: 68.7500% F1: 0.455 Time: 0.96s (0.03s)
Fold 7 train - epoch: 3/5 iter: 12/15 loss: 0.8575 Acc: 59.3750% F1: 0.496 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 13/15 loss: 0.8370 Acc: 56.2500% F1: 0.449 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 14/15 loss: 0.0963 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 64.8889% F1: 0.5479 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7334 Acc: 62.5000% F1: 0.256 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 3/5 iter: 1/2 loss: 1.8206 Acc: 27.7778% F1: 0.240 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3771 *
*********************************************************
Performing epoch 4 of 5
Fold 7 train - epoch: 4/5 iter: 0/15 loss: 0.5835 Acc: 71.8750% F1: 0.630 Time: 0.95s (0.00s)
Fold 7 train - epoch: 4/5 iter: 1/15 loss: 0.6072 Acc: 75.0000% F1: 0.717 Time: 0.94s (0.02s)
Fold 7 train - epoch: 4/5 iter: 2/15 loss: 0.6846 Acc: 65.6250% F1: 0.559 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 3/15 loss: 0.5368 Acc: 78.1250% F1: 0.808 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 4/15 loss: 0.7307 Acc: 65.6250% F1: 0.632 Time: 0.94s (0.02s)
Fold 7 train - epoch: 4/5 iter: 5/15 loss: 0.6151 Acc: 71.8750% F1: 0.595 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 6/15 loss: 0.5828 Acc: 78.1250% F1: 0.675 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 7/15 loss: 0.5861 Acc: 68.7500% F1: 0.773 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 8/15 loss: 0.6300 Acc: 78.1250% F1: 0.739 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 9/15 loss: 0.7154 Acc: 59.3750% F1: 0.596 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 10/15 loss: 0.6393 Acc: 62.5000% F1: 0.528 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 11/15 loss: 0.6761 Acc: 75.0000% F1: 0.623 Time: 0.96s (0.02s)
Fold 7 train - epoch: 4/5 iter: 12/15 loss: 0.6347 Acc: 78.1250% F1: 0.751 Time: 0.96s (0.02s)
Fold 7 train - epoch: 4/5 iter: 13/15 loss: 0.7738 Acc: 65.6250% F1: 0.549 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 14/15 loss: 0.0272 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 71.1111% F1: 0.6646 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/5 iter: 0/2 loss: 0.7609 Acc: 65.6250% F1: 0.322 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 4/5 iter: 1/2 loss: 2.1892 Acc: 16.6667% F1: 0.179 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3518 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/5 iter: 0/15 loss: 0.9210 Acc: 53.1250% F1: 0.489 Time: 0.96s (0.00s)
Fold 8 train - epoch: 0/5 iter: 1/15 loss: 0.9897 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 8 train - epoch: 0/5 iter: 2/15 loss: 0.9778 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 3/15 loss: 0.8560 Acc: 59.3750% F1: 0.298 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 4/15 loss: 1.0615 Acc: 53.1250% F1: 0.375 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 5/15 loss: 1.0157 Acc: 34.3750% F1: 0.232 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 6/15 loss: 0.9033 Acc: 40.6250% F1: 0.223 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 7/15 loss: 0.8865 Acc: 50.0000% F1: 0.282 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 8/15 loss: 1.1928 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 9/15 loss: 1.0675 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 10/15 loss: 0.9877 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 11/15 loss: 0.9227 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 12/15 loss: 0.9923 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 13/15 loss: 0.9569 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 14/15 loss: 0.8567 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 51.5556% F1: 0.3033 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6251 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6071 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 8 train - epoch: 1/5 iter: 0/15 loss: 0.7737 Acc: 62.5000% F1: 0.444 Time: 0.94s (0.00s)
Fold 8 train - epoch: 1/5 iter: 1/15 loss: 0.8872 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 8 train - epoch: 1/5 iter: 2/15 loss: 0.9503 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 8 train - epoch: 1/5 iter: 3/15 loss: 0.8246 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 4/15 loss: 0.9959 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 5/15 loss: 0.8961 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 8 train - epoch: 1/5 iter: 6/15 loss: 0.7680 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 7/15 loss: 0.9068 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 8/15 loss: 1.0497 Acc: 56.2500% F1: 0.310 Time: 0.95s (0.02s)
Fold 8 train - epoch: 1/5 iter: 9/15 loss: 0.9954 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 8 train - epoch: 1/5 iter: 10/15 loss: 0.9443 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 8 train - epoch: 1/5 iter: 11/15 loss: 0.8787 Acc: 56.2500% F1: 0.320 Time: 0.96s (0.02s)
Fold 8 train - epoch: 1/5 iter: 12/15 loss: 0.9320 Acc: 59.3750% F1: 0.367 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 13/15 loss: 0.9587 Acc: 50.0000% F1: 0.264 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 14/15 loss: 0.5761 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 54.6667% F1: 0.2779 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7301 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4046 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.2651 *
*********************************************************
Performing epoch 2 of 5
Fold 8 train - epoch: 2/5 iter: 0/15 loss: 0.7800 Acc: 65.6250% F1: 0.430 Time: 0.94s (0.00s)
Fold 8 train - epoch: 2/5 iter: 1/15 loss: 0.8524 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.02s)
Fold 8 train - epoch: 2/5 iter: 2/15 loss: 0.9023 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 3/15 loss: 0.7945 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 4/15 loss: 0.9037 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 5/15 loss: 0.8276 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 6/15 loss: 0.7197 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 7/15 loss: 0.8389 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 8/15 loss: 1.0025 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 9/15 loss: 0.9912 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 10/15 loss: 0.9131 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 11/15 loss: 0.8314 Acc: 62.5000% F1: 0.351 Time: 0.96s (0.02s)
Fold 8 train - epoch: 2/5 iter: 12/15 loss: 0.8610 Acc: 65.6250% F1: 0.430 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 13/15 loss: 0.8561 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 14/15 loss: 0.3756 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 56.2222% F1: 0.2803 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7116 Acc: 71.8750% F1: 0.566 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4593 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 48.0000% F1: 0.2828 *
*********************************************************
Performing epoch 3 of 5
Fold 8 train - epoch: 3/5 iter: 0/15 loss: 0.6764 Acc: 78.1250% F1: 0.788 Time: 0.94s (0.00s)
Fold 8 train - epoch: 3/5 iter: 1/15 loss: 0.8151 Acc: 62.5000% F1: 0.524 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 2/15 loss: 0.7619 Acc: 62.5000% F1: 0.534 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 3/15 loss: 0.7152 Acc: 59.3750% F1: 0.298 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 4/15 loss: 0.8348 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 5/15 loss: 0.7017 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 6/15 loss: 0.6664 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 7/15 loss: 0.7250 Acc: 62.5000% F1: 0.407 Time: 0.96s (0.03s)
Fold 8 train - epoch: 3/5 iter: 8/15 loss: 0.8660 Acc: 65.6250% F1: 0.508 Time: 0.96s (0.03s)
Fold 8 train - epoch: 3/5 iter: 9/15 loss: 0.9322 Acc: 46.8750% F1: 0.378 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 10/15 loss: 0.6709 Acc: 75.0000% F1: 0.641 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 11/15 loss: 0.8483 Acc: 50.0000% F1: 0.317 Time: 0.96s (0.03s)
Fold 8 train - epoch: 3/5 iter: 12/15 loss: 0.8022 Acc: 71.8750% F1: 0.623 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 13/15 loss: 0.8193 Acc: 68.7500% F1: 0.591 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 14/15 loss: 0.0858 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 63.3333% F1: 0.5186 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6052 Acc: 75.0000% F1: 0.358 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 3/5 iter: 1/2 loss: 1.8412 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.2447 *
*********************************************************
Performing epoch 4 of 5
Fold 8 train - epoch: 4/5 iter: 0/15 loss: 0.5683 Acc: 75.0000% F1: 0.720 Time: 0.94s (0.00s)
Fold 8 train - epoch: 4/5 iter: 1/15 loss: 0.7460 Acc: 71.8750% F1: 0.643 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 2/15 loss: 0.6376 Acc: 62.5000% F1: 0.622 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 3/15 loss: 0.6241 Acc: 71.8750% F1: 0.671 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 4/15 loss: 0.6395 Acc: 75.0000% F1: 0.724 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 5/15 loss: 0.6936 Acc: 65.6250% F1: 0.635 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 6/15 loss: 0.5748 Acc: 71.8750% F1: 0.622 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 7/15 loss: 0.5053 Acc: 84.3750% F1: 0.892 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 8/15 loss: 0.6953 Acc: 81.2500% F1: 0.761 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 9/15 loss: 0.6384 Acc: 75.0000% F1: 0.773 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 10/15 loss: 0.5658 Acc: 87.5000% F1: 0.820 Time: 0.96s (0.02s)
Fold 8 train - epoch: 4/5 iter: 11/15 loss: 0.5760 Acc: 75.0000% F1: 0.615 Time: 0.96s (0.02s)
Fold 8 train - epoch: 4/5 iter: 12/15 loss: 0.6753 Acc: 65.6250% F1: 0.627 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 13/15 loss: 0.8205 Acc: 62.5000% F1: 0.498 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 14/15 loss: 0.0285 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 73.3333% F1: 0.6982 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/5 iter: 0/2 loss: 0.6785 Acc: 71.8750% F1: 0.343 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 4/5 iter: 1/2 loss: 1.9177 Acc: 16.6667% F1: 0.169 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 52.0000% F1: 0.3722 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/5 iter: 0/15 loss: 0.9557 Acc: 56.2500% F1: 0.387 Time: 0.96s (0.00s)
Fold 9 train - epoch: 0/5 iter: 1/15 loss: 1.0123 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.04s)
Fold 9 train - epoch: 0/5 iter: 2/15 loss: 1.0992 Acc: 37.5000% F1: 0.216 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 3/15 loss: 0.9261 Acc: 43.7500% F1: 0.241 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 4/15 loss: 1.0856 Acc: 46.8750% F1: 0.306 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 5/15 loss: 0.9612 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 6/15 loss: 0.7961 Acc: 65.6250% F1: 0.264 Time: 0.95s (0.05s)
Fold 9 train - epoch: 0/5 iter: 7/15 loss: 0.9613 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 8/15 loss: 1.1105 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 9/15 loss: 1.0818 Acc: 40.6250% F1: 0.202 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 10/15 loss: 1.0729 Acc: 40.6250% F1: 0.197 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 11/15 loss: 0.8912 Acc: 68.7500% F1: 0.460 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 12/15 loss: 0.9863 Acc: 46.8750% F1: 0.254 Time: 0.96s (0.03s)
Fold 9 train - epoch: 0/5 iter: 13/15 loss: 0.9883 Acc: 53.1250% F1: 0.275 Time: 0.96s (0.03s)
Fold 9 train - epoch: 0/5 iter: 14/15 loss: 0.7407 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.03s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 50.4444% F1: 0.2804 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6930 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 0/5 iter: 1/2 loss: 1.1316 Acc: 27.7778% F1: 0.152 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2424 *
*********************************************************
Performing epoch 1 of 5
Fold 9 train - epoch: 1/5 iter: 0/15 loss: 0.8508 Acc: 56.2500% F1: 0.297 Time: 0.95s (0.00s)
Fold 9 train - epoch: 1/5 iter: 1/15 loss: 0.8965 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 2/15 loss: 1.0025 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 3/15 loss: 0.8020 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 4/15 loss: 1.0164 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 5/15 loss: 0.8849 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 6/15 loss: 0.7384 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 7/15 loss: 0.8976 Acc: 43.7500% F1: 0.203 Time: 0.96s (0.04s)
Fold 9 train - epoch: 1/5 iter: 8/15 loss: 1.0590 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 9/15 loss: 1.0411 Acc: 43.7500% F1: 0.207 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 10/15 loss: 1.0522 Acc: 43.7500% F1: 0.243 Time: 0.95s (0.04s)
Fold 9 train - epoch: 1/5 iter: 11/15 loss: 0.8532 Acc: 59.3750% F1: 0.296 Time: 0.96s (0.03s)
Fold 9 train - epoch: 1/5 iter: 12/15 loss: 0.9260 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 13/15 loss: 0.9608 Acc: 53.1250% F1: 0.306 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 14/15 loss: 0.6465 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 53.5556% F1: 0.2584 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7810 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 1/5 iter: 1/2 loss: 1.0551 Acc: 33.3333% F1: 0.210 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.2988 *
*********************************************************
Performing epoch 2 of 5
Fold 9 train - epoch: 2/5 iter: 0/15 loss: 0.8291 Acc: 68.7500% F1: 0.559 Time: 0.94s (0.00s)
Fold 9 train - epoch: 2/5 iter: 1/15 loss: 0.8508 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 2/15 loss: 0.8764 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 3/15 loss: 0.7471 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 4/15 loss: 0.9214 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 5/15 loss: 0.8203 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 6/15 loss: 0.6589 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 7/15 loss: 0.8976 Acc: 46.8750% F1: 0.247 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 8/15 loss: 1.0285 Acc: 56.2500% F1: 0.310 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 9/15 loss: 1.0941 Acc: 46.8750% F1: 0.257 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 10/15 loss: 0.9370 Acc: 50.0000% F1: 0.301 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 11/15 loss: 0.7570 Acc: 68.7500% F1: 0.378 Time: 0.96s (0.03s)
Fold 9 train - epoch: 2/5 iter: 12/15 loss: 0.8497 Acc: 62.5000% F1: 0.389 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 13/15 loss: 0.8638 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 14/15 loss: 0.3164 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 57.1111% F1: 0.3117 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8222 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 2/5 iter: 1/2 loss: 1.1543 Acc: 44.4444% F1: 0.309 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.3333 *
*********************************************************
Performing epoch 3 of 5
Fold 9 train - epoch: 3/5 iter: 0/15 loss: 0.7117 Acc: 78.1250% F1: 0.749 Time: 0.94s (0.00s)
Fold 9 train - epoch: 3/5 iter: 1/15 loss: 0.8312 Acc: 65.6250% F1: 0.548 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 2/15 loss: 0.8652 Acc: 53.1250% F1: 0.440 Time: 0.94s (0.03s)
Fold 9 train - epoch: 3/5 iter: 3/15 loss: 0.7065 Acc: 75.0000% F1: 0.649 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 4/15 loss: 0.8370 Acc: 75.0000% F1: 0.630 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 5/15 loss: 0.7192 Acc: 65.6250% F1: 0.459 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 6/15 loss: 0.6355 Acc: 75.0000% F1: 0.403 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 7/15 loss: 0.7770 Acc: 62.5000% F1: 0.413 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 8/15 loss: 0.9467 Acc: 59.3750% F1: 0.387 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 9/15 loss: 0.9990 Acc: 46.8750% F1: 0.287 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 10/15 loss: 0.9011 Acc: 50.0000% F1: 0.337 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 11/15 loss: 0.7736 Acc: 59.3750% F1: 0.355 Time: 0.96s (0.03s)
Fold 9 train - epoch: 3/5 iter: 12/15 loss: 0.7790 Acc: 59.3750% F1: 0.500 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 13/15 loss: 0.8875 Acc: 65.6250% F1: 0.601 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 14/15 loss: 0.0843 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 63.7778% F1: 0.5073 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8391 Acc: 62.5000% F1: 0.309 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3580 Acc: 33.3333% F1: 0.243 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 52.0000% F1: 0.3024 *
*********************************************************
Performing epoch 4 of 5
Fold 9 train - epoch: 4/5 iter: 0/15 loss: 0.6109 Acc: 75.0000% F1: 0.702 Time: 0.94s (0.00s)
Fold 9 train - epoch: 4/5 iter: 1/15 loss: 0.6857 Acc: 68.7500% F1: 0.567 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 2/15 loss: 0.7361 Acc: 62.5000% F1: 0.530 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 3/15 loss: 0.6265 Acc: 75.0000% F1: 0.654 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 4/15 loss: 0.7119 Acc: 71.8750% F1: 0.610 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 5/15 loss: 0.5852 Acc: 78.1250% F1: 0.749 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 6/15 loss: 0.5232 Acc: 84.3750% F1: 0.853 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 7/15 loss: 0.6289 Acc: 71.8750% F1: 0.493 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 8/15 loss: 0.6980 Acc: 59.3750% F1: 0.490 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 9/15 loss: 0.8824 Acc: 56.2500% F1: 0.587 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 10/15 loss: 0.6625 Acc: 65.6250% F1: 0.571 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 11/15 loss: 0.6156 Acc: 75.0000% F1: 0.699 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 12/15 loss: 0.5525 Acc: 84.3750% F1: 0.828 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 13/15 loss: 0.6742 Acc: 65.6250% F1: 0.552 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 14/15 loss: 0.0329 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 71.1111% F1: 0.6467 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/5 iter: 0/2 loss: 0.9291 Acc: 53.1250% F1: 0.317 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6048 Acc: 33.3333% F1: 0.267 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3149 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 54.8000% F1: 0.2366
	Epoch 1 Accuracy: 52.0000% F1: 0.2816
	Epoch 2 Accuracy: 51.2000% F1: 0.3187
	Epoch 3 Accuracy: 48.8000% F1: 0.3077
	Epoch 4 Accuracy: 46.0000% F1: 0.3511
all done :)
************************************
** MODEL TIME ID: 20220225-142959 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/5 iter: 0/15 loss: 1.2424 Acc: 43.7500% F1: 0.297 Time: 0.97s (0.00s)
Fold 0 train - epoch: 0/5 iter: 1/15 loss: 0.9546 Acc: 50.0000% F1: 0.272 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 2/15 loss: 1.1527 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 3/15 loss: 0.9955 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 0 train - epoch: 0/5 iter: 4/15 loss: 0.9965 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 0 train - epoch: 0/5 iter: 5/15 loss: 0.8704 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 0 train - epoch: 0/5 iter: 6/15 loss: 0.8675 Acc: 71.8750% F1: 0.396 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 7/15 loss: 0.8703 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 8/15 loss: 1.0494 Acc: 50.0000% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 9/15 loss: 0.9176 Acc: 65.6250% F1: 0.466 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 10/15 loss: 0.9849 Acc: 50.0000% F1: 0.343 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 11/15 loss: 0.9461 Acc: 43.7500% F1: 0.280 Time: 0.96s (0.02s)
Fold 0 train - epoch: 0/5 iter: 12/15 loss: 1.0402 Acc: 43.7500% F1: 0.269 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 13/15 loss: 0.8885 Acc: 50.0000% F1: 0.264 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 14/15 loss: 0.6808 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 53.3333% F1: 0.3273 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6245 Acc: 78.1250% F1: 0.439 Time: 0.32s (0.00s)
Fold 0 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5909 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 50.0000% F1: 0.2252 *
*********************************************************
Performing epoch 1 of 5
Fold 0 train - epoch: 1/5 iter: 0/15 loss: 0.9048 Acc: 56.2500% F1: 0.240 Time: 0.97s (0.00s)
Fold 0 train - epoch: 1/5 iter: 1/15 loss: 0.8363 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 2/15 loss: 0.9890 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 3/15 loss: 0.8049 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 4/15 loss: 0.9907 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 5/15 loss: 0.9231 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 6/15 loss: 0.7642 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 7/15 loss: 0.9328 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 8/15 loss: 0.9831 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 9/15 loss: 0.8840 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 10/15 loss: 0.9545 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 11/15 loss: 0.8321 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 12/15 loss: 0.9861 Acc: 53.1250% F1: 0.283 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 13/15 loss: 0.9221 Acc: 46.8750% F1: 0.251 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 14/15 loss: 0.5950 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 55.1111% F1: 0.2602 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/5 iter: 0/2 loss: 0.8043 Acc: 68.7500% F1: 0.583 Time: 0.32s (0.00s)
Fold 0 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3122 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 56.0000% F1: 0.3851 *
*********************************************************
Performing epoch 2 of 5
Fold 0 train - epoch: 2/5 iter: 0/15 loss: 0.8515 Acc: 56.2500% F1: 0.292 Time: 0.96s (0.00s)
Fold 0 train - epoch: 2/5 iter: 1/15 loss: 0.7945 Acc: 71.8750% F1: 0.599 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 2/15 loss: 0.8841 Acc: 53.1250% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 3/15 loss: 0.7713 Acc: 65.6250% F1: 0.361 Time: 0.94s (0.02s)
Fold 0 train - epoch: 2/5 iter: 4/15 loss: 0.8958 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 5/15 loss: 0.8334 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 6/15 loss: 0.6958 Acc: 71.8750% F1: 0.351 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 7/15 loss: 0.8382 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 8/15 loss: 0.9485 Acc: 59.3750% F1: 0.317 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 9/15 loss: 0.8070 Acc: 68.7500% F1: 0.479 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 10/15 loss: 0.9645 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 11/15 loss: 0.7851 Acc: 62.5000% F1: 0.351 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 12/15 loss: 0.9454 Acc: 56.2500% F1: 0.358 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 13/15 loss: 0.8016 Acc: 62.5000% F1: 0.377 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 14/15 loss: 0.2247 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 59.7778% F1: 0.3596 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8781 Acc: 56.2500% F1: 0.333 Time: 0.32s (0.00s)
Fold 0 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3402 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 48.0000% F1: 0.3390 *
*********************************************************
Performing epoch 3 of 5
Fold 0 train - epoch: 3/5 iter: 0/15 loss: 0.7619 Acc: 59.3750% F1: 0.389 Time: 0.94s (0.00s)
Fold 0 train - epoch: 3/5 iter: 1/15 loss: 0.7078 Acc: 78.1250% F1: 0.736 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 2/15 loss: 0.8260 Acc: 59.3750% F1: 0.400 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 3/15 loss: 0.6204 Acc: 75.0000% F1: 0.675 Time: 0.94s (0.02s)
Fold 0 train - epoch: 3/5 iter: 4/15 loss: 0.7153 Acc: 65.6250% F1: 0.559 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 5/15 loss: 0.7055 Acc: 65.6250% F1: 0.400 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 6/15 loss: 0.6200 Acc: 78.1250% F1: 0.482 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 7/15 loss: 0.7639 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 8/15 loss: 0.8570 Acc: 65.6250% F1: 0.477 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 9/15 loss: 0.7120 Acc: 71.8750% F1: 0.619 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 10/15 loss: 0.8498 Acc: 62.5000% F1: 0.442 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 11/15 loss: 0.6186 Acc: 68.7500% F1: 0.465 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 12/15 loss: 0.8080 Acc: 68.7500% F1: 0.582 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 13/15 loss: 0.6412 Acc: 68.7500% F1: 0.612 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 14/15 loss: 0.0889 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 67.5556% F1: 0.5321 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/5 iter: 0/2 loss: 1.1152 Acc: 40.6250% F1: 0.266 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4894 Acc: 44.4444% F1: 0.317 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 42.0000% F1: 0.3612 *
*********************************************************
Performing epoch 4 of 5
Fold 0 train - epoch: 4/5 iter: 0/15 loss: 0.6006 Acc: 75.0000% F1: 0.513 Time: 0.94s (0.00s)
Fold 0 train - epoch: 4/5 iter: 1/15 loss: 0.5284 Acc: 87.5000% F1: 0.876 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 2/15 loss: 0.6750 Acc: 78.1250% F1: 0.668 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 3/15 loss: 0.4231 Acc: 87.5000% F1: 0.835 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 4/15 loss: 0.5289 Acc: 84.3750% F1: 0.841 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 5/15 loss: 0.5245 Acc: 71.8750% F1: 0.674 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 6/15 loss: 0.5292 Acc: 78.1250% F1: 0.492 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 7/15 loss: 0.5487 Acc: 81.2500% F1: 0.764 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 8/15 loss: 0.7394 Acc: 71.8750% F1: 0.662 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 9/15 loss: 0.4631 Acc: 87.5000% F1: 0.744 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 10/15 loss: 0.8028 Acc: 62.5000% F1: 0.451 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 11/15 loss: 0.5161 Acc: 78.1250% F1: 0.517 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 12/15 loss: 0.7693 Acc: 56.2500% F1: 0.506 Time: 0.96s (0.02s)
Fold 0 train - epoch: 4/5 iter: 13/15 loss: 0.4954 Acc: 68.7500% F1: 0.522 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 14/15 loss: 0.0193 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 76.4444% F1: 0.6824 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/5 iter: 0/2 loss: 1.5034 Acc: 37.5000% F1: 0.264 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5838 Acc: 44.4444% F1: 0.306 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 40.0000% F1: 0.3420 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/5 iter: 0/15 loss: 1.1520 Acc: 28.1250% F1: 0.170 Time: 0.97s (0.00s)
Fold 1 train - epoch: 0/5 iter: 1/15 loss: 1.0144 Acc: 46.8750% F1: 0.261 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 2/15 loss: 1.1240 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 3/15 loss: 1.0193 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 4/15 loss: 1.0650 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/5 iter: 5/15 loss: 0.9213 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 6/15 loss: 0.8903 Acc: 65.6250% F1: 0.322 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 7/15 loss: 0.8934 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 8/15 loss: 0.9902 Acc: 56.2500% F1: 0.362 Time: 0.96s (0.02s)
Fold 1 train - epoch: 0/5 iter: 9/15 loss: 0.9274 Acc: 53.1250% F1: 0.375 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 10/15 loss: 0.9863 Acc: 53.1250% F1: 0.377 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 11/15 loss: 0.9896 Acc: 43.7500% F1: 0.239 Time: 0.96s (0.02s)
Fold 1 train - epoch: 0/5 iter: 12/15 loss: 1.0621 Acc: 50.0000% F1: 0.340 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 13/15 loss: 0.9285 Acc: 50.0000% F1: 0.291 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 14/15 loss: 0.6504 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 51.7778% F1: 0.3196 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6703 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5948 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 1 train - epoch: 1/5 iter: 0/15 loss: 0.8391 Acc: 59.3750% F1: 0.378 Time: 0.94s (0.00s)
Fold 1 train - epoch: 1/5 iter: 1/15 loss: 0.8468 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.02s)
Fold 1 train - epoch: 1/5 iter: 2/15 loss: 0.9468 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 3/15 loss: 0.8299 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 4/15 loss: 1.0027 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 5/15 loss: 0.9560 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 6/15 loss: 0.8225 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 7/15 loss: 0.9081 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 8/15 loss: 0.9617 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 9/15 loss: 0.9100 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 10/15 loss: 0.9774 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 11/15 loss: 0.9015 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 12/15 loss: 1.0100 Acc: 50.0000% F1: 0.269 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 13/15 loss: 0.8738 Acc: 59.3750% F1: 0.301 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 14/15 loss: 0.5167 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 55.5556% F1: 0.2745 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7974 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3192 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.2973 *
*********************************************************
Performing epoch 2 of 5
Fold 1 train - epoch: 2/5 iter: 0/15 loss: 0.8382 Acc: 62.5000% F1: 0.480 Time: 0.95s (0.00s)
Fold 1 train - epoch: 2/5 iter: 1/15 loss: 0.7997 Acc: 71.8750% F1: 0.492 Time: 0.95s (0.04s)
Fold 1 train - epoch: 2/5 iter: 2/15 loss: 0.8921 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.03s)
Fold 1 train - epoch: 2/5 iter: 3/15 loss: 0.8023 Acc: 62.5000% F1: 0.345 Time: 0.94s (0.02s)
Fold 1 train - epoch: 2/5 iter: 4/15 loss: 0.8845 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 5/15 loss: 0.8463 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 6/15 loss: 0.7505 Acc: 71.8750% F1: 0.351 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 7/15 loss: 0.8046 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 8/15 loss: 0.9627 Acc: 59.3750% F1: 0.319 Time: 0.95s (0.05s)
Fold 1 train - epoch: 2/5 iter: 9/15 loss: 0.8732 Acc: 56.2500% F1: 0.350 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 10/15 loss: 0.9935 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 11/15 loss: 0.8257 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 12/15 loss: 0.9115 Acc: 56.2500% F1: 0.297 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 13/15 loss: 0.8139 Acc: 53.1250% F1: 0.236 Time: 0.96s (0.02s)
Fold 1 train - epoch: 2/5 iter: 14/15 loss: 0.2871 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 58.4444% F1: 0.3281 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7899 Acc: 59.3750% F1: 0.335 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3811 Acc: 22.2222% F1: 0.157 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.3171 *
*********************************************************
Performing epoch 3 of 5
Fold 1 train - epoch: 3/5 iter: 0/15 loss: 0.7291 Acc: 65.6250% F1: 0.528 Time: 0.94s (0.00s)
Fold 1 train - epoch: 3/5 iter: 1/15 loss: 0.7092 Acc: 75.0000% F1: 0.640 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 2/15 loss: 0.8281 Acc: 53.1250% F1: 0.345 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 3/15 loss: 0.6813 Acc: 71.8750% F1: 0.470 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 4/15 loss: 0.7233 Acc: 68.7500% F1: 0.560 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 5/15 loss: 0.7212 Acc: 65.6250% F1: 0.548 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 6/15 loss: 0.6799 Acc: 78.1250% F1: 0.482 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 7/15 loss: 0.7178 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 8/15 loss: 0.8559 Acc: 65.6250% F1: 0.477 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 9/15 loss: 0.7291 Acc: 68.7500% F1: 0.479 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 10/15 loss: 0.9139 Acc: 46.8750% F1: 0.306 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 11/15 loss: 0.7288 Acc: 56.2500% F1: 0.344 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 12/15 loss: 0.8669 Acc: 59.3750% F1: 0.477 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 13/15 loss: 0.7326 Acc: 65.6250% F1: 0.542 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 14/15 loss: 0.1122 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 64.4444% F1: 0.4904 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/5 iter: 0/2 loss: 1.0496 Acc: 37.5000% F1: 0.234 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3425 Acc: 38.8889% F1: 0.212 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 38.0000% F1: 0.2877 *
*********************************************************
Performing epoch 4 of 5
Fold 1 train - epoch: 4/5 iter: 0/15 loss: 0.5999 Acc: 71.8750% F1: 0.683 Time: 0.95s (0.00s)
Fold 1 train - epoch: 4/5 iter: 1/15 loss: 0.6054 Acc: 78.1250% F1: 0.771 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/5 iter: 2/15 loss: 0.6985 Acc: 71.8750% F1: 0.703 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 3/15 loss: 0.5038 Acc: 87.5000% F1: 0.835 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 4/15 loss: 0.4918 Acc: 87.5000% F1: 0.817 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 5/15 loss: 0.5894 Acc: 75.0000% F1: 0.679 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 6/15 loss: 0.5987 Acc: 71.8750% F1: 0.391 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 7/15 loss: 0.6602 Acc: 59.3750% F1: 0.583 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 8/15 loss: 0.6830 Acc: 71.8750% F1: 0.662 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 9/15 loss: 0.5384 Acc: 78.1250% F1: 0.752 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 10/15 loss: 0.7198 Acc: 59.3750% F1: 0.539 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 11/15 loss: 0.5844 Acc: 68.7500% F1: 0.466 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 12/15 loss: 0.6021 Acc: 84.3750% F1: 0.813 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 13/15 loss: 0.4962 Acc: 75.0000% F1: 0.642 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 14/15 loss: 0.0354 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 74.4444% F1: 0.6961 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/5 iter: 0/2 loss: 0.9652 Acc: 46.8750% F1: 0.285 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 4/5 iter: 1/2 loss: 1.4937 Acc: 33.3333% F1: 0.241 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 42.0000% F1: 0.3510 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/5 iter: 0/15 loss: 1.1505 Acc: 37.5000% F1: 0.317 Time: 0.97s (0.00s)
Fold 2 train - epoch: 0/5 iter: 1/15 loss: 0.8985 Acc: 53.1250% F1: 0.280 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 2/15 loss: 1.1048 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 3/15 loss: 0.9776 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 4/15 loss: 1.0521 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 5/15 loss: 0.9978 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 6/15 loss: 0.9370 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 7/15 loss: 0.9325 Acc: 46.8750% F1: 0.286 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 8/15 loss: 1.0900 Acc: 46.8750% F1: 0.265 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 9/15 loss: 0.9581 Acc: 56.2500% F1: 0.400 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 10/15 loss: 0.9986 Acc: 43.7500% F1: 0.310 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 11/15 loss: 0.9440 Acc: 56.2500% F1: 0.344 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 12/15 loss: 1.0635 Acc: 59.3750% F1: 0.413 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 13/15 loss: 0.9348 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 14/15 loss: 0.6870 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.03s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 51.7778% F1: 0.3290 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6002 Acc: 87.5000% F1: 0.632 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5003 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2750 *
*********************************************************
Performing epoch 1 of 5
Fold 2 train - epoch: 1/5 iter: 0/15 loss: 0.8594 Acc: 59.3750% F1: 0.378 Time: 0.94s (0.00s)
Fold 2 train - epoch: 1/5 iter: 1/15 loss: 0.8386 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 2/15 loss: 0.9619 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 3/15 loss: 0.8379 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 4/15 loss: 0.9904 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 5/15 loss: 0.9145 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 6/15 loss: 0.7527 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 7/15 loss: 0.8952 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 8/15 loss: 1.0820 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 9/15 loss: 0.9721 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 10/15 loss: 1.0012 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 11/15 loss: 0.8367 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 12/15 loss: 0.9785 Acc: 59.3750% F1: 0.352 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 13/15 loss: 0.9090 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 14/15 loss: 0.6327 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 55.3333% F1: 0.2732 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7683 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2308 Acc: 16.6667% F1: 0.111 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 50.0000% F1: 0.3043 *
*********************************************************
Performing epoch 2 of 5
Fold 2 train - epoch: 2/5 iter: 0/15 loss: 0.8470 Acc: 59.3750% F1: 0.466 Time: 0.94s (0.00s)
Fold 2 train - epoch: 2/5 iter: 1/15 loss: 0.8382 Acc: 62.5000% F1: 0.389 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 2/15 loss: 0.9081 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 3/15 loss: 0.8067 Acc: 65.6250% F1: 0.361 Time: 0.94s (0.02s)
Fold 2 train - epoch: 2/5 iter: 4/15 loss: 0.9076 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 5/15 loss: 0.8412 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 6/15 loss: 0.7717 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 7/15 loss: 0.8592 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 8/15 loss: 0.9718 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 9/15 loss: 0.9360 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 10/15 loss: 0.9645 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 11/15 loss: 0.8080 Acc: 62.5000% F1: 0.314 Time: 0.96s (0.04s)
Fold 2 train - epoch: 2/5 iter: 12/15 loss: 0.9239 Acc: 56.2500% F1: 0.334 Time: 0.96s (0.04s)
Fold 2 train - epoch: 2/5 iter: 13/15 loss: 0.8289 Acc: 56.2500% F1: 0.287 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 14/15 loss: 0.2321 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 56.4444% F1: 0.3029 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7074 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3575 Acc: 22.2222% F1: 0.140 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.3192 *
*********************************************************
Performing epoch 3 of 5
Fold 2 train - epoch: 3/5 iter: 0/15 loss: 0.7862 Acc: 62.5000% F1: 0.480 Time: 0.94s (0.00s)
Fold 2 train - epoch: 3/5 iter: 1/15 loss: 0.7912 Acc: 75.0000% F1: 0.640 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 2/15 loss: 0.8607 Acc: 68.7500% F1: 0.472 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 3/15 loss: 0.7121 Acc: 75.0000% F1: 0.495 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 4/15 loss: 0.7714 Acc: 62.5000% F1: 0.419 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 5/15 loss: 0.7970 Acc: 71.8750% F1: 0.590 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 6/15 loss: 0.7151 Acc: 68.7500% F1: 0.346 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 7/15 loss: 0.7802 Acc: 53.1250% F1: 0.322 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 8/15 loss: 0.9444 Acc: 62.5000% F1: 0.469 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 9/15 loss: 0.7886 Acc: 68.7500% F1: 0.594 Time: 0.95s (0.03s)
Fold 2 train - epoch: 3/5 iter: 10/15 loss: 0.8951 Acc: 53.1250% F1: 0.376 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 11/15 loss: 0.6935 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 12/15 loss: 0.8483 Acc: 68.7500% F1: 0.573 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 13/15 loss: 0.8255 Acc: 53.1250% F1: 0.328 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 14/15 loss: 0.0896 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 64.6667% F1: 0.4917 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8064 Acc: 50.0000% F1: 0.446 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4577 Acc: 27.7778% F1: 0.159 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 42.0000% F1: 0.2940 *
*********************************************************
Performing epoch 4 of 5
Fold 2 train - epoch: 4/5 iter: 0/15 loss: 0.5646 Acc: 78.1250% F1: 0.728 Time: 0.94s (0.00s)
Fold 2 train - epoch: 4/5 iter: 1/15 loss: 0.6488 Acc: 81.2500% F1: 0.850 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 2/15 loss: 0.7452 Acc: 75.0000% F1: 0.526 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 3/15 loss: 0.5778 Acc: 75.0000% F1: 0.478 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 4/15 loss: 0.6186 Acc: 81.2500% F1: 0.769 Time: 0.95s (0.03s)
Fold 2 train - epoch: 4/5 iter: 5/15 loss: 0.5617 Acc: 75.0000% F1: 0.706 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 6/15 loss: 0.5435 Acc: 87.5000% F1: 0.569 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 7/15 loss: 0.6711 Acc: 65.6250% F1: 0.642 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 8/15 loss: 0.7167 Acc: 71.8750% F1: 0.689 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 9/15 loss: 0.7163 Acc: 75.0000% F1: 0.650 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 10/15 loss: 0.8275 Acc: 50.0000% F1: 0.360 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 11/15 loss: 0.6434 Acc: 65.6250% F1: 0.427 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 12/15 loss: 0.6939 Acc: 78.1250% F1: 0.717 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 13/15 loss: 0.6249 Acc: 78.1250% F1: 0.770 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 14/15 loss: 0.0295 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 74.2222% F1: 0.6749 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8643 Acc: 56.2500% F1: 0.338 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6034 Acc: 27.7778% F1: 0.152 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3256 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/5 iter: 0/15 loss: 1.2074 Acc: 34.3750% F1: 0.221 Time: 0.96s (0.00s)
Fold 3 train - epoch: 0/5 iter: 1/15 loss: 1.0029 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 2/15 loss: 1.1266 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 3/15 loss: 0.9706 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 4/15 loss: 1.0162 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 5/15 loss: 0.9339 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 6/15 loss: 0.8247 Acc: 71.8750% F1: 0.439 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 7/15 loss: 0.9265 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 8/15 loss: 1.0932 Acc: 43.7500% F1: 0.251 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 9/15 loss: 0.9672 Acc: 53.1250% F1: 0.349 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 10/15 loss: 1.0056 Acc: 37.5000% F1: 0.271 Time: 0.95s (0.03s)
Fold 3 train - epoch: 0/5 iter: 11/15 loss: 0.9059 Acc: 62.5000% F1: 0.377 Time: 0.96s (0.03s)
Fold 3 train - epoch: 0/5 iter: 12/15 loss: 0.9390 Acc: 62.5000% F1: 0.389 Time: 0.96s (0.02s)
Fold 3 train - epoch: 0/5 iter: 13/15 loss: 0.9255 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 14/15 loss: 0.8523 Acc: 0.0000% F1: 0.000 Time: 0.11s (0.02s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 52.6667% F1: 0.3138 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5962 Acc: 78.1250% F1: 0.439 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 0/5 iter: 1/2 loss: 1.7652 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 52.0000% F1: 0.2586 *
*********************************************************
Performing epoch 1 of 5
Fold 3 train - epoch: 1/5 iter: 0/15 loss: 0.8661 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.00s)
Fold 3 train - epoch: 1/5 iter: 1/15 loss: 0.8706 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 2/15 loss: 0.9439 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 3/15 loss: 0.8068 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 4/15 loss: 0.9818 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 5/15 loss: 0.9200 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 6/15 loss: 0.7559 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 7/15 loss: 0.8759 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 8/15 loss: 1.0657 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 9/15 loss: 0.9289 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 10/15 loss: 0.9818 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 11/15 loss: 0.8399 Acc: 62.5000% F1: 0.314 Time: 0.96s (0.03s)
Fold 3 train - epoch: 1/5 iter: 12/15 loss: 0.9140 Acc: 68.7500% F1: 0.466 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 13/15 loss: 0.8958 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 14/15 loss: 0.6297 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 56.0000% F1: 0.2666 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7387 Acc: 75.0000% F1: 0.526 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4027 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3024 *
*********************************************************
Performing epoch 2 of 5
Fold 3 train - epoch: 2/5 iter: 0/15 loss: 0.8561 Acc: 62.5000% F1: 0.444 Time: 0.95s (0.00s)
Fold 3 train - epoch: 2/5 iter: 1/15 loss: 0.8263 Acc: 75.0000% F1: 0.522 Time: 0.95s (0.03s)
Fold 3 train - epoch: 2/5 iter: 2/15 loss: 0.8866 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.03s)
Fold 3 train - epoch: 2/5 iter: 3/15 loss: 0.8037 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.03s)
Fold 3 train - epoch: 2/5 iter: 4/15 loss: 0.8728 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.03s)
Fold 3 train - epoch: 2/5 iter: 5/15 loss: 0.8429 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 3 train - epoch: 2/5 iter: 6/15 loss: 0.7334 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 7/15 loss: 0.8520 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 8/15 loss: 1.0381 Acc: 53.1250% F1: 0.231 Time: 0.96s (0.02s)
Fold 3 train - epoch: 2/5 iter: 9/15 loss: 0.9213 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.05s)
Fold 3 train - epoch: 2/5 iter: 10/15 loss: 0.9872 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 11/15 loss: 0.7756 Acc: 62.5000% F1: 0.314 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 12/15 loss: 0.8815 Acc: 59.3750% F1: 0.306 Time: 0.96s (0.02s)
Fold 3 train - epoch: 2/5 iter: 13/15 loss: 0.8033 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 14/15 loss: 0.4202 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 58.2222% F1: 0.3134 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6938 Acc: 71.8750% F1: 0.566 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/5 iter: 1/2 loss: 1.5331 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3533 *
*********************************************************
Performing epoch 3 of 5
Fold 3 train - epoch: 3/5 iter: 0/15 loss: 0.7463 Acc: 65.6250% F1: 0.500 Time: 0.94s (0.00s)
Fold 3 train - epoch: 3/5 iter: 1/15 loss: 0.7079 Acc: 75.0000% F1: 0.522 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 2/15 loss: 0.8755 Acc: 59.3750% F1: 0.362 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 3/15 loss: 0.7028 Acc: 75.0000% F1: 0.675 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 4/15 loss: 0.7682 Acc: 68.7500% F1: 0.579 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 5/15 loss: 0.7217 Acc: 68.7500% F1: 0.423 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 6/15 loss: 0.6797 Acc: 75.0000% F1: 0.440 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 7/15 loss: 0.8012 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 8/15 loss: 0.9323 Acc: 65.6250% F1: 0.556 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 9/15 loss: 0.8633 Acc: 65.6250% F1: 0.454 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 10/15 loss: 0.9476 Acc: 59.3750% F1: 0.408 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 11/15 loss: 0.7059 Acc: 65.6250% F1: 0.414 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 12/15 loss: 0.7097 Acc: 68.7500% F1: 0.583 Time: 0.96s (0.03s)
Fold 3 train - epoch: 3/5 iter: 13/15 loss: 0.7906 Acc: 62.5000% F1: 0.558 Time: 0.96s (0.02s)
Fold 3 train - epoch: 3/5 iter: 14/15 loss: 0.1422 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 66.6667% F1: 0.5117 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7645 Acc: 62.5000% F1: 0.339 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/5 iter: 1/2 loss: 1.7076 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.3290 *
*********************************************************
Performing epoch 4 of 5
Fold 3 train - epoch: 4/5 iter: 0/15 loss: 0.6697 Acc: 65.6250% F1: 0.512 Time: 0.94s (0.00s)
Fold 3 train - epoch: 4/5 iter: 1/15 loss: 0.5549 Acc: 81.2500% F1: 0.780 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 2/15 loss: 0.7037 Acc: 71.8750% F1: 0.651 Time: 0.95s (0.03s)
Fold 3 train - epoch: 4/5 iter: 3/15 loss: 0.4844 Acc: 87.5000% F1: 0.900 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 4/15 loss: 0.5631 Acc: 75.0000% F1: 0.682 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 5/15 loss: 0.5562 Acc: 84.3750% F1: 0.787 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 6/15 loss: 0.5409 Acc: 84.3750% F1: 0.562 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 7/15 loss: 0.6797 Acc: 65.6250% F1: 0.736 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 8/15 loss: 0.7983 Acc: 65.6250% F1: 0.565 Time: 0.95s (0.03s)
Fold 3 train - epoch: 4/5 iter: 9/15 loss: 0.6535 Acc: 75.0000% F1: 0.646 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 10/15 loss: 0.9720 Acc: 50.0000% F1: 0.356 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 11/15 loss: 0.4922 Acc: 81.2500% F1: 0.686 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 12/15 loss: 0.5304 Acc: 78.1250% F1: 0.741 Time: 0.96s (0.02s)
Fold 3 train - epoch: 4/5 iter: 13/15 loss: 0.5417 Acc: 81.2500% F1: 0.776 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 14/15 loss: 0.0531 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 74.8889% F1: 0.6833 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0075 Acc: 65.6250% F1: 0.419 Time: 0.32s (0.00s)
Fold 3 train-dev - epoch: 4/5 iter: 1/2 loss: 1.8110 Acc: 44.4444% F1: 0.320 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 58.0000% F1: 0.4789 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/5 iter: 0/15 loss: 1.1807 Acc: 37.5000% F1: 0.264 Time: 0.98s (0.00s)
Fold 4 train - epoch: 0/5 iter: 1/15 loss: 0.9569 Acc: 53.1250% F1: 0.311 Time: 0.94s (0.04s)
Fold 4 train - epoch: 0/5 iter: 2/15 loss: 1.0173 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 3/15 loss: 0.9754 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 4/15 loss: 1.0112 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 5/15 loss: 0.9499 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 6/15 loss: 0.9150 Acc: 53.1250% F1: 0.241 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 7/15 loss: 0.8710 Acc: 46.8750% F1: 0.286 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 8/15 loss: 1.1843 Acc: 37.5000% F1: 0.221 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 9/15 loss: 0.9451 Acc: 50.0000% F1: 0.350 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 10/15 loss: 1.0299 Acc: 43.7500% F1: 0.315 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 11/15 loss: 0.9490 Acc: 53.1250% F1: 0.327 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 12/15 loss: 1.0045 Acc: 50.0000% F1: 0.295 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 13/15 loss: 0.9058 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 14/15 loss: 0.6238 Acc: 100.0000% F1: 1.000 Time: 0.10s (0.02s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 49.3333% F1: 0.2947 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5905 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6215 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2739 *
*********************************************************
Performing epoch 1 of 5
Fold 4 train - epoch: 1/5 iter: 0/15 loss: 0.8375 Acc: 59.3750% F1: 0.378 Time: 0.94s (0.00s)
Fold 4 train - epoch: 1/5 iter: 1/15 loss: 0.8973 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 2/15 loss: 0.9240 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 3/15 loss: 0.7970 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 4/15 loss: 0.9860 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 4 train - epoch: 1/5 iter: 5/15 loss: 0.9165 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 6/15 loss: 0.7789 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 4 train - epoch: 1/5 iter: 7/15 loss: 0.9434 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 8/15 loss: 1.0784 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 9/15 loss: 0.9895 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 10/15 loss: 0.9893 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 11/15 loss: 0.8500 Acc: 62.5000% F1: 0.314 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 12/15 loss: 0.9246 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 4 train - epoch: 1/5 iter: 13/15 loss: 0.8939 Acc: 62.5000% F1: 0.396 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 14/15 loss: 0.6915 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 56.2222% F1: 0.2801 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7589 Acc: 68.7500% F1: 0.407 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3290 Acc: 22.2222% F1: 0.157 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3177 *
*********************************************************
Performing epoch 2 of 5
Fold 4 train - epoch: 2/5 iter: 0/15 loss: 0.8169 Acc: 65.6250% F1: 0.544 Time: 0.94s (0.00s)
Fold 4 train - epoch: 2/5 iter: 1/15 loss: 0.8936 Acc: 68.7500% F1: 0.543 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 2/15 loss: 0.8734 Acc: 50.0000% F1: 0.262 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 3/15 loss: 0.8145 Acc: 62.5000% F1: 0.310 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 4/15 loss: 0.8347 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 5/15 loss: 0.8199 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 6/15 loss: 0.7510 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 7/15 loss: 0.8258 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 8/15 loss: 1.0564 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 9/15 loss: 0.9602 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 4 train - epoch: 2/5 iter: 10/15 loss: 0.9790 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 11/15 loss: 0.7782 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 12/15 loss: 0.9062 Acc: 59.3750% F1: 0.306 Time: 0.96s (0.02s)
Fold 4 train - epoch: 2/5 iter: 13/15 loss: 0.8300 Acc: 56.2500% F1: 0.287 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 14/15 loss: 0.3554 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 57.1111% F1: 0.3121 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6938 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4404 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3119 *
*********************************************************
Performing epoch 3 of 5
Fold 4 train - epoch: 3/5 iter: 0/15 loss: 0.7479 Acc: 62.5000% F1: 0.444 Time: 0.94s (0.00s)
Fold 4 train - epoch: 3/5 iter: 1/15 loss: 0.7915 Acc: 71.8750% F1: 0.599 Time: 0.94s (0.03s)
Fold 4 train - epoch: 3/5 iter: 2/15 loss: 0.8387 Acc: 56.2500% F1: 0.345 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 3/15 loss: 0.7128 Acc: 75.0000% F1: 0.691 Time: 0.94s (0.02s)
Fold 4 train - epoch: 3/5 iter: 4/15 loss: 0.7264 Acc: 71.8750% F1: 0.649 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 5/15 loss: 0.7204 Acc: 71.8750% F1: 0.570 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 6/15 loss: 0.6914 Acc: 68.7500% F1: 0.335 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 7/15 loss: 0.7490 Acc: 59.3750% F1: 0.671 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 8/15 loss: 0.8622 Acc: 59.3750% F1: 0.393 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 9/15 loss: 0.8739 Acc: 56.2500% F1: 0.368 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 10/15 loss: 0.9005 Acc: 50.0000% F1: 0.350 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 11/15 loss: 0.6815 Acc: 68.7500% F1: 0.570 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 12/15 loss: 0.8658 Acc: 65.6250% F1: 0.456 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 13/15 loss: 0.7674 Acc: 65.6250% F1: 0.623 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 14/15 loss: 0.1242 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 64.6667% F1: 0.5082 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7919 Acc: 59.3750% F1: 0.248 Time: 0.32s (0.00s)
Fold 4 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4144 Acc: 27.7778% F1: 0.231 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.3706 *
*********************************************************
Performing epoch 4 of 5
Fold 4 train - epoch: 4/5 iter: 0/15 loss: 0.6076 Acc: 75.0000% F1: 0.716 Time: 0.96s (0.00s)
Fold 4 train - epoch: 4/5 iter: 1/15 loss: 0.6841 Acc: 75.0000% F1: 0.677 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 2/15 loss: 0.6377 Acc: 78.1250% F1: 0.745 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 3/15 loss: 0.5781 Acc: 84.3750% F1: 0.776 Time: 0.95s (0.03s)
Fold 4 train - epoch: 4/5 iter: 4/15 loss: 0.5748 Acc: 81.2500% F1: 0.784 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 5/15 loss: 0.5246 Acc: 78.1250% F1: 0.755 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 6/15 loss: 0.5120 Acc: 84.3750% F1: 0.554 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 7/15 loss: 0.6707 Acc: 65.6250% F1: 0.754 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 8/15 loss: 0.7881 Acc: 62.5000% F1: 0.504 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 9/15 loss: 0.7818 Acc: 65.6250% F1: 0.562 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 10/15 loss: 0.8717 Acc: 56.2500% F1: 0.502 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 11/15 loss: 0.4979 Acc: 81.2500% F1: 0.554 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 12/15 loss: 0.6915 Acc: 71.8750% F1: 0.600 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 13/15 loss: 0.5788 Acc: 75.0000% F1: 0.691 Time: 0.96s (0.02s)
Fold 4 train - epoch: 4/5 iter: 14/15 loss: 0.0338 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 74.0000% F1: 0.6726 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8629 Acc: 59.3750% F1: 0.253 Time: 0.32s (0.00s)
Fold 4 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6025 Acc: 27.7778% F1: 0.209 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3619 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/5 iter: 0/15 loss: 1.1857 Acc: 31.2500% F1: 0.201 Time: 0.97s (0.00s)
Fold 5 train - epoch: 0/5 iter: 1/15 loss: 1.0160 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/5 iter: 2/15 loss: 1.0888 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 3/15 loss: 0.9265 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/5 iter: 4/15 loss: 0.9581 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 5/15 loss: 0.8941 Acc: 62.5000% F1: 0.319 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 6/15 loss: 0.8658 Acc: 68.7500% F1: 0.343 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 7/15 loss: 0.8520 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 8/15 loss: 1.0928 Acc: 53.1250% F1: 0.327 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 9/15 loss: 1.0131 Acc: 53.1250% F1: 0.362 Time: 0.96s (0.03s)
Fold 5 train - epoch: 0/5 iter: 10/15 loss: 1.0000 Acc: 40.6250% F1: 0.285 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 11/15 loss: 0.9797 Acc: 53.1250% F1: 0.307 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 12/15 loss: 0.9825 Acc: 65.6250% F1: 0.439 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 13/15 loss: 0.9473 Acc: 50.0000% F1: 0.265 Time: 0.96s (0.03s)
Fold 5 train - epoch: 0/5 iter: 14/15 loss: 0.6701 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 53.5556% F1: 0.3226 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6418 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5168 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 58.0000% F1: 0.3102 *
*********************************************************
Performing epoch 1 of 5
Fold 5 train - epoch: 1/5 iter: 0/15 loss: 0.8338 Acc: 56.2500% F1: 0.369 Time: 0.94s (0.00s)
Fold 5 train - epoch: 1/5 iter: 1/15 loss: 0.8906 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 2/15 loss: 0.9578 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 3/15 loss: 0.8326 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 4/15 loss: 0.9801 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 5/15 loss: 0.8821 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 6/15 loss: 0.7681 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 7/15 loss: 0.9004 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 8/15 loss: 1.0743 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 9/15 loss: 0.9238 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 10/15 loss: 1.0070 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 11/15 loss: 0.8841 Acc: 65.6250% F1: 0.366 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 12/15 loss: 0.9445 Acc: 59.3750% F1: 0.306 Time: 0.96s (0.02s)
Fold 5 train - epoch: 1/5 iter: 13/15 loss: 0.9001 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 14/15 loss: 0.4964 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 55.7778% F1: 0.2750 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/5 iter: 0/2 loss: 0.8012 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2503 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 44.0000% F1: 0.2677 *
*********************************************************
Performing epoch 2 of 5
Fold 5 train - epoch: 2/5 iter: 0/15 loss: 0.8123 Acc: 68.7500% F1: 0.548 Time: 0.94s (0.00s)
Fold 5 train - epoch: 2/5 iter: 1/15 loss: 0.8757 Acc: 65.6250% F1: 0.439 Time: 0.95s (0.03s)
Fold 5 train - epoch: 2/5 iter: 2/15 loss: 0.8474 Acc: 59.3750% F1: 0.381 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 3/15 loss: 0.8045 Acc: 62.5000% F1: 0.369 Time: 0.94s (0.03s)
Fold 5 train - epoch: 2/5 iter: 4/15 loss: 0.8909 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 5/15 loss: 0.7912 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 6/15 loss: 0.7524 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 7/15 loss: 0.7987 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 8/15 loss: 1.0312 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 9/15 loss: 0.9647 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.03s)
Fold 5 train - epoch: 2/5 iter: 10/15 loss: 0.9881 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 11/15 loss: 0.7923 Acc: 65.6250% F1: 0.366 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 12/15 loss: 0.8946 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 13/15 loss: 0.8056 Acc: 59.3750% F1: 0.342 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 14/15 loss: 0.2596 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 58.8889% F1: 0.3447 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7514 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3221 Acc: 22.2222% F1: 0.157 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.3228 *
*********************************************************
Performing epoch 3 of 5
Fold 5 train - epoch: 3/5 iter: 0/15 loss: 0.7029 Acc: 65.6250% F1: 0.528 Time: 0.94s (0.00s)
Fold 5 train - epoch: 3/5 iter: 1/15 loss: 0.7543 Acc: 75.0000% F1: 0.632 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 2/15 loss: 0.8386 Acc: 65.6250% F1: 0.444 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 3/15 loss: 0.7595 Acc: 62.5000% F1: 0.582 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 4/15 loss: 0.7344 Acc: 71.8750% F1: 0.616 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 5/15 loss: 0.6821 Acc: 71.8750% F1: 0.678 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 6/15 loss: 0.6438 Acc: 75.0000% F1: 0.415 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 7/15 loss: 0.7048 Acc: 65.6250% F1: 0.736 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 8/15 loss: 0.9095 Acc: 56.2500% F1: 0.307 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 9/15 loss: 0.8059 Acc: 59.3750% F1: 0.506 Time: 0.96s (0.03s)
Fold 5 train - epoch: 3/5 iter: 10/15 loss: 0.9097 Acc: 53.1250% F1: 0.378 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 11/15 loss: 0.7516 Acc: 59.3750% F1: 0.517 Time: 0.96s (0.03s)
Fold 5 train - epoch: 3/5 iter: 12/15 loss: 0.7822 Acc: 68.7500% F1: 0.455 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 13/15 loss: 0.6468 Acc: 71.8750% F1: 0.619 Time: 0.96s (0.03s)
Fold 5 train - epoch: 3/5 iter: 14/15 loss: 0.0533 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 66.0000% F1: 0.5336 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/5 iter: 0/2 loss: 1.0128 Acc: 50.0000% F1: 0.282 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 3/5 iter: 1/2 loss: 1.2616 Acc: 44.4444% F1: 0.295 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.4071 *
*********************************************************
Performing epoch 4 of 5
Fold 5 train - epoch: 4/5 iter: 0/15 loss: 0.5719 Acc: 87.5000% F1: 0.865 Time: 0.95s (0.00s)
Fold 5 train - epoch: 4/5 iter: 1/15 loss: 0.6775 Acc: 75.0000% F1: 0.727 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/5 iter: 2/15 loss: 0.7215 Acc: 71.8750% F1: 0.614 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 3/15 loss: 0.6373 Acc: 68.7500% F1: 0.637 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 4/15 loss: 0.5492 Acc: 81.2500% F1: 0.764 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 5/15 loss: 0.5051 Acc: 78.1250% F1: 0.734 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 6/15 loss: 0.5225 Acc: 84.3750% F1: 0.556 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 7/15 loss: 0.6896 Acc: 68.7500% F1: 0.660 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 8/15 loss: 0.6022 Acc: 68.7500% F1: 0.593 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 9/15 loss: 0.6109 Acc: 78.1250% F1: 0.678 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 10/15 loss: 0.7698 Acc: 62.5000% F1: 0.446 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 11/15 loss: 0.6487 Acc: 68.7500% F1: 0.473 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 12/15 loss: 0.5287 Acc: 84.3750% F1: 0.765 Time: 0.96s (0.02s)
Fold 5 train - epoch: 4/5 iter: 13/15 loss: 0.4676 Acc: 81.2500% F1: 0.761 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 14/15 loss: 0.0314 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 75.7778% F1: 0.6849 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/5 iter: 0/2 loss: 1.2018 Acc: 43.7500% F1: 0.257 Time: 0.32s (0.00s)
Fold 5 train-dev - epoch: 4/5 iter: 1/2 loss: 1.4102 Acc: 33.3333% F1: 0.270 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 40.0000% F1: 0.3406 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/5 iter: 0/15 loss: 1.1665 Acc: 40.6250% F1: 0.288 Time: 0.98s (0.00s)
Fold 6 train - epoch: 0/5 iter: 1/15 loss: 0.9750 Acc: 59.3750% F1: 0.342 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/5 iter: 2/15 loss: 1.1113 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 3/15 loss: 0.9214 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 4/15 loss: 1.0530 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 5/15 loss: 0.9171 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 6/15 loss: 0.8487 Acc: 65.6250% F1: 0.328 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 7/15 loss: 0.8664 Acc: 53.1250% F1: 0.317 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 8/15 loss: 1.1402 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 9/15 loss: 1.0130 Acc: 53.1250% F1: 0.362 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 10/15 loss: 1.0256 Acc: 43.7500% F1: 0.310 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 11/15 loss: 0.9501 Acc: 53.1250% F1: 0.327 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 12/15 loss: 0.9766 Acc: 62.5000% F1: 0.391 Time: 0.95s (0.03s)
Fold 6 train - epoch: 0/5 iter: 13/15 loss: 0.9451 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 14/15 loss: 0.6507 Acc: 50.0000% F1: 0.333 Time: 0.10s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 52.4444% F1: 0.3130 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6315 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5293 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 6 train - epoch: 1/5 iter: 0/15 loss: 0.8643 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.00s)
Fold 6 train - epoch: 1/5 iter: 1/15 loss: 0.9033 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 2/15 loss: 0.9510 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 3/15 loss: 0.7806 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 4/15 loss: 0.9295 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 5/15 loss: 0.9123 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 6/15 loss: 0.7257 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 7/15 loss: 0.8918 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 8/15 loss: 1.0902 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 9/15 loss: 1.0220 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 10/15 loss: 0.9894 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 11/15 loss: 0.8506 Acc: 62.5000% F1: 0.261 Time: 0.96s (0.03s)
Fold 6 train - epoch: 1/5 iter: 12/15 loss: 0.9407 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 13/15 loss: 0.9009 Acc: 56.2500% F1: 0.287 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 14/15 loss: 0.6599 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 55.7778% F1: 0.2642 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7735 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2677 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3177 *
*********************************************************
Performing epoch 2 of 5
Fold 6 train - epoch: 2/5 iter: 0/15 loss: 0.8374 Acc: 59.3750% F1: 0.431 Time: 0.94s (0.00s)
Fold 6 train - epoch: 2/5 iter: 1/15 loss: 0.8516 Acc: 59.3750% F1: 0.342 Time: 0.95s (0.03s)
Fold 6 train - epoch: 2/5 iter: 2/15 loss: 0.8543 Acc: 56.2500% F1: 0.344 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 3/15 loss: 0.7710 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 4/15 loss: 0.8645 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 5/15 loss: 0.8216 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 6/15 loss: 0.7548 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 7/15 loss: 0.8232 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.03s)
Fold 6 train - epoch: 2/5 iter: 8/15 loss: 1.0525 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.03s)
Fold 6 train - epoch: 2/5 iter: 9/15 loss: 0.9909 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 10/15 loss: 1.0360 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 11/15 loss: 0.8274 Acc: 65.6250% F1: 0.327 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 12/15 loss: 0.9003 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.02s)
Fold 6 train - epoch: 2/5 iter: 13/15 loss: 0.8225 Acc: 68.7500% F1: 0.439 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 14/15 loss: 0.2804 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 57.7778% F1: 0.3116 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7593 Acc: 59.3750% F1: 0.434 Time: 0.32s (0.00s)
Fold 6 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3052 Acc: 22.2222% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.2986 *
*********************************************************
Performing epoch 3 of 5
Fold 6 train - epoch: 3/5 iter: 0/15 loss: 0.8165 Acc: 56.2500% F1: 0.416 Time: 0.96s (0.00s)
Fold 6 train - epoch: 3/5 iter: 1/15 loss: 0.7638 Acc: 71.8750% F1: 0.599 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 2/15 loss: 0.8002 Acc: 65.6250% F1: 0.604 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 3/15 loss: 0.7181 Acc: 71.8750% F1: 0.671 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 4/15 loss: 0.7494 Acc: 75.0000% F1: 0.650 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 5/15 loss: 0.6857 Acc: 75.0000% F1: 0.614 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 6/15 loss: 0.6391 Acc: 81.2500% F1: 0.520 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 7/15 loss: 0.7176 Acc: 68.7500% F1: 0.453 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 8/15 loss: 0.9335 Acc: 56.2500% F1: 0.309 Time: 0.95s (0.03s)
Fold 6 train - epoch: 3/5 iter: 9/15 loss: 0.9252 Acc: 56.2500% F1: 0.370 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 10/15 loss: 0.9331 Acc: 46.8750% F1: 0.315 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 11/15 loss: 0.6918 Acc: 75.0000% F1: 0.490 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 12/15 loss: 0.8136 Acc: 71.8750% F1: 0.492 Time: 0.96s (0.03s)
Fold 6 train - epoch: 3/5 iter: 13/15 loss: 0.7536 Acc: 68.7500% F1: 0.575 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 14/15 loss: 0.0666 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 67.3333% F1: 0.5186 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9619 Acc: 37.5000% F1: 0.211 Time: 0.32s (0.00s)
Fold 6 train-dev - epoch: 3/5 iter: 1/2 loss: 1.1859 Acc: 44.4444% F1: 0.222 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 40.0000% F1: 0.2928 *
*********************************************************
Performing epoch 4 of 5
Fold 6 train - epoch: 4/5 iter: 0/15 loss: 0.6099 Acc: 71.8750% F1: 0.738 Time: 0.96s (0.00s)
Fold 6 train - epoch: 4/5 iter: 1/15 loss: 0.6154 Acc: 71.8750% F1: 0.669 Time: 0.95s (0.04s)
Fold 6 train - epoch: 4/5 iter: 2/15 loss: 0.6967 Acc: 78.1250% F1: 0.701 Time: 0.94s (0.02s)
Fold 6 train - epoch: 4/5 iter: 3/15 loss: 0.5765 Acc: 87.5000% F1: 0.808 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 4/15 loss: 0.5420 Acc: 75.0000% F1: 0.665 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 5/15 loss: 0.5059 Acc: 78.1250% F1: 0.724 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 6/15 loss: 0.5395 Acc: 78.1250% F1: 0.491 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 7/15 loss: 0.6536 Acc: 75.0000% F1: 0.819 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 8/15 loss: 0.6718 Acc: 71.8750% F1: 0.615 Time: 0.96s (0.03s)
Fold 6 train - epoch: 4/5 iter: 9/15 loss: 0.7849 Acc: 62.5000% F1: 0.440 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 10/15 loss: 0.7752 Acc: 65.6250% F1: 0.467 Time: 0.96s (0.02s)
Fold 6 train - epoch: 4/5 iter: 11/15 loss: 0.5393 Acc: 75.0000% F1: 0.667 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 12/15 loss: 0.8008 Acc: 65.6250% F1: 0.463 Time: 0.96s (0.02s)
Fold 6 train - epoch: 4/5 iter: 13/15 loss: 0.6648 Acc: 68.7500% F1: 0.681 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 14/15 loss: 0.0228 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 73.3333% F1: 0.6575 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8631 Acc: 56.2500% F1: 0.286 Time: 0.32s (0.00s)
Fold 6 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5619 Acc: 38.8889% F1: 0.274 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 50.0000% F1: 0.4062 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/5 iter: 0/15 loss: 1.1741 Acc: 34.3750% F1: 0.331 Time: 0.97s (0.00s)
Fold 7 train - epoch: 0/5 iter: 1/15 loss: 0.9655 Acc: 62.5000% F1: 0.389 Time: 0.94s (0.03s)
Fold 7 train - epoch: 0/5 iter: 2/15 loss: 1.0890 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 3/15 loss: 0.9445 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.03s)
Fold 7 train - epoch: 0/5 iter: 4/15 loss: 1.0355 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 7 train - epoch: 0/5 iter: 5/15 loss: 0.9544 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 6/15 loss: 0.8422 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 7/15 loss: 0.9042 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 8/15 loss: 1.1152 Acc: 46.8750% F1: 0.295 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 9/15 loss: 1.0167 Acc: 34.3750% F1: 0.231 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 10/15 loss: 1.0306 Acc: 46.8750% F1: 0.333 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 11/15 loss: 0.9504 Acc: 53.1250% F1: 0.344 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 12/15 loss: 0.9930 Acc: 50.0000% F1: 0.332 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 13/15 loss: 0.9239 Acc: 46.8750% F1: 0.249 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 14/15 loss: 0.7280 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.02s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 50.6667% F1: 0.3154 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6654 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 0/5 iter: 1/2 loss: 1.3597 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2739 *
*********************************************************
Performing epoch 1 of 5
Fold 7 train - epoch: 1/5 iter: 0/15 loss: 0.8480 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.00s)
Fold 7 train - epoch: 1/5 iter: 1/15 loss: 0.8838 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 2/15 loss: 0.9053 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 3/15 loss: 0.8319 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 4/15 loss: 0.9858 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 5/15 loss: 0.9332 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 6/15 loss: 0.7440 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 7/15 loss: 0.9463 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 8/15 loss: 1.0642 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 9/15 loss: 1.0318 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 10/15 loss: 1.0521 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 11/15 loss: 0.8461 Acc: 65.6250% F1: 0.264 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 12/15 loss: 0.9173 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 13/15 loss: 0.9181 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 14/15 loss: 0.5605 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 55.3333% F1: 0.2378 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7201 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3048 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 2 of 5
Fold 7 train - epoch: 2/5 iter: 0/15 loss: 0.8397 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.00s)
Fold 7 train - epoch: 2/5 iter: 1/15 loss: 0.8717 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 2/15 loss: 0.8945 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 3/15 loss: 0.8139 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/5 iter: 4/15 loss: 0.9059 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 5/15 loss: 0.9021 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 6/15 loss: 0.7801 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 7/15 loss: 0.8385 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 8/15 loss: 1.0521 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 9/15 loss: 0.9882 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 10/15 loss: 0.9741 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 11/15 loss: 0.8110 Acc: 68.7500% F1: 0.341 Time: 0.96s (0.02s)
Fold 7 train - epoch: 2/5 iter: 12/15 loss: 0.9706 Acc: 59.3750% F1: 0.344 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 13/15 loss: 0.8712 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 14/15 loss: 0.2807 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 56.4444% F1: 0.2647 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6886 Acc: 78.1250% F1: 0.439 Time: 0.32s (0.00s)
Fold 7 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3072 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 56.0000% F1: 0.3217 *
*********************************************************
Performing epoch 3 of 5
Fold 7 train - epoch: 3/5 iter: 0/15 loss: 0.8075 Acc: 56.2500% F1: 0.369 Time: 0.95s (0.00s)
Fold 7 train - epoch: 3/5 iter: 1/15 loss: 0.8165 Acc: 65.6250% F1: 0.404 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 2/15 loss: 0.7971 Acc: 53.1250% F1: 0.306 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 3/15 loss: 0.7483 Acc: 62.5000% F1: 0.476 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 4/15 loss: 0.7830 Acc: 56.2500% F1: 0.416 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 5/15 loss: 0.7977 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 6/15 loss: 0.7351 Acc: 71.8750% F1: 0.356 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 7/15 loss: 0.8046 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 8/15 loss: 0.8557 Acc: 56.2500% F1: 0.308 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 9/15 loss: 0.9168 Acc: 53.1250% F1: 0.352 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 10/15 loss: 0.9001 Acc: 53.1250% F1: 0.350 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 11/15 loss: 0.7346 Acc: 71.8750% F1: 0.606 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 12/15 loss: 0.7571 Acc: 62.5000% F1: 0.436 Time: 0.96s (0.02s)
Fold 7 train - epoch: 3/5 iter: 13/15 loss: 0.8119 Acc: 56.2500% F1: 0.550 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 14/15 loss: 0.1635 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 60.2222% F1: 0.4208 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7631 Acc: 65.6250% F1: 0.359 Time: 0.32s (0.00s)
Fold 7 train-dev - epoch: 3/5 iter: 1/2 loss: 1.1860 Acc: 44.4444% F1: 0.377 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 58.0000% F1: 0.5172 *
*********************************************************
Performing epoch 4 of 5
Fold 7 train - epoch: 4/5 iter: 0/15 loss: 0.6323 Acc: 65.6250% F1: 0.577 Time: 0.96s (0.00s)
Fold 7 train - epoch: 4/5 iter: 1/15 loss: 0.6722 Acc: 71.8750% F1: 0.584 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 2/15 loss: 0.6706 Acc: 71.8750% F1: 0.623 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 3/15 loss: 0.5919 Acc: 75.0000% F1: 0.691 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 4/15 loss: 0.5967 Acc: 65.6250% F1: 0.647 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 5/15 loss: 0.6123 Acc: 78.1250% F1: 0.654 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 6/15 loss: 0.5798 Acc: 78.1250% F1: 0.482 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 7/15 loss: 0.7081 Acc: 68.7500% F1: 0.462 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 8/15 loss: 0.5933 Acc: 78.1250% F1: 0.698 Time: 0.96s (0.03s)
Fold 7 train - epoch: 4/5 iter: 9/15 loss: 0.6555 Acc: 75.0000% F1: 0.727 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 10/15 loss: 0.7534 Acc: 68.7500% F1: 0.683 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 11/15 loss: 0.6210 Acc: 81.2500% F1: 0.678 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 12/15 loss: 0.5782 Acc: 78.1250% F1: 0.712 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 13/15 loss: 0.6056 Acc: 68.7500% F1: 0.600 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 14/15 loss: 0.0172 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 73.3333% F1: 0.6521 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8196 Acc: 65.6250% F1: 0.353 Time: 0.32s (0.00s)
Fold 7 train-dev - epoch: 4/5 iter: 1/2 loss: 1.3635 Acc: 44.4444% F1: 0.377 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 58.0000% F1: 0.5272 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/5 iter: 0/15 loss: 1.1484 Acc: 43.7500% F1: 0.297 Time: 0.98s (0.00s)
Fold 8 train - epoch: 0/5 iter: 1/15 loss: 0.9643 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 8 train - epoch: 0/5 iter: 2/15 loss: 1.1168 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 3/15 loss: 0.9005 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.04s)
Fold 8 train - epoch: 0/5 iter: 4/15 loss: 1.0529 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 5/15 loss: 0.9429 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 6/15 loss: 0.8749 Acc: 65.6250% F1: 0.264 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 7/15 loss: 0.9377 Acc: 43.7500% F1: 0.235 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 8/15 loss: 1.0867 Acc: 46.8750% F1: 0.295 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 9/15 loss: 1.0471 Acc: 40.6250% F1: 0.252 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 10/15 loss: 1.0217 Acc: 40.6250% F1: 0.278 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 11/15 loss: 0.9207 Acc: 59.3750% F1: 0.359 Time: 0.96s (0.03s)
Fold 8 train - epoch: 0/5 iter: 12/15 loss: 0.9506 Acc: 62.5000% F1: 0.409 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 13/15 loss: 0.9047 Acc: 59.3750% F1: 0.335 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 14/15 loss: 0.6813 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 52.6667% F1: 0.3095 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5991 Acc: 84.3750% F1: 0.599 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5706 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2662 *
*********************************************************
Performing epoch 1 of 5
Fold 8 train - epoch: 1/5 iter: 0/15 loss: 0.8231 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.00s)
Fold 8 train - epoch: 1/5 iter: 1/15 loss: 0.8863 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 2/15 loss: 0.9274 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 3/15 loss: 0.8042 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 4/15 loss: 0.9642 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 5/15 loss: 0.8778 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 6/15 loss: 0.7831 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 7/15 loss: 0.9072 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 8/15 loss: 1.0590 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 8 train - epoch: 1/5 iter: 9/15 loss: 1.0416 Acc: 46.8750% F1: 0.213 Time: 0.96s (0.03s)
Fold 8 train - epoch: 1/5 iter: 10/15 loss: 1.0057 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 11/15 loss: 0.8795 Acc: 68.7500% F1: 0.383 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 12/15 loss: 0.9573 Acc: 62.5000% F1: 0.358 Time: 0.95s (0.02s)
Fold 8 train - epoch: 1/5 iter: 13/15 loss: 0.8923 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 14/15 loss: 0.6613 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 56.2222% F1: 0.2605 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7286 Acc: 71.8750% F1: 0.506 Time: 0.32s (0.00s)
Fold 8 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2805 Acc: 27.7778% F1: 0.185 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 56.0000% F1: 0.3636 *
*********************************************************
Performing epoch 2 of 5
Fold 8 train - epoch: 2/5 iter: 0/15 loss: 0.8621 Acc: 53.1250% F1: 0.311 Time: 0.96s (0.00s)
Fold 8 train - epoch: 2/5 iter: 1/15 loss: 0.8488 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 2/15 loss: 0.8640 Acc: 62.5000% F1: 0.399 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 3/15 loss: 0.8074 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 4/15 loss: 0.9040 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 5/15 loss: 0.8590 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 6/15 loss: 0.7876 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 7/15 loss: 0.8932 Acc: 46.8750% F1: 0.213 Time: 0.96s (0.04s)
Fold 8 train - epoch: 2/5 iter: 8/15 loss: 1.0685 Acc: 53.1250% F1: 0.231 Time: 0.96s (0.04s)
Fold 8 train - epoch: 2/5 iter: 9/15 loss: 1.0099 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 10/15 loss: 0.9975 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.04s)
Fold 8 train - epoch: 2/5 iter: 11/15 loss: 0.7648 Acc: 65.6250% F1: 0.394 Time: 0.96s (0.03s)
Fold 8 train - epoch: 2/5 iter: 12/15 loss: 0.8427 Acc: 59.3750% F1: 0.342 Time: 0.96s (0.03s)
Fold 8 train - epoch: 2/5 iter: 13/15 loss: 0.8144 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.03s)
Fold 8 train - epoch: 2/5 iter: 14/15 loss: 0.2924 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 56.6667% F1: 0.2968 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6551 Acc: 78.1250% F1: 0.423 Time: 0.32s (0.00s)
Fold 8 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3450 Acc: 27.7778% F1: 0.185 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 60.0000% F1: 0.4026 *
*********************************************************
Performing epoch 3 of 5
Fold 8 train - epoch: 3/5 iter: 0/15 loss: 0.7668 Acc: 59.3750% F1: 0.430 Time: 0.96s (0.00s)
Fold 8 train - epoch: 3/5 iter: 1/15 loss: 0.8003 Acc: 65.6250% F1: 0.409 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 2/15 loss: 0.8350 Acc: 71.8750% F1: 0.510 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 3/15 loss: 0.7175 Acc: 65.6250% F1: 0.536 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 4/15 loss: 0.7919 Acc: 56.2500% F1: 0.416 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 5/15 loss: 0.7316 Acc: 65.6250% F1: 0.459 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 6/15 loss: 0.6816 Acc: 71.8750% F1: 0.351 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 7/15 loss: 0.8200 Acc: 59.3750% F1: 0.365 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 8/15 loss: 0.9131 Acc: 59.3750% F1: 0.369 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 9/15 loss: 0.8771 Acc: 53.1250% F1: 0.434 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 10/15 loss: 0.8518 Acc: 56.2500% F1: 0.396 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 11/15 loss: 0.7634 Acc: 62.5000% F1: 0.396 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 12/15 loss: 0.7301 Acc: 75.0000% F1: 0.650 Time: 0.96s (0.03s)
Fold 8 train - epoch: 3/5 iter: 13/15 loss: 0.7509 Acc: 71.8750% F1: 0.731 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 14/15 loss: 0.1286 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 64.0000% F1: 0.4875 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6071 Acc: 65.6250% F1: 0.380 Time: 0.32s (0.00s)
Fold 8 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3908 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.3772 *
*********************************************************
Performing epoch 4 of 5
Fold 8 train - epoch: 4/5 iter: 0/15 loss: 0.6046 Acc: 71.8750% F1: 0.643 Time: 0.94s (0.00s)
Fold 8 train - epoch: 4/5 iter: 1/15 loss: 0.6782 Acc: 71.8750% F1: 0.616 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 2/15 loss: 0.7143 Acc: 65.6250% F1: 0.559 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 3/15 loss: 0.5856 Acc: 71.8750% F1: 0.442 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 4/15 loss: 0.6120 Acc: 84.3750% F1: 0.844 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 5/15 loss: 0.6019 Acc: 71.8750% F1: 0.634 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 6/15 loss: 0.5614 Acc: 68.7500% F1: 0.378 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 7/15 loss: 0.7677 Acc: 62.5000% F1: 0.412 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 8/15 loss: 0.6539 Acc: 78.1250% F1: 0.713 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 9/15 loss: 0.7225 Acc: 62.5000% F1: 0.561 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 10/15 loss: 0.6507 Acc: 71.8750% F1: 0.632 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 11/15 loss: 0.4930 Acc: 78.1250% F1: 0.770 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 12/15 loss: 0.6315 Acc: 75.0000% F1: 0.719 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 13/15 loss: 0.5605 Acc: 78.1250% F1: 0.774 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 14/15 loss: 0.0295 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 72.4444% F1: 0.6556 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/5 iter: 0/2 loss: 0.6008 Acc: 65.6250% F1: 0.380 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7206 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3281 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-2.0626e-02, -4.4060e-02, -2.6161e-02, -5.2382e-02, -1.1801e-02,
         -2.1239e-02, -1.8071e-02, -3.0182e-02, -3.2382e-02, -2.0418e-02,
         -2.6720e-02, -4.3357e-02, -2.5472e-02,  5.5979e-03,  1.2318e-02,
         -3.4176e-02, -4.4977e-02, -2.8053e-02, -2.1231e-02, -6.0782e-02,
         -4.0279e-02, -3.8476e-02, -1.6334e-02, -3.2569e-02, -2.5926e-02,
         -2.7391e-02, -2.3964e-02, -3.0589e-02, -2.9190e-02, -3.3341e-02,
         -1.8691e-02, -3.5133e-02, -6.1570e-02, -1.4964e-02, -2.1376e-02,
         -3.1147e-02, -3.0501e-02,  1.4628e-03, -1.8252e-02, -1.9517e-02,
         -1.1622e-02, -3.5739e-02, -3.6812e-02, -1.9371e-02, -1.1859e-02,
         -8.8161e-03, -3.1636e-02, -2.0528e-02, -1.1131e-02, -3.5867e-02,
         -3.8365e-02, -2.5705e-02, -2.1792e-02, -4.4399e-02, -4.0270e-02,
         -5.4638e-02, -2.8454e-02, -2.5333e-02, -2.5547e-02, -1.5287e-02,
         -2.4182e-02, -3.0630e-02,  1.5069e-02, -3.6271e-02, -2.8401e-02,
         -1.5753e-02, -3.0542e-02, -4.4240e-02, -2.0518e-02,  1.3146e-02,
         -2.6623e-02, -5.8353e-03, -6.2991e-02, -2.9645e-02, -5.2723e-02,
         -6.1875e-03, -4.6658e-02, -1.1706e-02, -2.0831e-02, -2.1911e-02,
         -3.0621e-02, -3.0604e-02, -3.1597e-02, -1.4894e-02, -3.7422e-02,
         -1.8669e-02, -3.9261e-02, -2.2540e-02, -3.5005e-02, -1.8804e-02,
         -3.9859e-02, -2.3636e-02, -4.4098e-02, -2.2001e-02, -4.0159e-02,
         -4.1603e-02, -6.0479e-02, -3.4732e-02, -3.5348e-02, -1.1855e-02,
         -1.1457e-02, -1.8017e-02, -2.5695e-02, -3.2170e-02, -1.8815e-02,
          6.0778e-04, -2.9451e-02, -1.5095e-02, -3.4335e-02, -1.8684e-02,
         -5.2852e-02, -2.8976e-02, -4.0829e-02, -1.9680e-02, -1.9389e-02,
         -2.6357e-02, -1.0519e-02, -3.3680e-02, -3.3897e-02, -1.6169e-02,
         -2.5509e-02,  6.1193e-03, -2.6227e-02, -4.9142e-02, -5.1203e-02,
         -2.7614e-02, -2.7042e-02, -8.2370e-02, -3.2448e-02, -6.2488e-02,
         -2.4354e-02, -5.3159e-02,  2.3348e-02, -3.0051e-02, -2.0879e-02,
         -1.0396e-02, -1.9578e-02, -2.8474e-02, -2.0294e-02, -6.0514e-03,
         -3.1489e-02, -2.7566e-02, -8.3959e-03, -8.8849e-03, -3.2843e-02,
         -3.9592e-02, -2.8364e-02, -3.0351e-02, -3.9907e-02, -2.9026e-02,
         -4.1853e-02, -1.4049e-02, -5.6452e-02, -2.7035e-02, -3.0331e-02,
         -2.6986e-02, -1.8532e-02, -1.1369e-02, -4.2319e-02, -1.0535e-01,
         -2.5081e-02, -2.4970e-02, -1.3560e-02, -3.2619e-02, -2.3543e-02,
         -3.1439e-02, -1.0665e-02, -2.8571e-02, -2.4338e-02, -2.9927e-02,
         -2.2083e-02, -1.0367e-02, -3.6159e-02, -3.5902e-02, -3.8141e-02,
          2.2199e-02, -3.4954e-02, -4.2879e-02, -2.6929e-02, -6.4845e-03,
         -2.5604e-02, -2.3864e-02, -6.5536e-02, -1.1395e-02, -2.0720e-02,
         -1.9895e-02, -3.2456e-02, -4.2429e-02, -2.7246e-02, -7.3045e-03,
         -7.3728e-03, -4.4053e-02, -4.0074e-02, -3.2210e-02, -5.1612e-02,
         -3.5234e-02, -4.5111e-02, -1.9451e-02, -3.2387e-02, -4.2629e-02,
         -2.9188e-02, -9.1431e-03, -5.6937e-03, -3.6857e-02, -3.5655e-02,
         -1.4412e-02, -1.6586e-02, -4.8305e-02, -2.0054e-02, -2.2196e-03,
         -2.6737e-02, -2.1182e-02, -3.0844e-02, -2.2878e-02, -2.6762e-02,
         -2.2593e-02, -1.2894e-01, -2.6952e-03, -4.7206e-02, -1.9133e-02,
         -1.6194e-02, -3.4907e-02, -3.0068e-02, -8.7365e-03, -2.4353e-02,
         -1.0010e-01, -3.6861e-02, -4.0455e-02, -3.1136e-02, -5.0729e-02,
         -2.6141e-02, -2.6761e-02, -4.6217e-02, -3.6219e-02, -2.3151e-02,
         -1.7108e-02, -3.9074e-02, -1.9915e-02, -1.2001e-02, -3.7315e-02,
         -3.3087e-02, -2.6547e-02, -4.3930e-02, -4.6760e-02, -2.4470e-02,
          5.5595e-05, -4.3790e-02, -2.8079e-02, -3.8258e-02, -4.1491e-02,
         -3.3981e-02, -2.7939e-02, -2.7030e-02, -1.2909e-02, -4.0956e-02,
         -2.1886e-02, -2.3271e-02, -2.3112e-02, -2.3970e-02, -1.5977e-02,
         -1.6904e-02, -3.9638e-02, -1.4792e-02, -4.5347e-02, -3.1461e-02,
         -2.8825e-02, -1.2313e-02, -3.7781e-02,  2.4980e-03, -2.8167e-02,
         -3.1846e-02, -3.3554e-02, -2.1765e-02, -2.0045e-02, -4.7427e-02,
         -1.8519e-02, -2.8280e-02, -4.4778e-02, -1.3524e-02, -3.2319e-03,
         -2.9903e-02, -3.1917e-02, -2.2511e-02, -5.9006e-02,  1.5946e-03,
         -2.4006e-02, -1.7079e-02, -2.9951e-02, -2.5281e-02, -5.0775e-02,
         -4.3378e-02,  8.9525e-04, -1.6057e-02, -1.3209e-03, -1.9282e-02,
         -3.7466e-02, -7.5034e-02, -3.4903e-02, -2.6046e-02, -6.0244e-02,
         -2.3133e-02, -3.6307e-02, -1.1735e-02, -1.5822e-02, -5.8465e-02,
         -9.4279e-03, -2.4606e-02, -1.6206e-02,  6.8120e-03, -3.1115e-02,
         -2.9297e-02, -3.0197e-02, -2.0291e-02, -1.5730e-02, -2.9399e-02,
         -5.8700e-02, -4.9440e-02, -1.4964e-02, -1.4682e-02, -4.7326e-02,
          1.9171e-03, -2.7425e-02, -3.2663e-02, -4.0381e-02, -2.2076e-02,
         -7.2282e-03, -1.7059e-02, -1.3517e-02, -2.6359e-02, -4.3907e-03,
         -3.9467e-02, -6.9909e-02, -3.6269e-02, -3.8785e-02, -2.7625e-02,
         -3.0422e-02, -3.4326e-02, -4.1373e-03, -2.0417e-02, -2.2334e-02,
         -1.4535e-02, -1.3532e-02, -3.6567e-02, -2.2748e-02, -1.6989e-02,
          1.4371e-03, -4.1189e-02, -4.5399e-02, -3.9660e-02, -2.7661e-02,
         -2.3461e-02, -2.6766e-02, -3.6177e-02, -5.6810e-02, -2.0876e-02,
         -3.5100e-02, -4.1278e-02, -2.1627e-02, -3.7129e-03, -1.9762e-02,
         -1.7184e-02, -2.7223e-02, -2.7576e-02, -9.1357e-04, -1.6475e-02,
         -4.0762e-02, -2.2704e-02, -3.2532e-02, -2.5827e-02, -3.1545e-02,
         -1.7120e-02, -3.9241e-02, -2.4638e-02, -2.9520e-02, -3.0369e-02,
         -2.5346e-02, -2.1294e-02, -4.9278e-02, -2.7448e-02, -3.7034e-02,
         -6.7724e-03, -5.7087e-02, -2.8008e-02,  6.8865e-03, -4.9772e-02,
         -4.4891e-02, -3.4240e-02, -5.1939e-02, -2.3135e-02, -1.4994e-02,
         -3.6481e-02, -1.6013e-02, -4.1644e-02, -3.7349e-02, -1.8343e-02,
         -2.8021e-02, -2.3944e-02, -3.5851e-02, -4.4354e-02, -1.8169e-02,
         -2.3095e-02, -9.4431e-02, -3.8140e-02, -3.4660e-02, -3.6032e-02,
         -2.0042e-02, -2.4378e-02, -1.2028e-02, -2.0953e-02, -2.1721e-02,
         -3.2877e-02, -3.8856e-02, -4.7931e-02, -3.5312e-02, -2.8105e-02,
         -4.7969e-02, -1.0577e-02, -5.4178e-02, -1.9335e-02,  1.4594e-02,
         -5.6146e-03, -2.8620e-02, -3.7851e-02, -1.9913e-02, -4.2189e-02,
         -1.0320e-02, -2.1976e-02, -5.2133e-02, -4.3729e-02, -5.4553e-02,
         -3.5142e-02, -3.7726e-02, -2.6058e-02, -1.1231e-02, -3.2618e-02,
         -4.3355e-02, -1.7774e-02, -3.1622e-02, -2.8229e-02, -2.1421e-02,
         -2.9999e-02, -3.1010e-02, -2.5497e-02, -4.5435e-03, -4.1343e-02,
         -3.2268e-02, -2.0374e-02, -1.9872e-02, -4.6008e-02, -2.8307e-02,
         -1.1835e-02, -2.2528e-02, -2.8635e-02, -2.5755e-02, -2.2962e-02,
         -1.1313e-02, -3.7581e-02, -1.8534e-02, -1.6939e-02, -2.5694e-02,
         -4.5719e-02,  2.1832e-02, -1.5461e-02, -1.8565e-02, -2.3459e-02,
         -6.4967e-02, -2.7417e-02, -2.3490e-02, -2.3226e-02, -4.4696e-02,
         -6.2122e-03, -6.0652e-02, -5.4693e-02, -2.0045e-02, -3.9188e-02,
         -1.6182e-02, -1.6512e-02, -8.1976e-03, -3.2709e-02, -2.5799e-02,
         -2.4841e-03, -2.1700e-02,  4.9436e-03, -2.0033e-02, -1.6611e-02,
         -2.2917e-02, -3.5269e-02, -2.3938e-02, -2.5108e-02, -2.5692e-02,
         -1.4544e-02, -1.7141e-02, -1.6639e-02, -3.4002e-02, -2.0499e-02,
         -3.2127e-02, -6.7373e-02, -1.1025e-02, -3.1973e-02, -2.3749e-02,
         -9.4430e-03, -4.9020e-02, -4.0693e-02, -3.4095e-02, -3.2654e-02,
         -4.3039e-02, -3.7553e-02, -3.2732e-02, -2.8386e-02, -2.5889e-02,
         -4.1052e-02, -4.8210e-02, -3.5973e-02, -3.5697e-02, -3.5337e-02,
         -3.0920e-02, -8.5859e-03,  1.3807e-02, -2.3515e-02, -3.5362e-02,
         -2.4852e-02, -2.1957e-02, -1.4952e-02, -2.1925e-02, -3.4758e-02,
         -2.8220e-02, -3.7455e-02, -4.0865e-02, -3.8104e-02,  1.3347e-03,
         -5.2198e-02, -2.5467e-02, -5.1656e-02, -1.5240e-02, -1.6143e-02,
         -1.1116e-02, -2.9076e-02, -1.5411e-02, -3.7430e-02, -2.3534e-02,
         -3.0279e-02, -1.0146e-01, -2.3272e-02, -4.5229e-02, -3.8890e-02,
          1.7913e-03, -6.2969e-03, -4.1688e-02, -3.0044e-02, -9.7487e-03,
         -2.1045e-02, -1.4235e-02, -2.4313e-02, -1.7216e-02, -3.1402e-02,
         -3.4427e-02, -2.1556e-02, -2.8665e-02, -3.3832e-02, -3.4066e-02,
         -2.2086e-02, -2.8477e-02, -5.0919e-02, -3.4791e-02, -1.7713e-02,
         -1.8561e-02, -3.6231e-02, -3.8923e-02, -3.6720e-02, -1.8680e-02,
         -1.9310e-02, -5.3375e-02, -2.2816e-02, -2.2705e-02, -3.0119e-02,
         -1.6189e-02, -3.3022e-02, -3.1215e-02, -3.2107e-02, -3.4758e-02,
         -2.5874e-02, -5.6180e-02, -4.0951e-02, -2.7341e-02, -2.5415e-02,
         -3.5619e-02, -3.5732e-02, -2.5828e-02, -3.6025e-02, -4.4503e-02,
         -7.0487e-03, -8.6296e-03, -4.8478e-02, -3.6195e-02, -3.2900e-02,
         -3.9823e-02, -1.5362e-02, -5.5768e-03, -2.8645e-02, -2.9967e-02,
         -4.2924e-02, -3.2294e-02, -2.9039e-02, -3.4810e-02, -1.9517e-02,
         -2.9691e-02, -2.8602e-02, -4.0939e-02, -5.1788e-02, -2.9741e-02,
         -3.6167e-02, -2.9228e-02, -2.2982e-02, -5.7735e-02, -1.8834e-02,
         -1.7366e-02, -3.5482e-02, -2.1608e-02, -4.6328e-02, -1.2155e-02,
         -3.7046e-02, -4.9812e-02, -1.5587e-02, -4.5353e-02, -1.4188e-02,
         -4.2994e-02, -3.1072e-02, -4.7449e-02, -3.5341e-02, -2.4476e-02,
         -2.2945e-02, -1.8691e-02, -2.2915e-02, -3.1375e-02, -2.5264e-02,
         -2.0859e-02, -2.5641e-02, -3.1363e-02, -2.8384e-02, -4.3555e-02,
         -3.4732e-02, -3.9482e-02, -1.7701e-02, -2.6898e-02, -1.4156e-02,
         -1.7624e-02, -1.3957e-02, -2.4311e-02, -2.0646e-02, -2.5072e-02,
         -2.4137e-02, -1.8694e-02, -1.8910e-02, -1.0031e-02, -1.3634e-02,
          5.8649e-04, -2.2889e-02, -3.8469e-02, -2.8486e-02, -4.6490e-02,
         -1.9475e-02, -1.1686e-02, -3.8871e-02, -2.8555e-02, -3.2153e-02,
         -2.2332e-02, -1.1427e-02, -3.9309e-02, -2.2623e-02, -1.3303e-02,
         -5.9567e-03, -4.0263e-02, -2.7895e-02, -3.2989e-02, -2.7573e-02,
         -2.0798e-02, -4.4355e-02, -3.5247e-02, -4.0966e-02, -7.5861e-04,
         -2.7611e-02, -5.1424e-02, -4.2651e-02, -3.6131e-02, -1.9154e-02,
         -1.5637e-02, -2.3545e-02, -1.4531e-02, -3.4187e-02, -3.3429e-02,
         -2.7579e-02, -3.3237e-02, -1.5961e-02, -2.8258e-02, -2.0247e-02,
         -2.2522e-02, -3.1527e-02, -2.9972e-02, -2.4582e-02, -8.2861e-02,
         -2.8782e-02, -1.5376e-02, -3.8141e-02, -3.1824e-02, -3.6612e-02,
         -3.9245e-02, -1.9609e-02, -3.0501e-02, -7.7609e-03, -8.0570e-03,
         -2.3806e-02, -2.6526e-02, -2.4266e-02, -2.4878e-02, -3.3844e-02,
         -2.9566e-02, -3.6479e-02, -1.4832e-02, -2.4131e-02,  1.2522e-02,
         -3.0995e-02, -1.1293e-02, -2.4523e-02, -3.9244e-02, -3.0727e-02,
         -4.0406e-02, -2.8647e-02, -9.1868e-03, -4.6082e-02, -4.8844e-02,
         -4.1209e-02, -1.3117e-02, -2.7125e-02, -3.0641e-02, -6.2721e-02,
         -1.6724e-02, -1.2205e-02, -7.4066e-03, -2.2215e-02, -2.2350e-02,
         -1.5625e-02, -2.4046e-02, -1.9222e-02, -4.8427e-02, -2.3804e-02,
         -1.7038e-02, -8.9022e-03, -9.0834e-03, -9.2919e-03, -5.0812e-03,
         -2.9265e-02, -1.5873e-02, -1.5446e-02, -3.2083e-02, -5.7845e-02,
         -1.7572e-02, -5.2507e-02,  4.4114e-04, -4.0418e-02, -4.0858e-02,
         -2.4568e-02, -4.6892e-02, -2.0101e-02, -4.1925e-02, -1.5755e-02,
         -2.7231e-02, -2.4822e-02, -2.8439e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/5 iter: 0/15 loss: 1.1455 Acc: 40.6250% F1: 0.296 Time: 0.96s (0.00s)
Fold 9 train - epoch: 0/5 iter: 1/15 loss: 1.0049 Acc: 50.0000% F1: 0.265 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/5 iter: 2/15 loss: 1.1138 Acc: 46.8750% F1: 0.255 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 3/15 loss: 0.8263 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 4/15 loss: 0.9986 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 5/15 loss: 0.9192 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 6/15 loss: 0.7997 Acc: 68.7500% F1: 0.336 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 7/15 loss: 1.0014 Acc: 46.8750% F1: 0.272 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 8/15 loss: 1.0936 Acc: 50.0000% F1: 0.315 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 9/15 loss: 1.0720 Acc: 34.3750% F1: 0.202 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 10/15 loss: 1.0431 Acc: 46.8750% F1: 0.329 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 11/15 loss: 0.8819 Acc: 56.2500% F1: 0.338 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 12/15 loss: 0.9841 Acc: 53.1250% F1: 0.333 Time: 0.96s (0.02s)
Fold 9 train - epoch: 0/5 iter: 13/15 loss: 0.9752 Acc: 46.8750% F1: 0.249 Time: 0.95s (0.03s)
Fold 9 train - epoch: 0/5 iter: 14/15 loss: 0.6430 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 51.1111% F1: 0.3005 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/5 iter: 0/2 loss: 0.7185 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 0/5 iter: 1/2 loss: 1.1069 Acc: 27.7778% F1: 0.152 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 58.0000% F1: 0.2479 *
*********************************************************
Performing epoch 1 of 5
Fold 9 train - epoch: 1/5 iter: 0/15 loss: 0.8700 Acc: 59.3750% F1: 0.413 Time: 0.94s (0.00s)
Fold 9 train - epoch: 1/5 iter: 1/15 loss: 0.8901 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 2/15 loss: 0.9520 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 3/15 loss: 0.7569 Acc: 62.5000% F1: 0.261 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 4/15 loss: 0.9966 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 5/15 loss: 0.8627 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 6/15 loss: 0.7265 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 7/15 loss: 0.9605 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 8/15 loss: 0.9846 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 9/15 loss: 1.0324 Acc: 46.8750% F1: 0.213 Time: 0.96s (0.03s)
Fold 9 train - epoch: 1/5 iter: 10/15 loss: 1.0053 Acc: 40.6250% F1: 0.253 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 11/15 loss: 0.7939 Acc: 71.8750% F1: 0.426 Time: 0.95s (0.02s)
Fold 9 train - epoch: 1/5 iter: 12/15 loss: 0.8737 Acc: 71.8750% F1: 0.492 Time: 0.95s (0.02s)
Fold 9 train - epoch: 1/5 iter: 13/15 loss: 0.8986 Acc: 65.6250% F1: 0.449 Time: 0.95s (0.02s)
Fold 9 train - epoch: 1/5 iter: 14/15 loss: 0.5405 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 57.1111% F1: 0.3225 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/5 iter: 0/2 loss: 0.8093 Acc: 62.5000% F1: 0.536 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 1/5 iter: 1/2 loss: 1.0261 Acc: 38.8889% F1: 0.262 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3264 *
*********************************************************
Performing epoch 2 of 5
Fold 9 train - epoch: 2/5 iter: 0/15 loss: 0.8106 Acc: 65.6250% F1: 0.532 Time: 0.94s (0.00s)
Fold 9 train - epoch: 2/5 iter: 1/15 loss: 0.8487 Acc: 59.3750% F1: 0.370 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 2/15 loss: 0.8826 Acc: 40.6250% F1: 0.228 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 3/15 loss: 0.7517 Acc: 62.5000% F1: 0.310 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 4/15 loss: 0.8748 Acc: 53.1250% F1: 0.283 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 5/15 loss: 0.7876 Acc: 65.6250% F1: 0.380 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 6/15 loss: 0.6654 Acc: 71.8750% F1: 0.343 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 7/15 loss: 0.9348 Acc: 50.0000% F1: 0.286 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 8/15 loss: 0.9380 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 9/15 loss: 1.0010 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 10/15 loss: 0.9967 Acc: 43.7500% F1: 0.369 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 11/15 loss: 0.7356 Acc: 65.6250% F1: 0.414 Time: 0.96s (0.03s)
Fold 9 train - epoch: 2/5 iter: 12/15 loss: 0.8443 Acc: 65.6250% F1: 0.427 Time: 0.96s (0.02s)
Fold 9 train - epoch: 2/5 iter: 13/15 loss: 0.8112 Acc: 65.6250% F1: 0.659 Time: 0.96s (0.03s)
Fold 9 train - epoch: 2/5 iter: 14/15 loss: 0.1898 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 57.7778% F1: 0.3774 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8226 Acc: 56.2500% F1: 0.333 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 2/5 iter: 1/2 loss: 1.0487 Acc: 38.8889% F1: 0.262 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.3093 *
*********************************************************
Performing epoch 3 of 5
Fold 9 train - epoch: 3/5 iter: 0/15 loss: 0.7161 Acc: 68.7500% F1: 0.622 Time: 0.94s (0.00s)
Fold 9 train - epoch: 3/5 iter: 1/15 loss: 0.7275 Acc: 78.1250% F1: 0.669 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 2/15 loss: 0.7501 Acc: 62.5000% F1: 0.543 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 3/15 loss: 0.6125 Acc: 75.0000% F1: 0.654 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 4/15 loss: 0.6984 Acc: 75.0000% F1: 0.720 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 5/15 loss: 0.7035 Acc: 65.6250% F1: 0.459 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 6/15 loss: 0.5635 Acc: 71.8750% F1: 0.415 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 7/15 loss: 0.7671 Acc: 68.7500% F1: 0.464 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 8/15 loss: 0.7665 Acc: 59.3750% F1: 0.474 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 9/15 loss: 0.9192 Acc: 56.2500% F1: 0.493 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 10/15 loss: 0.7377 Acc: 65.6250% F1: 0.566 Time: 0.96s (0.02s)
Fold 9 train - epoch: 3/5 iter: 11/15 loss: 0.7318 Acc: 68.7500% F1: 0.652 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 12/15 loss: 0.7226 Acc: 75.0000% F1: 0.731 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 13/15 loss: 0.7222 Acc: 65.6250% F1: 0.578 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 14/15 loss: 0.0380 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 68.4444% F1: 0.5970 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7808 Acc: 59.3750% F1: 0.348 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 3/5 iter: 1/2 loss: 1.2736 Acc: 33.3333% F1: 0.236 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3127 *
*********************************************************
Performing epoch 4 of 5
Fold 9 train - epoch: 4/5 iter: 0/15 loss: 0.6238 Acc: 71.8750% F1: 0.627 Time: 0.94s (0.00s)
Fold 9 train - epoch: 4/5 iter: 1/15 loss: 0.7082 Acc: 71.8750% F1: 0.589 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 2/15 loss: 0.5851 Acc: 75.0000% F1: 0.703 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 3/15 loss: 0.5572 Acc: 81.2500% F1: 0.788 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 4/15 loss: 0.5389 Acc: 81.2500% F1: 0.780 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 5/15 loss: 0.5263 Acc: 84.3750% F1: 0.774 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 6/15 loss: 0.4727 Acc: 84.3750% F1: 0.769 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 7/15 loss: 0.6188 Acc: 71.8750% F1: 0.702 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 8/15 loss: 0.5155 Acc: 78.1250% F1: 0.696 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 9/15 loss: 0.7260 Acc: 71.8750% F1: 0.610 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 10/15 loss: 0.6237 Acc: 62.5000% F1: 0.540 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 11/15 loss: 0.4330 Acc: 81.2500% F1: 0.532 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 12/15 loss: 0.5227 Acc: 81.2500% F1: 0.784 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 13/15 loss: 0.5312 Acc: 75.0000% F1: 0.712 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 14/15 loss: 0.0206 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 76.6667% F1: 0.6938 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/5 iter: 0/2 loss: 1.1593 Acc: 43.7500% F1: 0.289 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 4/5 iter: 1/2 loss: 1.2642 Acc: 44.4444% F1: 0.324 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 44.0000% F1: 0.3152 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 54.8000% F1: 0.2598
	Epoch 1 Accuracy: 51.8000% F1: 0.3116
	Epoch 2 Accuracy: 51.4000% F1: 0.3296
	Epoch 3 Accuracy: 46.8000% F1: 0.3549
	Epoch 4 Accuracy: 47.4000% F1: 0.3777
all done :)
************************************
** MODEL TIME ID: 20220225-150856 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[7.3088e-01, 5.0749e-01, 4.3455e-01, 7.6472e-01, 6.9369e-01, 6.2000e-01,
         5.6472e-01, 9.2290e-01, 9.1472e-01, 9.9707e-01, 2.0255e-01, 6.0742e-02,
         4.7933e-01, 3.8971e-01, 5.4505e-01, 3.5584e-01, 1.9544e-01, 6.2289e-01,
         1.2769e-01, 6.7061e-01, 4.5010e-01, 8.8128e-01, 3.0709e-01, 4.5170e-01,
         4.5216e-01, 3.3059e-01, 8.4138e-01, 4.0841e-03, 4.7888e-01, 1.3131e-01,
         7.0188e-02, 2.7769e-01, 8.0502e-02, 2.2254e-01, 5.8066e-01, 6.7405e-01,
         9.0179e-01, 4.2018e-02, 5.5829e-01, 3.5957e-01, 7.7319e-01, 7.5424e-01,
         5.0828e-01, 2.2169e-01, 5.3094e-01, 7.9370e-01, 8.8657e-01, 6.0048e-01,
         7.8504e-01, 1.4019e-01, 4.4821e-01, 2.9975e-01, 7.6301e-01, 6.3399e-01,
         1.1931e-01, 1.2053e-01, 1.5601e-01, 4.0072e-01, 9.9672e-01, 6.2115e-02,
         2.8568e-01, 6.8355e-01, 6.1238e-01, 2.2421e-01, 9.3985e-01, 8.3979e-02,
         1.6627e-01, 5.8774e-01, 8.5452e-01, 3.5277e-01, 6.7234e-01, 2.4764e-01,
         7.6313e-01, 1.0535e-01, 7.3096e-01, 4.9347e-01, 6.1428e-01, 4.2024e-01,
         5.7544e-01, 1.2036e-01, 6.3013e-01, 6.6447e-01, 9.3675e-01, 3.6890e-01,
         4.2897e-01, 2.6278e-01, 8.0353e-01, 8.2643e-01, 1.2139e-01, 9.7928e-01,
         7.4772e-01, 4.5944e-01, 3.6350e-01, 5.6427e-01, 2.0039e-01, 7.2551e-01,
         3.9836e-02, 6.1704e-02, 4.1273e-01, 4.4535e-01, 9.2358e-02, 8.7110e-01,
         4.2424e-01, 8.7976e-01, 4.7130e-01, 1.0820e-01, 5.5810e-02, 3.1545e-01,
         2.2365e-01, 7.4150e-01, 3.5854e-01, 9.0878e-01, 6.7754e-01, 7.8689e-01,
         2.2539e-01, 8.1950e-01, 6.6943e-01, 4.9970e-02, 5.3973e-01, 3.6736e-01,
         9.3187e-01, 6.0934e-01, 9.4311e-01, 4.1357e-01, 3.6496e-01, 2.4235e-01,
         8.1232e-01, 1.4158e-02, 5.7282e-02, 6.1452e-01, 6.3840e-01, 6.5476e-01,
         1.6618e-01, 6.0709e-01, 6.6217e-01, 8.2543e-01, 5.9790e-01, 9.5338e-01,
         9.3017e-02, 7.6107e-01, 1.8151e-01, 9.2700e-01, 3.2890e-01, 8.5589e-01,
         5.4850e-01, 8.0487e-01, 3.0081e-01, 5.6507e-01, 1.7669e-01, 7.5054e-01,
         1.6071e-01, 4.2825e-01, 4.3251e-01, 7.5749e-01, 3.3296e-01, 4.0123e-01,
         6.9051e-01, 6.6205e-01, 1.1056e-01, 5.9807e-01, 1.1854e-01, 7.7121e-01,
         8.5935e-01, 5.0915e-01, 1.2116e-01, 5.6986e-01, 3.4829e-01, 7.3829e-01,
         2.5531e-01, 2.2218e-02, 3.5945e-01, 8.7510e-01, 5.3369e-01, 8.3820e-01,
         9.4300e-01, 6.2514e-01, 4.1653e-01, 4.6382e-01, 8.1835e-01, 2.9089e-01,
         7.2602e-01, 7.4600e-01, 6.1533e-01, 9.0121e-01, 1.6047e-01, 1.1106e-01,
         1.6283e-01, 1.0487e-01, 2.9452e-01, 6.1695e-01, 9.8384e-01, 4.4944e-01,
         5.4462e-01, 3.9401e-01, 4.4418e-01, 8.7369e-01, 4.9764e-01, 7.9002e-01,
         9.3912e-01, 7.7963e-01, 8.7893e-02, 3.6396e-01, 4.1000e-01, 6.3618e-01,
         1.6678e-01, 1.9849e-01, 1.6753e-03, 6.4854e-01, 5.7021e-01, 7.7750e-01,
         2.9826e-01, 5.0268e-01, 3.2567e-01, 1.1563e-01, 1.6213e-01, 3.3370e-01,
         2.9720e-01, 8.5428e-01, 5.0080e-02, 4.6431e-01, 1.9325e-01, 2.0948e-01,
         3.3258e-01, 2.2001e-01, 5.1631e-01, 8.9828e-01, 2.4326e-01, 6.1670e-01,
         1.0175e-02, 7.6577e-01, 3.0455e-01, 5.4958e-01, 5.7247e-01, 8.0227e-01,
         9.8628e-01, 1.2278e-01, 2.1841e-01, 7.0230e-01, 9.7946e-01, 3.8389e-01,
         1.8921e-01, 1.0293e-01, 1.1904e-01, 6.1322e-01, 4.8208e-01, 6.4112e-01,
         5.2311e-01, 2.0710e-01, 4.6959e-01, 1.2849e-01, 1.4823e-01, 5.6401e-01,
         5.6378e-01, 4.8962e-01, 7.1251e-01, 6.8856e-01, 7.1186e-01, 6.6923e-01,
         7.3119e-02, 4.7180e-01, 5.2828e-01, 6.0395e-01, 5.1799e-01, 7.7438e-01,
         7.6378e-01, 4.8697e-01, 1.0962e-01, 9.7520e-02, 5.5753e-01, 6.4672e-01,
         3.4871e-02, 8.8728e-01, 2.7198e-01, 6.9921e-01, 1.1644e-01, 8.0029e-02,
         8.7297e-01, 3.3016e-01, 1.5131e-01, 7.9632e-01, 5.7806e-01, 9.6364e-01,
         4.3630e-01, 9.2723e-01, 8.8012e-01, 1.2471e-01, 3.4697e-01, 6.5417e-01,
         8.2205e-01, 8.1964e-01, 8.0740e-01, 7.5439e-01, 9.7985e-01, 9.3742e-01,
         7.0448e-01, 7.1428e-01, 9.5566e-01, 5.9470e-01, 3.5922e-01, 4.3782e-01,
         9.2372e-01, 6.4801e-01, 5.9766e-01, 2.5100e-02, 1.3882e-01, 8.3568e-01,
         2.1751e-01, 7.5278e-01, 8.2731e-01, 7.6641e-01, 4.8038e-01, 1.9839e-01,
         1.3134e-01, 4.7052e-01, 9.4436e-01, 4.2847e-01, 8.8119e-01, 6.2900e-01,
         8.6041e-01, 1.0800e-01, 6.3905e-01, 3.6659e-01, 7.7021e-02, 3.7296e-02,
         1.3537e-01, 2.4282e-01, 2.1765e-01, 8.2196e-01, 4.6050e-01, 3.6509e-01,
         8.2359e-01, 3.2055e-01, 4.0459e-03, 6.0718e-01, 9.3051e-01, 3.9695e-01,
         5.0511e-01, 2.5463e-04, 2.8710e-01, 9.8516e-01, 1.8307e-01, 1.8092e-01,
         7.4965e-01, 9.9601e-01, 5.1587e-01, 2.8176e-01, 4.6686e-01, 4.9593e-01,
         4.5512e-01, 2.3540e-01, 3.9777e-02, 5.7744e-01, 1.2856e-01, 8.9649e-01,
         3.0794e-01, 8.7931e-01, 3.0355e-01, 2.4820e-01, 9.7103e-01, 2.6139e-01,
         1.4375e-01, 6.0226e-01, 7.1387e-01, 1.0774e-01, 4.2014e-01, 1.4787e-02,
         6.4876e-01, 6.2314e-01, 5.2751e-01, 4.8160e-01, 5.2677e-01, 8.9545e-01,
         8.0206e-01, 4.7391e-01, 1.5073e-01, 5.4221e-01, 3.7919e-01, 6.4297e-01,
         8.6874e-01, 9.4839e-01, 2.9220e-01, 4.6977e-01, 7.4181e-01, 8.6362e-01,
         3.7040e-01, 7.1547e-01, 9.6011e-01, 8.0419e-01, 6.0251e-01, 3.2333e-01,
         1.9766e-01, 4.5795e-01, 1.7440e-02, 6.6379e-02, 7.2989e-01, 7.9225e-01,
         4.4740e-01, 5.0771e-01, 5.4156e-01, 1.8526e-01, 7.8696e-01, 9.8286e-01,
         6.0492e-01, 9.4285e-02, 2.6235e-01, 8.3236e-01, 4.3168e-01, 7.8561e-01,
         8.4130e-01, 9.0742e-01, 6.9789e-01, 7.9801e-01, 6.8561e-01, 2.7370e-02,
         6.1173e-01, 5.7470e-01, 4.0099e-01, 7.4934e-01, 5.2049e-01, 6.4744e-01,
         3.9356e-01, 5.2488e-01, 4.5996e-01, 5.8619e-01, 8.9401e-01, 8.5808e-01,
         2.1838e-01, 3.2394e-01, 3.5680e-01, 7.2328e-01, 2.3724e-01, 3.3317e-01,
         6.8931e-01, 8.8839e-01, 7.4934e-01, 2.9914e-01, 5.4101e-01, 8.9109e-01,
         6.1517e-01, 9.6560e-01, 3.0279e-01, 2.1182e-01, 1.1913e-02, 3.2106e-01,
         8.3754e-01, 9.9002e-01, 2.1279e-01, 9.4823e-01, 4.8947e-01, 3.3153e-01,
         6.2563e-01, 7.6669e-01, 4.0916e-01, 9.5340e-01, 4.5046e-01, 9.9113e-01,
         7.4065e-01, 1.4724e-01, 7.8538e-01, 9.5075e-01, 3.3580e-01, 6.8371e-01,
         7.8270e-01, 1.8541e-01, 4.2198e-01, 2.4392e-01, 6.7243e-02, 2.7165e-01,
         2.8067e-01, 1.2055e-01, 2.5470e-02, 8.8472e-01, 9.6647e-01, 9.7960e-01,
         1.4920e-02, 2.1177e-01, 7.8708e-02, 1.7282e-02, 7.5658e-01, 9.1769e-01,
         9.4757e-01, 4.6565e-01, 2.8363e-01, 7.4839e-01, 8.9557e-03, 2.2171e-01,
         5.7871e-01, 5.8153e-01, 1.7189e-01, 2.2616e-01, 2.3947e-01, 5.2286e-01,
         8.2790e-01, 8.0713e-01, 5.8684e-01, 2.5825e-01, 6.5690e-01, 9.4680e-01,
         6.3067e-01, 6.6393e-01, 5.1623e-01, 3.8933e-01, 1.0081e-01, 9.0240e-01,
         8.1030e-01, 8.7332e-01, 4.8561e-01, 8.9447e-01, 9.1774e-01, 2.5926e-01,
         1.3623e-02, 8.9306e-01, 7.0015e-01, 3.6260e-01, 1.8771e-01, 7.3108e-01,
         9.9672e-01, 7.4909e-01, 5.9934e-01, 5.5912e-01, 4.6435e-02, 8.0155e-01,
         2.2385e-01, 2.0994e-01, 8.3020e-01, 2.1302e-01, 8.5930e-01, 4.4207e-01,
         7.2020e-01, 9.1413e-01, 1.6263e-01, 1.5348e-01, 1.4104e-01, 4.6332e-01,
         2.4296e-01, 7.1876e-01, 7.9202e-01, 1.3180e-02, 7.4113e-01, 2.4485e-01,
         3.3919e-01, 7.3538e-01, 4.4580e-01, 5.8659e-01, 2.2254e-01, 8.1192e-01,
         9.6492e-01, 3.8093e-01, 4.3876e-01, 8.0936e-01, 6.6638e-01, 5.4630e-01,
         8.7462e-01, 6.7063e-01, 2.3750e-01, 5.9071e-01, 4.8560e-01, 3.7653e-01,
         9.9609e-01, 3.8017e-01, 6.8270e-01, 5.1714e-01, 1.8583e-01, 2.4403e-01,
         9.3361e-01, 9.8415e-01, 6.7672e-01, 1.5548e-01, 2.9308e-01, 5.9891e-01,
         7.3738e-01, 5.3801e-02, 1.6344e-01, 7.0443e-01, 2.0321e-01, 8.2624e-02,
         7.3735e-01, 7.7533e-02, 6.0923e-01, 4.5088e-01, 6.8291e-01, 4.5514e-01,
         9.9106e-01, 9.8262e-01, 3.1797e-01, 2.5348e-01, 9.6957e-01, 8.6786e-01,
         7.0807e-01, 6.4801e-01, 9.5370e-01, 1.4185e-01, 8.5426e-01, 7.0249e-01,
         5.9696e-01, 1.5468e-01, 3.4653e-01, 5.0303e-01, 9.7943e-01, 2.9054e-01,
         4.0006e-01, 6.8972e-01, 4.6535e-01, 9.0308e-01, 8.6183e-01, 5.2387e-01,
         4.2363e-01, 7.8479e-01, 9.9355e-01, 2.5145e-01, 7.3650e-01, 6.2145e-01,
         1.6912e-01, 5.9522e-01, 3.6420e-01, 7.2844e-01, 7.7961e-01, 4.7766e-01,
         2.3460e-01, 6.5044e-01, 9.9726e-01, 3.8192e-01, 1.9211e-01, 9.6745e-01,
         7.2414e-01, 1.4044e-01, 2.6623e-01, 6.4132e-01, 5.7868e-02, 8.9798e-02,
         4.3117e-01, 1.6439e-01, 4.1278e-01, 1.0357e-01, 5.1162e-01, 1.7191e-02,
         1.5664e-01, 7.6168e-01, 6.1847e-01, 9.4315e-01, 2.6122e-01, 1.2726e-01,
         8.2466e-01, 2.0351e-01, 2.9765e-01, 5.0399e-01, 4.3297e-01, 5.2831e-01,
         1.8924e-01, 5.3605e-01, 1.7530e-01, 9.5484e-01, 5.5651e-01, 4.4243e-01,
         6.7346e-01, 1.4663e-01, 6.0868e-01, 2.8841e-01, 9.1939e-01, 4.7430e-01,
         3.7688e-01, 5.3414e-01, 3.4480e-01, 5.0210e-01, 1.7326e-01, 1.9903e-01,
         3.1572e-02, 3.3677e-01, 9.0481e-01, 3.3442e-01, 4.0980e-01, 6.3619e-01,
         9.7106e-01, 1.5669e-01, 8.4796e-01, 3.8596e-01, 2.6969e-01, 5.4375e-01,
         3.3182e-01, 5.0644e-01, 2.9240e-01, 3.4969e-01, 1.5914e-01, 9.3665e-01,
         4.9064e-01, 3.4057e-01, 5.5403e-01, 7.5571e-01, 8.5708e-01, 5.7715e-01,
         2.8072e-02, 5.7120e-01, 8.5326e-01, 1.2849e-01, 6.5417e-01, 5.0805e-01,
         4.0498e-01, 1.4197e-01, 4.4199e-01, 9.4113e-02, 8.1144e-01, 2.1551e-01,
         4.7790e-01, 4.3634e-01, 3.6010e-01, 4.6273e-01, 6.9831e-01, 6.8269e-01,
         3.8227e-01, 4.9132e-01, 5.0340e-01, 3.7868e-01, 6.6024e-01, 1.4451e-01,
         9.8165e-01, 3.4334e-01, 5.4898e-01, 3.5625e-01, 5.3970e-01, 4.5853e-01,
         9.0889e-01, 5.5937e-01, 1.4952e-01, 2.9803e-01, 7.7340e-01, 6.5006e-01,
         8.1541e-01, 7.3714e-01, 2.9028e-01, 4.9227e-01, 5.1226e-01, 1.7272e-01,
         4.7990e-01, 6.1604e-01, 3.6108e-01, 2.6405e-01, 6.0130e-01, 1.5335e-01,
         6.3906e-01, 3.0463e-01, 9.5654e-01, 8.1805e-01, 7.3676e-01, 4.4142e-01,
         6.3770e-01, 3.5470e-01, 3.7407e-01, 1.0295e-02, 4.4112e-01, 2.5229e-01,
         7.9086e-01, 1.2433e-01, 1.1700e-01, 1.0575e-02, 4.1196e-01, 3.0267e-01,
         4.4174e-01, 6.2764e-01, 1.0019e-02, 9.5788e-01, 4.2058e-02, 3.8274e-01,
         4.9439e-01, 1.5998e-01, 7.7725e-01, 8.5263e-01, 8.7023e-01, 1.2442e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/5 iter: 0/15 loss: 1.1306 Acc: 46.8750% F1: 0.324 Time: 0.98s (0.00s)
Fold 0 train - epoch: 0/5 iter: 1/15 loss: 0.9829 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.03s)
Fold 0 train - epoch: 0/5 iter: 2/15 loss: 1.1478 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 3/15 loss: 0.9294 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 4/15 loss: 0.9189 Acc: 56.2500% F1: 0.352 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 5/15 loss: 0.9822 Acc: 37.5000% F1: 0.234 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 6/15 loss: 0.9685 Acc: 31.2500% F1: 0.209 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 7/15 loss: 0.8610 Acc: 46.8750% F1: 0.286 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 8/15 loss: 1.0936 Acc: 65.6250% F1: 0.441 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 9/15 loss: 0.9595 Acc: 59.3750% F1: 0.386 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 10/15 loss: 0.9800 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 11/15 loss: 0.9053 Acc: 65.6250% F1: 0.327 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 12/15 loss: 1.0183 Acc: 59.3750% F1: 0.312 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 13/15 loss: 0.8885 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 0 train - epoch: 0/5 iter: 14/15 loss: 0.6759 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 52.6667% F1: 0.3066 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6537 Acc: 81.2500% F1: 0.448 Time: 0.32s (0.00s)
Fold 0 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5367 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 52.0000% F1: 0.2281 *
*********************************************************
Performing epoch 1 of 5
Fold 0 train - epoch: 1/5 iter: 0/15 loss: 0.8520 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 0 train - epoch: 1/5 iter: 1/15 loss: 0.8508 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 2/15 loss: 1.0114 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 3/15 loss: 0.8518 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.02s)
Fold 0 train - epoch: 1/5 iter: 4/15 loss: 0.9993 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 0 train - epoch: 1/5 iter: 5/15 loss: 0.9430 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 6/15 loss: 0.7607 Acc: 71.8750% F1: 0.351 Time: 0.95s (0.03s)
Fold 0 train - epoch: 1/5 iter: 7/15 loss: 0.8986 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 8/15 loss: 0.9563 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 9/15 loss: 0.9507 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 10/15 loss: 0.9340 Acc: 56.2500% F1: 0.352 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 11/15 loss: 0.8839 Acc: 53.1250% F1: 0.276 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 12/15 loss: 0.9680 Acc: 65.6250% F1: 0.450 Time: 0.95s (0.02s)
Fold 0 train - epoch: 1/5 iter: 13/15 loss: 0.8965 Acc: 50.0000% F1: 0.326 Time: 0.96s (0.02s)
Fold 0 train - epoch: 1/5 iter: 14/15 loss: 0.6780 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 55.7778% F1: 0.2930 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7580 Acc: 71.8750% F1: 0.566 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4124 Acc: 11.1111% F1: 0.089 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 50.0000% F1: 0.3074 *
*********************************************************
Performing epoch 2 of 5
Fold 0 train - epoch: 2/5 iter: 0/15 loss: 0.8289 Acc: 65.6250% F1: 0.409 Time: 0.95s (0.00s)
Fold 0 train - epoch: 2/5 iter: 1/15 loss: 0.8120 Acc: 62.5000% F1: 0.389 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 2/15 loss: 0.9457 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 3/15 loss: 0.7495 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 4/15 loss: 0.8859 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 0 train - epoch: 2/5 iter: 5/15 loss: 0.9249 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 6/15 loss: 0.7070 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 7/15 loss: 0.8773 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 8/15 loss: 0.9262 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 9/15 loss: 0.9343 Acc: 56.2500% F1: 0.350 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 10/15 loss: 0.9440 Acc: 43.7500% F1: 0.239 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 11/15 loss: 0.8019 Acc: 62.5000% F1: 0.351 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 12/15 loss: 0.8965 Acc: 53.1250% F1: 0.316 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 13/15 loss: 0.8385 Acc: 59.3750% F1: 0.335 Time: 0.95s (0.02s)
Fold 0 train - epoch: 2/5 iter: 14/15 loss: 0.3073 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 56.6667% F1: 0.2968 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8233 Acc: 65.6250% F1: 0.521 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4086 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.2868 *
*********************************************************
Performing epoch 3 of 5
Fold 0 train - epoch: 3/5 iter: 0/15 loss: 0.7685 Acc: 65.6250% F1: 0.424 Time: 0.95s (0.00s)
Fold 0 train - epoch: 3/5 iter: 1/15 loss: 0.7696 Acc: 65.6250% F1: 0.528 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 2/15 loss: 0.8865 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 3/15 loss: 0.6601 Acc: 71.8750% F1: 0.447 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 4/15 loss: 0.7764 Acc: 59.3750% F1: 0.488 Time: 0.95s (0.03s)
Fold 0 train - epoch: 3/5 iter: 5/15 loss: 0.7996 Acc: 62.5000% F1: 0.358 Time: 0.95s (0.03s)
Fold 0 train - epoch: 3/5 iter: 6/15 loss: 0.6116 Acc: 78.1250% F1: 0.460 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 7/15 loss: 0.7635 Acc: 46.8750% F1: 0.268 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 8/15 loss: 0.8572 Acc: 71.8750% F1: 0.580 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 9/15 loss: 0.8548 Acc: 68.7500% F1: 0.481 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 10/15 loss: 0.8599 Acc: 56.2500% F1: 0.402 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 11/15 loss: 0.7125 Acc: 75.0000% F1: 0.650 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 12/15 loss: 0.7732 Acc: 59.3750% F1: 0.491 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 13/15 loss: 0.7854 Acc: 56.2500% F1: 0.289 Time: 0.95s (0.02s)
Fold 0 train - epoch: 3/5 iter: 14/15 loss: 0.0828 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 63.3333% F1: 0.4563 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7961 Acc: 68.7500% F1: 0.336 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 3/5 iter: 1/2 loss: 2.0291 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.2639 *
*********************************************************
Performing epoch 4 of 5
Fold 0 train - epoch: 4/5 iter: 0/15 loss: 0.6370 Acc: 71.8750% F1: 0.663 Time: 0.95s (0.00s)
Fold 0 train - epoch: 4/5 iter: 1/15 loss: 0.5558 Acc: 75.0000% F1: 0.701 Time: 0.94s (0.02s)
Fold 0 train - epoch: 4/5 iter: 2/15 loss: 0.7524 Acc: 68.7500% F1: 0.677 Time: 0.94s (0.02s)
Fold 0 train - epoch: 4/5 iter: 3/15 loss: 0.5455 Acc: 90.6250% F1: 0.923 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 4/15 loss: 0.6584 Acc: 71.8750% F1: 0.699 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 5/15 loss: 0.6429 Acc: 75.0000% F1: 0.526 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 6/15 loss: 0.6118 Acc: 68.7500% F1: 0.464 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 7/15 loss: 0.5857 Acc: 71.8750% F1: 0.485 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 8/15 loss: 0.7309 Acc: 71.8750% F1: 0.617 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 9/15 loss: 0.6589 Acc: 78.1250% F1: 0.672 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 10/15 loss: 0.7770 Acc: 59.3750% F1: 0.493 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 11/15 loss: 0.4236 Acc: 78.1250% F1: 0.670 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 12/15 loss: 0.5916 Acc: 75.0000% F1: 0.694 Time: 0.95s (0.02s)
Fold 0 train - epoch: 4/5 iter: 13/15 loss: 0.5244 Acc: 75.0000% F1: 0.740 Time: 0.96s (0.02s)
Fold 0 train - epoch: 4/5 iter: 14/15 loss: 0.0363 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 73.7778% F1: 0.6770 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/5 iter: 0/2 loss: 1.9716 Acc: 15.6250% F1: 0.119 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 4/5 iter: 1/2 loss: 1.4005 Acc: 38.8889% F1: 0.257 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 24.0000% F1: 0.2023 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[9.9390e-01, 3.8436e-01, 8.4522e-01, 9.6224e-01, 2.1875e-01, 3.4381e-01,
         6.8542e-01, 7.9031e-01, 3.5955e-01, 5.3455e-01, 9.0128e-01, 1.7884e-01,
         7.3115e-01, 8.4396e-01, 9.1865e-01, 7.7580e-01, 6.9260e-01, 1.8465e-01,
         2.9619e-01, 1.1186e-01, 8.9418e-01, 7.5969e-01, 1.3128e-01, 3.0066e-01,
         5.0445e-01, 9.0151e-01, 7.5344e-01, 9.3098e-01, 7.3989e-01, 1.7502e-01,
         3.8451e-01, 3.7135e-03, 6.6900e-01, 2.1174e-01, 5.6073e-01, 9.2508e-01,
         2.3304e-01, 3.9985e-02, 7.5070e-02, 2.7732e-01, 8.8664e-01, 2.6336e-01,
         7.7359e-01, 6.4286e-02, 3.2206e-01, 6.8342e-01, 7.1928e-01, 5.5214e-01,
         3.1262e-01, 7.3536e-01, 6.0121e-01, 8.1438e-01, 9.9742e-01, 1.3356e-01,
         3.9235e-01, 1.8184e-02, 2.9128e-01, 9.5617e-01, 4.5644e-01, 3.6505e-01,
         6.6564e-01, 3.1783e-01, 9.8322e-01, 1.6974e-01, 2.1271e-01, 2.5782e-01,
         4.3064e-01, 2.4264e-01, 3.8289e-03, 5.9827e-01, 1.1608e-01, 6.7500e-01,
         9.7720e-01, 2.3221e-01, 3.8155e-01, 4.0594e-01, 7.1562e-01, 6.2914e-01,
         9.7487e-01, 4.5880e-01, 7.1693e-01, 8.1173e-01, 7.3872e-01, 2.0564e-01,
         9.7513e-02, 6.7725e-01, 3.9238e-01, 4.4254e-01, 4.0902e-01, 2.4482e-01,
         8.9135e-01, 6.8632e-01, 4.1004e-01, 7.9548e-02, 7.5108e-01, 9.8615e-01,
         2.4690e-01, 7.8003e-01, 6.4838e-01, 4.1123e-01, 3.9951e-01, 5.9617e-02,
         9.9197e-01, 4.6787e-01, 4.8766e-01, 6.2278e-01, 5.7723e-02, 6.8371e-01,
         4.3742e-01, 7.8601e-01, 7.8912e-01, 7.0284e-01, 1.5612e-01, 6.6331e-01,
         7.3453e-01, 1.3504e-01, 9.7230e-01, 7.2939e-01, 8.9440e-01, 7.3668e-01,
         3.4838e-01, 2.7079e-01, 9.0597e-01, 3.3737e-01, 9.0205e-01, 9.1627e-01,
         7.6261e-01, 9.3302e-01, 7.5805e-01, 4.8474e-01, 6.0298e-01, 3.0078e-01,
         8.6479e-01, 6.4705e-01, 2.2671e-01, 3.9490e-01, 9.9901e-01, 3.6348e-01,
         9.9149e-01, 7.0817e-02, 6.3929e-01, 6.7449e-01, 2.1198e-02, 1.0177e-01,
         7.8141e-01, 4.1939e-01, 3.4773e-01, 5.7392e-01, 9.2635e-01, 2.0825e-01,
         6.2501e-01, 1.1024e-01, 2.9115e-01, 8.6455e-01, 9.4947e-01, 4.6062e-01,
         2.2091e-01, 9.0577e-01, 8.6404e-02, 6.8881e-01, 7.5250e-01, 6.3816e-01,
         4.7826e-01, 9.2804e-02, 7.0733e-01, 8.6512e-01, 4.7789e-01, 2.3002e-01,
         5.6099e-01, 4.3701e-01, 5.3997e-01, 4.9786e-01, 9.8002e-01, 3.6487e-01,
         3.8121e-01, 2.0580e-01, 3.8216e-01, 8.2379e-02, 5.5608e-01, 1.0714e-01,
         1.3617e-01, 2.0599e-01, 1.7935e-04, 1.5692e-01, 5.9433e-01, 5.4633e-01,
         4.3184e-01, 8.6595e-01, 1.1845e-02, 7.4938e-01, 9.3314e-01, 2.4615e-01,
         6.2606e-01, 6.1918e-01, 6.3613e-01, 5.1676e-01, 3.5558e-01, 6.6363e-01,
         1.2618e-01, 1.1112e-01, 3.2299e-02, 1.0329e-01, 8.8767e-01, 8.5168e-01,
         3.7001e-01, 5.1047e-01, 8.3693e-01, 8.2778e-01, 2.9250e-02, 9.4203e-01,
         6.9601e-01, 8.5910e-01, 4.9515e-01, 8.8630e-01, 4.6422e-01, 3.5479e-02,
         1.8216e-01, 1.8276e-01, 7.4665e-01, 8.2825e-01, 4.3752e-03, 4.2946e-01,
         2.2083e-02, 7.5081e-01, 4.4198e-02, 1.9940e-01, 2.3033e-01, 4.0715e-01,
         1.9795e-01, 3.8303e-01, 3.9817e-01, 3.8653e-01, 7.2435e-01, 4.3471e-02,
         6.0367e-01, 5.3898e-01, 9.0040e-01, 1.0268e-01, 5.0066e-02, 6.1239e-01,
         8.7629e-01, 7.7259e-01, 1.0262e-01, 2.2588e-01, 2.2912e-01, 1.4556e-01,
         7.7466e-01, 5.9715e-01, 4.7100e-01, 7.1911e-01, 6.9698e-01, 9.8634e-01,
         9.0210e-01, 9.7006e-01, 6.3575e-01, 7.0752e-01, 9.8018e-01, 7.1599e-01,
         1.4323e-01, 2.7959e-01, 9.0366e-01, 6.2573e-01, 5.1653e-01, 2.1503e-01,
         5.3468e-03, 2.4476e-01, 5.0267e-01, 6.8101e-01, 1.9017e-01, 8.2177e-01,
         8.4196e-01, 8.2689e-01, 2.6867e-01, 6.2317e-01, 7.2797e-01, 6.4095e-02,
         5.2329e-01, 6.1604e-01, 8.8494e-01, 5.4743e-01, 6.2186e-01, 7.6959e-01,
         2.2381e-01, 1.9959e-01, 8.5037e-01, 7.1549e-01, 8.7599e-01, 4.2078e-01,
         7.2874e-01, 2.7448e-01, 5.0081e-02, 4.8108e-01, 7.3461e-01, 2.5267e-01,
         7.0280e-01, 8.3044e-01, 3.2241e-01, 7.6925e-01, 3.2413e-01, 4.8011e-01,
         3.0949e-01, 2.6846e-01, 5.6297e-02, 7.2205e-01, 9.2871e-01, 6.7618e-01,
         8.5553e-01, 8.3050e-01, 4.5234e-02, 3.3425e-01, 3.1242e-01, 4.9895e-01,
         1.4306e-01, 8.7619e-01, 1.2940e-01, 6.1422e-01, 7.8179e-01, 5.1090e-01,
         9.2323e-03, 2.6678e-01, 1.5517e-02, 5.1358e-01, 1.0694e-01, 2.7881e-01,
         3.4599e-01, 8.3063e-01, 6.1265e-01, 6.0227e-01, 4.5816e-01, 1.9999e-01,
         4.7349e-01, 8.4107e-01, 8.0412e-01, 2.2456e-01, 4.9248e-01, 7.3784e-01,
         9.9356e-01, 7.6304e-02, 3.0351e-01, 4.6362e-01, 4.0078e-01, 5.7668e-01,
         4.2863e-02, 3.2830e-01, 8.2761e-01, 7.3656e-01, 3.9297e-01, 3.3263e-01,
         4.1492e-01, 5.0390e-01, 1.0648e-01, 7.9953e-01, 9.6251e-01, 2.8291e-02,
         6.0020e-01, 3.6937e-01, 8.6439e-03, 3.6701e-02, 5.6393e-01, 4.0073e-02,
         9.1533e-01, 6.6946e-01, 3.6240e-01, 1.2292e-02, 5.2310e-01, 9.7227e-01,
         4.2175e-01, 3.6931e-01, 8.0406e-01, 6.4227e-02, 3.1421e-01, 3.7608e-01,
         3.2731e-03, 6.0263e-01, 7.4466e-01, 6.6157e-01, 4.1456e-01, 1.3553e-02,
         6.2202e-01, 1.1259e-01, 1.1150e-01, 1.4193e-01, 2.0714e-01, 2.6464e-01,
         7.0191e-01, 8.4020e-01, 2.7502e-01, 5.0510e-01, 8.2394e-01, 8.6408e-02,
         9.7154e-01, 7.9020e-01, 8.8920e-01, 4.6218e-01, 2.7187e-01, 6.4953e-01,
         2.9775e-01, 3.1663e-01, 7.0577e-01, 3.3169e-01, 6.6225e-01, 2.5813e-01,
         2.6476e-01, 1.8912e-01, 8.3026e-01, 1.4658e-01, 7.9756e-01, 1.4859e-01,
         9.7014e-01, 1.2636e-01, 4.6733e-01, 7.1188e-01, 6.0763e-01, 5.9298e-03,
         6.7679e-01, 9.0769e-01, 6.9686e-01, 3.0861e-01, 1.9162e-01, 3.1959e-01,
         4.7565e-01, 5.4500e-01, 7.0561e-01, 8.9912e-01, 9.9587e-02, 8.5200e-01,
         5.7003e-02, 5.3428e-01, 6.8941e-01, 1.4616e-01, 3.2616e-02, 3.0206e-02,
         2.5240e-01, 9.4304e-01, 6.6598e-02, 5.2063e-01, 6.1847e-01, 4.7345e-01,
         6.9972e-01, 5.4275e-01, 3.8443e-01, 7.3306e-01, 7.8729e-03, 5.9416e-01,
         5.2080e-01, 3.0842e-01, 1.4641e-01, 1.6362e-01, 9.8539e-01, 4.5886e-01,
         5.0359e-01, 4.4349e-02, 7.2311e-01, 9.9778e-01, 3.4260e-02, 4.4264e-01,
         6.5136e-01, 9.2338e-01, 8.4743e-01, 2.1900e-01, 8.2290e-01, 1.5741e-01,
         3.8260e-01, 7.8533e-02, 2.1232e-01, 7.4188e-01, 9.5026e-01, 1.2651e-01,
         4.5446e-01, 6.8838e-01, 8.9409e-01, 9.0938e-01, 5.9916e-01, 2.3968e-01,
         1.7627e-01, 1.1013e-01, 7.4649e-01, 5.0754e-01, 1.2176e-01, 5.8598e-02,
         4.5232e-01, 3.6519e-01, 6.4909e-01, 2.9008e-01, 9.8192e-01, 8.3229e-01,
         8.0591e-01, 6.3982e-01, 8.0218e-01, 3.3342e-01, 2.7827e-01, 6.3173e-01,
         1.4524e-01, 2.9684e-01, 7.3667e-01, 8.8997e-01, 2.7787e-01, 7.4977e-01,
         1.4054e-01, 5.9692e-01, 6.9933e-02, 6.6919e-01, 9.1800e-01, 2.0753e-02,
         5.3759e-01, 3.6277e-01, 3.4465e-01, 4.0629e-01, 8.2584e-02, 8.5585e-01,
         8.3220e-01, 3.4767e-01, 4.3084e-01, 6.6182e-01, 3.5860e-01, 2.1426e-02,
         9.6245e-01, 2.1137e-01, 6.0753e-01, 9.3206e-01, 3.5283e-01, 4.6101e-02,
         5.6943e-01, 1.3739e-01, 7.2178e-01, 8.9963e-01, 6.1270e-01, 4.7730e-01,
         5.2588e-01, 4.1484e-01, 8.1771e-01, 5.3548e-01, 3.7905e-01, 3.6475e-01,
         8.5332e-01, 9.0629e-02, 4.5189e-01, 3.5497e-01, 5.9797e-01, 5.9659e-01,
         3.9204e-01, 3.8858e-02, 9.2854e-01, 1.9669e-02, 6.1448e-01, 9.1728e-01,
         9.3377e-01, 2.2591e-01, 3.4030e-01, 6.4232e-01, 4.5414e-01, 8.5960e-01,
         6.7804e-01, 4.5319e-01, 2.1772e-01, 7.2642e-01, 3.6619e-01, 9.8444e-01,
         1.2002e-01, 2.8212e-01, 7.3740e-01, 3.1445e-02, 7.3026e-01, 4.2094e-01,
         2.4441e-01, 9.2773e-01, 7.2271e-01, 8.8094e-01, 5.1170e-01, 8.9661e-02,
         1.8217e-01, 7.2316e-01, 2.6359e-01, 7.7910e-01, 5.1507e-01, 5.7806e-01,
         8.6768e-01, 7.1866e-02, 2.6370e-01, 3.7991e-01, 8.9887e-01, 8.8480e-02,
         3.6916e-01, 2.3582e-01, 2.9396e-01, 6.5270e-01, 9.9239e-02, 4.4151e-01,
         6.4544e-01, 6.1477e-01, 2.2128e-01, 5.1585e-01, 4.7930e-01, 4.7360e-01,
         9.1022e-01, 9.2501e-01, 4.4125e-01, 1.4966e-01, 5.6360e-03, 5.9204e-01,
         8.5097e-01, 9.0536e-01, 5.0866e-01, 3.2136e-01, 5.7599e-01, 4.5667e-01,
         2.4119e-01, 5.4061e-01, 3.0550e-01, 7.5699e-02, 6.7739e-01, 8.4428e-01,
         9.9773e-01, 5.4109e-01, 4.6124e-01, 3.1701e-01, 1.0181e-01, 3.1451e-01,
         6.9198e-01, 1.9568e-01, 8.1660e-01, 3.3204e-01, 5.2432e-01, 2.4178e-01,
         9.3707e-01, 1.8293e-01, 8.3541e-01, 5.8999e-01, 3.4606e-01, 5.8669e-01,
         9.9955e-01, 2.6619e-01, 8.6694e-01, 8.8354e-01, 5.9483e-01, 6.9260e-01,
         4.3407e-01, 1.2677e-01, 5.1566e-01, 6.8631e-01, 9.3709e-03, 6.9160e-01,
         2.8803e-01, 5.1567e-01, 2.5304e-02, 2.2343e-01, 1.9874e-01, 9.6719e-01,
         6.8275e-01, 8.9465e-01, 6.4725e-01, 2.7752e-01, 4.6393e-01, 9.3789e-01,
         2.5038e-01, 7.4076e-01, 2.8553e-01, 8.4270e-02, 7.2093e-01, 2.8857e-01,
         9.5200e-02, 1.7738e-01, 7.7745e-01, 8.6409e-01, 6.5042e-01, 9.8694e-01,
         6.1059e-01, 5.0942e-01, 7.8674e-01, 4.7629e-01, 7.7082e-01, 6.8017e-01,
         5.1853e-01, 4.0713e-01, 2.4926e-01, 1.7039e-01, 8.5985e-02, 4.1239e-01,
         4.8488e-02, 9.0061e-01, 6.8213e-01, 3.3231e-01, 3.9817e-01, 2.1499e-01,
         1.7389e-01, 8.0944e-01, 2.9186e-01, 5.2734e-02, 4.3843e-01, 4.8815e-02,
         2.0884e-01, 9.3540e-01, 4.3446e-01, 2.6956e-02, 3.4313e-01, 6.3321e-01,
         7.9675e-01, 8.7743e-02, 6.9581e-01, 1.6760e-01, 5.1295e-01, 9.3114e-01,
         7.5180e-01, 6.3219e-01, 7.9379e-01, 4.1821e-01, 6.2748e-01, 9.4372e-01,
         5.3998e-02, 2.8967e-01, 9.7983e-01, 8.9470e-01, 7.2409e-01, 3.9087e-01,
         6.5306e-01, 9.7956e-01, 5.7718e-01, 7.8673e-02, 9.3873e-02, 9.6368e-01,
         6.6263e-01, 1.9510e-01, 6.4078e-01, 1.0875e-01, 1.0281e-01, 8.8112e-01,
         7.3279e-01, 3.8062e-01, 7.7944e-01, 5.4021e-01, 3.2770e-01, 2.6612e-01,
         5.1207e-02, 6.5797e-01, 3.9679e-01, 5.4962e-01, 2.3603e-01, 8.0075e-01,
         2.7019e-01, 1.3916e-01, 6.5900e-01, 3.5900e-01, 9.7704e-01, 7.6445e-01,
         4.4017e-01, 5.9736e-01, 6.3432e-01, 3.6867e-01, 3.2171e-01, 3.4152e-01,
         6.9239e-01, 6.8038e-02, 3.1861e-01, 1.0928e-01, 8.0061e-01, 7.3649e-01,
         7.7027e-01, 5.0427e-01, 3.9470e-01, 6.3582e-01, 8.4007e-01, 4.4741e-01,
         4.6210e-01, 7.7415e-01, 5.4330e-01, 4.4520e-01, 7.4629e-01, 5.0220e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/5 iter: 0/15 loss: 0.9516 Acc: 53.1250% F1: 0.376 Time: 0.96s (0.00s)
Fold 1 train - epoch: 0/5 iter: 1/15 loss: 1.0344 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 1 train - epoch: 0/5 iter: 2/15 loss: 1.1579 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 3/15 loss: 0.9513 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/5 iter: 4/15 loss: 0.9667 Acc: 56.2500% F1: 0.370 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/5 iter: 5/15 loss: 0.9387 Acc: 50.0000% F1: 0.332 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 6/15 loss: 0.9389 Acc: 34.3750% F1: 0.215 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 7/15 loss: 0.8744 Acc: 43.7500% F1: 0.270 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 8/15 loss: 1.0560 Acc: 50.0000% F1: 0.306 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 9/15 loss: 0.9525 Acc: 62.5000% F1: 0.419 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 10/15 loss: 1.0204 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 11/15 loss: 0.9265 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 12/15 loss: 1.0597 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 13/15 loss: 0.9196 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 1 train - epoch: 0/5 iter: 14/15 loss: 0.6738 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 51.5556% F1: 0.2987 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6171 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/5 iter: 1/2 loss: 1.7207 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 1 train - epoch: 1/5 iter: 0/15 loss: 0.8551 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 1 train - epoch: 1/5 iter: 1/15 loss: 0.8440 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 2/15 loss: 0.9837 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 3/15 loss: 0.8977 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 1 train - epoch: 1/5 iter: 4/15 loss: 0.9552 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 5/15 loss: 0.9578 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.04s)
Fold 1 train - epoch: 1/5 iter: 6/15 loss: 0.7988 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 7/15 loss: 0.9177 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 8/15 loss: 0.9329 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 9/15 loss: 0.9541 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 10/15 loss: 0.9459 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 11/15 loss: 0.8658 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 12/15 loss: 0.9934 Acc: 59.3750% F1: 0.399 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 13/15 loss: 0.9015 Acc: 43.7500% F1: 0.207 Time: 0.95s (0.02s)
Fold 1 train - epoch: 1/5 iter: 14/15 loss: 0.6095 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 54.2222% F1: 0.2534 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7544 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4281 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3175 *
*********************************************************
Performing epoch 2 of 5
Fold 1 train - epoch: 2/5 iter: 0/15 loss: 0.8457 Acc: 62.5000% F1: 0.389 Time: 0.94s (0.00s)
Fold 1 train - epoch: 2/5 iter: 1/15 loss: 0.7875 Acc: 65.6250% F1: 0.409 Time: 0.94s (0.02s)
Fold 1 train - epoch: 2/5 iter: 2/15 loss: 0.8936 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 3/15 loss: 0.7955 Acc: 62.5000% F1: 0.309 Time: 0.94s (0.02s)
Fold 1 train - epoch: 2/5 iter: 4/15 loss: 0.8802 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 5/15 loss: 0.9112 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 6/15 loss: 0.7829 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 7/15 loss: 0.8813 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 8/15 loss: 0.9564 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 9/15 loss: 0.9459 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 10/15 loss: 0.9346 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 11/15 loss: 0.8134 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 12/15 loss: 0.8664 Acc: 62.5000% F1: 0.371 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 13/15 loss: 0.8840 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 1 train - epoch: 2/5 iter: 14/15 loss: 0.4760 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 56.8889% F1: 0.2796 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7394 Acc: 65.6250% F1: 0.264 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 2/5 iter: 1/2 loss: 1.5604 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.2592 *
*********************************************************
Performing epoch 3 of 5
Fold 1 train - epoch: 3/5 iter: 0/15 loss: 0.7487 Acc: 75.0000% F1: 0.712 Time: 0.95s (0.00s)
Fold 1 train - epoch: 3/5 iter: 1/15 loss: 0.7412 Acc: 62.5000% F1: 0.482 Time: 0.95s (0.04s)
Fold 1 train - epoch: 3/5 iter: 2/15 loss: 0.8752 Acc: 53.1250% F1: 0.306 Time: 0.95s (0.04s)
Fold 1 train - epoch: 3/5 iter: 3/15 loss: 0.7250 Acc: 59.3750% F1: 0.296 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 4/15 loss: 0.8100 Acc: 59.3750% F1: 0.491 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 5/15 loss: 0.8610 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 6/15 loss: 0.6895 Acc: 68.7500% F1: 0.336 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 7/15 loss: 0.7626 Acc: 56.2500% F1: 0.333 Time: 0.95s (0.04s)
Fold 1 train - epoch: 3/5 iter: 8/15 loss: 0.8927 Acc: 53.1250% F1: 0.321 Time: 0.95s (0.04s)
Fold 1 train - epoch: 3/5 iter: 9/15 loss: 0.8401 Acc: 65.6250% F1: 0.455 Time: 0.95s (0.03s)
Fold 1 train - epoch: 3/5 iter: 10/15 loss: 0.8363 Acc: 50.0000% F1: 0.436 Time: 0.95s (0.04s)
Fold 1 train - epoch: 3/5 iter: 11/15 loss: 0.7900 Acc: 53.1250% F1: 0.327 Time: 0.96s (0.03s)
Fold 1 train - epoch: 3/5 iter: 12/15 loss: 0.7782 Acc: 62.5000% F1: 0.573 Time: 0.95s (0.02s)
Fold 1 train - epoch: 3/5 iter: 13/15 loss: 0.8252 Acc: 59.3750% F1: 0.489 Time: 0.96s (0.02s)
Fold 1 train - epoch: 3/5 iter: 14/15 loss: 0.1632 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 60.2222% F1: 0.4666 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6986 Acc: 65.6250% F1: 0.269 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 3/5 iter: 1/2 loss: 1.8084 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 42.0000% F1: 0.2059 *
*********************************************************
Performing epoch 4 of 5
Fold 1 train - epoch: 4/5 iter: 0/15 loss: 0.6557 Acc: 75.0000% F1: 0.712 Time: 0.94s (0.00s)
Fold 1 train - epoch: 4/5 iter: 1/15 loss: 0.5830 Acc: 84.3750% F1: 0.859 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/5 iter: 2/15 loss: 0.8323 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 3/15 loss: 0.6295 Acc: 75.0000% F1: 0.675 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/5 iter: 4/15 loss: 0.6788 Acc: 75.0000% F1: 0.719 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 5/15 loss: 0.7149 Acc: 71.8750% F1: 0.487 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 6/15 loss: 0.6350 Acc: 71.8750% F1: 0.668 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 7/15 loss: 0.6340 Acc: 71.8750% F1: 0.486 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 8/15 loss: 0.8039 Acc: 56.2500% F1: 0.583 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 9/15 loss: 0.6976 Acc: 75.0000% F1: 0.634 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 10/15 loss: 0.7037 Acc: 78.1250% F1: 0.773 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 11/15 loss: 0.6035 Acc: 68.7500% F1: 0.587 Time: 0.95s (0.03s)
Fold 1 train - epoch: 4/5 iter: 12/15 loss: 0.6555 Acc: 68.7500% F1: 0.549 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 13/15 loss: 0.7183 Acc: 65.6250% F1: 0.523 Time: 0.95s (0.02s)
Fold 1 train - epoch: 4/5 iter: 14/15 loss: 0.0731 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 71.1111% F1: 0.6488 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/5 iter: 0/2 loss: 0.9809 Acc: 53.1250% F1: 0.241 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 4/5 iter: 1/2 loss: 1.9543 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 36.0000% F1: 0.2271 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[9.0472e-01, 8.5082e-01, 8.4399e-01, 8.1798e-01, 3.8942e-01, 3.6146e-02,
         2.4375e-01, 3.4955e-01, 9.7677e-01, 7.6385e-01, 8.8542e-02, 2.6687e-01,
         1.7659e-01, 7.7008e-01, 8.7154e-01, 6.3455e-01, 2.0929e-01, 3.4577e-01,
         7.5235e-01, 6.3589e-01, 1.3521e-01, 4.9341e-02, 5.0651e-01, 2.2337e-01,
         9.7465e-01, 8.9970e-01, 2.2499e-01, 2.1411e-01, 2.9321e-01, 5.9308e-01,
         9.4990e-01, 7.2549e-01, 3.5815e-01, 3.7683e-01, 9.7525e-01, 6.1215e-02,
         8.8516e-01, 4.2768e-01, 3.5697e-02, 2.2224e-01, 2.7577e-01, 1.5943e-01,
         5.9362e-01, 4.8265e-01, 4.7676e-01, 3.6083e-01, 7.5111e-01, 5.8282e-01,
         1.0532e-01, 2.6021e-01, 8.3740e-02, 2.9131e-01, 6.0965e-01, 2.8629e-01,
         7.9729e-01, 5.7571e-01, 8.6256e-01, 1.8949e-01, 2.2149e-01, 5.8938e-01,
         4.6214e-01, 1.1864e-01, 3.8833e-02, 8.5527e-01, 5.1010e-01, 9.5353e-01,
         2.6745e-01, 4.2155e-01, 6.0244e-01, 7.9174e-01, 8.9649e-02, 5.8478e-01,
         1.1375e-01, 2.7738e-01, 7.5310e-01, 8.3787e-01, 1.7683e-01, 5.6116e-01,
         8.9901e-02, 7.0564e-01, 8.8879e-01, 4.4895e-01, 7.8960e-01, 9.5240e-01,
         8.3999e-01, 9.6983e-01, 4.4665e-01, 1.8782e-01, 3.9296e-01, 5.3474e-01,
         4.3416e-01, 5.0388e-01, 6.0477e-01, 6.9653e-01, 3.0658e-01, 9.5918e-01,
         3.0398e-02, 2.6487e-01, 6.6911e-01, 8.0130e-01, 3.4073e-01, 6.7924e-01,
         4.6893e-01, 3.8958e-01, 2.4776e-01, 2.0273e-01, 9.1904e-01, 6.5798e-01,
         8.2299e-01, 1.0955e-01, 4.6195e-01, 3.7716e-01, 8.2723e-01, 6.6324e-01,
         8.4529e-02, 1.4476e-01, 4.2143e-02, 3.7360e-01, 8.8147e-01, 1.9720e-01,
         1.4513e-01, 9.9179e-01, 9.5385e-01, 8.1758e-01, 5.7893e-01, 2.4997e-02,
         8.3999e-01, 4.0397e-01, 7.3729e-01, 1.9092e-01, 4.8906e-01, 7.7778e-02,
         2.2420e-01, 8.5821e-01, 8.2901e-01, 5.1216e-01, 9.7632e-01, 8.7325e-01,
         6.1863e-01, 1.3193e-01, 3.9413e-01, 9.8032e-02, 9.7486e-01, 1.2970e-01,
         3.2177e-01, 4.2362e-02, 9.0971e-01, 6.6420e-01, 5.7091e-01, 5.2143e-01,
         3.9071e-01, 1.0988e-01, 5.8602e-01, 6.8005e-01, 9.0283e-01, 2.7412e-01,
         5.1488e-01, 2.1364e-01, 1.5930e-01, 1.9453e-01, 4.9628e-01, 8.2089e-01,
         4.7641e-01, 6.8696e-02, 2.7215e-01, 3.9426e-01, 9.1841e-01, 9.3246e-01,
         8.8409e-01, 7.7796e-01, 8.0058e-02, 6.3199e-01, 3.2836e-01, 9.9047e-01,
         6.0303e-01, 7.4057e-01, 4.7613e-01, 4.0162e-01, 2.9693e-01, 3.6008e-01,
         4.6937e-01, 6.7713e-01, 6.8107e-01, 9.7578e-01, 7.3692e-01, 3.5985e-01,
         6.5404e-01, 3.8591e-01, 9.9522e-01, 4.2177e-01, 5.4098e-01, 4.1102e-01,
         4.5823e-01, 3.7745e-01, 8.1712e-01, 4.8110e-01, 4.5866e-01, 2.9191e-01,
         7.2641e-02, 2.2025e-01, 9.7667e-01, 9.2606e-01, 4.8641e-02, 6.0152e-01,
         8.3336e-01, 9.2514e-02, 1.6680e-02, 2.4342e-01, 8.8848e-01, 8.0828e-01,
         3.8366e-02, 6.7128e-01, 2.3947e-01, 3.7491e-01, 5.8586e-01, 2.7776e-01,
         3.5172e-01, 8.2119e-01, 8.8940e-01, 5.5419e-01, 7.4873e-01, 2.6185e-01,
         8.0919e-01, 8.1626e-01, 3.0168e-01, 4.2125e-01, 3.6204e-01, 2.7107e-02,
         7.5147e-01, 9.1590e-01, 7.5054e-01, 4.1471e-01, 5.2790e-01, 7.0234e-01,
         6.3737e-01, 8.0040e-01, 1.8895e-01, 7.5228e-01, 8.0265e-01, 3.0566e-01,
         9.3444e-01, 8.9146e-01, 3.4609e-02, 1.7018e-01, 7.6391e-01, 5.8852e-01,
         1.9975e-01, 2.0785e-01, 2.5052e-01, 6.7875e-01, 4.4619e-01, 9.4167e-01,
         9.9088e-01, 3.5711e-01, 2.9871e-01, 8.9340e-01, 6.2769e-01, 1.3832e-01,
         6.0754e-01, 5.5766e-01, 1.4267e-01, 3.0308e-01, 7.8934e-01, 3.4903e-01,
         8.9235e-01, 8.7123e-01, 7.1823e-02, 8.8933e-01, 2.3449e-01, 3.6977e-01,
         5.1459e-01, 7.1542e-01, 6.2812e-01, 8.8631e-01, 9.2555e-01, 1.3074e-02,
         7.2030e-01, 4.0345e-01, 4.4488e-01, 5.6538e-02, 8.0565e-01, 5.8555e-01,
         2.3309e-01, 7.9810e-01, 4.6608e-01, 9.9696e-01, 2.5271e-01, 1.3873e-01,
         2.1589e-01, 9.4618e-01, 9.1350e-01, 3.5052e-01, 1.3134e-01, 4.2536e-02,
         8.2802e-01, 1.4673e-01, 7.8278e-01, 2.0826e-01, 5.5823e-01, 1.4002e-01,
         5.6170e-01, 6.4126e-01, 4.2540e-01, 2.0431e-01, 7.1228e-01, 4.3143e-01,
         3.6497e-01, 8.5635e-01, 1.0812e-01, 5.0823e-01, 5.9825e-01, 4.0035e-01,
         7.5165e-01, 1.7011e-01, 4.6902e-01, 4.8028e-01, 2.7903e-01, 1.8875e-01,
         1.4325e-01, 1.8517e-02, 1.9054e-01, 9.8599e-01, 6.0302e-01, 8.3036e-01,
         7.4047e-01, 9.9228e-01, 6.4588e-01, 8.1595e-01, 9.5221e-01, 9.6251e-01,
         9.0223e-01, 2.0791e-01, 7.4681e-01, 1.9092e-01, 6.9008e-01, 7.3831e-01,
         6.8673e-01, 6.8472e-01, 8.7436e-01, 7.7719e-01, 7.8863e-01, 8.3482e-02,
         3.9864e-01, 9.6070e-02, 8.0065e-01, 9.1588e-01, 2.7384e-01, 2.8423e-01,
         3.9926e-02, 7.4388e-01, 9.0680e-01, 4.1470e-01, 2.2216e-01, 3.7501e-01,
         1.6447e-01, 2.7081e-01, 6.2631e-01, 8.4449e-02, 2.4200e-01, 3.9424e-01,
         7.7483e-01, 2.7480e-01, 7.5074e-01, 3.8136e-01, 5.7536e-01, 2.1466e-01,
         9.2667e-01, 1.6770e-01, 7.9143e-01, 5.2256e-01, 4.4918e-01, 6.4148e-01,
         9.5856e-01, 9.4183e-02, 2.1882e-02, 3.0949e-01, 6.3251e-01, 5.1822e-01,
         9.8470e-01, 6.7872e-01, 6.6957e-01, 5.8383e-01, 8.8996e-01, 7.6853e-02,
         9.2563e-01, 8.3256e-01, 5.2052e-01, 1.4017e-01, 4.7111e-01, 4.9373e-01,
         7.4922e-01, 8.9209e-01, 6.5457e-01, 9.2190e-01, 7.3641e-01, 5.7572e-01,
         8.6075e-01, 7.4677e-01, 6.5676e-03, 8.7067e-02, 6.2565e-01, 7.3724e-01,
         2.3570e-01, 6.3588e-01, 1.7963e-01, 3.6999e-01, 1.5073e-01, 4.2186e-01,
         1.3967e-02, 6.2240e-02, 4.3611e-01, 4.7816e-01, 7.1281e-01, 5.8257e-01,
         3.9397e-01, 3.5353e-01, 1.5236e-01, 7.0421e-01, 1.6115e-02, 9.0997e-02,
         9.6208e-01, 7.8001e-01, 1.6983e-01, 8.3630e-01, 3.8119e-01, 9.4338e-01,
         1.1572e-02, 2.9689e-01, 2.1209e-01, 9.8085e-04, 3.5811e-01, 1.2501e-01,
         7.5130e-01, 8.5185e-01, 1.1304e-01, 6.6058e-01, 2.7595e-01, 1.5236e-02,
         4.4804e-01, 3.6328e-01, 9.7432e-02, 8.0393e-01, 1.1657e-01, 6.4142e-01,
         2.5025e-01, 9.8536e-01, 8.5198e-01, 4.0289e-01, 5.8317e-01, 2.1403e-01,
         1.6028e-01, 6.8397e-01, 6.1812e-01, 5.1582e-01, 9.9126e-01, 8.9834e-02,
         2.2303e-01, 2.9927e-01, 2.1191e-01, 1.3647e-01, 8.1264e-01, 5.7133e-01,
         4.8701e-02, 8.4461e-01, 2.3377e-01, 7.9233e-01, 6.2118e-01, 2.9690e-01,
         4.4749e-01, 6.3189e-01, 3.2258e-01, 3.9528e-01, 9.3399e-01, 4.4941e-01,
         1.3028e-01, 5.0182e-01, 8.5310e-01, 4.3028e-01, 9.9345e-01, 5.5541e-01,
         8.2564e-01, 7.8782e-01, 8.3940e-01, 1.4275e-01, 8.3217e-01, 2.7817e-01,
         5.3073e-01, 5.9761e-01, 1.2685e-01, 2.1206e-01, 3.2254e-01, 8.5849e-02,
         4.9327e-01, 5.6978e-01, 7.2431e-01, 7.7287e-01, 1.0332e-01, 5.4856e-01,
         4.6692e-01, 9.3687e-01, 3.7325e-02, 5.9374e-01, 8.2328e-01, 2.5287e-01,
         4.1167e-01, 5.2467e-01, 4.2293e-01, 2.7126e-01, 7.5338e-01, 2.1160e-01,
         2.2416e-02, 3.7294e-01, 3.6963e-01, 3.2697e-01, 2.8441e-01, 4.2010e-01,
         5.4856e-01, 9.8247e-01, 4.4359e-01, 3.3100e-01, 4.6161e-01, 2.9558e-02,
         5.5457e-01, 3.2773e-01, 9.9383e-01, 5.0196e-01, 1.5601e-01, 1.0477e-01,
         7.5232e-01, 9.7643e-02, 5.3604e-01, 1.7796e-01, 1.0074e-01, 5.9617e-02,
         1.3472e-01, 7.0445e-01, 5.3198e-02, 6.7988e-01, 2.7952e-01, 4.5862e-01,
         2.2363e-01, 4.0893e-01, 7.5846e-01, 9.0584e-01, 7.0698e-01, 7.6487e-01,
         6.3421e-01, 8.2787e-01, 5.6856e-01, 7.5699e-01, 3.5103e-01, 2.9268e-01,
         1.0945e-01, 8.4317e-01, 6.7536e-01, 5.4214e-01, 7.7754e-01, 3.4936e-01,
         9.5742e-01, 2.4673e-01, 1.8074e-01, 1.5919e-01, 8.9202e-01, 9.9663e-01,
         5.3675e-01, 9.4077e-02, 8.4849e-01, 5.1953e-02, 2.0981e-01, 5.0893e-01,
         6.7305e-01, 5.8631e-01, 1.5608e-02, 7.6381e-01, 5.7169e-01, 9.1044e-01,
         1.5222e-01, 8.3825e-01, 2.4924e-01, 9.0414e-01, 1.6218e-01, 8.6224e-01,
         7.9529e-01, 5.7515e-01, 1.8317e-01, 5.1556e-01, 1.7964e-01, 8.8263e-01,
         2.0219e-01, 1.2234e-01, 7.9044e-01, 1.3450e-01, 3.0902e-01, 9.2645e-01,
         3.1837e-01, 2.0286e-01, 7.5707e-01, 1.8317e-01, 4.7372e-02, 7.1174e-01,
         6.6486e-01, 4.6377e-01, 7.2038e-01, 1.7023e-02, 3.0018e-01, 4.0115e-02,
         2.7851e-01, 7.2510e-01, 6.4988e-01, 5.0353e-01, 5.1198e-01, 1.6466e-02,
         8.1887e-01, 7.9713e-01, 2.2359e-01, 7.5924e-01, 6.3986e-01, 7.7829e-01,
         5.3742e-01, 5.3337e-02, 4.9326e-01, 1.6053e-01, 8.7014e-01, 4.8551e-01,
         2.8855e-01, 9.5590e-01, 8.5943e-01, 4.3013e-01, 1.3883e-02, 2.9543e-01,
         9.7455e-01, 2.4827e-01, 3.4342e-01, 2.0145e-01, 5.1883e-01, 8.3019e-02,
         4.1289e-01, 2.4892e-01, 1.0130e-01, 3.0549e-01, 3.3552e-01, 9.5952e-01,
         7.5666e-01, 5.9726e-01, 7.8085e-01, 8.0891e-01, 5.9738e-01, 4.4280e-01,
         7.6498e-01, 1.8926e-01, 8.6941e-01, 2.3463e-01, 7.0738e-01, 7.0792e-01,
         5.1784e-02, 4.0132e-01, 5.6459e-02, 8.2047e-01, 5.0023e-01, 3.2519e-01,
         7.2371e-01, 2.3858e-01, 2.0527e-01, 9.2685e-01, 5.6973e-01, 9.5936e-01,
         3.1519e-01, 4.2726e-02, 8.9211e-01, 8.3117e-01, 1.6061e-01, 3.9261e-01,
         8.4033e-01, 7.2039e-01, 4.2767e-01, 2.8264e-01, 4.7607e-01, 7.1528e-01,
         4.9593e-01, 7.7341e-01, 3.7355e-02, 2.3531e-01, 5.7338e-01, 3.0559e-01,
         2.3475e-01, 2.2845e-01, 1.9117e-01, 4.8447e-01, 1.2426e-02, 5.8392e-01,
         7.1964e-01, 6.1019e-01, 5.8958e-01, 5.3273e-01, 8.9105e-01, 3.4523e-01,
         8.9464e-01, 5.0721e-01, 9.4803e-01, 7.4415e-01, 4.7562e-01, 9.1677e-01,
         5.6145e-01, 6.8394e-01, 3.5056e-01, 9.3750e-01, 1.5864e-01, 2.3590e-01,
         2.2610e-01, 1.4035e-01, 6.9716e-01, 1.2187e-01, 8.1135e-01, 1.5432e-01,
         9.6351e-01, 6.3293e-02, 7.1008e-01, 9.1998e-01, 5.6126e-01, 3.1320e-01,
         1.8920e-01, 4.3613e-01, 3.9246e-01, 2.1702e-01, 4.8702e-01, 3.1261e-01,
         7.6819e-02, 2.7322e-01, 4.2325e-01, 2.8400e-01, 7.3503e-01, 8.9917e-01,
         2.3757e-01, 5.1130e-01, 6.8176e-01, 3.9083e-01, 3.5975e-01, 1.7901e-01,
         7.5877e-01, 4.8806e-02, 1.4085e-01, 1.0106e-01, 6.0102e-01, 1.3710e-01,
         7.1028e-01, 5.9789e-02, 4.4176e-01, 5.3430e-02, 9.4571e-01, 7.6130e-01,
         8.6877e-01, 2.4127e-01, 1.1429e-01, 1.7502e-02, 5.5420e-01, 9.8685e-01,
         6.2211e-01, 1.1276e-01, 1.7689e-01, 6.8648e-01, 6.4047e-01, 9.7295e-01,
         5.5199e-01, 6.4990e-01, 5.6134e-01, 1.4655e-01, 9.6854e-01, 6.3845e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/5 iter: 0/15 loss: 1.0078 Acc: 40.6250% F1: 0.284 Time: 0.97s (0.00s)
Fold 2 train - epoch: 0/5 iter: 1/15 loss: 1.0911 Acc: 53.1250% F1: 0.236 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/5 iter: 2/15 loss: 1.0225 Acc: 50.0000% F1: 0.222 Time: 0.96s (0.02s)
Fold 2 train - epoch: 0/5 iter: 3/15 loss: 0.9037 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 2 train - epoch: 0/5 iter: 4/15 loss: 0.9218 Acc: 65.6250% F1: 0.462 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 5/15 loss: 0.9825 Acc: 34.3750% F1: 0.232 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 6/15 loss: 0.9763 Acc: 43.7500% F1: 0.300 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 7/15 loss: 0.9054 Acc: 46.8750% F1: 0.299 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 8/15 loss: 1.1498 Acc: 46.8750% F1: 0.217 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 9/15 loss: 1.0140 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 10/15 loss: 1.0527 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 11/15 loss: 0.8869 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 12/15 loss: 1.0117 Acc: 59.3750% F1: 0.312 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 13/15 loss: 0.9428 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 2 train - epoch: 0/5 iter: 14/15 loss: 0.6325 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 50.8889% F1: 0.2944 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5837 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5450 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2424 *
*********************************************************
Performing epoch 1 of 5
Fold 2 train - epoch: 1/5 iter: 0/15 loss: 0.8337 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 2 train - epoch: 1/5 iter: 1/15 loss: 0.8490 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 2/15 loss: 0.9727 Acc: 50.0000% F1: 0.222 Time: 0.96s (0.04s)
Fold 2 train - epoch: 1/5 iter: 3/15 loss: 0.8294 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.04s)
Fold 2 train - epoch: 1/5 iter: 4/15 loss: 0.9451 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.04s)
Fold 2 train - epoch: 1/5 iter: 5/15 loss: 0.9314 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 6/15 loss: 0.7439 Acc: 68.7500% F1: 0.272 Time: 0.96s (0.03s)
Fold 2 train - epoch: 1/5 iter: 7/15 loss: 0.8946 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 8/15 loss: 1.0679 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 2 train - epoch: 1/5 iter: 9/15 loss: 0.9644 Acc: 53.1250% F1: 0.335 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 10/15 loss: 0.9721 Acc: 56.2500% F1: 0.389 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 11/15 loss: 0.8848 Acc: 56.2500% F1: 0.361 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 12/15 loss: 0.9898 Acc: 56.2500% F1: 0.387 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 13/15 loss: 0.9147 Acc: 46.8750% F1: 0.277 Time: 0.95s (0.02s)
Fold 2 train - epoch: 1/5 iter: 14/15 loss: 0.6427 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 55.3333% F1: 0.3084 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7415 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2639 Acc: 5.5556% F1: 0.044 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 44.0000% F1: 0.2466 *
*********************************************************
Performing epoch 2 of 5
Fold 2 train - epoch: 2/5 iter: 0/15 loss: 0.8194 Acc: 65.6250% F1: 0.404 Time: 0.94s (0.00s)
Fold 2 train - epoch: 2/5 iter: 1/15 loss: 0.8223 Acc: 65.6250% F1: 0.409 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 2/15 loss: 0.9017 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 3/15 loss: 0.8001 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 4/15 loss: 0.8704 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 5/15 loss: 0.8794 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 6/15 loss: 0.7170 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 7/15 loss: 0.9290 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 8/15 loss: 1.0393 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 9/15 loss: 0.9668 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 2 train - epoch: 2/5 iter: 10/15 loss: 0.9351 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 11/15 loss: 0.8465 Acc: 56.2500% F1: 0.289 Time: 0.95s (0.03s)
Fold 2 train - epoch: 2/5 iter: 12/15 loss: 0.9087 Acc: 59.3750% F1: 0.422 Time: 0.96s (0.03s)
Fold 2 train - epoch: 2/5 iter: 13/15 loss: 0.8646 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 2 train - epoch: 2/5 iter: 14/15 loss: 0.3872 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 56.0000% F1: 0.2914 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7638 Acc: 68.7500% F1: 0.543 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3039 Acc: 11.1111% F1: 0.083 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 48.0000% F1: 0.2940 *
*********************************************************
Performing epoch 3 of 5
Fold 2 train - epoch: 3/5 iter: 0/15 loss: 0.7349 Acc: 71.8750% F1: 0.638 Time: 0.94s (0.00s)
Fold 2 train - epoch: 3/5 iter: 1/15 loss: 0.7413 Acc: 71.8750% F1: 0.643 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 2/15 loss: 0.8518 Acc: 59.3750% F1: 0.362 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 3/15 loss: 0.7505 Acc: 56.2500% F1: 0.315 Time: 0.94s (0.02s)
Fold 2 train - epoch: 3/5 iter: 4/15 loss: 0.7640 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 5/15 loss: 0.7778 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 6/15 loss: 0.6423 Acc: 78.1250% F1: 0.460 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 7/15 loss: 0.8515 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 8/15 loss: 0.9694 Acc: 56.2500% F1: 0.381 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 9/15 loss: 0.9560 Acc: 68.7500% F1: 0.598 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 10/15 loss: 0.8646 Acc: 46.8750% F1: 0.336 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 11/15 loss: 0.7618 Acc: 68.7500% F1: 0.460 Time: 0.96s (0.02s)
Fold 2 train - epoch: 3/5 iter: 12/15 loss: 0.8284 Acc: 59.3750% F1: 0.499 Time: 0.96s (0.02s)
Fold 2 train - epoch: 3/5 iter: 13/15 loss: 0.7152 Acc: 71.8750% F1: 0.611 Time: 0.95s (0.02s)
Fold 2 train - epoch: 3/5 iter: 14/15 loss: 0.1116 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 62.6667% F1: 0.4752 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6859 Acc: 68.7500% F1: 0.369 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6082 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 44.0000% F1: 0.2514 *
*********************************************************
Performing epoch 4 of 5
Fold 2 train - epoch: 4/5 iter: 0/15 loss: 0.6361 Acc: 68.7500% F1: 0.527 Time: 0.94s (0.00s)
Fold 2 train - epoch: 4/5 iter: 1/15 loss: 0.6436 Acc: 71.8750% F1: 0.667 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 2/15 loss: 0.6828 Acc: 65.6250% F1: 0.439 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 3/15 loss: 0.5302 Acc: 81.2500% F1: 0.850 Time: 0.94s (0.02s)
Fold 2 train - epoch: 4/5 iter: 4/15 loss: 0.5987 Acc: 75.0000% F1: 0.706 Time: 0.95s (0.03s)
Fold 2 train - epoch: 4/5 iter: 5/15 loss: 0.6473 Acc: 81.2500% F1: 0.685 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 6/15 loss: 0.5388 Acc: 75.0000% F1: 0.670 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 7/15 loss: 0.7385 Acc: 62.5000% F1: 0.630 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 8/15 loss: 0.8541 Acc: 65.6250% F1: 0.609 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 9/15 loss: 0.7124 Acc: 75.0000% F1: 0.652 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 10/15 loss: 0.6538 Acc: 68.7500% F1: 0.628 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 11/15 loss: 0.5482 Acc: 81.2500% F1: 0.847 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 12/15 loss: 0.6089 Acc: 75.0000% F1: 0.675 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 13/15 loss: 0.6299 Acc: 65.6250% F1: 0.441 Time: 0.95s (0.02s)
Fold 2 train - epoch: 4/5 iter: 14/15 loss: 0.0289 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 72.4444% F1: 0.6534 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/5 iter: 0/2 loss: 0.7973 Acc: 71.8750% F1: 0.439 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 4/5 iter: 1/2 loss: 1.8680 Acc: 5.5556% F1: 0.083 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3492 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[1.8387e-01, 1.2878e-01, 3.8244e-01, 3.8105e-02, 5.9548e-01, 1.1894e-01,
         3.2034e-01, 8.5772e-01, 1.0545e-03, 2.9237e-01, 1.0810e-01, 8.8661e-02,
         3.4192e-01, 6.4353e-01, 7.6847e-01, 8.6696e-01, 2.8620e-02, 8.6792e-02,
         3.6431e-01, 1.0577e-01, 7.2635e-01, 4.3259e-02, 8.6312e-01, 5.7245e-01,
         5.6826e-01, 2.9756e-01, 1.8321e-01, 9.3625e-01, 4.4602e-01, 7.0783e-01,
         6.4191e-01, 7.3452e-01, 1.0977e-01, 3.2681e-01, 3.5866e-01, 5.3065e-01,
         2.6311e-01, 1.7333e-01, 9.6070e-01, 4.0773e-01, 2.2880e-01, 5.6822e-01,
         4.4518e-01, 5.6169e-01, 4.9033e-02, 3.7451e-01, 5.4937e-02, 2.3629e-01,
         4.7125e-01, 7.1586e-01, 6.7694e-01, 9.3472e-02, 6.2857e-01, 7.1387e-02,
         9.2750e-01, 4.9017e-01, 8.8389e-01, 6.2303e-01, 6.0018e-01, 2.6029e-01,
         6.3358e-02, 8.1902e-01, 9.7884e-01, 3.3617e-01, 8.3987e-01, 8.0699e-01,
         3.6056e-01, 4.1166e-02, 7.6121e-01, 6.0002e-01, 8.5960e-01, 6.5054e-01,
         3.3587e-02, 1.7205e-01, 4.0179e-01, 5.7580e-01, 4.3907e-01, 4.5154e-02,
         3.0924e-01, 4.8640e-01, 8.2432e-01, 6.3716e-01, 7.4602e-01, 8.0128e-01,
         3.8917e-01, 2.6928e-01, 2.4583e-02, 2.6849e-01, 2.7164e-01, 3.0209e-01,
         5.5712e-01, 1.0642e-01, 9.1818e-01, 4.2800e-01, 4.1737e-01, 8.0661e-02,
         7.8291e-01, 1.2990e-01, 6.7490e-01, 3.0222e-01, 6.7291e-01, 3.1316e-01,
         6.0882e-01, 4.0306e-01, 5.6379e-01, 9.7602e-01, 6.9744e-02, 1.5811e-01,
         5.6704e-02, 4.4257e-02, 2.0122e-01, 7.2914e-01, 3.1755e-01, 3.9703e-01,
         9.2795e-01, 4.5774e-01, 4.3430e-01, 2.1827e-01, 1.6731e-01, 9.8908e-01,
         3.4648e-01, 2.9559e-01, 1.0896e-02, 5.6766e-03, 7.7373e-01, 4.0835e-01,
         9.2659e-01, 6.4064e-01, 6.9030e-01, 2.8028e-01, 5.3506e-01, 5.5261e-01,
         1.5761e-01, 7.3988e-01, 9.4769e-01, 7.3324e-01, 5.2547e-01, 6.7942e-01,
         9.7847e-01, 7.6616e-01, 3.8912e-01, 2.0473e-01, 4.6950e-01, 4.3278e-02,
         3.5874e-01, 2.4840e-01, 5.9965e-01, 6.5768e-01, 6.4462e-02, 5.1905e-01,
         3.8976e-01, 6.4002e-02, 2.3798e-01, 1.3144e-01, 7.1192e-01, 9.5442e-01,
         6.9723e-01, 1.6022e-01, 5.2367e-01, 3.8687e-01, 1.7562e-01, 4.3402e-01,
         9.2003e-01, 2.7744e-01, 7.7445e-03, 4.2138e-01, 2.0777e-01, 1.4581e-01,
         2.5672e-01, 8.9320e-01, 4.3802e-01, 2.1201e-01, 8.9898e-01, 3.0675e-01,
         1.7649e-01, 4.2896e-02, 9.5866e-01, 3.2883e-01, 7.0569e-01, 1.0881e-01,
         4.2261e-01, 2.0847e-01, 2.6737e-01, 6.8480e-02, 4.7302e-01, 5.9587e-01,
         3.1363e-01, 7.6498e-01, 6.8730e-02, 5.8170e-01, 7.7043e-01, 9.1860e-01,
         8.2735e-01, 4.9788e-01, 9.3119e-01, 3.2135e-01, 4.8403e-01, 1.5548e-01,
         3.3052e-01, 4.8027e-01, 3.1690e-01, 1.3701e-01, 1.7213e-01, 9.8235e-01,
         8.6324e-01, 4.5692e-01, 1.9475e-01, 8.2497e-01, 5.7088e-01, 9.6827e-01,
         4.7443e-01, 1.4210e-01, 5.8707e-01, 9.9649e-01, 4.5827e-01, 1.7817e-01,
         5.8909e-01, 7.4926e-01, 7.4019e-01, 3.9037e-01, 2.5541e-01, 4.7195e-01,
         9.2090e-01, 9.5578e-01, 2.2710e-01, 6.1538e-01, 5.5728e-01, 2.8152e-01,
         3.1619e-01, 6.1117e-01, 1.6485e-01, 7.4452e-01, 5.3049e-01, 1.8774e-01,
         6.3128e-02, 1.6841e-02, 1.1834e-01, 9.0977e-01, 1.3014e-01, 4.5289e-01,
         5.0374e-01, 2.3845e-01, 1.3303e-01, 8.8260e-01, 5.8624e-01, 7.0258e-02,
         7.4918e-01, 9.1036e-01, 9.3271e-01, 9.0934e-01, 6.1848e-01, 4.7312e-01,
         4.1013e-01, 6.1354e-01, 6.7026e-01, 5.6350e-01, 8.0793e-01, 9.2310e-01,
         9.0621e-01, 8.4621e-01, 6.8700e-02, 1.3652e-01, 4.0077e-01, 1.7699e-01,
         2.2270e-01, 8.1085e-01, 2.3486e-01, 5.8925e-01, 8.5591e-01, 9.0538e-01,
         5.4052e-01, 6.4479e-01, 9.9712e-01, 2.0547e-01, 9.7093e-01, 1.5213e-01,
         1.3153e-01, 4.3102e-01, 4.0533e-01, 9.5567e-01, 4.1036e-01, 7.8183e-02,
         7.9612e-01, 5.8398e-01, 5.1580e-02, 9.4270e-01, 7.2094e-01, 7.8454e-01,
         6.5643e-01, 4.8555e-01, 8.3400e-01, 7.1257e-01, 7.6979e-01, 1.0672e-01,
         9.2764e-01, 2.1441e-01, 7.4073e-01, 6.8986e-01, 1.2449e-01, 9.5650e-01,
         7.2501e-01, 6.9341e-01, 8.4703e-01, 5.2092e-01, 4.1580e-01, 4.2624e-01,
         8.9356e-01, 9.5431e-01, 8.8347e-01, 5.6768e-01, 9.4384e-01, 3.6444e-01,
         5.3602e-01, 2.5783e-01, 7.7453e-02, 1.3315e-01, 8.6937e-03, 6.0736e-01,
         9.5200e-01, 9.2494e-01, 2.0200e-01, 4.9034e-03, 9.1086e-01, 7.4326e-01,
         6.3329e-01, 3.3907e-01, 5.0319e-01, 6.3643e-01, 4.2709e-01, 9.8070e-01,
         9.6968e-02, 9.1897e-01, 3.0631e-01, 4.9410e-02, 5.0535e-01, 9.4883e-01,
         3.9405e-01, 3.2441e-01, 2.1353e-01, 8.1944e-01, 9.6539e-01, 7.5876e-02,
         1.3656e-01, 1.5488e-01, 8.6499e-01, 9.7724e-01, 1.6742e-01, 5.7401e-01,
         4.6107e-01, 6.6995e-01, 1.9777e-01, 4.9767e-01, 3.8118e-03, 1.4163e-01,
         3.4760e-01, 5.7657e-01, 2.9097e-01, 3.0916e-01, 4.3239e-01, 5.0633e-01,
         8.8408e-01, 2.2598e-02, 4.1493e-02, 2.9978e-01, 8.1569e-01, 5.4833e-01,
         9.1835e-01, 9.5021e-01, 8.2104e-01, 7.9766e-01, 9.2793e-01, 6.9796e-01,
         3.9302e-01, 1.5547e-01, 8.3688e-01, 2.4063e-01, 1.4312e-01, 8.4984e-01,
         3.5203e-01, 3.7271e-01, 7.9971e-01, 6.1894e-01, 5.5808e-01, 6.8988e-01,
         3.5873e-01, 4.5469e-01, 4.4201e-01, 5.4138e-02, 3.1249e-01, 7.9019e-01,
         6.7549e-01, 3.1018e-01, 1.7665e-01, 5.7101e-01, 1.8270e-01, 3.3248e-01,
         6.6081e-02, 2.2727e-01, 2.6731e-01, 7.8940e-01, 6.2217e-02, 9.2157e-01,
         7.3804e-01, 7.3166e-01, 9.1431e-01, 3.5472e-01, 2.1620e-01, 1.2958e-01,
         7.0236e-01, 5.8272e-01, 2.6654e-01, 3.1988e-01, 4.5991e-02, 2.9462e-01,
         1.6617e-01, 6.8480e-01, 1.1063e-01, 1.8162e-01, 1.4680e-01, 1.7298e-02,
         3.0182e-01, 1.5101e-01, 4.2088e-01, 7.7574e-01, 3.3259e-01, 4.4731e-01,
         6.6298e-01, 2.2328e-01, 9.6383e-01, 9.3632e-01, 8.6273e-01, 5.2677e-01,
         2.5148e-01, 9.7216e-01, 5.4927e-01, 2.2366e-01, 4.0832e-01, 9.9547e-01,
         4.4103e-01, 4.5387e-01, 2.0451e-01, 8.7113e-01, 8.8792e-01, 8.4241e-01,
         6.9482e-01, 1.8805e-01, 3.6249e-01, 6.5291e-01, 5.2990e-01, 4.1524e-01,
         8.1117e-01, 7.6745e-01, 3.0166e-02, 1.3411e-01, 3.8055e-01, 6.7173e-01,
         1.8603e-01, 6.9024e-01, 6.1087e-01, 8.8293e-01, 7.6508e-02, 3.0093e-01,
         3.5325e-01, 5.1409e-01, 2.8666e-02, 2.2300e-01, 4.9283e-01, 7.8134e-01,
         3.9787e-01, 4.7894e-01, 4.7340e-01, 6.5211e-01, 5.5619e-01, 4.7746e-03,
         3.7620e-01, 6.2053e-01, 1.9333e-01, 4.1038e-01, 2.5668e-01, 3.7474e-01,
         9.3170e-01, 9.9513e-01, 9.5040e-01, 3.1794e-01, 6.7776e-01, 4.0337e-01,
         5.3665e-01, 9.0489e-01, 5.7782e-01, 2.6840e-01, 5.8644e-01, 2.8258e-02,
         4.7882e-01, 8.7321e-01, 7.5358e-01, 5.1693e-01, 2.4471e-01, 1.0426e-01,
         9.7403e-01, 6.2994e-01, 3.6065e-01, 5.2812e-01, 6.6355e-01, 5.3298e-01,
         5.5384e-01, 7.2999e-01, 1.5860e-01, 4.8176e-01, 5.8869e-01, 8.3970e-01,
         6.4838e-02, 6.5616e-01, 5.4021e-01, 3.7540e-02, 9.6069e-01, 5.3650e-02,
         8.3805e-02, 1.4707e-01, 4.6424e-01, 8.9087e-03, 9.4398e-01, 9.8371e-01,
         6.8431e-01, 5.8075e-01, 7.1484e-01, 8.8781e-01, 4.5856e-02, 6.8545e-01,
         4.8370e-01, 9.7395e-01, 5.2240e-01, 4.2345e-01, 9.7966e-01, 3.9077e-01,
         4.1921e-01, 5.1670e-01, 2.9508e-03, 9.1061e-01, 9.9310e-02, 6.0789e-01,
         1.4155e-01, 2.2280e-01, 5.5842e-01, 4.4204e-02, 6.4222e-01, 3.9172e-01,
         4.2740e-01, 2.4187e-01, 3.6170e-01, 7.8204e-01, 6.6897e-01, 8.2557e-01,
         9.5274e-01, 8.0410e-02, 1.6719e-01, 1.0316e-01, 5.5393e-01, 5.9471e-01,
         6.5466e-01, 5.2669e-01, 5.2617e-01, 2.3212e-01, 5.6428e-04, 2.6302e-01,
         4.1210e-01, 2.2576e-01, 5.1174e-01, 6.9911e-01, 4.6642e-01, 2.2701e-01,
         4.9629e-01, 4.7453e-01, 9.2084e-01, 4.7518e-01, 6.1844e-01, 1.3826e-02,
         6.2409e-01, 9.8305e-01, 5.2792e-01, 5.8097e-01, 8.0717e-01, 2.5384e-01,
         8.7329e-01, 6.7783e-01, 7.3062e-01, 3.4365e-01, 4.3590e-02, 8.5212e-01,
         3.9023e-01, 9.3285e-01, 5.9549e-01, 4.9415e-01, 9.2986e-01, 3.2788e-01,
         8.1046e-01, 7.0487e-01, 8.6957e-01, 7.2882e-01, 6.5029e-01, 3.8408e-01,
         4.9803e-01, 8.6414e-01, 4.9263e-02, 8.5967e-01, 1.9354e-02, 9.6952e-01,
         8.5151e-02, 9.4749e-01, 8.5569e-01, 1.7857e-01, 5.8834e-01, 1.0301e-01,
         7.2684e-01, 9.0560e-01, 9.0408e-01, 7.9621e-01, 4.4297e-01, 5.6278e-01,
         2.4982e-01, 2.6223e-01, 8.9848e-01, 3.7232e-01, 6.2483e-01, 5.9274e-01,
         3.1240e-01, 8.9609e-01, 9.0353e-01, 9.7379e-01, 3.7268e-01, 9.6305e-01,
         1.0314e-01, 6.9361e-01, 2.2058e-01, 6.1966e-01, 9.9571e-01, 9.6001e-02,
         8.6991e-01, 1.2234e-01, 7.5473e-01, 6.9188e-01, 5.7510e-01, 5.1726e-01,
         4.1708e-01, 3.1825e-02, 1.2966e-01, 9.6492e-01, 3.4177e-01, 6.1411e-02,
         4.2134e-02, 4.2906e-01, 6.4009e-01, 3.8773e-01, 5.9742e-01, 3.0262e-01,
         3.1371e-01, 1.3073e-01, 6.5392e-02, 2.7763e-01, 7.2277e-01, 6.0852e-01,
         3.6508e-01, 7.5100e-01, 5.6998e-01, 9.3177e-01, 8.8765e-01, 2.8202e-01,
         7.0010e-01, 2.3125e-01, 1.1058e-02, 3.2210e-01, 2.9239e-01, 2.4371e-01,
         5.5073e-01, 6.0041e-01, 3.7674e-01, 5.1074e-01, 1.7171e-01, 4.6307e-01,
         4.5791e-01, 6.7470e-02, 4.7084e-01, 7.0824e-01, 6.5861e-01, 9.0703e-02,
         3.4625e-01, 5.4429e-01, 5.5950e-01, 1.5480e-01, 1.0101e-01, 4.2903e-01,
         7.3810e-01, 5.4953e-01, 5.5109e-01, 1.1875e-01, 3.7636e-01, 5.3477e-01,
         8.2859e-01, 1.3424e-01, 8.8562e-01, 2.9170e-01, 8.3249e-01, 7.9342e-01,
         5.1477e-01, 7.0844e-01, 1.9636e-02, 4.6257e-02, 4.9362e-01, 7.1709e-01,
         5.7132e-01, 1.8697e-01, 3.5081e-02, 6.8611e-01, 6.0967e-01, 8.9850e-01,
         2.9079e-02, 5.5260e-01, 5.8803e-01, 4.8886e-01, 2.8641e-01, 2.1865e-01,
         9.5994e-01, 1.0270e-01, 2.4108e-01, 8.8991e-01, 4.1111e-01, 2.6776e-01,
         4.7202e-01, 1.9628e-01, 8.9994e-01, 2.0333e-01, 1.0124e-01, 3.8590e-01,
         9.0021e-01, 5.9278e-01, 5.8001e-01, 9.3222e-01, 1.8874e-01, 7.2618e-01,
         4.1648e-01, 7.6217e-01, 1.2033e-01, 5.8178e-01, 8.9316e-02, 8.8022e-01,
         8.0955e-01, 5.0433e-01, 1.5014e-01, 7.9548e-01, 8.1312e-01, 6.5014e-01,
         4.9404e-01, 7.6957e-01, 4.7000e-01, 8.3309e-01, 8.3767e-01, 8.8204e-01,
         1.4843e-01, 2.0415e-01, 7.7098e-01, 5.4859e-01, 9.0024e-02, 2.4046e-01,
         4.7464e-01, 8.5105e-01, 1.5308e-01, 2.3500e-01, 8.6171e-01, 2.5616e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/5 iter: 0/15 loss: 1.0488 Acc: 40.6250% F1: 0.267 Time: 0.96s (0.00s)
Fold 3 train - epoch: 0/5 iter: 1/15 loss: 1.0144 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 3 train - epoch: 0/5 iter: 2/15 loss: 1.0636 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 3/15 loss: 0.8869 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 4/15 loss: 0.9044 Acc: 65.6250% F1: 0.454 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 5/15 loss: 0.9719 Acc: 46.8750% F1: 0.301 Time: 0.95s (0.02s)
Fold 3 train - epoch: 0/5 iter: 6/15 loss: 0.9398 Acc: 56.2500% F1: 0.355 Time: 0.95s (0.04s)
Fold 3 train - epoch: 0/5 iter: 7/15 loss: 0.8849 Acc: 46.8750% F1: 0.217 Time: 0.95s (0.04s)
Fold 3 train - epoch: 0/5 iter: 8/15 loss: 1.2405 Acc: 50.0000% F1: 0.222 Time: 0.96s (0.04s)
Fold 3 train - epoch: 0/5 iter: 9/15 loss: 1.0647 Acc: 46.8750% F1: 0.213 Time: 0.96s (0.04s)
Fold 3 train - epoch: 0/5 iter: 10/15 loss: 1.0382 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.04s)
Fold 3 train - epoch: 0/5 iter: 11/15 loss: 0.9093 Acc: 59.3750% F1: 0.253 Time: 0.96s (0.04s)
Fold 3 train - epoch: 0/5 iter: 12/15 loss: 0.9695 Acc: 56.2500% F1: 0.240 Time: 0.96s (0.04s)
Fold 3 train - epoch: 0/5 iter: 13/15 loss: 0.9565 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.04s)
Fold 3 train - epoch: 0/5 iter: 14/15 loss: 0.7959 Acc: 100.0000% F1: 1.000 Time: 0.10s (0.04s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 52.2222% F1: 0.2783 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5992 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5688 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 3 train - epoch: 1/5 iter: 0/15 loss: 0.8443 Acc: 59.3750% F1: 0.378 Time: 0.94s (0.00s)
Fold 3 train - epoch: 1/5 iter: 1/15 loss: 0.8704 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 2/15 loss: 0.9660 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 1/5 iter: 3/15 loss: 0.8196 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/5 iter: 4/15 loss: 0.9548 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 5/15 loss: 0.9232 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 6/15 loss: 0.7520 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 7/15 loss: 0.9252 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 8/15 loss: 1.0479 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 9/15 loss: 0.9615 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 10/15 loss: 0.9517 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 11/15 loss: 0.9012 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 12/15 loss: 0.9002 Acc: 68.7500% F1: 0.466 Time: 0.95s (0.02s)
Fold 3 train - epoch: 1/5 iter: 13/15 loss: 0.9121 Acc: 62.5000% F1: 0.399 Time: 0.96s (0.02s)
Fold 3 train - epoch: 1/5 iter: 14/15 loss: 0.7443 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 56.4444% F1: 0.2937 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7381 Acc: 81.2500% F1: 0.692 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3327 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.3752 *
*********************************************************
Performing epoch 2 of 5
Fold 3 train - epoch: 2/5 iter: 0/15 loss: 0.8474 Acc: 68.7500% F1: 0.548 Time: 0.94s (0.00s)
Fold 3 train - epoch: 2/5 iter: 1/15 loss: 0.8256 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 2/15 loss: 0.9109 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 3 train - epoch: 2/5 iter: 3/15 loss: 0.7559 Acc: 59.3750% F1: 0.253 Time: 0.94s (0.02s)
Fold 3 train - epoch: 2/5 iter: 4/15 loss: 0.9352 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 5/15 loss: 0.8768 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 6/15 loss: 0.7220 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 7/15 loss: 0.9116 Acc: 46.8750% F1: 0.213 Time: 0.97s (0.02s)
Fold 3 train - epoch: 2/5 iter: 8/15 loss: 1.0752 Acc: 53.1250% F1: 0.231 Time: 0.96s (0.05s)
Fold 3 train - epoch: 2/5 iter: 9/15 loss: 0.9750 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 10/15 loss: 0.9550 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 11/15 loss: 0.8482 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 12/15 loss: 0.8637 Acc: 62.5000% F1: 0.361 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 13/15 loss: 0.8763 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 3 train - epoch: 2/5 iter: 14/15 loss: 0.4010 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 56.8889% F1: 0.2823 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7169 Acc: 84.3750% F1: 0.726 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3416 Acc: 11.1111% F1: 0.089 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.3635 *
*********************************************************
Performing epoch 3 of 5
Fold 3 train - epoch: 3/5 iter: 0/15 loss: 0.7498 Acc: 68.7500% F1: 0.646 Time: 0.94s (0.00s)
Fold 3 train - epoch: 3/5 iter: 1/15 loss: 0.7761 Acc: 65.6250% F1: 0.577 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 2/15 loss: 0.8461 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 3/15 loss: 0.7293 Acc: 56.2500% F1: 0.286 Time: 0.94s (0.02s)
Fold 3 train - epoch: 3/5 iter: 4/15 loss: 0.8023 Acc: 53.1250% F1: 0.360 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 5/15 loss: 0.7778 Acc: 71.8750% F1: 0.475 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 6/15 loss: 0.6731 Acc: 75.0000% F1: 0.415 Time: 0.95s (0.03s)
Fold 3 train - epoch: 3/5 iter: 7/15 loss: 0.8435 Acc: 53.1250% F1: 0.296 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 8/15 loss: 1.0710 Acc: 59.3750% F1: 0.437 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 9/15 loss: 0.8221 Acc: 56.2500% F1: 0.473 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 10/15 loss: 0.8774 Acc: 46.8750% F1: 0.305 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 11/15 loss: 0.7405 Acc: 68.7500% F1: 0.655 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 12/15 loss: 0.8174 Acc: 62.5000% F1: 0.578 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 13/15 loss: 0.8760 Acc: 59.3750% F1: 0.466 Time: 0.95s (0.02s)
Fold 3 train - epoch: 3/5 iter: 14/15 loss: 0.1741 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 61.1111% F1: 0.4807 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6977 Acc: 78.1250% F1: 0.469 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4470 Acc: 27.7778% F1: 0.241 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 60.0000% F1: 0.4700 *
*********************************************************
Performing epoch 4 of 5
Fold 3 train - epoch: 4/5 iter: 0/15 loss: 0.6457 Acc: 71.8750% F1: 0.663 Time: 0.94s (0.00s)
Fold 3 train - epoch: 4/5 iter: 1/15 loss: 0.6080 Acc: 78.1250% F1: 0.788 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/5 iter: 2/15 loss: 0.8564 Acc: 59.3750% F1: 0.493 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 3/15 loss: 0.6029 Acc: 75.0000% F1: 0.773 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 4/15 loss: 0.6153 Acc: 71.8750% F1: 0.674 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 5/15 loss: 0.6155 Acc: 75.0000% F1: 0.605 Time: 0.95s (0.03s)
Fold 3 train - epoch: 4/5 iter: 6/15 loss: 0.6064 Acc: 75.0000% F1: 0.633 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 7/15 loss: 0.7284 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 8/15 loss: 0.9735 Acc: 53.1250% F1: 0.456 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 9/15 loss: 0.7177 Acc: 81.2500% F1: 0.755 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 10/15 loss: 0.7404 Acc: 71.8750% F1: 0.679 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 11/15 loss: 0.5758 Acc: 78.1250% F1: 0.765 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 12/15 loss: 0.6568 Acc: 71.8750% F1: 0.589 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 13/15 loss: 0.7213 Acc: 68.7500% F1: 0.543 Time: 0.95s (0.02s)
Fold 3 train - epoch: 4/5 iter: 14/15 loss: 0.0465 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 70.8889% F1: 0.6496 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/5 iter: 0/2 loss: 0.7077 Acc: 75.0000% F1: 0.462 Time: 0.32s (0.00s)
Fold 3 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6521 Acc: 22.2222% F1: 0.207 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 56.0000% F1: 0.4352 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[0.6638, 0.2207, 0.2993, 0.4511, 0.5529, 0.0888, 0.4180, 0.7200, 0.0781,
         0.1784, 0.8392, 0.3481, 0.8336, 0.2246, 0.8529, 0.6622, 0.9791, 0.2501,
         0.9599, 0.1770, 0.2682, 0.0475, 0.7577, 0.0903, 0.4298, 0.6733, 0.2294,
         0.6279, 0.5909, 0.8978, 0.7103, 0.1513, 0.3031, 0.1032, 0.8854, 0.7780,
         0.0213, 0.1912, 0.6418, 0.6122, 0.1446, 0.5614, 0.3624, 0.7002, 0.6594,
         0.5852, 0.4832, 0.6035, 0.1128, 0.1972, 0.6633, 0.7370, 0.1158, 0.2597,
         0.5633, 0.5165, 0.4722, 0.3074, 0.2932, 0.7552, 0.6757, 0.1317, 0.0019,
         0.2990, 0.8620, 0.3151, 0.1314, 0.2583, 0.7603, 0.6718, 0.3628, 0.6189,
         0.2839, 0.1656, 0.4542, 0.0708, 0.4458, 0.5161, 0.5678, 0.4260, 0.0230,
         0.5833, 0.2558, 0.3446, 0.9674, 0.9437, 0.5539, 0.9016, 0.8192, 0.2348,
         0.3049, 0.4723, 0.4863, 0.2014, 0.2832, 0.7049, 0.4405, 0.6437, 0.5962,
         0.4992, 0.8139, 0.9756, 0.4587, 0.9656, 0.9058, 0.1730, 0.5711, 0.3586,
         0.7880, 0.4991, 0.5505, 0.9751, 0.1468, 0.5417, 0.9808, 0.1576, 0.0082,
         0.8503, 0.8823, 0.4077, 0.3601, 0.3233, 0.7946, 0.4269, 0.7543, 0.0054,
         0.0532, 0.0445, 0.8492, 0.4362, 0.0611, 0.8580, 0.1944, 0.8844, 0.3270,
         0.7503, 0.4924, 0.3375, 0.3292, 0.5282, 0.5924, 0.9782, 0.1044, 0.2762,
         0.5501, 0.4950, 0.4789, 0.5825, 0.6066, 0.9508, 0.5632, 0.7464, 0.9400,
         0.4185, 0.0134, 0.2631, 0.2353, 0.8705, 0.9787, 0.5221, 0.8280, 0.0609,
         0.6825, 0.2502, 0.4689, 0.0972, 0.4134, 0.5915, 0.1449, 0.5454, 0.7127,
         0.0252, 0.3196, 0.1291, 0.1609, 0.9259, 0.0541, 0.4123, 0.4640, 0.4917,
         0.0876, 0.8334, 0.0661, 0.5265, 0.0023, 0.6098, 0.5862, 0.8809, 0.8442,
         0.7298, 0.4825, 0.9323, 0.3783, 0.8504, 0.0806, 0.5707, 0.0546, 0.4084,
         0.8283, 0.1598, 0.5557, 0.1951, 0.0520, 0.9964, 0.5311, 0.6532, 0.3828,
         0.0861, 0.3434, 0.7324, 0.3748, 0.8446, 0.8748, 0.2421, 0.0386, 0.9057,
         0.8700, 0.4014, 0.3228, 0.1844, 0.8927, 0.4127, 0.9503, 0.8606, 0.8379,
         0.0950, 0.9712, 0.4513, 0.7519, 0.2969, 0.7457, 0.9616, 0.0978, 0.6078,
         0.9487, 0.7478, 0.0781, 0.9559, 0.3718, 0.6445, 0.3426, 0.2344, 0.4541,
         0.3170, 0.8618, 0.9202, 0.1393, 0.6702, 0.5107, 0.8365, 0.8075, 0.2671,
         0.7559, 0.5508, 0.2635, 0.0352, 0.0133, 0.3149, 0.1944, 0.9575, 0.1818,
         0.2573, 0.7587, 0.9950, 0.0070, 0.8241, 0.5020, 0.3056, 0.7899, 0.3065,
         0.0967, 0.2215, 0.9590, 0.9934, 0.5074, 0.2447, 0.8988, 0.0550, 0.8233,
         0.6480, 0.8024, 0.9786, 0.6343, 0.9799, 0.4430, 0.9030, 0.2220, 0.0561,
         0.1227, 0.3806, 0.0706, 0.4408, 0.7960, 0.6764, 0.4283, 0.5916, 0.5593,
         0.2707, 0.2229, 0.1815, 0.3465, 0.0060, 0.7380, 0.3329, 0.5361, 0.4697,
         0.4329, 0.5529, 0.3864, 0.5168, 0.4336, 0.7138, 0.0042, 0.2595, 0.3435,
         0.4542, 0.3166, 0.3299, 0.7539, 0.1927, 0.1149, 0.7276, 0.4536, 0.6488,
         0.5523, 0.0605, 0.7927, 0.9944, 0.1340, 0.1751, 0.5146, 0.3096, 0.9700,
         0.6628, 0.0013, 0.4253, 0.2835, 0.7202, 0.5127, 0.6914, 0.6451, 0.9417,
         0.5480, 0.0155, 0.3917, 0.0850, 0.9884, 0.1183, 0.3674, 0.7187, 0.9994,
         0.3515, 0.4598, 0.4179, 0.3086, 0.9911, 0.4293, 0.0304, 0.1720, 0.7760,
         0.2369, 0.4780, 0.7998, 0.8222, 0.6766, 0.5696, 0.6305, 0.4865, 0.1812,
         0.3706, 0.0179, 0.1999, 0.7060, 0.7767, 0.3247, 0.2369, 0.8142, 0.1872,
         0.7258, 0.4052, 0.1138, 0.7028, 0.9050, 0.0356, 0.5852, 0.7849, 0.7823,
         0.7174, 0.0196, 0.5064, 0.9876, 0.4010, 0.5996, 0.6759, 0.8097, 0.8422,
         0.5481, 0.5204, 0.7732, 0.7960, 0.0048, 0.9433, 0.8551, 0.4480, 0.6086,
         0.2750, 0.8858, 0.5884, 0.9640, 0.1893, 0.9796, 0.9157, 0.3811, 0.1267,
         0.7766, 0.0772, 0.9634, 0.1321, 0.5569, 0.9063, 0.8993, 0.1634, 0.1755,
         0.5769, 0.2206, 0.8022, 0.2337, 0.8544, 0.7584, 0.5398, 0.7093, 0.5748,
         0.2612, 0.4731, 0.9264, 0.0112, 0.9867, 0.8229, 0.8792, 0.7422, 0.3534,
         0.2561, 0.5816, 0.0483, 0.9132, 0.8209, 0.1803, 0.3765, 0.2148, 0.2029,
         0.4077, 0.7224, 0.1742, 0.7538, 0.9478, 0.4332, 0.3789, 0.5221, 0.0425,
         0.0828, 0.7854, 0.1563, 0.5956, 0.9394, 0.4508, 0.9277, 0.1124, 0.5385,
         0.2450, 0.3378, 0.5409, 0.5470, 0.7813, 0.4980, 0.0961, 0.4627, 0.6657,
         0.1042, 0.0109, 0.1088, 0.4394, 0.8758, 0.9482, 0.5995, 0.2054, 0.3868,
         0.0789, 0.5336, 0.4312, 0.9193, 0.4116, 0.6970, 0.6460, 0.3424, 0.1962,
         0.7732, 0.5881, 0.8196, 0.0650, 0.7568, 0.6143, 0.4266, 0.5514, 0.1894,
         0.2699, 0.1320, 0.2887, 0.4174, 0.0793, 0.2997, 0.9446, 0.2054, 0.8752,
         0.3966, 0.8127, 0.7703, 0.5661, 0.9163, 0.6175, 0.7880, 0.4569, 0.7162,
         0.1782, 0.7836, 0.4970, 0.7481, 0.1211, 0.8256, 0.8741, 0.9356, 0.6936,
         0.2986, 0.1996, 0.2228, 0.1091, 0.9785, 0.4048, 0.3568, 0.1416, 0.8570,
         0.5543, 0.0891, 0.5373, 0.3577, 0.7682, 0.8055, 0.7877, 0.2917, 0.8230,
         0.3509, 0.4512, 0.4748, 0.6042, 0.1922, 0.8503, 0.1504, 0.2361, 0.9207,
         0.2577, 0.4911, 0.5587, 0.6055, 0.3102, 0.0864, 0.8794, 0.9866, 0.9141,
         0.7006, 0.6734, 0.6655, 0.7725, 0.4812, 0.0818, 0.3808, 0.1178, 0.3852,
         0.0739, 0.1033, 0.5653, 0.7656, 0.8583, 0.5317, 0.9339, 0.2995, 0.3141,
         0.0842, 0.8619, 0.1025, 0.6447, 0.9655, 0.3726, 0.2255, 0.2863, 0.0782,
         0.0256, 0.1587, 0.8249, 0.3114, 0.7221, 0.7511, 0.0954, 0.8025, 0.7772,
         0.1730, 0.8983, 0.2232, 0.5901, 0.0346, 0.8302, 0.0912, 0.0549, 0.3629,
         0.4740, 0.6877, 0.6930, 0.4068, 0.4799, 0.0773, 0.0834, 0.6838, 0.0806,
         0.4912, 0.6688, 0.9636, 0.3755, 0.1589, 0.2764, 0.2642, 0.8687, 0.1415,
         0.4250, 0.8891, 0.0353, 0.4831, 0.8398, 0.1591, 0.0579, 0.5409, 0.9486,
         0.7904, 0.1041, 0.9070, 0.3227, 0.7834, 0.8016, 0.4149, 0.4020, 0.2253,
         0.6022, 0.7988, 0.9053, 0.1706, 0.2787, 0.8432, 0.4954, 0.6505, 0.4414,
         0.5159, 0.6539, 0.7990, 0.6079, 0.7587, 0.1923, 0.7936, 0.2182, 0.5237,
         0.7597, 0.0803, 0.8079, 0.5986, 0.0496, 0.8324, 0.0324, 0.4816, 0.7313,
         0.5551, 0.3543, 0.7616, 0.5404, 0.8305, 0.6542, 0.1038, 0.6792, 0.7591,
         0.1876, 0.2342, 0.8232, 0.6023, 0.9854, 0.4026, 0.4607, 0.0643, 0.1533,
         0.1336, 0.6840, 0.9666, 0.4207, 0.2873, 0.4867, 0.9777, 0.7330, 0.6154,
         0.4412, 0.5580, 0.7215, 0.3969, 0.5300, 0.2879, 0.0983, 0.9412, 0.1135,
         0.9066, 0.6242, 0.7695, 0.8910, 0.1111, 0.6542, 0.1337, 0.8179, 0.8331,
         0.9755, 0.9023, 0.2968, 0.6843, 0.6183, 0.9003, 0.8479, 0.1000, 0.2200,
         0.9570, 0.9654, 0.0968, 0.4205, 0.3640, 0.1079, 0.7467, 0.6370, 0.3170,
         0.3211, 0.8899, 0.3785, 0.3837, 0.8093, 0.8916, 0.3467, 0.4287, 0.8526,
         0.4170, 0.3891, 0.0788, 0.5178, 0.7679, 0.7718, 0.8590, 0.8914, 0.1218,
         0.4959, 0.2766, 0.8607, 0.8973, 0.6945, 0.6865, 0.0924, 0.8658, 0.9534,
         0.5567, 0.5657, 0.2693]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/5 iter: 0/15 loss: 1.0421 Acc: 46.8750% F1: 0.320 Time: 0.98s (0.00s)
Fold 4 train - epoch: 0/5 iter: 1/15 loss: 1.0258 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 2/15 loss: 1.0441 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 3/15 loss: 0.9436 Acc: 53.1250% F1: 0.236 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 4/15 loss: 0.8986 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 5/15 loss: 0.9833 Acc: 40.6250% F1: 0.250 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 6/15 loss: 0.9319 Acc: 59.3750% F1: 0.392 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 7/15 loss: 0.8814 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 8/15 loss: 1.2063 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 9/15 loss: 1.0931 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 10/15 loss: 1.0709 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 11/15 loss: 0.9244 Acc: 65.6250% F1: 0.327 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 12/15 loss: 1.0015 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 13/15 loss: 0.9362 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 4 train - epoch: 0/5 iter: 14/15 loss: 0.6587 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 52.2222% F1: 0.2857 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6007 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5473 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 4 train - epoch: 1/5 iter: 0/15 loss: 0.8454 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.00s)
Fold 4 train - epoch: 1/5 iter: 1/15 loss: 0.8729 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 4 train - epoch: 1/5 iter: 2/15 loss: 0.9371 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 3/15 loss: 0.8062 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.04s)
Fold 4 train - epoch: 1/5 iter: 4/15 loss: 0.9469 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 5/15 loss: 0.8769 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 6/15 loss: 0.7879 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 7/15 loss: 0.8912 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 8/15 loss: 1.0531 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 9/15 loss: 0.9697 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 10/15 loss: 0.9537 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 11/15 loss: 0.8683 Acc: 62.5000% F1: 0.351 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 12/15 loss: 0.9641 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 13/15 loss: 0.8817 Acc: 59.3750% F1: 0.300 Time: 0.95s (0.02s)
Fold 4 train - epoch: 1/5 iter: 14/15 loss: 0.6228 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 55.1111% F1: 0.2533 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7306 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3341 Acc: 5.5556% F1: 0.048 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.2659 *
*********************************************************
Performing epoch 2 of 5
Fold 4 train - epoch: 2/5 iter: 0/15 loss: 0.8503 Acc: 59.3750% F1: 0.342 Time: 0.94s (0.00s)
Fold 4 train - epoch: 2/5 iter: 1/15 loss: 0.8303 Acc: 65.6250% F1: 0.409 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 2/15 loss: 0.8732 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 3/15 loss: 0.7599 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 4/15 loss: 0.9068 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 5/15 loss: 0.8567 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 6/15 loss: 0.7341 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 7/15 loss: 0.8501 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 8/15 loss: 1.0085 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 9/15 loss: 0.9696 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 10/15 loss: 0.9905 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 11/15 loss: 0.8257 Acc: 59.3750% F1: 0.301 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 12/15 loss: 0.8855 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 13/15 loss: 0.8718 Acc: 56.2500% F1: 0.326 Time: 0.95s (0.02s)
Fold 4 train - epoch: 2/5 iter: 14/15 loss: 0.4527 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 56.2222% F1: 0.2807 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7531 Acc: 68.7500% F1: 0.487 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3133 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.3073 *
*********************************************************
Performing epoch 3 of 5
Fold 4 train - epoch: 3/5 iter: 0/15 loss: 0.7430 Acc: 62.5000% F1: 0.560 Time: 0.95s (0.00s)
Fold 4 train - epoch: 3/5 iter: 1/15 loss: 0.7579 Acc: 71.8750% F1: 0.584 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 2/15 loss: 0.7914 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 3/15 loss: 0.7142 Acc: 59.3750% F1: 0.298 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 4/15 loss: 0.7653 Acc: 56.2500% F1: 0.416 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 5/15 loss: 0.7171 Acc: 65.6250% F1: 0.459 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 6/15 loss: 0.6349 Acc: 81.2500% F1: 0.697 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 7/15 loss: 0.7419 Acc: 59.3750% F1: 0.366 Time: 0.96s (0.02s)
Fold 4 train - epoch: 3/5 iter: 8/15 loss: 0.9314 Acc: 59.3750% F1: 0.396 Time: 0.95s (0.03s)
Fold 4 train - epoch: 3/5 iter: 9/15 loss: 1.0054 Acc: 56.2500% F1: 0.368 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 10/15 loss: 0.8199 Acc: 59.3750% F1: 0.415 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 11/15 loss: 0.7530 Acc: 68.7500% F1: 0.449 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 12/15 loss: 0.8417 Acc: 59.3750% F1: 0.483 Time: 0.95s (0.02s)
Fold 4 train - epoch: 3/5 iter: 13/15 loss: 0.8511 Acc: 56.2500% F1: 0.456 Time: 0.96s (0.03s)
Fold 4 train - epoch: 3/5 iter: 14/15 loss: 0.1185 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 62.4444% F1: 0.4710 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7427 Acc: 62.5000% F1: 0.305 Time: 0.32s (0.00s)
Fold 4 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4721 Acc: 16.6667% F1: 0.172 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.3466 *
*********************************************************
Performing epoch 4 of 5
Fold 4 train - epoch: 4/5 iter: 0/15 loss: 0.5976 Acc: 71.8750% F1: 0.643 Time: 0.95s (0.00s)
Fold 4 train - epoch: 4/5 iter: 1/15 loss: 0.6078 Acc: 78.1250% F1: 0.770 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 2/15 loss: 0.7224 Acc: 68.7500% F1: 0.606 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 3/15 loss: 0.5450 Acc: 71.8750% F1: 0.733 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 4/15 loss: 0.5689 Acc: 78.1250% F1: 0.654 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 5/15 loss: 0.6061 Acc: 81.2500% F1: 0.748 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 6/15 loss: 0.6818 Acc: 71.8750% F1: 0.664 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 7/15 loss: 0.6682 Acc: 75.0000% F1: 0.715 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 8/15 loss: 0.7716 Acc: 65.6250% F1: 0.638 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 9/15 loss: 0.7452 Acc: 65.6250% F1: 0.461 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 10/15 loss: 0.7502 Acc: 65.6250% F1: 0.620 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 11/15 loss: 0.5600 Acc: 75.0000% F1: 0.510 Time: 0.95s (0.02s)
Fold 4 train - epoch: 4/5 iter: 12/15 loss: 0.6105 Acc: 75.0000% F1: 0.720 Time: 0.97s (0.02s)
Fold 4 train - epoch: 4/5 iter: 13/15 loss: 0.7120 Acc: 59.3750% F1: 0.552 Time: 0.95s (0.03s)
Fold 4 train - epoch: 4/5 iter: 14/15 loss: 0.0444 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 71.7778% F1: 0.6627 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/5 iter: 0/2 loss: 0.9859 Acc: 53.1250% F1: 0.296 Time: 0.32s (0.00s)
Fold 4 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5737 Acc: 27.7778% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 44.0000% F1: 0.3619 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[0.4015, 0.7158, 0.3370, 0.3102, 0.2796, 0.6118, 0.6579, 0.2617, 0.2093,
         0.6913, 0.7039, 0.1670, 0.6938, 0.6511, 0.3001, 0.4689, 0.4369, 0.8152,
         0.3005, 0.3372, 0.4759, 0.4669, 0.9516, 0.6651, 0.7426, 0.4731, 0.4770,
         0.6626, 0.7526, 0.6531, 0.8670, 0.3363, 0.6174, 0.6757, 0.8437, 0.6624,
         0.0203, 0.1592, 0.2982, 0.6198, 0.4899, 0.8619, 0.1307, 0.5680, 0.9274,
         0.7647, 0.5519, 0.2370, 0.0320, 0.4792, 0.4559, 0.4301, 0.5013, 0.7443,
         0.0592, 0.6363, 0.8982, 0.2210, 0.1594, 0.0427, 0.6452, 0.8255, 0.1528,
         0.4703, 0.8761, 0.6730, 0.5216, 0.7552, 0.3814, 0.3853, 0.7755, 0.9955,
         0.7380, 0.5305, 0.0620, 0.1236, 0.3098, 0.2072, 0.4347, 0.5141, 0.9332,
         0.4082, 0.5225, 0.1654, 0.8340, 0.2278, 0.6887, 0.6177, 0.5452, 0.5546,
         0.8972, 0.0384, 0.1412, 0.0908, 0.2903, 0.2361, 0.4014, 0.6200, 0.4781,
         0.3043, 0.7292, 0.5535, 0.9033, 0.3942, 0.5865, 0.3900, 0.2253, 0.9184,
         0.7948, 0.6097, 0.0946, 0.3731, 0.1628, 0.4106, 0.8023, 0.7122, 0.8440,
         0.4909, 0.8262, 0.9996, 0.1591, 0.9197, 0.1401, 0.1139, 0.9501, 0.5387,
         0.2568, 0.5952, 0.3872, 0.6946, 0.5872, 0.1360, 0.3290, 0.8144, 0.9917,
         0.6170, 0.4190, 0.6769, 0.3397, 0.2428, 0.9045, 0.0012, 0.1226, 0.9575,
         0.1368, 0.7095, 0.4343, 0.1761, 0.6004, 0.6379, 0.4271, 0.5503, 0.5516,
         0.0408, 0.8887, 0.8829, 0.0910, 0.8636, 0.5256, 0.8188, 0.4439, 0.9246,
         0.8476, 0.7445, 0.1343, 0.8375, 0.1714, 0.3083, 0.8265, 0.3685, 0.7678,
         0.4923, 0.5503, 0.9128, 0.7020, 0.4904, 0.5996, 0.1718, 0.8689, 0.0218,
         0.7319, 0.3669, 0.5409, 0.1866, 0.7107, 0.8771, 0.4008, 0.9232, 0.7664,
         0.9697, 0.3423, 0.2978, 0.5183, 0.5573, 0.0033, 0.3519, 0.6878, 0.7139,
         0.8059, 0.8893, 0.1356, 0.0243, 0.4515, 0.1439, 0.2230, 0.2020, 0.6816,
         0.5507, 0.2130, 0.0998, 0.8664, 0.8328, 0.8554, 0.7172, 0.5946, 0.1476,
         0.9638, 0.1473, 0.3982, 0.4374, 0.2831, 0.6906, 0.4573, 0.7214, 0.4226,
         0.9500, 0.7856, 0.1484, 0.2303, 0.4040, 0.1846, 0.4058, 0.2291, 0.7179,
         0.5474, 0.3209, 0.3409, 0.9810, 0.0203, 0.5735, 0.2014, 0.8700, 0.5008,
         0.8570, 0.1725, 0.9200, 0.9533, 0.4886, 0.5506, 0.9090, 0.2747, 0.9961,
         0.2945, 0.3532, 0.9962, 0.4675, 0.8100, 0.8027, 0.7533, 0.1265, 0.5100,
         0.2863, 0.1030, 0.7428, 0.5762, 0.3512, 0.8354, 0.3606, 0.0273, 0.6998,
         0.1553, 0.6832, 0.7578, 0.8888, 0.2949, 0.3170, 0.4033, 0.9511, 0.1457,
         0.5641, 0.3720, 0.6902, 0.5225, 0.9048, 0.6358, 0.6461, 0.9228, 0.8150,
         0.8462, 0.2064, 0.3699, 0.5060, 0.9445, 0.7565, 0.8427, 0.8602, 0.1703,
         0.9144, 0.1928, 0.5052, 0.2528, 0.8590, 0.3052, 0.2975, 0.5498, 0.5506,
         0.1435, 0.3866, 0.3737, 0.8495, 0.8958, 0.3680, 0.2786, 0.1130, 0.9286,
         0.8768, 0.6276, 0.6635, 0.9664, 0.0167, 0.4181, 0.2142, 0.3001, 0.4178,
         0.3055, 0.1433, 0.1343, 0.7966, 0.8669, 0.4981, 0.9408, 0.7567, 0.2662,
         0.6725, 0.4429, 0.9655, 0.1971, 0.8092, 0.6845, 0.6560, 0.6212, 0.9542,
         0.1618, 0.8473, 0.7161, 0.8312, 0.1836, 0.4081, 0.3280, 0.9817, 0.8245,
         0.3732, 0.3887, 0.1271, 0.2630, 0.8020, 0.4078, 0.3026, 0.9464, 0.7005,
         0.8676, 0.7133, 0.8478, 0.9592, 0.1934, 0.8768, 0.6806, 0.3713, 0.5802,
         0.6745, 0.1488, 0.3341, 0.0744, 0.6498, 0.4293, 0.5736, 0.2941, 0.9987,
         0.2398, 0.8806, 0.0796, 0.2442, 0.2177, 0.1849, 0.7979, 0.2062, 0.0591,
         0.6806, 0.9947, 0.9202, 0.0803, 0.5656, 0.3894, 0.9733, 0.0767, 0.3903,
         0.9754, 0.2922, 0.8871, 0.3719, 0.4063, 0.9656, 0.2069, 0.5791, 0.4828,
         0.4906, 0.8461, 0.9833, 0.9899, 0.5527, 0.4284, 0.4160, 0.3885, 0.2727,
         0.6086, 0.2298, 0.6474, 0.0038, 0.8311, 0.6362, 0.8113, 0.1177, 0.9033,
         0.6491, 0.7708, 0.6836, 0.1445, 0.6976, 0.8956, 0.9316, 0.3555, 0.7129,
         0.1656, 0.1193, 0.4941, 0.4949, 0.0280, 0.6569, 0.5861, 0.2661, 0.6807,
         0.8337, 0.2083, 0.0397, 0.1263, 0.0843, 0.6138, 0.2346, 0.5295, 0.4210,
         0.9378, 0.7283, 0.9908, 0.7074, 0.3663, 0.9499, 0.1767, 0.9590, 0.4338,
         0.6187, 0.2368, 0.5853, 0.5736, 0.9881, 0.5516, 0.4218, 0.9106, 0.2436,
         0.0695, 0.6285, 0.9024, 0.2640, 0.9001, 0.7764, 0.5090, 0.0886, 0.0544,
         0.4147, 0.5712, 0.8281, 0.9569, 0.6252, 0.8053, 0.5192, 0.9344, 0.8267,
         0.3448, 0.2768, 0.0903, 0.4528, 0.0371, 0.6833, 0.7051, 0.0423, 0.3938,
         0.8621, 0.4070, 0.7042, 0.9338, 0.2874, 0.4115, 0.5311, 0.2556, 0.1113,
         0.3160, 0.0483, 0.2776, 0.1826, 0.0206, 0.5343, 0.2182, 0.5700, 0.3843,
         0.4143, 0.0275, 0.6678, 0.5254, 0.7853, 0.0510, 0.4895, 0.2753, 0.9722,
         0.4044, 0.7284, 0.1239, 0.4516, 0.8622, 0.7415, 0.9391, 0.6632, 0.3997,
         0.8500, 0.8895, 0.0565, 0.8033, 0.0575, 0.3134, 0.5258, 0.4581, 0.8153,
         0.9927, 0.4425, 0.8952, 0.3502, 0.5793, 0.2169, 0.7617, 0.0211, 0.5109,
         0.4620, 0.2884, 0.3110, 0.7232, 0.2125, 0.8667, 0.3314, 0.7854, 0.3608,
         0.1406, 0.8358, 0.8661, 0.8724, 0.4399, 0.5189, 0.0671, 0.0092, 0.0882,
         0.1917, 0.2892, 0.8380, 0.8028, 0.1484, 0.8601, 0.0229, 0.6356, 0.7540,
         0.7260, 0.3316, 0.1537, 0.7886, 0.5869, 0.7287, 0.5987, 0.9993, 0.4546,
         0.0232, 0.2620, 0.6717, 0.7156, 0.2721, 0.8580, 0.0024, 0.1447, 0.1234,
         0.5754, 0.1153, 0.3274, 0.2566, 0.0292, 0.4036, 0.9691, 0.6892, 0.2116,
         0.3047, 0.7019, 0.1301, 0.5937, 0.2700, 0.3241, 0.0752, 0.0891, 0.5537,
         0.0268, 0.1728, 0.2984, 0.5773, 0.7445, 0.5022, 0.5523, 0.1338, 0.1401,
         0.4895, 0.1870, 0.3615, 0.1466, 0.3202, 0.8562, 0.3982, 0.5230, 0.6220,
         0.6238, 0.3760, 0.5719, 0.8621, 0.5773, 0.5990, 0.8948, 0.5660, 0.1649,
         0.8924, 0.0486, 0.4374, 0.4931, 0.2083, 0.1192, 0.1428, 0.0172, 0.5474,
         0.4886, 0.4195, 0.8998, 0.9853, 0.4258, 0.4427, 0.9676, 0.5334, 0.7610,
         0.9573, 0.4850, 0.3984, 0.8006, 0.2692, 0.0040, 0.6994, 0.6677, 0.0836,
         0.6228, 0.8210, 0.3762, 0.6010, 0.1332, 0.8950, 0.7313, 0.9180, 0.9410,
         0.0786, 0.7057, 0.6788, 0.8524, 0.5537, 0.8794, 0.1579, 0.3608, 0.8292,
         0.6247, 0.7434, 0.0398, 0.0243, 0.9839, 0.5730, 0.6080, 0.4951, 0.2464,
         0.6185, 0.1518, 0.6587, 0.6944, 0.6074, 0.7654, 0.1645, 0.1080, 0.2853,
         0.6418, 0.8588, 0.3659, 0.9390, 0.5952, 0.4307, 0.5865, 0.6649, 0.8147,
         0.1259, 0.0147, 0.6840, 0.6551, 0.3754, 0.6636, 0.2894, 0.6588, 0.9468,
         0.5934, 0.7160, 0.6394, 0.2629, 0.6285, 0.2476, 0.8649, 0.2130, 0.3353,
         0.2806, 0.8084, 0.3600, 0.6790, 0.3651, 0.5237, 0.7825, 0.3197, 0.5501,
         0.7718, 0.4070, 0.2592, 0.5216, 0.9300, 0.7845, 0.4777, 0.0600, 0.2591,
         0.7263, 0.4216, 0.5256, 0.5530, 0.3523, 0.4461, 0.8908, 0.4274, 0.1752,
         0.5023, 0.0520, 0.9156, 0.7403, 0.2561, 0.0302, 0.1969, 0.7686, 0.9889,
         0.0453, 0.1305, 0.6448]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/5 iter: 0/15 loss: 0.9775 Acc: 53.1250% F1: 0.356 Time: 0.97s (0.00s)
Fold 5 train - epoch: 0/5 iter: 1/15 loss: 1.0743 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.04s)
Fold 5 train - epoch: 0/5 iter: 2/15 loss: 1.0666 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 3/15 loss: 0.9104 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 5 train - epoch: 0/5 iter: 4/15 loss: 0.9083 Acc: 50.0000% F1: 0.322 Time: 0.95s (0.03s)
Fold 5 train - epoch: 0/5 iter: 5/15 loss: 0.9782 Acc: 43.7500% F1: 0.296 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 6/15 loss: 0.8992 Acc: 46.8750% F1: 0.291 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 7/15 loss: 0.8495 Acc: 53.1250% F1: 0.317 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 8/15 loss: 1.2000 Acc: 43.7500% F1: 0.212 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 9/15 loss: 1.0167 Acc: 50.0000% F1: 0.295 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 10/15 loss: 1.0158 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 11/15 loss: 0.9116 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 12/15 loss: 0.9786 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 13/15 loss: 0.9466 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 0/5 iter: 14/15 loss: 0.5971 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 51.7778% F1: 0.2861 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6421 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6329 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 5 train - epoch: 1/5 iter: 0/15 loss: 0.8295 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.00s)
Fold 5 train - epoch: 1/5 iter: 1/15 loss: 0.8733 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/5 iter: 2/15 loss: 0.9347 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 3/15 loss: 0.8275 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 4/15 loss: 0.9680 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 5/15 loss: 0.8442 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 6/15 loss: 0.7604 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 7/15 loss: 0.8966 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 8/15 loss: 1.0081 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 9/15 loss: 0.9427 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 10/15 loss: 0.9980 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 11/15 loss: 0.9097 Acc: 56.2500% F1: 0.289 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 12/15 loss: 0.9404 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 13/15 loss: 0.9098 Acc: 53.1250% F1: 0.276 Time: 0.95s (0.02s)
Fold 5 train - epoch: 1/5 iter: 14/15 loss: 0.6524 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 54.0000% F1: 0.2462 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7550 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4364 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3022 *
*********************************************************
Performing epoch 2 of 5
Fold 5 train - epoch: 2/5 iter: 0/15 loss: 0.7808 Acc: 62.5000% F1: 0.358 Time: 0.95s (0.00s)
Fold 5 train - epoch: 2/5 iter: 1/15 loss: 0.8381 Acc: 56.2500% F1: 0.327 Time: 0.94s (0.02s)
Fold 5 train - epoch: 2/5 iter: 2/15 loss: 0.8379 Acc: 50.0000% F1: 0.262 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 3/15 loss: 0.7656 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 4/15 loss: 0.8674 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 5/15 loss: 0.8408 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 6/15 loss: 0.7256 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 7/15 loss: 0.8472 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 8/15 loss: 1.0117 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 9/15 loss: 0.9693 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 10/15 loss: 0.9937 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 11/15 loss: 0.8575 Acc: 59.3750% F1: 0.335 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 12/15 loss: 0.8769 Acc: 59.3750% F1: 0.342 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 13/15 loss: 0.9183 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.02s)
Fold 5 train - epoch: 2/5 iter: 14/15 loss: 0.3874 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 55.5556% F1: 0.2814 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8179 Acc: 59.3750% F1: 0.248 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4549 Acc: 16.6667% F1: 0.184 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 44.0000% F1: 0.3202 *
*********************************************************
Performing epoch 3 of 5
Fold 5 train - epoch: 3/5 iter: 0/15 loss: 0.7188 Acc: 65.6250% F1: 0.604 Time: 0.94s (0.00s)
Fold 5 train - epoch: 3/5 iter: 1/15 loss: 0.7568 Acc: 71.8750% F1: 0.664 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 2/15 loss: 0.8205 Acc: 56.2500% F1: 0.478 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 3/15 loss: 0.7513 Acc: 62.5000% F1: 0.344 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 4/15 loss: 0.7327 Acc: 59.3750% F1: 0.491 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 5/15 loss: 0.7329 Acc: 65.6250% F1: 0.380 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 6/15 loss: 0.6472 Acc: 71.8750% F1: 0.499 Time: 0.95s (0.03s)
Fold 5 train - epoch: 3/5 iter: 7/15 loss: 0.7578 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 8/15 loss: 0.8812 Acc: 62.5000% F1: 0.480 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 9/15 loss: 0.8518 Acc: 62.5000% F1: 0.525 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 10/15 loss: 0.8908 Acc: 56.2500% F1: 0.404 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 11/15 loss: 0.8030 Acc: 59.3750% F1: 0.392 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 12/15 loss: 0.8178 Acc: 56.2500% F1: 0.460 Time: 0.95s (0.02s)
Fold 5 train - epoch: 3/5 iter: 13/15 loss: 0.8280 Acc: 62.5000% F1: 0.480 Time: 0.96s (0.02s)
Fold 5 train - epoch: 3/5 iter: 14/15 loss: 0.1018 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 62.4444% F1: 0.4908 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7576 Acc: 65.6250% F1: 0.264 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 3/5 iter: 1/2 loss: 1.8729 Acc: 22.2222% F1: 0.256 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3913 *
*********************************************************
Performing epoch 4 of 5
Fold 5 train - epoch: 4/5 iter: 0/15 loss: 0.5700 Acc: 81.2500% F1: 0.810 Time: 0.94s (0.00s)
Fold 5 train - epoch: 4/5 iter: 1/15 loss: 0.5297 Acc: 75.0000% F1: 0.733 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/5 iter: 2/15 loss: 0.6541 Acc: 68.7500% F1: 0.677 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 3/15 loss: 0.6789 Acc: 65.6250% F1: 0.386 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 4/15 loss: 0.5512 Acc: 78.1250% F1: 0.747 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 5/15 loss: 0.4927 Acc: 84.3750% F1: 0.781 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 6/15 loss: 0.5950 Acc: 75.0000% F1: 0.604 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 7/15 loss: 0.6138 Acc: 71.8750% F1: 0.799 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 8/15 loss: 0.6510 Acc: 75.0000% F1: 0.715 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 9/15 loss: 0.7932 Acc: 62.5000% F1: 0.513 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 10/15 loss: 0.7034 Acc: 62.5000% F1: 0.579 Time: 0.95s (0.03s)
Fold 5 train - epoch: 4/5 iter: 11/15 loss: 0.6058 Acc: 65.6250% F1: 0.436 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 12/15 loss: 0.7080 Acc: 65.6250% F1: 0.548 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 13/15 loss: 0.7606 Acc: 56.2500% F1: 0.416 Time: 0.95s (0.02s)
Fold 5 train - epoch: 4/5 iter: 14/15 loss: 0.0402 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 70.6667% F1: 0.6389 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/5 iter: 0/2 loss: 1.3381 Acc: 43.7500% F1: 0.212 Time: 0.32s (0.00s)
Fold 5 train-dev - epoch: 4/5 iter: 1/2 loss: 2.0210 Acc: 22.2222% F1: 0.210 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 36.0000% F1: 0.2924 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[0.4356, 0.9390, 0.5770, 0.1655, 0.5204, 0.2504, 0.0866, 0.4627, 0.3890,
         0.1417, 0.7046, 0.3556, 0.4710, 0.5681, 0.9757, 0.3693, 0.0191, 0.5457,
         0.8462, 0.8237, 0.5909, 0.1849, 0.2326, 0.6561, 0.0342, 0.8967, 0.4892,
         0.1324, 0.1580, 0.0987, 0.2225, 0.1075, 0.6581, 0.5439, 0.8633, 0.4874,
         0.9079, 0.1735, 0.3101, 0.9354, 0.2295, 0.2252, 0.1246, 0.8776, 0.6822,
         0.7592, 0.1334, 0.9037, 0.2319, 0.3512, 0.3722, 0.1622, 0.1317, 0.2845,
         0.3480, 0.3592, 0.2746, 0.5478, 0.9255, 0.4445, 0.4394, 0.3677, 0.8855,
         0.7478, 0.6495, 0.3166, 0.7793, 0.9905, 0.0979, 0.7391, 0.0961, 0.5611,
         0.7834, 0.4160, 0.8659, 0.9556, 0.4140, 0.1759, 0.1966, 0.9812, 0.8475,
         0.4308, 0.5435, 0.6285, 0.7039, 0.8837, 0.5418, 0.7093, 0.3441, 0.9411,
         0.9196, 0.6618, 0.0309, 0.1742, 0.6975, 0.9728, 0.2466, 0.4680, 0.3188,
         0.6052, 0.6334, 0.6525, 0.1176, 0.3608, 0.5468, 0.8005, 0.5607, 0.8561,
         0.6666, 0.0736, 0.0197, 0.6233, 0.2772, 0.8765, 0.3068, 0.1764, 0.5790,
         0.4207, 0.3326, 0.7372, 0.8536, 0.3499, 0.1749, 0.1139, 0.9566, 0.6682,
         0.4244, 0.6001, 0.3042, 0.4483, 0.6131, 0.7405, 0.4285, 0.2975, 0.6257,
         0.3694, 0.5041, 0.7640, 0.7660, 0.9578, 0.9958, 0.3201, 0.8514, 0.5452,
         0.9619, 0.3245, 0.7544, 0.0535, 0.4468, 0.6021, 0.7124, 0.5569, 0.3492,
         0.6470, 0.0454, 0.1554, 0.5957, 0.7335, 0.2034, 0.6608, 0.7623, 0.1165,
         0.1023, 0.1801, 0.2895, 0.4180, 0.8037, 0.9103, 0.4678, 0.2037, 0.3318,
         0.2611, 0.7145, 0.5335, 0.9932, 0.1058, 0.0551, 0.3513, 0.8674, 0.6770,
         0.2294, 0.0548, 0.4820, 0.3942, 0.1733, 0.4323, 0.6986, 0.0550, 0.9398,
         0.5569, 0.0388, 0.1528, 0.6060, 0.7892, 0.5303, 0.6603, 0.3744, 0.6734,
         0.3064, 0.1652, 0.0212, 0.3843, 0.2590, 0.9654, 0.7557, 0.2588, 0.7276,
         0.1756, 0.3829, 0.4149, 0.7760, 0.6986, 0.2468, 0.5665, 0.6557, 0.0645,
         0.4681, 0.0423, 0.2135, 0.0543, 0.2995, 0.1798, 0.3549, 0.5738, 0.0946,
         0.2589, 0.4395, 0.9262, 0.1315, 0.4664, 0.2584, 0.2085, 0.3096, 0.8025,
         0.3998, 0.0753, 0.9844, 0.7015, 0.9899, 0.9498, 0.7232, 0.0587, 0.9335,
         0.5132, 0.3472, 0.9743, 0.9913, 0.6386, 0.0590, 0.7779, 0.7280, 0.4280,
         0.9320, 0.3243, 0.0597, 0.4344, 0.3382, 0.0736, 0.0222, 0.8612, 0.2784,
         0.5912, 0.7011, 0.2778, 0.7620, 0.5578, 0.2589, 0.5866, 0.6222, 0.6659,
         0.9496, 0.6382, 0.1834, 0.4896, 0.3137, 0.5601, 0.8297, 0.9900, 0.2179,
         0.9117, 0.9248, 0.2884, 0.9596, 0.9655, 0.1707, 0.3473, 0.5623, 0.6191,
         0.1265, 0.3969, 0.1806, 0.3880, 0.9312, 0.1308, 0.2313, 0.2578, 0.0093,
         0.1379, 0.5014, 0.0399, 0.3792, 0.2923, 0.2671, 0.6406, 0.0752, 0.4107,
         0.2488, 0.6484, 0.5210, 0.7932, 0.4085, 0.2390, 0.1040, 0.9673, 0.7471,
         0.8244, 0.7411, 0.4339, 0.7261, 0.9170, 0.6806, 0.6723, 0.8694, 0.4839,
         0.5755, 0.2266, 0.1745, 0.0083, 0.3090, 0.3590, 0.4673, 0.2123, 0.8971,
         0.1426, 0.2401, 0.2654, 0.4298, 0.2388, 0.3422, 0.5010, 0.7236, 0.8776,
         0.7643, 0.9383, 0.3396, 0.7366, 0.8066, 0.3777, 0.3347, 0.7063, 0.1349,
         0.1734, 0.3411, 0.3812, 0.4829, 0.8486, 0.9313, 0.8790, 0.2755, 0.5108,
         0.8436, 0.4680, 0.1262, 0.2006, 0.5303, 0.8014, 0.8329, 0.7732, 0.0017,
         0.3076, 0.5784, 0.5910, 0.0805, 0.6057, 0.5421, 0.2348, 0.9507, 0.2251,
         0.6640, 0.8428, 0.4684, 0.0639, 0.1117, 0.1801, 0.7469, 0.0082, 0.6562,
         0.9247, 0.2559, 0.8864, 0.0137, 0.1742, 0.9007, 0.3669, 0.3878, 0.4107,
         0.2388, 0.9746, 0.4923, 0.1315, 0.7884, 0.8798, 0.8272, 0.4310, 0.3291,
         0.3565, 0.5094, 0.7192, 0.8173, 0.1325, 0.1947, 0.7417, 0.3487, 0.9572,
         0.3045, 0.8350, 0.8884, 0.7506, 0.4138, 0.3331, 0.8888, 0.7695, 0.9872,
         0.3447, 0.8733, 0.7261, 0.8394, 0.4349, 0.0778, 0.5082, 0.4153, 0.3323,
         0.1802, 0.2409, 0.3967, 0.3200, 0.3619, 0.7253, 0.0801, 0.2323, 0.0289,
         0.0070, 0.8834, 0.1027, 0.6119, 0.5796, 0.9587, 0.5623, 0.8786, 0.3611,
         0.1234, 0.5812, 0.8199, 0.7232, 0.1578, 0.4129, 0.5590, 0.4248, 0.9286,
         0.9441, 0.5905, 0.3248, 0.2744, 0.6005, 0.9417, 0.9250, 0.0909, 0.3526,
         0.3731, 0.9887, 0.1639, 0.6861, 0.3303, 0.7881, 0.0270, 0.4679, 0.2637,
         0.4082, 0.0782, 0.3573, 0.3274, 0.8159, 0.7830, 0.8731, 0.3585, 0.7350,
         0.6140, 0.2597, 0.0999, 0.6416, 0.8774, 0.7000, 0.4014, 0.8187, 0.4438,
         0.7127, 0.5544, 0.2112, 0.4692, 0.4243, 0.4584, 0.0105, 0.6194, 0.7662,
         0.0964, 0.9470, 0.6057, 0.2572, 0.1001, 0.7542, 0.4773, 0.1919, 0.5419,
         0.0604, 0.4548, 0.7059, 0.4638, 0.6935, 0.8784, 0.0262, 0.2692, 0.4214,
         0.3951, 0.3223, 0.9812, 0.2281, 0.3567, 0.9677, 0.1966, 0.2867, 0.2891,
         0.3825, 0.6332, 0.7463, 0.9772, 0.1388, 0.4372, 0.3108, 0.3239, 0.1241,
         0.4230, 0.8583, 0.7990, 0.8401, 0.6289, 0.4660, 0.5295, 0.8803, 0.4655,
         0.0857, 0.8819, 0.6755, 0.3251, 0.3125, 0.7529, 0.2053, 0.4891, 0.4221,
         0.9405, 0.8241, 0.9033, 0.4855, 0.1617, 0.0684, 0.8412, 0.4314, 0.1857,
         0.3130, 0.2100, 0.2494, 0.9700, 0.3725, 0.1459, 0.5440, 0.0298, 0.3082,
         0.1730, 0.6502, 0.9396, 0.4669, 0.4359, 0.4513, 0.6093, 0.0014, 0.6058,
         0.4671, 0.1216, 0.3022, 0.4672, 0.1074, 0.9929, 0.4654, 0.3657, 0.8425,
         0.7856, 0.7053, 0.9161, 0.0765, 0.5077, 0.8674, 0.0687, 0.9933, 0.4747,
         0.9809, 0.5248, 0.6166, 0.2466, 0.5595, 0.9687, 0.6005, 0.7972, 0.1993,
         0.2420, 0.5657, 0.1366, 0.2014, 0.0650, 0.4630, 0.4162, 0.2885, 0.7982,
         0.6417, 0.0435, 0.4099, 0.6760, 0.7741, 0.6790, 0.1474, 0.8638, 0.1553,
         0.6808, 0.7829, 0.7303, 0.7370, 0.0375, 0.3637, 0.3129, 0.7414, 0.4935,
         0.6622, 0.6559, 0.9755, 0.4909, 0.4722, 0.1553, 0.5631, 0.1376, 0.6355,
         0.0722, 0.4915, 0.1603, 0.0635, 0.1225, 0.3835, 0.8068, 0.4160, 0.7414,
         0.2426, 0.3789, 0.1298, 0.0124, 0.6435, 0.9346, 0.9305, 0.5456, 0.5664,
         0.4783, 0.5828, 0.1807, 0.1057, 0.4581, 0.4254, 0.8958, 0.1756, 0.3401,
         0.3231, 0.4577, 0.1444, 0.9524, 0.8483, 0.0517, 0.0806, 0.3697, 0.9269,
         0.7359, 0.5825, 0.7062, 0.9477, 0.4909, 0.7487, 0.7830, 0.7824, 0.1033,
         0.4811, 0.4926, 0.6460, 0.3569, 0.4175, 0.3432, 0.5006, 0.7408, 0.6857,
         0.9259, 0.1511, 0.2266, 0.6412, 0.4449, 0.5068, 0.0856, 0.1457, 0.0479,
         0.8498, 0.1819, 0.2383, 0.1407, 0.1276, 0.7474, 0.1398, 0.2592, 0.3423,
         0.7567, 0.3212, 0.3046, 0.7609, 0.7626, 0.0314, 0.5468, 0.0796, 0.5248,
         0.8706, 0.5138, 0.5384, 0.2017, 0.2314, 0.3029, 0.4127, 0.3549, 0.8303,
         0.6617, 0.7292, 0.8955, 0.8255, 0.9936, 0.8917, 0.7162, 0.5001, 0.9200,
         0.8439, 0.7215, 0.9745, 0.2435, 0.4971, 0.9187, 0.6520, 0.9065, 0.3664,
         0.5331, 0.5517, 0.5895, 0.6511, 0.7266, 0.4922, 0.9875, 0.8411, 0.7181,
         0.9864, 0.6896, 0.4699]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/5 iter: 0/15 loss: 0.9722 Acc: 43.7500% F1: 0.305 Time: 0.99s (0.00s)
Fold 6 train - epoch: 0/5 iter: 1/15 loss: 1.0756 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/5 iter: 2/15 loss: 1.0597 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 3/15 loss: 0.8523 Acc: 59.3750% F1: 0.298 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 4/15 loss: 1.0255 Acc: 53.1250% F1: 0.352 Time: 0.94s (0.02s)
Fold 6 train - epoch: 0/5 iter: 5/15 loss: 0.9923 Acc: 37.5000% F1: 0.250 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 6/15 loss: 0.8725 Acc: 50.0000% F1: 0.311 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 7/15 loss: 0.8266 Acc: 53.1250% F1: 0.317 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 8/15 loss: 1.1711 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 9/15 loss: 1.0601 Acc: 43.7500% F1: 0.207 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 10/15 loss: 1.0805 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 11/15 loss: 0.8935 Acc: 59.3750% F1: 0.301 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 12/15 loss: 0.9841 Acc: 56.2500% F1: 0.245 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 13/15 loss: 0.9289 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 6 train - epoch: 0/5 iter: 14/15 loss: 0.6752 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 50.4444% F1: 0.2798 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5868 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5350 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 6 train - epoch: 1/5 iter: 0/15 loss: 0.8175 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.00s)
Fold 6 train - epoch: 1/5 iter: 1/15 loss: 0.8398 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/5 iter: 2/15 loss: 0.9359 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 3/15 loss: 0.8134 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 4/15 loss: 0.9971 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 5/15 loss: 0.8874 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 6/15 loss: 0.7723 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 7/15 loss: 0.8584 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 8/15 loss: 1.0267 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 9/15 loss: 0.9963 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 10/15 loss: 0.9613 Acc: 46.8750% F1: 0.280 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 11/15 loss: 0.9084 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.02s)
Fold 6 train - epoch: 1/5 iter: 12/15 loss: 0.9455 Acc: 50.0000% F1: 0.317 Time: 0.96s (0.02s)
Fold 6 train - epoch: 1/5 iter: 13/15 loss: 0.9149 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 6 train - epoch: 1/5 iter: 14/15 loss: 0.5950 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 53.7778% F1: 0.2586 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7276 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3191 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3281 *
*********************************************************
Performing epoch 2 of 5
Fold 6 train - epoch: 2/5 iter: 0/15 loss: 0.7771 Acc: 71.8750% F1: 0.584 Time: 0.95s (0.00s)
Fold 6 train - epoch: 2/5 iter: 1/15 loss: 0.8382 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 2/15 loss: 0.8615 Acc: 50.0000% F1: 0.262 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 3/15 loss: 0.7613 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 4/15 loss: 0.8982 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/5 iter: 5/15 loss: 0.8464 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 6/15 loss: 0.7043 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 7/15 loss: 0.8650 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 8/15 loss: 0.9627 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 9/15 loss: 1.0243 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 10/15 loss: 0.9318 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 11/15 loss: 0.8339 Acc: 65.6250% F1: 0.394 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 12/15 loss: 0.9223 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 13/15 loss: 0.9148 Acc: 53.1250% F1: 0.306 Time: 0.95s (0.02s)
Fold 6 train - epoch: 2/5 iter: 14/15 loss: 0.3940 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 56.0000% F1: 0.2955 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7797 Acc: 68.7500% F1: 0.583 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3260 Acc: 27.7778% F1: 0.243 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.4472 *
*********************************************************
Performing epoch 3 of 5
Fold 6 train - epoch: 3/5 iter: 0/15 loss: 0.6894 Acc: 81.2500% F1: 0.758 Time: 0.95s (0.00s)
Fold 6 train - epoch: 3/5 iter: 1/15 loss: 0.7705 Acc: 62.5000% F1: 0.560 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 2/15 loss: 0.7944 Acc: 62.5000% F1: 0.553 Time: 0.94s (0.02s)
Fold 6 train - epoch: 3/5 iter: 3/15 loss: 0.7474 Acc: 56.2500% F1: 0.286 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 4/15 loss: 0.7584 Acc: 65.6250% F1: 0.604 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 5/15 loss: 0.7439 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 6/15 loss: 0.6328 Acc: 75.0000% F1: 0.415 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 7/15 loss: 0.7553 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 8/15 loss: 0.8592 Acc: 56.2500% F1: 0.335 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 9/15 loss: 0.9460 Acc: 53.1250% F1: 0.434 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 10/15 loss: 0.8067 Acc: 59.3750% F1: 0.398 Time: 0.96s (0.02s)
Fold 6 train - epoch: 3/5 iter: 11/15 loss: 0.7849 Acc: 62.5000% F1: 0.430 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 12/15 loss: 0.8305 Acc: 62.5000% F1: 0.581 Time: 0.96s (0.02s)
Fold 6 train - epoch: 3/5 iter: 13/15 loss: 0.8888 Acc: 50.0000% F1: 0.439 Time: 0.95s (0.02s)
Fold 6 train - epoch: 3/5 iter: 14/15 loss: 0.0964 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 62.0000% F1: 0.4954 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8028 Acc: 62.5000% F1: 0.339 Time: 0.32s (0.00s)
Fold 6 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4808 Acc: 27.7778% F1: 0.266 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.4449 *
*********************************************************
Performing epoch 4 of 5
Fold 6 train - epoch: 4/5 iter: 0/15 loss: 0.5200 Acc: 81.2500% F1: 0.779 Time: 0.96s (0.00s)
Fold 6 train - epoch: 4/5 iter: 1/15 loss: 0.7106 Acc: 71.8750% F1: 0.621 Time: 0.94s (0.03s)
Fold 6 train - epoch: 4/5 iter: 2/15 loss: 0.6935 Acc: 65.6250% F1: 0.577 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 3/15 loss: 0.6043 Acc: 68.7500% F1: 0.637 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 4/15 loss: 0.6494 Acc: 84.3750% F1: 0.725 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 5/15 loss: 0.5482 Acc: 81.2500% F1: 0.678 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 6/15 loss: 0.4756 Acc: 87.5000% F1: 0.784 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 7/15 loss: 0.5523 Acc: 84.3750% F1: 0.891 Time: 0.95s (0.03s)
Fold 6 train - epoch: 4/5 iter: 8/15 loss: 0.6057 Acc: 78.1250% F1: 0.731 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 9/15 loss: 0.7795 Acc: 56.2500% F1: 0.542 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 10/15 loss: 0.6423 Acc: 75.0000% F1: 0.690 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 11/15 loss: 0.6526 Acc: 78.1250% F1: 0.526 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 12/15 loss: 0.7284 Acc: 62.5000% F1: 0.534 Time: 0.96s (0.02s)
Fold 6 train - epoch: 4/5 iter: 13/15 loss: 0.7524 Acc: 65.6250% F1: 0.512 Time: 0.95s (0.02s)
Fold 6 train - epoch: 4/5 iter: 14/15 loss: 0.0110 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 74.4444% F1: 0.6723 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/5 iter: 0/2 loss: 1.1387 Acc: 50.0000% F1: 0.295 Time: 0.32s (0.00s)
Fold 6 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7338 Acc: 38.8889% F1: 0.324 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.4152 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[9.6409e-01, 2.3409e-01, 7.7782e-01, 7.3936e-01, 9.3843e-01, 2.9401e-02,
         1.9174e-02, 6.2126e-02, 4.1132e-01, 6.0639e-02, 4.5079e-01, 6.5133e-01,
         3.5569e-01, 5.1257e-01, 2.4400e-01, 5.5696e-01, 3.4771e-01, 2.4516e-02,
         3.7761e-01, 9.0012e-01, 1.5192e-01, 8.9427e-02, 6.8718e-01, 6.8924e-01,
         9.0303e-01, 4.6885e-01, 5.9641e-01, 5.4856e-01, 3.6705e-01, 2.1136e-01,
         2.6817e-01, 9.5953e-01, 9.3140e-01, 5.6250e-01, 1.2839e-01, 6.0365e-01,
         4.9533e-01, 4.2681e-01, 9.0901e-02, 1.8912e-01, 2.0991e-01, 1.2614e-01,
         8.3062e-01, 6.3485e-01, 7.7841e-01, 6.9937e-01, 2.1080e-01, 3.0151e-01,
         2.0595e-01, 7.8297e-01, 3.9217e-01, 6.5336e-01, 6.1843e-01, 1.9368e-01,
         4.6319e-04, 1.5126e-01, 5.2725e-02, 5.9280e-01, 2.4513e-01, 1.4587e-01,
         2.4143e-01, 3.2208e-01, 5.7897e-02, 2.4726e-01, 9.2938e-01, 5.8585e-01,
         4.4918e-01, 9.2667e-01, 5.1199e-01, 7.1520e-01, 6.8100e-02, 4.9003e-01,
         1.7541e-01, 2.2578e-01, 6.4011e-01, 8.1000e-01, 7.7788e-01, 8.3111e-01,
         7.4994e-01, 9.0507e-01, 1.4142e-01, 6.8512e-01, 1.1825e-01, 6.0660e-01,
         8.5113e-01, 1.5616e-01, 4.9722e-01, 1.0882e-01, 4.5891e-01, 4.0439e-01,
         3.3035e-01, 8.0437e-02, 6.6437e-01, 1.5770e-01, 6.1174e-01, 9.6147e-02,
         2.8285e-01, 9.5707e-01, 7.1836e-01, 3.5849e-01, 3.4981e-01, 2.0152e-02,
         1.2034e-01, 4.4366e-01, 8.2573e-02, 4.3322e-01, 7.6361e-02, 3.1142e-01,
         3.2043e-01, 1.9509e-01, 8.5307e-01, 3.3439e-01, 6.2327e-01, 5.6896e-01,
         7.8903e-01, 2.4031e-01, 4.7675e-01, 7.4032e-01, 1.1408e-01, 3.2255e-01,
         7.0711e-01, 1.2776e-01, 8.2747e-01, 4.6841e-01, 3.9080e-01, 6.9897e-01,
         9.3252e-01, 4.2678e-01, 4.9567e-02, 3.8639e-01, 7.7066e-01, 7.9482e-01,
         5.6405e-01, 6.7199e-01, 8.8936e-01, 8.5206e-01, 2.7933e-01, 1.8859e-01,
         9.7209e-01, 9.5051e-01, 1.5926e-01, 1.4551e-01, 4.4943e-01, 9.2660e-01,
         2.2608e-01, 8.1369e-01, 5.1416e-01, 2.3592e-01, 7.9736e-01, 1.3801e-02,
         7.0766e-01, 8.5966e-01, 1.2970e-01, 2.8683e-02, 7.6039e-01, 8.4987e-01,
         3.9185e-01, 6.6480e-01, 1.0517e-01, 9.6389e-02, 4.6750e-01, 6.6895e-01,
         1.4130e-01, 6.5283e-02, 5.3379e-01, 2.1282e-01, 3.7772e-01, 2.2222e-01,
         6.3289e-01, 4.1165e-01, 7.1289e-01, 7.8432e-01, 6.8021e-01, 3.7549e-01,
         2.6168e-01, 7.9167e-01, 6.6702e-01, 4.4406e-01, 9.7377e-01, 8.5519e-01,
         5.9228e-02, 3.1402e-01, 3.6602e-01, 6.3020e-01, 6.1940e-01, 4.6158e-01,
         7.4854e-01, 1.1966e-01, 9.9162e-01, 6.2327e-01, 9.4713e-01, 8.8078e-03,
         2.0480e-01, 9.7545e-01, 7.4209e-01, 5.6938e-01, 1.4704e-01, 9.8013e-01,
         3.4617e-01, 1.0529e-01, 8.2267e-01, 6.2217e-01, 1.5813e-02, 3.5846e-02,
         1.4941e-01, 7.1836e-01, 8.1540e-01, 1.3897e-01, 4.2728e-02, 5.7461e-01,
         9.9441e-02, 8.5133e-01, 8.7535e-01, 6.3353e-01, 2.6304e-02, 1.3548e-01,
         5.4117e-01, 5.2577e-01, 1.4278e-01, 3.3851e-01, 9.9189e-01, 8.7320e-01,
         4.5484e-01, 9.4516e-01, 3.0484e-01, 1.6891e-01, 6.4696e-01, 6.8551e-02,
         6.3379e-01, 7.1550e-01, 4.1446e-01, 7.3671e-01, 7.8368e-02, 4.1831e-01,
         6.3646e-01, 4.0652e-01, 8.0442e-01, 3.9171e-01, 8.7693e-01, 1.5337e-01,
         2.0189e-01, 6.0705e-01, 9.8944e-01, 2.1519e-02, 5.8390e-01, 3.1836e-01,
         4.6478e-01, 1.5268e-01, 7.1968e-01, 2.6311e-01, 5.2811e-01, 5.1577e-01,
         6.6476e-01, 1.8243e-01, 1.5889e-01, 8.2419e-01, 1.1282e-01, 6.6010e-01,
         1.9043e-01, 1.4582e-01, 3.3175e-01, 7.4982e-01, 1.1478e-01, 1.3624e-01,
         9.9758e-01, 9.1442e-02, 9.9539e-01, 9.3112e-01, 9.7207e-01, 9.8865e-01,
         9.4021e-01, 5.3387e-01, 9.0765e-01, 3.8040e-02, 3.0089e-01, 8.4091e-01,
         9.9925e-01, 8.3145e-01, 8.3656e-01, 2.4295e-01, 1.9127e-01, 5.0475e-01,
         2.7132e-01, 6.1951e-01, 1.1420e-01, 8.9072e-02, 4.5833e-01, 4.5300e-01,
         6.6677e-01, 7.2560e-01, 5.1243e-02, 8.2782e-02, 6.5211e-01, 8.9943e-02,
         8.8990e-01, 4.4502e-01, 8.1969e-01, 3.9772e-01, 2.4242e-02, 7.8810e-01,
         6.0991e-01, 6.8567e-01, 6.0816e-01, 4.3153e-01, 7.0750e-01, 9.4866e-01,
         6.1118e-01, 8.7516e-01, 3.7517e-01, 7.5363e-01, 5.7691e-01, 5.1416e-01,
         1.0014e-01, 7.5497e-01, 9.8888e-01, 8.3238e-01, 6.0329e-01, 6.9286e-02,
         4.1225e-01, 1.7521e-01, 6.5081e-01, 8.5271e-01, 2.9178e-01, 1.9147e-01,
         9.1840e-03, 8.3644e-02, 3.1937e-02, 7.3898e-01, 2.4293e-03, 3.9242e-01,
         3.7896e-01, 5.9132e-01, 4.4670e-01, 5.8624e-01, 1.2079e-01, 8.5508e-01,
         5.4648e-01, 1.9790e-01, 4.3626e-01, 4.1773e-01, 3.9050e-01, 4.6906e-01,
         2.0093e-01, 3.2853e-01, 9.8947e-01, 3.0250e-01, 3.0197e-02, 6.6824e-01,
         8.2794e-01, 5.0793e-01, 6.3400e-03, 6.8029e-01, 5.8827e-01, 7.5248e-01,
         7.1918e-01, 1.6971e-01, 9.3335e-01, 7.0589e-01, 1.0977e-01, 8.3826e-01,
         3.8092e-01, 4.6667e-01, 9.7018e-01, 8.8208e-01, 8.2610e-01, 7.7563e-01,
         2.8153e-01, 9.5332e-01, 5.7645e-01, 6.4003e-01, 3.5616e-01, 6.2918e-01,
         1.1848e-01, 5.1549e-01, 2.8275e-01, 6.0935e-01, 6.1362e-01, 7.3530e-02,
         4.6159e-01, 2.4664e-01, 1.8525e-01, 8.9839e-01, 9.0162e-01, 7.0842e-01,
         3.4387e-01, 9.1609e-01, 6.5833e-01, 1.7739e-01, 1.3435e-02, 5.5452e-01,
         8.0713e-01, 3.5206e-01, 8.6654e-01, 7.9313e-01, 7.5388e-01, 8.5247e-02,
         8.6164e-01, 8.0202e-02, 5.2265e-01, 3.0295e-01, 7.5763e-01, 7.6480e-01,
         8.0926e-01, 3.5147e-01, 6.3255e-01, 1.9984e-01, 8.3185e-01, 9.8942e-01,
         2.0789e-01, 6.7251e-01, 7.5401e-01, 4.9334e-01, 3.1933e-01, 7.4052e-01,
         1.6239e-01, 5.8366e-01, 4.0312e-01, 2.4456e-02, 3.4626e-01, 4.1003e-01,
         5.3238e-02, 4.8235e-02, 2.2214e-01, 5.6701e-01, 4.5558e-01, 9.3669e-01,
         5.8240e-01, 8.1156e-01, 8.2490e-01, 8.4824e-01, 7.3503e-01, 5.6580e-01,
         9.5377e-01, 9.4240e-01, 1.7484e-01, 7.2340e-01, 1.0734e-02, 9.2018e-01,
         7.6992e-01, 5.5831e-01, 1.9738e-01, 9.2149e-01, 9.3729e-01, 7.8511e-01,
         8.4989e-01, 9.8375e-01, 5.0168e-01, 6.1266e-01, 5.4558e-01, 1.1808e-01,
         3.9593e-01, 2.6387e-01, 5.2108e-01, 5.7701e-01, 8.9211e-01, 6.8235e-01,
         1.3990e-01, 8.5560e-01, 9.5911e-01, 9.2955e-01, 6.9785e-01, 3.4783e-01,
         9.9303e-01, 2.3106e-01, 2.1196e-01, 8.7110e-01, 2.0063e-01, 8.1893e-01,
         7.9051e-01, 5.3735e-01, 1.5918e-01, 1.8671e-01, 8.0504e-01, 5.8102e-01,
         3.7533e-01, 1.1502e-01, 9.0730e-01, 6.8512e-01, 4.9602e-01, 9.8460e-01,
         3.0011e-01, 6.7838e-01, 5.2345e-01, 4.3706e-01, 2.9217e-01, 4.3287e-01,
         9.3160e-01, 3.5990e-01, 1.4579e-01, 5.6126e-01, 9.3443e-01, 4.3447e-01,
         1.4514e-01, 7.4259e-01, 7.0336e-01, 8.7666e-02, 5.2161e-01, 3.6799e-01,
         7.6069e-02, 7.8189e-01, 7.3775e-01, 4.6370e-01, 9.8794e-01, 4.0010e-01,
         8.6031e-01, 8.8989e-01, 1.3071e-01, 8.5855e-01, 8.2987e-02, 5.0162e-01,
         1.8837e-01, 8.8861e-01, 4.1895e-01, 7.1790e-01, 9.9750e-01, 1.6322e-01,
         4.0614e-01, 9.4014e-01, 3.0779e-01, 3.0383e-01, 6.4235e-01, 7.1328e-01,
         1.7543e-01, 7.0473e-01, 2.0954e-01, 4.7519e-01, 1.6905e-01, 1.1189e-01,
         6.8488e-01, 1.2668e-01, 9.4028e-01, 5.5682e-01, 7.7523e-01, 7.0742e-01,
         9.9364e-01, 8.9922e-01, 9.0790e-01, 5.1029e-01, 7.3021e-01, 4.7979e-01,
         6.4040e-01, 9.9163e-01, 9.9402e-01, 2.4835e-01, 3.2914e-01, 5.2524e-01,
         7.7148e-01, 5.7432e-01, 3.2295e-01, 1.2449e-02, 4.7681e-02, 9.2040e-01,
         2.0570e-01, 2.6871e-01, 1.7327e-01, 5.7088e-01, 3.2402e-01, 9.7828e-01,
         2.4841e-02, 1.1846e-01, 4.9122e-01, 2.3958e-01, 4.8644e-01, 3.2977e-02,
         4.8301e-01, 6.5114e-01, 5.4591e-01, 5.8563e-01, 8.3626e-01, 5.8472e-02,
         5.9630e-01, 1.4746e-01, 9.7115e-01, 7.6382e-01, 2.4545e-01, 7.3884e-01,
         4.5024e-01, 9.6163e-01, 1.3969e-01, 6.8295e-01, 2.4581e-01, 5.8129e-01,
         5.2922e-01, 8.9921e-01, 7.2542e-01, 9.8561e-01, 9.9805e-01, 1.8482e-02,
         7.7252e-01, 6.8012e-01, 4.6508e-01, 5.2918e-01, 5.8821e-01, 9.3508e-01,
         1.8049e-02, 4.3614e-01, 1.5860e-01, 5.0855e-01, 3.7176e-02, 3.5803e-01,
         9.9104e-01, 9.4532e-01, 3.7443e-01, 4.9624e-02, 9.3070e-01, 8.7930e-01,
         6.1581e-01, 2.4900e-01, 9.6812e-01, 2.3799e-01, 1.4964e-02, 7.3571e-01,
         5.7378e-01, 9.7557e-01, 4.3354e-01, 7.4501e-01, 8.9956e-01, 4.8850e-01,
         6.7369e-02, 8.8053e-01, 9.2482e-02, 6.8937e-02, 7.8087e-01, 4.7644e-01,
         7.3437e-01, 2.8658e-01, 2.3002e-01, 8.3223e-01, 9.0125e-01, 3.8289e-01,
         8.3846e-01, 2.3364e-01, 7.5501e-01, 9.6116e-01, 7.8903e-01, 6.9872e-01,
         8.4923e-01, 2.0358e-01, 6.5872e-01, 9.0759e-01, 3.2118e-01, 6.6336e-01,
         4.5394e-01, 5.2611e-02, 9.9933e-01, 8.3056e-01, 4.5707e-01, 6.0384e-01,
         5.1545e-01, 8.6212e-01, 4.5155e-01, 8.6796e-01, 1.9023e-01, 6.9922e-01,
         1.6062e-01, 6.8602e-01, 2.0359e-01, 2.3397e-01, 7.9660e-01, 9.8688e-02,
         4.4046e-01, 3.3745e-01, 6.3207e-01, 2.0770e-01, 5.7511e-02, 9.9773e-01,
         4.0996e-01, 6.6925e-01, 1.0311e-01, 6.0090e-01, 6.7115e-01, 2.9812e-01,
         4.2864e-01, 5.3017e-01, 1.9995e-01, 5.0023e-01, 6.9706e-01, 2.3134e-01,
         7.8105e-01, 5.5562e-01, 6.3039e-01, 4.1481e-01, 3.4704e-01, 1.4575e-02,
         6.0713e-01, 7.6550e-01, 5.7721e-01, 6.7677e-01, 1.9461e-01, 4.6824e-01,
         2.2699e-01, 5.2775e-01, 4.8992e-02, 7.8058e-01, 5.8837e-01, 2.8264e-01,
         5.1979e-01, 3.7692e-01, 4.2380e-01, 9.0748e-01, 9.7912e-01, 6.2675e-01,
         4.6415e-01, 8.8652e-01, 8.8269e-01, 6.4314e-02, 4.3048e-01, 9.6488e-01,
         3.4944e-02, 1.4097e-01, 6.9352e-01, 6.2910e-02, 2.5992e-01, 2.4462e-01,
         8.9051e-01, 3.3840e-02, 8.0738e-01, 5.6181e-01, 2.0407e-01, 9.6278e-01,
         1.7113e-01, 7.8441e-01, 1.7910e-01, 4.9811e-01, 9.1643e-01, 4.4129e-01,
         2.2362e-01, 5.8643e-01, 9.0365e-01, 6.5371e-01, 7.3797e-01, 4.2078e-01,
         9.3881e-01, 2.3067e-01, 8.1206e-01, 3.5534e-01, 6.4452e-01, 3.6872e-01,
         7.5145e-01, 3.8126e-01, 6.9497e-01, 6.6292e-02, 5.6063e-01, 3.3975e-01,
         7.2978e-02, 4.8289e-01, 1.1758e-01, 9.4056e-02, 8.1642e-01, 5.3068e-01,
         3.4632e-01, 2.8413e-01, 5.2439e-01, 2.8350e-01, 4.3837e-01, 7.7704e-01,
         3.5504e-02, 1.0790e-01, 6.4726e-01, 1.5206e-01, 3.0377e-01, 5.5442e-01,
         1.8461e-01, 7.4661e-01, 1.6619e-02, 7.0618e-01, 2.1358e-01, 7.8405e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/5 iter: 0/15 loss: 0.9330 Acc: 50.0000% F1: 0.349 Time: 0.98s (0.00s)
Fold 7 train - epoch: 0/5 iter: 1/15 loss: 1.0135 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 7 train - epoch: 0/5 iter: 2/15 loss: 0.9972 Acc: 50.0000% F1: 0.262 Time: 0.95s (0.03s)
Fold 7 train - epoch: 0/5 iter: 3/15 loss: 0.8856 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.03s)
Fold 7 train - epoch: 0/5 iter: 4/15 loss: 1.0047 Acc: 46.8750% F1: 0.314 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 5/15 loss: 1.0069 Acc: 34.3750% F1: 0.240 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 6/15 loss: 0.8504 Acc: 65.6250% F1: 0.404 Time: 0.94s (0.02s)
Fold 7 train - epoch: 0/5 iter: 7/15 loss: 0.8330 Acc: 59.3750% F1: 0.366 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 8/15 loss: 1.1215 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 7 train - epoch: 0/5 iter: 9/15 loss: 1.0947 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.03s)
Fold 7 train - epoch: 0/5 iter: 10/15 loss: 1.0642 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 7 train - epoch: 0/5 iter: 11/15 loss: 0.9143 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 12/15 loss: 1.0082 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 7 train - epoch: 0/5 iter: 13/15 loss: 0.9475 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 7 train - epoch: 0/5 iter: 14/15 loss: 0.7294 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 52.6667% F1: 0.2960 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6215 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 0/5 iter: 1/2 loss: 1.4805 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 7 train - epoch: 1/5 iter: 0/15 loss: 0.8174 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.00s)
Fold 7 train - epoch: 1/5 iter: 1/15 loss: 0.8464 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 2/15 loss: 0.9089 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 3/15 loss: 0.7777 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 4/15 loss: 0.9920 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 5/15 loss: 0.9047 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 6/15 loss: 0.7612 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 7/15 loss: 0.8443 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 8/15 loss: 1.0172 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 9/15 loss: 0.9985 Acc: 46.8750% F1: 0.217 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 10/15 loss: 1.0267 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 11/15 loss: 0.8890 Acc: 65.6250% F1: 0.366 Time: 0.95s (0.02s)
Fold 7 train - epoch: 1/5 iter: 12/15 loss: 0.9504 Acc: 50.0000% F1: 0.295 Time: 0.96s (0.02s)
Fold 7 train - epoch: 1/5 iter: 13/15 loss: 0.8996 Acc: 59.3750% F1: 0.362 Time: 0.95s (0.03s)
Fold 7 train - epoch: 1/5 iter: 14/15 loss: 0.6026 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 55.1111% F1: 0.2668 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7036 Acc: 81.2500% F1: 0.448 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3605 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.2662 *
*********************************************************
Performing epoch 2 of 5
Fold 7 train - epoch: 2/5 iter: 0/15 loss: 0.7617 Acc: 65.6250% F1: 0.538 Time: 0.95s (0.00s)
Fold 7 train - epoch: 2/5 iter: 1/15 loss: 0.8526 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.03s)
Fold 7 train - epoch: 2/5 iter: 2/15 loss: 0.8489 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 3/15 loss: 0.7804 Acc: 59.3750% F1: 0.253 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 4/15 loss: 0.9267 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 5/15 loss: 0.8717 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 6/15 loss: 0.6981 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 7/15 loss: 0.8468 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 7 train - epoch: 2/5 iter: 8/15 loss: 0.9898 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 9/15 loss: 1.0360 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 10/15 loss: 0.9541 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 11/15 loss: 0.8667 Acc: 59.3750% F1: 0.301 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 12/15 loss: 0.9343 Acc: 59.3750% F1: 0.342 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 13/15 loss: 0.8944 Acc: 56.2500% F1: 0.345 Time: 0.95s (0.02s)
Fold 7 train - epoch: 2/5 iter: 14/15 loss: 0.4017 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 56.2222% F1: 0.2992 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7342 Acc: 78.1250% F1: 0.547 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3761 Acc: 27.7778% F1: 0.185 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 60.0000% F1: 0.3870 *
*********************************************************
Performing epoch 3 of 5
Fold 7 train - epoch: 3/5 iter: 0/15 loss: 0.7216 Acc: 71.8750% F1: 0.663 Time: 0.94s (0.00s)
Fold 7 train - epoch: 3/5 iter: 1/15 loss: 0.7603 Acc: 65.6250% F1: 0.500 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 2/15 loss: 0.7869 Acc: 53.1250% F1: 0.433 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 3/15 loss: 0.7040 Acc: 59.3750% F1: 0.298 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 4/15 loss: 0.8412 Acc: 59.3750% F1: 0.370 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 5/15 loss: 0.7565 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 6/15 loss: 0.6947 Acc: 65.6250% F1: 0.264 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 7/15 loss: 0.7290 Acc: 62.5000% F1: 0.705 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 8/15 loss: 0.8215 Acc: 71.8750% F1: 0.645 Time: 0.95s (0.02s)
Fold 7 train - epoch: 3/5 iter: 9/15 loss: 0.9444 Acc: 46.8750% F1: 0.345 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 10/15 loss: 0.8394 Acc: 50.0000% F1: 0.333 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 11/15 loss: 0.7959 Acc: 56.2500% F1: 0.361 Time: 0.96s (0.03s)
Fold 7 train - epoch: 3/5 iter: 12/15 loss: 0.8280 Acc: 56.2500% F1: 0.490 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 13/15 loss: 0.8276 Acc: 59.3750% F1: 0.488 Time: 0.95s (0.03s)
Fold 7 train - epoch: 3/5 iter: 14/15 loss: 0.1253 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 60.0000% F1: 0.4704 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7326 Acc: 65.6250% F1: 0.396 Time: 0.32s (0.00s)
Fold 7 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6572 Acc: 33.3333% F1: 0.269 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.4205 *
*********************************************************
Performing epoch 4 of 5
Fold 7 train - epoch: 4/5 iter: 0/15 loss: 0.5790 Acc: 71.8750% F1: 0.634 Time: 0.96s (0.00s)
Fold 7 train - epoch: 4/5 iter: 1/15 loss: 0.6068 Acc: 78.1250% F1: 0.736 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 2/15 loss: 0.7608 Acc: 56.2500% F1: 0.472 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 3/15 loss: 0.5572 Acc: 71.8750% F1: 0.631 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 4/15 loss: 0.7412 Acc: 75.0000% F1: 0.715 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 5/15 loss: 0.6527 Acc: 68.7500% F1: 0.548 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 6/15 loss: 0.5789 Acc: 75.0000% F1: 0.649 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 7/15 loss: 0.5725 Acc: 78.1250% F1: 0.849 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 8/15 loss: 0.6465 Acc: 75.0000% F1: 0.737 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 9/15 loss: 0.7502 Acc: 56.2500% F1: 0.582 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 10/15 loss: 0.6264 Acc: 65.6250% F1: 0.470 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 11/15 loss: 0.6295 Acc: 75.0000% F1: 0.501 Time: 0.95s (0.03s)
Fold 7 train - epoch: 4/5 iter: 12/15 loss: 0.6392 Acc: 65.6250% F1: 0.601 Time: 0.96s (0.03s)
Fold 7 train - epoch: 4/5 iter: 13/15 loss: 0.6861 Acc: 62.5000% F1: 0.444 Time: 0.95s (0.02s)
Fold 7 train - epoch: 4/5 iter: 14/15 loss: 0.0274 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 69.7778% F1: 0.6370 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8736 Acc: 59.3750% F1: 0.296 Time: 0.32s (0.00s)
Fold 7 train-dev - epoch: 4/5 iter: 1/2 loss: 2.0317 Acc: 33.3333% F1: 0.278 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 50.0000% F1: 0.4242 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[0.8374, 0.0111, 0.1950, 0.8811, 0.8470, 0.6411, 0.9295, 0.7077, 0.3241,
         0.9696, 0.2145, 0.7007, 0.2060, 0.0673, 0.9875, 0.6274, 0.7211, 0.3465,
         0.6519, 0.3368, 0.4575, 0.0226, 0.1339, 0.3993, 0.3795, 0.5897, 0.1812,
         0.9251, 0.7686, 0.0936, 0.8190, 0.9543, 0.8487, 0.2598, 0.0462, 0.0435,
         0.8685, 0.6998, 0.5542, 0.7677, 0.4782, 0.1556, 0.4332, 0.4337, 0.5615,
         0.0883, 0.5632, 0.6041, 0.6594, 0.2869, 0.1199, 0.0789, 0.1022, 0.4583,
         0.1901, 0.5498, 0.8926, 0.6921, 0.6075, 0.0411, 0.5199, 0.3763, 0.6826,
         0.8976, 0.6255, 0.1884, 0.9160, 0.4761, 0.2305, 0.6836, 0.9138, 0.6980,
         0.4032, 0.2634, 0.8015, 0.6974, 0.2190, 0.5967, 0.7573, 0.5889, 0.8838,
         0.5871, 0.2313, 0.6999, 0.8150, 0.4823, 0.4090, 0.2502, 0.8920, 0.0902,
         0.5632, 0.7273, 0.7876, 0.8580, 0.1811, 0.6807, 0.2698, 0.1448, 0.7311,
         0.4994, 0.9766, 0.7990, 0.7253, 0.6511, 0.2942, 0.6115, 0.6291, 0.4838,
         0.0038, 0.7713, 0.3948, 0.9527, 0.8861, 0.4282, 0.7195, 0.2475, 0.9902,
         0.3054, 0.7479, 0.3281, 0.9655, 0.8189, 0.8571, 0.5280, 0.5028, 0.9544,
         0.1478, 0.5153, 0.5498, 0.3816, 0.5036, 0.5956, 0.2893, 0.4583, 0.1737,
         0.3983, 0.5479, 0.0914, 0.4848, 0.1062, 0.0083, 0.5241, 0.3925, 0.7602,
         0.7956, 0.5428, 0.3483, 0.9035, 0.8623, 0.1462, 0.9210, 0.8947, 0.3350,
         0.8785, 0.3885, 0.3696, 0.8978, 0.8626, 0.6921, 0.7175, 0.2821, 0.9314,
         0.9663, 0.1043, 0.4950, 0.0405, 0.4524, 0.4125, 0.1638, 0.1424, 0.6685,
         0.6280, 0.7125, 0.8116, 0.7518, 0.6016, 0.1307, 0.8100, 0.2568, 0.9304,
         0.4455, 0.2662, 0.6962, 0.3625, 0.5074, 0.6879, 0.3423, 0.2529, 0.5606,
         0.8103, 0.3749, 0.9715, 0.8982, 0.6531, 0.1669, 0.2453, 0.2035, 0.1082,
         0.3139, 0.2447, 0.3840, 0.9127, 0.9340, 0.8242, 0.7115, 0.6839, 0.2984,
         0.4346, 0.9294, 0.0478, 0.6501, 0.1720, 0.9970, 0.5950, 0.9660, 0.6888,
         0.0833, 0.7884, 0.4299, 0.6582, 0.1182, 0.0671, 0.7491, 0.8567, 0.4865,
         0.4528, 0.0764, 0.2521, 0.3819, 0.6187, 0.9652, 0.0220, 0.6115, 0.7029,
         0.8704, 0.4558, 0.5573, 0.9716, 0.5904, 0.8104, 0.9527, 0.0680, 0.1095,
         0.6525, 0.5999, 0.4827, 0.5707, 0.5015, 0.9261, 0.2938, 0.3912, 0.7576,
         0.7652, 0.2723, 0.6127, 0.8639, 0.2783, 0.4188, 0.7600, 0.4324, 0.5550,
         0.7858, 0.7808, 0.0088, 0.3467, 0.9872, 0.1408, 0.1784, 0.4288, 0.3385,
         0.8772, 0.7018, 0.7842, 0.3169, 0.7455, 0.3400, 0.0933, 0.1850, 0.6353,
         0.0510, 0.2924, 0.9002, 0.9315, 0.8961, 0.7957, 0.3454, 0.8706, 0.5576,
         0.0460, 0.7801, 0.1661, 0.2241, 0.2336, 0.3537, 0.3433, 0.7904, 0.6060,
         0.9625, 0.8734, 0.7497, 0.0840, 0.6677, 0.3877, 0.1031, 0.3069, 0.7872,
         0.7594, 0.4994, 0.3890, 0.3804, 0.7774, 0.2068, 0.2648, 0.3455, 0.5677,
         0.8434, 0.5019, 0.2014, 0.3626, 0.8097, 0.1358, 0.4607, 0.9263, 0.5369,
         0.8907, 0.8072, 0.7295, 0.4393, 0.6687, 0.9103, 0.5314, 0.3905, 0.7929,
         0.3345, 0.6822, 0.6004, 0.4291, 0.8058, 0.2026, 0.8686, 0.9559, 0.8573,
         0.8658, 0.1465, 0.7616, 0.1997, 0.5098, 0.1619, 0.8140, 0.4650, 0.8705,
         0.1440, 0.0719, 0.6394, 0.4096, 0.1806, 0.8966, 0.7655, 0.8324, 0.6542,
         0.7847, 0.4410, 0.4282, 0.5057, 0.9950, 0.6072, 0.8074, 0.9475, 0.2858,
         0.2950, 0.9281, 0.8006, 0.9254, 0.5731, 0.1099, 0.2651, 0.0449, 0.1692,
         0.6440, 0.2553, 0.8644, 0.9024, 0.5960, 0.1748, 0.7946, 0.4071, 0.6618,
         0.1512, 0.4591, 0.9643, 0.0209, 0.7240, 0.4556, 0.9836, 0.1210, 0.1300,
         0.4351, 0.2488, 0.8056, 0.3780, 0.8825, 0.7557, 0.3057, 0.3458, 0.6464,
         0.4214, 0.0271, 0.8417, 0.0240, 0.8784, 0.1582, 0.5693, 0.2290, 0.9531,
         0.1163, 0.9048, 0.2425, 0.6182, 0.5063, 0.5578, 0.5050, 0.6337, 0.6427,
         0.7502, 0.8435, 0.8552, 0.1580, 0.5866, 0.2111, 0.3616, 0.3996, 0.5810,
         0.4108, 0.1036, 0.4913, 0.7482, 0.0117, 0.4436, 0.2208, 0.0147, 0.2321,
         0.5185, 0.8018, 0.1595, 0.0274, 0.6426, 0.7787, 0.7936, 0.4911, 0.1163,
         0.8880, 0.5424, 0.8803, 0.4945, 0.9617, 0.4337, 0.3259, 0.2194, 0.0984,
         0.5334, 0.7610, 0.0036, 0.7134, 0.7134, 0.5267, 0.3738, 0.9071, 0.9735,
         0.4301, 0.2323, 0.6741, 0.3118, 0.1708, 0.0493, 0.3704, 0.1498, 0.3652,
         0.6928, 0.2636, 0.9482, 0.8961, 0.5559, 0.9846, 0.4187, 0.7871, 0.7510,
         0.4649, 0.1354, 0.2163, 0.8256, 0.4997, 0.1318, 0.0667, 0.8849, 0.0161,
         0.9599, 0.9498, 0.0965, 0.0280, 0.1981, 0.8612, 0.8485, 0.6942, 0.5614,
         0.2095, 0.4070, 0.6137, 0.7986, 0.3481, 0.0632, 0.9306, 0.1356, 0.0682,
         0.8590, 0.6361, 0.7860, 0.0818, 0.0335, 0.8716, 0.7161, 0.7405, 0.1192,
         0.2232, 0.8209, 0.8243, 0.5073, 0.3433, 0.1841, 0.2769, 0.8123, 0.9380,
         0.7630, 0.9867, 0.4553, 0.0934, 0.6367, 0.2743, 0.6545, 0.3754, 0.2642,
         0.5116, 0.6627, 0.9061, 0.0892, 0.6506, 0.1251, 0.0565, 0.5446, 0.1187,
         0.6064, 0.1804, 0.2187, 0.9068, 0.5117, 0.8862, 0.9894, 0.3394, 0.8825,
         0.6643, 0.2344, 0.3834, 0.5943, 0.8022, 0.4325, 0.0096, 0.9346, 0.9612,
         0.1577, 0.4578, 0.6785, 0.5361, 0.9748, 0.1379, 0.1940, 0.7807, 0.3955,
         0.6611, 0.3282, 0.8655, 0.5370, 0.8242, 0.8526, 0.0604, 0.7894, 0.9320,
         0.1866, 0.6838, 0.9581, 0.5031, 0.5755, 0.2746, 0.4494, 0.7457, 0.9258,
         0.6973, 0.4708, 0.5394, 0.1071, 0.7853, 0.5819, 0.9952, 0.3099, 0.3392,
         0.5130, 0.7340, 0.7371, 0.9822, 0.0190, 0.8524, 0.8780, 0.8445, 0.3790,
         0.6936, 0.6272, 0.4147, 0.1171, 0.2366, 0.2036, 0.3504, 0.7597, 0.2171,
         0.8208, 0.3594, 0.1712, 0.5753, 0.6662, 0.0717, 0.2747, 0.8668, 0.4403,
         0.7087, 0.9050, 0.4206, 0.3702, 0.8269, 0.5787, 0.2466, 0.4505, 0.4741,
         0.1995, 0.4613, 0.8880, 0.2761, 0.5875, 0.0729, 0.8133, 0.6473, 0.9430,
         0.3430, 0.4464, 0.8558, 0.6173, 0.8016, 0.8155, 0.7501, 0.1491, 0.6490,
         0.9093, 0.4981, 0.8001, 0.5984, 0.2561, 0.8179, 0.4772, 0.7970, 0.5113,
         0.8307, 0.0716, 0.2221, 0.9235, 0.3322, 0.6638, 0.0589, 0.1418, 0.8207,
         0.3730, 0.6321, 0.9767, 0.7749, 0.2780, 0.6281, 0.2948, 0.7203, 0.7011,
         0.1830, 0.5726, 0.1175, 0.2883, 0.5416, 0.1486, 0.6050, 0.6676, 0.2100,
         0.3465, 0.7164, 0.4150, 0.8271, 0.9451, 0.2843, 0.2342, 0.8273, 0.0259,
         0.3265, 0.3952, 0.3331, 0.2354, 0.8551, 0.1846, 0.8424, 0.8997, 0.8208,
         0.5642, 0.0174, 0.2966, 0.2391, 0.7711, 0.4659, 0.8570, 0.7668, 0.4805,
         0.9512, 0.1522, 0.7891, 0.8020, 0.3117, 0.3293, 0.2495, 0.9109, 0.5467,
         0.0575, 0.8789, 0.9381, 0.4334, 0.6309, 0.6030, 0.4104, 0.5402, 0.9966,
         0.1063, 0.2095, 0.4417, 0.0974, 0.1148, 0.2433, 0.3001, 0.6600, 0.9073,
         0.8081, 0.1503, 0.4194, 0.5354, 0.8324, 0.8118, 0.0204, 0.4451, 0.3547,
         0.8700, 0.1414, 0.6464, 0.0684, 0.4944, 0.1183, 0.5802, 0.8234, 0.2530,
         0.1960, 0.1097, 0.4978]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/5 iter: 0/15 loss: 0.9595 Acc: 53.1250% F1: 0.466 Time: 0.98s (0.00s)
Fold 8 train - epoch: 0/5 iter: 1/15 loss: 1.0198 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 8 train - epoch: 0/5 iter: 2/15 loss: 0.9939 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 3/15 loss: 0.9173 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 4/15 loss: 1.0133 Acc: 53.1250% F1: 0.350 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 5/15 loss: 1.0039 Acc: 50.0000% F1: 0.343 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 6/15 loss: 0.8524 Acc: 71.8750% F1: 0.421 Time: 0.94s (0.02s)
Fold 8 train - epoch: 0/5 iter: 7/15 loss: 0.9013 Acc: 46.8750% F1: 0.244 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 8/15 loss: 1.1317 Acc: 59.3750% F1: 0.374 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 9/15 loss: 1.1356 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 10/15 loss: 1.0338 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 11/15 loss: 0.9146 Acc: 62.5000% F1: 0.351 Time: 0.95s (0.03s)
Fold 8 train - epoch: 0/5 iter: 12/15 loss: 1.0102 Acc: 53.1250% F1: 0.236 Time: 0.96s (0.02s)
Fold 8 train - epoch: 0/5 iter: 13/15 loss: 0.9587 Acc: 59.3750% F1: 0.336 Time: 0.95s (0.02s)
Fold 8 train - epoch: 0/5 iter: 14/15 loss: 0.6917 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 54.4444% F1: 0.3209 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6065 Acc: 87.5000% F1: 0.632 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6411 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2739 *
*********************************************************
Performing epoch 1 of 5
Fold 8 train - epoch: 1/5 iter: 0/15 loss: 0.8086 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.00s)
Fold 8 train - epoch: 1/5 iter: 1/15 loss: 0.8836 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 2/15 loss: 0.9135 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 3/15 loss: 0.8095 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 4/15 loss: 0.9870 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 5/15 loss: 0.8579 Acc: 59.3750% F1: 0.248 Time: 0.96s (0.03s)
Fold 8 train - epoch: 1/5 iter: 6/15 loss: 0.7430 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 7/15 loss: 0.8726 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 8/15 loss: 1.0332 Acc: 56.2500% F1: 0.310 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 9/15 loss: 0.9766 Acc: 50.0000% F1: 0.265 Time: 0.96s (0.03s)
Fold 8 train - epoch: 1/5 iter: 10/15 loss: 0.9180 Acc: 46.8750% F1: 0.280 Time: 0.96s (0.03s)
Fold 8 train - epoch: 1/5 iter: 11/15 loss: 0.8903 Acc: 53.1250% F1: 0.344 Time: 0.96s (0.03s)
Fold 8 train - epoch: 1/5 iter: 12/15 loss: 0.9644 Acc: 53.1250% F1: 0.333 Time: 0.96s (0.03s)
Fold 8 train - epoch: 1/5 iter: 13/15 loss: 0.9130 Acc: 50.0000% F1: 0.262 Time: 0.95s (0.03s)
Fold 8 train - epoch: 1/5 iter: 14/15 loss: 0.5565 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 54.4444% F1: 0.2818 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/5 iter: 0/2 loss: 0.6960 Acc: 78.1250% F1: 0.616 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4866 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3024 *
*********************************************************
Performing epoch 2 of 5
Fold 8 train - epoch: 2/5 iter: 0/15 loss: 0.7422 Acc: 62.5000% F1: 0.480 Time: 0.95s (0.00s)
Fold 8 train - epoch: 2/5 iter: 1/15 loss: 0.8347 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.02s)
Fold 8 train - epoch: 2/5 iter: 2/15 loss: 0.8781 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 3/15 loss: 0.7809 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 4/15 loss: 0.9154 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 5/15 loss: 0.8411 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 6/15 loss: 0.7275 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 7/15 loss: 0.8530 Acc: 50.0000% F1: 0.257 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 8/15 loss: 0.9954 Acc: 53.1250% F1: 0.294 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 9/15 loss: 0.9328 Acc: 50.0000% F1: 0.265 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 10/15 loss: 0.8909 Acc: 56.2500% F1: 0.383 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 11/15 loss: 0.8543 Acc: 71.8750% F1: 0.453 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 12/15 loss: 0.8998 Acc: 56.2500% F1: 0.380 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 13/15 loss: 0.8983 Acc: 46.8750% F1: 0.277 Time: 0.95s (0.02s)
Fold 8 train - epoch: 2/5 iter: 14/15 loss: 0.3395 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 56.8889% F1: 0.3280 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7323 Acc: 71.8750% F1: 0.566 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 2/5 iter: 1/2 loss: 1.5067 Acc: 11.1111% F1: 0.089 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.3074 *
*********************************************************
Performing epoch 3 of 5
Fold 8 train - epoch: 3/5 iter: 0/15 loss: 0.6469 Acc: 81.2500% F1: 0.787 Time: 0.94s (0.00s)
Fold 8 train - epoch: 3/5 iter: 1/15 loss: 0.7718 Acc: 65.6250% F1: 0.577 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 2/15 loss: 0.7896 Acc: 56.2500% F1: 0.477 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 3/15 loss: 0.6989 Acc: 65.6250% F1: 0.386 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 4/15 loss: 0.7672 Acc: 56.2500% F1: 0.418 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 5/15 loss: 0.7629 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 6/15 loss: 0.5856 Acc: 78.1250% F1: 0.469 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 7/15 loss: 0.6940 Acc: 65.6250% F1: 0.434 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 8/15 loss: 0.8991 Acc: 62.5000% F1: 0.432 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 9/15 loss: 0.8618 Acc: 56.2500% F1: 0.389 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 10/15 loss: 0.7724 Acc: 59.3750% F1: 0.427 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 11/15 loss: 0.7673 Acc: 59.3750% F1: 0.391 Time: 0.95s (0.03s)
Fold 8 train - epoch: 3/5 iter: 12/15 loss: 0.8970 Acc: 62.5000% F1: 0.587 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 13/15 loss: 0.8441 Acc: 68.7500% F1: 0.619 Time: 0.95s (0.02s)
Fold 8 train - epoch: 3/5 iter: 14/15 loss: 0.0823 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 64.0000% F1: 0.5187 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/5 iter: 0/2 loss: 0.6373 Acc: 71.8750% F1: 0.279 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 3/5 iter: 1/2 loss: 1.9123 Acc: 5.5556% F1: 0.048 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.2463 *
*********************************************************
Performing epoch 4 of 5
Fold 8 train - epoch: 4/5 iter: 0/15 loss: 0.5691 Acc: 71.8750% F1: 0.699 Time: 0.94s (0.00s)
Fold 8 train - epoch: 4/5 iter: 1/15 loss: 0.6668 Acc: 68.7500% F1: 0.594 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 2/15 loss: 0.6398 Acc: 68.7500% F1: 0.690 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 3/15 loss: 0.5863 Acc: 75.0000% F1: 0.800 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 4/15 loss: 0.6841 Acc: 75.0000% F1: 0.648 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 5/15 loss: 0.6418 Acc: 68.7500% F1: 0.574 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 6/15 loss: 0.5609 Acc: 71.8750% F1: 0.438 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 7/15 loss: 0.5598 Acc: 84.3750% F1: 0.892 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 8/15 loss: 0.5973 Acc: 75.0000% F1: 0.672 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 9/15 loss: 0.7732 Acc: 65.6250% F1: 0.644 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 10/15 loss: 0.5622 Acc: 75.0000% F1: 0.704 Time: 0.95s (0.03s)
Fold 8 train - epoch: 4/5 iter: 11/15 loss: 0.6373 Acc: 75.0000% F1: 0.669 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 12/15 loss: 0.5944 Acc: 78.1250% F1: 0.741 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 13/15 loss: 0.6971 Acc: 75.0000% F1: 0.675 Time: 0.95s (0.02s)
Fold 8 train - epoch: 4/5 iter: 14/15 loss: 0.0366 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 73.5556% F1: 0.6884 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/5 iter: 0/2 loss: 0.7574 Acc: 62.5000% F1: 0.353 Time: 0.32s (0.00s)
Fold 8 train-dev - epoch: 4/5 iter: 1/2 loss: 1.9430 Acc: 11.1111% F1: 0.131 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 44.0000% F1: 0.3282 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
Replacing DTEs with random tensors...
dtes[0]: tensor([[0.2259, 0.5537, 0.9010, 0.9806, 0.3937, 0.9193, 0.2539, 0.1111, 0.8907,
         0.2863, 0.3478, 0.0996, 0.1047, 0.8255, 0.0526, 0.5701, 0.3621, 0.0069,
         0.0262, 0.4378, 0.8987, 0.4298, 0.2327, 0.8294, 0.4003, 0.5757, 0.2231,
         0.4283, 0.6248, 0.9837, 0.5113, 0.6415, 0.8253, 0.4958, 0.5323, 0.7992,
         0.8137, 0.9128, 0.2917, 0.2866, 0.1412, 0.6473, 0.3108, 0.7052, 0.8503,
         0.1922, 0.0944, 0.6588, 0.8760, 0.5383, 0.6090, 0.5495, 0.9223, 0.8959,
         0.6608, 0.6346, 0.7635, 0.2638, 0.3596, 0.5830, 0.2440, 0.0923, 0.3277,
         0.9746, 0.6390, 0.6493, 0.2157, 0.2960, 0.7024, 0.0980, 0.5786, 0.7389,
         0.6825, 0.3234, 0.5558, 0.5328, 0.9776, 0.2788, 0.7678, 0.9913, 0.2411,
         0.3700, 0.8785, 0.0051, 0.5348, 0.0173, 0.1760, 0.7921, 0.0861, 0.0395,
         0.1869, 0.1148, 0.4035, 0.1872, 0.4945, 0.2364, 0.0503, 0.8560, 0.7709,
         0.1174, 0.0213, 0.7047, 0.3403, 0.5368, 0.5542, 0.7582, 0.2361, 0.0314,
         0.1616, 0.5687, 0.2637, 0.9063, 0.1888, 0.4717, 0.6536, 0.6512, 0.1498,
         0.4411, 0.0831, 0.0135, 0.7309, 0.3486, 0.3650, 0.6707, 0.5785, 0.1661,
         0.4563, 0.0295, 0.8665, 0.3732, 0.2713, 0.5469, 0.2659, 0.3759, 0.9216,
         0.3025, 0.0645, 0.5340, 0.3616, 0.8077, 0.4011, 0.4840, 0.0222, 0.7623,
         0.9514, 0.8152, 0.2653, 0.5570, 0.3766, 0.3746, 0.0023, 0.4077, 0.9141,
         0.9111, 0.2763, 0.9584, 0.2313, 0.0432, 0.0990, 0.1790, 0.2109, 0.8397,
         0.2805, 0.2885, 0.5485, 0.4760, 0.2348, 0.6116, 0.3302, 0.3946, 0.5317,
         0.6955, 0.1482, 0.3048, 0.7524, 0.5987, 0.0766, 0.6142, 0.6529, 0.2057,
         0.5625, 0.1163, 0.0911, 0.6396, 0.3220, 0.6979, 0.1796, 0.2333, 0.6131,
         0.2922, 0.4430, 0.4936, 0.8387, 0.6939, 0.3647, 0.5574, 0.6187, 0.9723,
         0.7690, 0.6616, 0.0518, 0.5195, 0.3510, 0.5687, 0.7550, 0.6633, 0.6999,
         0.4845, 0.4789, 0.8786, 0.2912, 0.3122, 0.6064, 0.4802, 0.3318, 0.7297,
         0.4161, 0.5004, 0.3517, 0.4434, 0.6917, 0.7550, 0.1612, 0.7259, 0.8752,
         0.4826, 0.1571, 0.5977, 0.2358, 0.8721, 0.0594, 0.5938, 0.9962, 0.7323,
         0.0686, 0.9595, 0.3986, 0.4757, 0.1449, 0.6142, 0.1992, 0.3754, 0.2743,
         0.6717, 0.8396, 0.4320, 0.5924, 0.6845, 0.2324, 0.5033, 0.2580, 0.6472,
         0.7960, 0.4358, 0.9983, 0.7923, 0.7260, 0.5583, 0.2943, 0.7534, 0.0273,
         0.6768, 0.6188, 0.6446, 0.4245, 0.1113, 0.7587, 0.4750, 0.0684, 0.4349,
         0.7907, 0.7539, 0.1695, 0.7751, 0.9984, 0.6414, 0.5195, 0.5473, 0.9046,
         0.8380, 0.2679, 0.3419, 0.6089, 0.4711, 0.6868, 0.7527, 0.8097, 0.5750,
         0.2415, 0.2398, 0.6174, 0.7240, 0.6410, 0.0964, 0.5029, 0.5607, 0.2535,
         0.7429, 0.8558, 0.0690, 0.2675, 0.6397, 0.3136, 0.7061, 0.8518, 0.5989,
         0.8760, 0.6657, 0.6263, 0.4345, 0.7983, 0.8437, 0.5640, 0.8083, 0.6682,
         0.4964, 0.3161, 0.8635, 0.7580, 0.9168, 0.6015, 0.4879, 0.0638, 0.7199,
         0.0278, 0.7168, 0.1737, 0.9816, 0.6540, 0.5483, 0.2545, 0.9823, 0.1610,
         0.1565, 0.5692, 0.9879, 0.4759, 0.2449, 0.7476, 0.0746, 0.3678, 0.0484,
         0.0875, 0.4846, 0.3446, 0.7172, 0.2445, 0.3729, 0.0963, 0.4287, 0.0108,
         0.1026, 0.9751, 0.6172, 0.0291, 0.4137, 0.3557, 0.0644, 0.0643, 0.8955,
         0.0128, 0.1328, 0.0863, 0.1317, 0.6406, 0.4452, 0.8213, 0.0980, 0.4605,
         0.0193, 0.8131, 0.7212, 0.0100, 0.0774, 0.2473, 0.4233, 0.3137, 0.7930,
         0.3865, 0.6812, 0.4946, 0.5348, 0.2647, 0.4725, 0.7121, 0.7214, 0.5781,
         0.8711, 0.9700, 0.3533, 0.8309, 0.9148, 0.9828, 0.9925, 0.8515, 0.3378,
         0.6031, 0.8230, 0.3936, 0.6855, 0.7853, 0.8554, 0.8911, 0.7444, 0.6357,
         0.1512, 0.1999, 0.8658, 0.5500, 0.4305, 0.5770, 0.5157, 0.4493, 0.9827,
         0.1044, 0.4939, 0.8785, 0.6819, 0.3848, 0.3526, 0.9986, 0.3744, 0.9635,
         0.5365, 0.0637, 0.1108, 0.0238, 0.8162, 0.5167, 0.3053, 0.4096, 0.8378,
         0.2437, 0.5326, 0.2183, 0.2994, 0.9683, 0.0932, 0.8489, 0.2030, 0.2479,
         0.3630, 0.5296, 0.8161, 0.7507, 0.8451, 0.1078, 0.0219, 0.1494, 0.3245,
         0.6079, 0.3733, 0.3123, 0.8646, 0.2713, 0.0510, 0.6260, 0.0470, 0.1797,
         0.1927, 0.0638, 0.9701, 0.2541, 0.9884, 0.9659, 0.2782, 0.9591, 0.4014,
         0.0565, 0.4741, 0.7721, 0.4477, 0.6662, 0.1426, 0.3895, 0.2583, 0.4315,
         0.1686, 0.2444, 0.0754, 0.8208, 0.1678, 0.6382, 0.0692, 0.6691, 0.7531,
         0.4906, 0.0340, 0.7003, 0.3537, 0.0960, 0.4446, 0.0905, 0.8612, 0.5236,
         0.6487, 0.0688, 0.8476, 0.1880, 0.2821, 0.7219, 0.0191, 0.7545, 0.3883,
         0.9377, 0.2823, 0.8962, 0.6654, 0.3826, 0.3277, 0.3068, 0.7296, 0.0833,
         0.9077, 0.2093, 0.6161, 0.2287, 0.3260, 0.8435, 0.1748, 0.3120, 0.2321,
         0.4435, 0.0567, 0.0202, 0.6155, 0.0183, 0.0317, 0.5481, 0.7658, 0.2582,
         0.5337, 0.0683, 0.7462, 0.4567, 0.2850, 0.2986, 0.1331, 0.3559, 0.7233,
         0.0410, 0.4004, 0.2968, 0.5312, 0.8087, 0.0841, 0.2067, 0.1332, 0.7064,
         0.0869, 0.0602, 0.9883, 0.0849, 0.3571, 0.3611, 0.8243, 0.1157, 0.6845,
         0.5684, 0.8550, 0.2673, 0.6995, 0.3457, 0.8838, 0.8844, 0.7124, 0.1576,
         0.0339, 0.1420, 0.1096, 0.3403, 0.9703, 0.7269, 0.9603, 0.7193, 0.7346,
         0.5637, 0.1595, 0.9156, 0.7656, 0.8712, 0.1889, 0.2390, 0.7036, 0.5795,
         0.3601, 0.1927, 0.9267, 0.3737, 0.7204, 0.1582, 0.9965, 0.1643, 0.6466,
         0.0126, 0.8183, 0.2841, 0.5390, 0.4752, 0.2413, 0.3274, 0.7812, 0.8707,
         0.5933, 0.0067, 0.3466, 0.6406, 0.0136, 0.5115, 0.2583, 0.5335, 0.0115,
         0.1611, 0.1271, 0.1074, 0.3485, 0.5034, 0.8297, 0.2104, 0.9429, 0.5018,
         0.7158, 0.7696, 0.0396, 0.8219, 0.4160, 0.0722, 0.0612, 0.3228, 0.6131,
         0.3064, 0.1747, 0.3262, 0.0958, 0.9837, 0.7711, 0.7440, 0.0455, 0.5141,
         0.7211, 0.6840, 0.6994, 0.5663, 0.7429, 0.6496, 0.2150, 0.4892, 0.6320,
         0.2633, 0.3337, 0.9817, 0.3557, 0.6638, 0.5040, 0.8117, 0.1587, 0.1474,
         0.3726, 0.1965, 0.9407, 0.1597, 0.0814, 0.5617, 0.7497, 0.8311, 0.8511,
         0.8543, 0.3206, 0.2829, 0.8524, 0.9136, 0.2085, 0.5668, 0.1588, 0.3619,
         0.0304, 0.4842, 0.9131, 0.1538, 0.1660, 0.2525, 0.5018, 0.6313, 0.4632,
         0.3959, 0.9614, 0.1140, 0.7781, 0.1782, 0.3104, 0.4176, 0.5940, 0.5112,
         0.0117, 0.3868, 0.6107, 0.0947, 0.6343, 0.6862, 0.8133, 0.3129, 0.9613,
         0.8191, 0.6163, 0.8004, 0.2856, 0.1962, 0.9696, 0.9873, 0.0698, 0.4906,
         0.9623, 0.0087, 0.0263, 0.1162, 0.3982, 0.8966, 0.9730, 0.4271, 0.4925,
         0.6378, 0.8731, 0.0412, 0.3591, 0.9892, 0.9917, 0.2418, 0.8134, 0.3313,
         0.4094, 0.0381, 0.9263, 0.5396, 0.4627, 0.4138, 0.0280, 0.9548, 0.4807,
         0.4338, 0.6154, 0.8384, 0.2991, 0.5847, 0.2330, 0.0479, 0.2991, 0.1140,
         0.1837, 0.4122, 0.2421, 0.2815, 0.8792, 0.5187, 0.5830, 0.7029, 0.7955,
         0.2066, 0.0888, 0.6921, 0.9159, 0.9373, 0.7277, 0.2209, 0.3591, 0.9252,
         0.9753, 0.8058, 0.2497]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/5 iter: 0/15 loss: 1.0054 Acc: 40.6250% F1: 0.288 Time: 0.97s (0.00s)
Fold 9 train - epoch: 0/5 iter: 1/15 loss: 1.0243 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.04s)
Fold 9 train - epoch: 0/5 iter: 2/15 loss: 1.0607 Acc: 50.0000% F1: 0.373 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 3/15 loss: 0.9133 Acc: 53.1250% F1: 0.274 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 4/15 loss: 1.0575 Acc: 50.0000% F1: 0.349 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 5/15 loss: 1.0342 Acc: 46.8750% F1: 0.282 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 6/15 loss: 0.7621 Acc: 71.8750% F1: 0.343 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 7/15 loss: 1.0019 Acc: 40.6250% F1: 0.193 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 8/15 loss: 1.1563 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 9/15 loss: 1.1180 Acc: 46.8750% F1: 0.255 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 10/15 loss: 1.0440 Acc: 46.8750% F1: 0.255 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 11/15 loss: 0.8441 Acc: 68.7500% F1: 0.378 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 12/15 loss: 0.9482 Acc: 50.0000% F1: 0.227 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 13/15 loss: 0.9627 Acc: 53.1250% F1: 0.275 Time: 0.95s (0.02s)
Fold 9 train - epoch: 0/5 iter: 14/15 loss: 0.7792 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 52.2222% F1: 0.3005 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/5 iter: 0/2 loss: 0.7136 Acc: 71.8750% F1: 0.418 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 0/5 iter: 1/2 loss: 1.1202 Acc: 27.7778% F1: 0.152 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2424 *
*********************************************************
Performing epoch 1 of 5
Fold 9 train - epoch: 1/5 iter: 0/15 loss: 0.8347 Acc: 56.2500% F1: 0.297 Time: 0.94s (0.00s)
Fold 9 train - epoch: 1/5 iter: 1/15 loss: 0.8829 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 2/15 loss: 0.9859 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 3/15 loss: 0.7972 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 4/15 loss: 1.0121 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 5/15 loss: 0.8670 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 6/15 loss: 0.7257 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 7/15 loss: 0.9294 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 8/15 loss: 1.0974 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 9/15 loss: 1.0651 Acc: 46.8750% F1: 0.255 Time: 0.95s (0.03s)
Fold 9 train - epoch: 1/5 iter: 10/15 loss: 1.0069 Acc: 46.8750% F1: 0.255 Time: 0.95s (0.02s)
Fold 9 train - epoch: 1/5 iter: 11/15 loss: 0.8369 Acc: 68.7500% F1: 0.378 Time: 0.96s (0.03s)
Fold 9 train - epoch: 1/5 iter: 12/15 loss: 0.9019 Acc: 56.2500% F1: 0.294 Time: 0.95s (0.02s)
Fold 9 train - epoch: 1/5 iter: 13/15 loss: 0.9488 Acc: 50.0000% F1: 0.262 Time: 0.95s (0.02s)
Fold 9 train - epoch: 1/5 iter: 14/15 loss: 0.5432 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 54.6667% F1: 0.2624 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7669 Acc: 65.6250% F1: 0.396 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 1/5 iter: 1/2 loss: 1.0634 Acc: 27.7778% F1: 0.152 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.2311 *
*********************************************************
Performing epoch 2 of 5
Fold 9 train - epoch: 2/5 iter: 0/15 loss: 0.8426 Acc: 65.6250% F1: 0.517 Time: 0.94s (0.00s)
Fold 9 train - epoch: 2/5 iter: 1/15 loss: 0.8752 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 2/15 loss: 0.9165 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 3/15 loss: 0.7855 Acc: 62.5000% F1: 0.256 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 4/15 loss: 0.9521 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 5/15 loss: 0.8502 Acc: 59.3750% F1: 0.248 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 6/15 loss: 0.7047 Acc: 68.7500% F1: 0.272 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 7/15 loss: 0.8732 Acc: 43.7500% F1: 0.203 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 8/15 loss: 1.0236 Acc: 56.2500% F1: 0.310 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 9/15 loss: 1.0551 Acc: 46.8750% F1: 0.213 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 10/15 loss: 1.0100 Acc: 56.2500% F1: 0.361 Time: 0.95s (0.02s)
Fold 9 train - epoch: 2/5 iter: 11/15 loss: 0.7764 Acc: 68.7500% F1: 0.378 Time: 0.96s (0.02s)
Fold 9 train - epoch: 2/5 iter: 12/15 loss: 0.8723 Acc: 71.8750% F1: 0.492 Time: 0.96s (0.02s)
Fold 9 train - epoch: 2/5 iter: 13/15 loss: 0.8933 Acc: 56.2500% F1: 0.321 Time: 0.95s (0.03s)
Fold 9 train - epoch: 2/5 iter: 14/15 loss: 0.3767 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 58.4444% F1: 0.3289 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7576 Acc: 65.6250% F1: 0.396 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 2/5 iter: 1/2 loss: 1.1525 Acc: 44.4444% F1: 0.309 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.3177 *
*********************************************************
Performing epoch 3 of 5
Fold 9 train - epoch: 3/5 iter: 0/15 loss: 0.7182 Acc: 71.8750% F1: 0.597 Time: 0.95s (0.00s)
Fold 9 train - epoch: 3/5 iter: 1/15 loss: 0.8265 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.02s)
Fold 9 train - epoch: 3/5 iter: 2/15 loss: 0.8248 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 3/15 loss: 0.7269 Acc: 68.7500% F1: 0.375 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 4/15 loss: 0.8393 Acc: 62.5000% F1: 0.420 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 5/15 loss: 0.7292 Acc: 62.5000% F1: 0.320 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 6/15 loss: 0.6056 Acc: 71.8750% F1: 0.386 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 7/15 loss: 0.7194 Acc: 62.5000% F1: 0.415 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 8/15 loss: 0.9344 Acc: 56.2500% F1: 0.387 Time: 0.95s (0.03s)
Fold 9 train - epoch: 3/5 iter: 9/15 loss: 0.9184 Acc: 50.0000% F1: 0.401 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 10/15 loss: 0.8268 Acc: 56.2500% F1: 0.404 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 11/15 loss: 0.8164 Acc: 62.5000% F1: 0.371 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 12/15 loss: 0.7095 Acc: 71.8750% F1: 0.638 Time: 0.95s (0.04s)
Fold 9 train - epoch: 3/5 iter: 13/15 loss: 0.8110 Acc: 62.5000% F1: 0.534 Time: 0.95s (0.02s)
Fold 9 train - epoch: 3/5 iter: 14/15 loss: 0.0871 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 62.2222% F1: 0.4417 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7890 Acc: 65.6250% F1: 0.328 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 3/5 iter: 1/2 loss: 1.4735 Acc: 33.3333% F1: 0.218 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.2927 *
*********************************************************
Performing epoch 4 of 5
Fold 9 train - epoch: 4/5 iter: 0/15 loss: 0.5664 Acc: 78.1250% F1: 0.762 Time: 0.94s (0.00s)
Fold 9 train - epoch: 4/5 iter: 1/15 loss: 0.6759 Acc: 71.8750% F1: 0.664 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 2/15 loss: 0.6461 Acc: 59.3750% F1: 0.585 Time: 0.95s (0.02s)
Fold 9 train - epoch: 4/5 iter: 3/15 loss: 0.5782 Acc: 78.1250% F1: 0.718 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 4/15 loss: 0.7285 Acc: 78.1250% F1: 0.659 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 5/15 loss: 0.6470 Acc: 68.7500% F1: 0.569 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 6/15 loss: 0.4990 Acc: 81.2500% F1: 0.519 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 7/15 loss: 0.6192 Acc: 78.1250% F1: 0.843 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 8/15 loss: 0.6939 Acc: 78.1250% F1: 0.689 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 9/15 loss: 0.8457 Acc: 62.5000% F1: 0.589 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 10/15 loss: 0.6146 Acc: 78.1250% F1: 0.744 Time: 0.96s (0.03s)
Fold 9 train - epoch: 4/5 iter: 11/15 loss: 0.6907 Acc: 65.6250% F1: 0.436 Time: 0.96s (0.03s)
Fold 9 train - epoch: 4/5 iter: 12/15 loss: 0.6347 Acc: 75.0000% F1: 0.715 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 13/15 loss: 0.7908 Acc: 65.6250% F1: 0.604 Time: 0.95s (0.03s)
Fold 9 train - epoch: 4/5 iter: 14/15 loss: 0.0259 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 72.8889% F1: 0.6710 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8053 Acc: 62.5000% F1: 0.353 Time: 0.32s (0.00s)
Fold 9 train-dev - epoch: 4/5 iter: 1/2 loss: 1.6080 Acc: 38.8889% F1: 0.271 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.3276 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 55.0000% F1: 0.2406
	Epoch 1 Accuracy: 51.6000% F1: 0.2942
	Epoch 2 Accuracy: 51.4000% F1: 0.3290
	Epoch 3 Accuracy: 49.4000% F1: 0.3333
	Epoch 4 Accuracy: 43.8000% F1: 0.3363
all done :)
