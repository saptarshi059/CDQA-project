{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at navteca/roberta-base-squad2 were not used when initializing RobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at navteca/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Extracting embeddings for non-domain terms. I'm simply using BERT's tokenizer for the nDT's.\n",
    "#Creating question representations in this block.\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#DTE_Model_Lookup_Table = pd.read_pickle(os.path.join(os.path.abspath('UMLS_KG'), 'embeddings/distmult/DTE_to_BERT.pkl'))\n",
    "\n",
    "DTE_Model_Lookup_Table = pd.read_pickle('DTE_to_RoBERTa.pkl')\n",
    "Metamap_Tokenizations = pd.read_pickle('Metamap_Tokenizations.pkl')\n",
    "\n",
    "model_name = 'navteca/roberta-base-squad2'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_embeddings = model.get_input_embeddings()\n",
    "\n",
    "CLS_embedding = model_embeddings(torch.LongTensor([tokenizer.cls_token_id]))\n",
    "SEP_embedding = model_embeddings(torch.LongTensor([tokenizer.sep_token_id]))\n",
    "\n",
    "all_entities = DTE_Model_Lookup_Table['Term'].to_list()\n",
    "\n",
    "def custom_input_rep(ques, context):\n",
    "    ques = re.sub(' +', ' ', ques).strip()\n",
    "    \n",
    "    def clean_term(word):\n",
    "        return re.sub(r'[\\W\\s]', '', word).lower()\n",
    "\n",
    "    tup = Metamap_Tokenizations.query(\"Question==@ques\")\n",
    "    \n",
    "    metamap_tokenized_question = tup['Tokenization'].values[0]\n",
    "\n",
    "    #Removing punctuations/spaces from domain-terms for easy comparison\n",
    "    mappings = tup['Mappings'].values[0]\n",
    "    for i,x in enumerate(mappings):\n",
    "        mappings[i][0] = clean_term(x[0])\n",
    "\n",
    "    domain_terms = [x[0] for x in mappings]\n",
    "\n",
    "    question_embeddings = []\n",
    "    for word in metamap_tokenized_question:\n",
    "        '''\n",
    "        This is done to easily check if the current word is a DT or not since DT form of the same word \n",
    "        are obtained bit differently.\n",
    "        '''\n",
    "        filtered_word = clean_term(word)\n",
    "\n",
    "        '''\n",
    "        This means that the filtered_word has to be a domain term which also has a KG expansion. If if does not,\n",
    "        then use its BERT embeddings.\n",
    "        '''\n",
    "\n",
    "        if filtered_word in domain_terms: #Use DTE_BERT_Matrix\n",
    "            mapped_concept = mappings[domain_terms.index(filtered_word)][1]\n",
    "            if mapped_concept in all_entities: \n",
    "                question_embeddings.append(DTE_Model_Lookup_Table.query(\"Term==@mapped_concept\")['Embedding'].values[0])\n",
    "            \n",
    "        #The mapped_concept doesn't have an expansion in the KG or the term isn't a DT. Thus, its BERT embeddings are used.\n",
    "        else:\n",
    "            subword_indices = tokenizer(word)['input_ids'][1:-1] #Take all tokens between [CLS] & [SEP]\n",
    "            for index in subword_indices:\n",
    "                question_embeddings.append(model_embeddings(torch.LongTensor([index])))\n",
    "    \n",
    "    #Since our total i/p's can only be 512 tokens long, the context has to be adjusted accordingly.\n",
    "    len_custom_question = len(question_embeddings)\n",
    "    max_length = 512\n",
    "    limit_for_context = max_length - (len_custom_question + 2) #2 to account for [CLS] & [SEP]\n",
    "    \n",
    "    context_embeddings = []\n",
    "    \n",
    "    #Taking all tokens b/w 1 & limit_for_context\n",
    "    reduced_context_indices = tokenizer(context, truncation=True)['input_ids'][1:limit_for_context+1]\n",
    "    \n",
    "    for index in reduced_context_indices:\n",
    "        context_embeddings.append(model_embeddings(torch.LongTensor([index])))\n",
    "        \n",
    "    #In this way, I don't have to add the CLS & SEP embeddings during fine-tuning.\n",
    "    final_representation = torch.unsqueeze(torch.cat((CLS_embedding,\\\n",
    "                                                      torch.cat([*question_embeddings]),\\\n",
    "                                                      torch.cat([*context_embeddings]),\\\n",
    "                                                      SEP_embedding)), dim=1)\n",
    "    \n",
    "    #This difference will be used to adjust the start/end indices of the answers in context.\n",
    "    token_diff = len(tokenizer(ques)['input_ids']) - len(question_embeddings)\n",
    "       \n",
    "    return final_representation, token_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
