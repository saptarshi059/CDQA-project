{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210be83",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Generate tuples (Question, Token, SemType) - old approach + old code - don't run this cell now\n",
    "%cd ~/Desktop/CDQA-project\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "s = pd.read_json('text.out')\n",
    "\n",
    "documents = s.iloc[0][0]['Document']['Utterances']\n",
    "Metamap_Tokenizations = [] #To be used for final QA\n",
    "\n",
    "def retrieve_tokens(SyntaxUnits):\n",
    "    tokens = []\n",
    "    for i in range(len(SyntaxUnits)):\n",
    "        tokens.append(SyntaxUnits[i]['InputMatch'])\n",
    "    return tokens\n",
    "\n",
    "def retrieve_mappings(Mappings):\n",
    "    mapped_semantic_types = []\n",
    "    #No mappings found\n",
    "    if len(Mappings) == 0:\n",
    "        return [] #These words will get their embeddings from BERT\n",
    "    else:\n",
    "        candidates = Mappings[0]['MappingCandidates'] #Choosing Only top mappings\n",
    "        for cnd in candidates:\n",
    "            mapped_semantic_types.append([' '.join(cnd['MatchedWords']), cnd['CandidateCUI'], \\\n",
    "                                              cnd['CandidatePreferred'], cnd['SemTypes'][0]])\n",
    "            entities.add(cnd['CandidatePreferred'])\n",
    "    return mapped_semantic_types\n",
    "\n",
    "entities = set()\n",
    "for doc in tqdm(documents):\n",
    "    Phrases = doc['Phrases']\n",
    "    Phrase_Tokenizations = []\n",
    "    Mappings = []\n",
    "    for ph in Phrases:\n",
    "        Phrase_Tokenizations.append(retrieve_tokens(ph['SyntaxUnits']))\n",
    "        Mappings.append(retrieve_mappings(ph['Mappings']))\n",
    "    #Flattening the Lists\n",
    "    Phrase_Tokenizations = [item for sublist in Phrase_Tokenizations for item in sublist]\n",
    "    Mappings = [item for sublist in Mappings for item in sublist]\n",
    "    #Creating the final list\n",
    "    Metamap_Tokenizations.append((doc['UttText'], Phrase_Tokenizations, Mappings))\n",
    "    \n",
    "#Removing extra spaces from each question\n",
    "for index, tup in enumerate(Metamap_Tokenizations):\n",
    "    temp = list(tup)\n",
    "    #The only question which was causing an issue\n",
    "    if type(temp[0]) != str:\n",
    "        temp[0] = 'What were the impact of event scaleâ€“revised scores?'\n",
    "    else:\n",
    "        temp[0] = temp[0].strip()\n",
    "    Metamap_Tokenizations[index] = tuple(temp)\n",
    "    \n",
    "print(f\"Number of entities discovered: {len(entities)}\")\n",
    "\n",
    "#Replacing each shorthand mapping with KG concept - IF USING SN\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi1234!\", database=\"umls\")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "for i in range(len(Metamap_Tokenizations)):\n",
    "    for j in range(len(Metamap_Tokenizations[i][2])):\n",
    "        mycursor.execute(\"select STY_RL from SRDEF where ABR = '%s' \" % Metamap_Tokenizations[i][2][j][3])\n",
    "        Metamap_Tokenizations[i][2][j][3] = mycursor.fetchall()[0][0]\n",
    "\n",
    "mycursor.close()\n",
    "\n",
    "#Saving Metamap_Tokenizations for use during question embedding creation\n",
    "pd.DataFrame(Metamap_Tokenizations, columns=['Question','Tokenization',\\\n",
    "                                            'Mappings']).to_pickle('Metamap_Tokenizations.pkl')\n",
    "\n",
    "'''--->Extracting relations from the Semantic Network<---'''\n",
    "    #Generating possible semantic types connected to the current one\n",
    "    mycursor.execute('''SELECT STY_RL2, RL\n",
    "                        FROM SRSTR \n",
    "                        WHERE STY_RL1 = '%s';''' % row.Semantic_Type)\n",
    "    possible_semantic_types_2 = [STREL(*x) for x in mycursor.fetchall()]\n",
    "    for sem_type_2 in possible_semantic_types_2:\n",
    "        if sem_type_2.ST == None:\n",
    "            continue\n",
    "        ST2 = sem_type_2.ST.decode()\n",
    "        REL = sem_type_2.REL.decode()\n",
    "        for result in CUI_Preferred_Concept_Semantic_Type_Lookup_Table.query('Semantic_Type==@ST2').itertuples(index=False):\n",
    "            KGT.add((row.Preferred_Concept, REL, result.Preferred_Concept))      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212ae73",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#old code - don't run\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "s = pd.read_pickle('NN-DTE-to-phiyodr-bert-base-finetuned-squad2.pkl')\n",
    "conn = sqlite3.connect('umls.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "entity = []\n",
    "definition = []\n",
    "UMLS_Embedding = []\n",
    "c = 0\n",
    "for row in tqdm(s.itertuples(index=False)):\n",
    "    cursor.execute('''SELECT DEF FROM MRDEF WHERE CUI = '%s' ''' % row.CUI)\n",
    "    results = cursor.fetchall()\n",
    "    if results == []:\n",
    "        c += 1\n",
    "    else:\n",
    "        entity.append(row.Entity)\n",
    "        UMLS_Embedding.append(row.UMLS_Embedding)\n",
    "        definition.append(results[0][0])\n",
    "    \n",
    "print(f\"Number of entities with no definition: {c}\")            \n",
    "pd.DataFrame(zip(entity, UMLS_Embedding, definition), columns=['Entity', 'UMLS_Embedding' ,'Definition']).to_csv('Entity_Definition.csv', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274d62b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#New version of creating KGT (only Metathesaurus) - don't run\n",
    "import sqlite3\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"Saptarshi1234!\", database=\"umls\")\n",
    "mydb.set_charset_collation('latin1')\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "#For sqlite (Metathesaurus)\n",
    "conn = sqlite3.connect('umls.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#All Knowledge Graph Triples\n",
    "KGT = set()\n",
    "\n",
    "#To make the code more readable (pythonic)\n",
    "CUIREL = namedtuple('CUIREL', ['CUI', 'REL'])\n",
    "#STREL = namedtuple('STREL', ['ST', 'REL'])\n",
    "\n",
    "for row in tqdm(CUI_Preferred_Concept_Lookup_Table.itertuples(index=False)):\n",
    "    '''--->Extracting relations from the Metathesaurus<---'''\n",
    "    #This gives the \"incoming\" relations i.e. CUI2 - RELA - CUI1\n",
    "    incoming_relations = []\n",
    "    cursor.execute('''SELECT DISTINCT CUI2, RELA\n",
    "                                  FROM MRREL\n",
    "                                  WHERE CUI1 = '%s' AND RELA <> '';''' % row.CUI)\n",
    "    results = cursor.fetchall()\n",
    "    #Checking which of the returned tuples are present in our lookup table \n",
    "    for tupA in results:\n",
    "        CUIREL2 = CUIREL(*tupA)\n",
    "        for tupB in CUI_Preferred_Concept_Lookup_Table.query('CUI==@CUIREL2.CUI').itertuples(index=False):\n",
    "            KGT.add((tupB.Preferred_Concept, CUIREL2.REL, row.Preferred_Concept))\n",
    "\n",
    "    #This gives the \"outgoing\" relations i.e. CUI1 - RELA - CUI2\n",
    "    outgoing_relations = []\n",
    "    cursor.execute('''SELECT DISTINCT CUI1, RELA \n",
    "                                  FROM MRREL\n",
    "                                  WHERE CUI2 = '%s' AND STYPE2 = 'CUI' AND RELA <> '';''' % row.CUI)\n",
    "    results = cursor.fetchall()\n",
    "    #Checking which of the returned tuples are present in our lookup table\n",
    "    for tupA in results:\n",
    "        CUIREL2 = CUIREL(*tupA)\n",
    "        for tupB in CUI_Preferred_Concept_Lookup_Table.query('CUI==@CUIREL2.CUI').itertuples(index=False):\n",
    "            KGT.add((row.Preferred_Concept, CUIREL2.REL, tupB.Preferred_Concept))\n",
    "    \n",
    "    \n",
    "conn.close()\n",
    "mycursor.close()\n",
    "\n",
    "print(f\"Number of triples extracted: {len(KGT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2ec36",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Creating Train/Validation/Test splits for training KGE's - don't run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Converting set to pandas dataframe for easy splitting\n",
    "KGT = pd.DataFrame(KGT)\n",
    "\n",
    "#Giving the KGT dataframe meaningful column names\n",
    "KGT.rename(columns={0: \"E1\", 1: \"Rel\", 2: \"E2\"}, inplace=True)\n",
    "\n",
    "#80/10/10 split\n",
    "train, validation, test = np.split(KGT.sample(frac=1, random_state=42), [int(.8*len(KGT)), int(.9*len(KGT))])\n",
    "\n",
    "source_of_KG = 'MT'\n",
    "\n",
    "#Creating folder where dataset files will be saved\n",
    "try:\n",
    "    os.mkdir(os.path.join(os.path.abspath(os.getcwd()), f\"UMLS_KG_{source_of_KG}\"))\n",
    "    print(\"KG Directory Created\")\n",
    "except:\n",
    "    print(\"KG Directory Already Exists\")\n",
    "\n",
    "dataset_path = os.path.abspath(f\"UMLS_KG_{source_of_KG}\")\n",
    "\n",
    "#Saving datasets as .txt files to be used for training the KGE\n",
    "np.savetxt(os.path.join(dataset_path, f'UMLS_KG_{source_of_KG}-train.txt'), train.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, f'UMLS_KG_{source_of_KG}-valid.txt'), validation.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "np.savetxt(os.path.join(dataset_path, f'UMLS_KG_{source_of_KG}-test.txt'), test.values, delimiter=\"\\t\", fmt=\"%s\")\n",
    "\n",
    "print('KG dataset saved...')\n",
    "\n",
    "KGT.to_pickle(os.path.join(dataset_path, \"KGT.pkl\"))\n",
    "print('KGT saved as dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e289cc6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Just need to run this cell once to update the dataset with the cleaned questions - done\n",
    "import json\n",
    "import os\n",
    "data_path = os.path.relpath('data/COVID-QA-original.json')\n",
    "\n",
    "dataset = json.load(open(data_path, 'r'))\n",
    "with open('Cleaned_Questions_utf8_replaced.txt','r') as file:\n",
    "    cleaned_questions = file.read().split('\\n')\n",
    "    \n",
    "index = 0\n",
    "for i in range(len(dataset['data'])):\n",
    "    for question_dictionary in dataset['data'][i]['paragraphs'][0]['qas']:\n",
    "        question_dictionary['question'] = cleaned_questions[index]\n",
    "        index += 1\n",
    "        \n",
    "with open('COVID-QA_cleaned.json', 'w') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f75f276",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 131661 column 1 (char 5080968)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5215/4122355822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MM_PubMed.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllDocuments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Utterances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mMetamap_Tokenizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 131661 column 1 (char 5080968)"
     ]
    }
   ],
   "source": [
    "#Generate tuples (question, (matched_text, CUI, preferred candidate)) - new approach\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "texts = json.load(open('MM_PubMed.json'))['AllDocuments'][0]['Document']['Utterances']\n",
    "\n",
    "Metamap_Tokenizations = []\n",
    "for ques in texts:\n",
    "    mappings = []\n",
    "    question_text = ques['UttText']\n",
    "    ques_start_idx = int(ques['UttStartPos'])\n",
    "    for phr in ques['Phrases']:\n",
    "        if phr['Mappings'] != []:\n",
    "            for phr_dict in phr[\"Mappings\"][0]['MappingCandidates']: #Choosing the first candidate\n",
    "                start_idx = int(phr_dict['ConceptPIs'][0]['StartPos']) - ques_start_idx\n",
    "                end_idx = start_idx + int(phr_dict['ConceptPIs'][0]['Length'])\n",
    "                mappings.append((question_text[start_idx:end_idx], phr_dict['CandidateCUI'], \\\n",
    "                                 phr_dict['CandidatePreferred']))\n",
    "    Metamap_Tokenizations.append((question_text, mappings))\n",
    "\n",
    "entities = set()\n",
    "for mappings in Metamap_Tokenizations:\n",
    "    for tup in mappings[1]:\n",
    "        entities.add(tup[2])\n",
    "print(f\"Number of entities discovered: {len(entities)}\")\n",
    "\n",
    "#Saving Metamap_Tokenizations for use during question embedding creation\n",
    "#pd.DataFrame(Metamap_Tokenizations, columns=['Question', 'Mappings']).to_pickle('Metamap_Tokenizations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc2fc9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#New version of CUI-PC_Lookup table\n",
    "cuis = [y[1] for x in Metamap_Tokenizations for y in x[1]]\n",
    "pc = [y[2] for x in Metamap_Tokenizations for y in x[1]]\n",
    "CUI_Preferred_Concept_Lookup_Table = pd.DataFrame(zip(cuis, pc), columns=['CUI','Preferred_Concept']).drop_duplicates()\n",
    "CUI_Preferred_Concept_Lookup_Table.to_csv('CUI_PC.csv', index=False)\n",
    "print('Our_CUI_PC table generated...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
