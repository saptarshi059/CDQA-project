{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62fe8e",
   "metadata": {
    "code_folding": [
     3,
     14,
     24
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "\n",
    "with open('entity2idx.pkl', 'rb') as file:\n",
    "    entity2id = pickle.load(file)\n",
    "\n",
    "entity_embeddings = pd.read_csv('ent_embedding.tsv', sep='\\t', header=None)\n",
    "\n",
    "all_entities = set(entity2id.keys())\n",
    "\n",
    "print('All Files Loaded...')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def common_terms_gen(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_vocab = set(tokenizer.get_vocab().keys())\n",
    "    common_terms = list(all_entities.intersection(model_vocab))\n",
    "    print(f\"Number of KGE's common with {model_name} vocabulary: {len(common_terms)}\")\n",
    "    return common_terms\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "\n",
    "def data_gen(common_terms, model_name):\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    model_embeddings = model.get_input_embeddings()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_vocab = tokenizer.get_vocab()\n",
    "    \n",
    "    src = []\n",
    "    tgt = []\n",
    "    for term in tqdm(common_terms):\n",
    "        src.append(entity_embeddings.iloc[entity2id[term]].to_numpy())\n",
    "        tgt.append(model_embeddings(torch.LongTensor([model_vocab[term]])).detach().cpu().numpy())\n",
    "    \n",
    "    return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76a516",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def weight_matrix_compute(model_name):\n",
    "    KGE_embeddings, Model_embeddings = data_gen(common_terms_gen(model_name), model_name)\n",
    "    W = np.linalg.lstsq(np.vstack(KGE_embeddings),np.vstack(Model_embeddings), rcond=None)[0]\n",
    "    return W\n",
    "\n",
    "bert_W = weight_matrix_compute('phiyodr/bert-base-finetuned-squad2')\n",
    "scibert_W = weight_matrix_compute('ktrapeznikov/scibert_scivocab_uncased_squad_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052c9d3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def homogenizer(weight_matrix, model_name):\n",
    "    homogenized_embeddings = {}\n",
    "    for entity_name, index in entity2id.items():\n",
    "        homogenized_embeddings[entity_name] = torch.FloatTensor(\\\n",
    "                                              np.matmul(weight_matrix.T, \\\n",
    "                                                        entity_embeddings.iloc[index].to_numpy()).reshape(1,-1))\n",
    "    \n",
    "    print(f'Saving Homogenized Embeddings for {model_name}...')\n",
    "    pd.DataFrame(list(homogenized_embeddings.items()), \\\n",
    "                 columns = ['Entity', 'Embedding']).to_pickle(f'Mikolov_to_{model_name.replace(\"/\",\"_\")}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "homogenizer(bert_W, 'phiyodr/bert-base-finetuned-squad2')\n",
    "homogenizer(scibert_W, 'ktrapeznikov/scibert_scivocab_uncased_squad_v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
