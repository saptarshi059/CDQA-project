remove excess spaces
captialize acronyms
remove duplicate words
fix spellings
fix grammar

pipeline for PT programs

1st 4 programs & 8 - run on dgx1
6 - local

A. Now use CUI_PC+MM_Tokenizations_gen.py to generate the tokenizations table and CUI_PC for respective task.
B. scp CUI_PC.csv & MetaMap_Tokenizations.pkl to dgx

1. train_test_gen.py - once (to generate the train/test samples overall) - for the resp. task
2. train_test_gen_NN.py - can repeat this to get the desired number of samples for the NN
3. trainer.py - trains the NN, can run multiple times
4. converter.py - generates the homogenized UMLS_Embeddings, can run multiple times depending on trainer.py -- stop here for only converted UMLS_Embeddings; Continue for dictionary_embeds too
5. scp NN-DTE-to-ktrapeznikov-biobert_v1.1_pubmed_squad_v2.pkl to local
6. ent_def_gen.py - once to add defintions to our NN matrix
7. scp Entity_Definition.pkl to dgx1
8. def_embed.py - run once to generate definition embeddings


export CUDA_VISIBLE_DEVICES=2

python run_pubmed_qa.py --data /home/CDQA-project/data/ --model_name boychaboy/SNLI_roberta-base --use_kge True --concat_kge True --random_kge True --dte_lookup_table_fp /home/CDQA-project/pretrained\ stuff/for_pubmedqa/roberta/NN-DTE-to-boychaboy-SNLI_roberta-base.pkl --gpus 2 --n_epochs 5 --port 10329

python apply_pubmed_qa.py --data /home/CDQA-project/data/test_set.json --model_id 20220218-162931 --epoch 4

nohup ./on3.sh > bertandroberta.out &
nohup ./on7.sh > roberta.out &

nohup ./runpub_all.sh 20220301-152312 > pubtests.out &