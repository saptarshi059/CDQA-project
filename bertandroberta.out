************************************
** MODEL TIME ID: 20220225-064325 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/10 iter: 0/15 loss: 3.7980 Acc: 15.6250% F1: 0.116 Time: 0.96s (0.00s)
Fold 0 train - epoch: 0/10 iter: 1/15 loss: 2.7399 Acc: 25.0000% F1: 0.263 Time: 0.93s (0.04s)
Fold 0 train - epoch: 0/10 iter: 2/15 loss: 2.4719 Acc: 15.6250% F1: 0.152 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 3/15 loss: 1.6286 Acc: 34.3750% F1: 0.252 Time: 0.92s (0.03s)
Fold 0 train - epoch: 0/10 iter: 4/15 loss: 1.4019 Acc: 46.8750% F1: 0.300 Time: 0.93s (0.02s)
Fold 0 train - epoch: 0/10 iter: 5/15 loss: 1.1660 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 6/15 loss: 0.9782 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 0 train - epoch: 0/10 iter: 7/15 loss: 1.0258 Acc: 50.0000% F1: 0.333 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 8/15 loss: 1.1632 Acc: 46.8750% F1: 0.342 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 9/15 loss: 1.2385 Acc: 37.5000% F1: 0.267 Time: 0.93s (0.02s)
Fold 0 train - epoch: 0/10 iter: 10/15 loss: 1.1222 Acc: 50.0000% F1: 0.350 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 11/15 loss: 1.1763 Acc: 37.5000% F1: 0.260 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 12/15 loss: 1.2223 Acc: 37.5000% F1: 0.270 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 13/15 loss: 0.9368 Acc: 46.8750% F1: 0.328 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/10 iter: 14/15 loss: 0.4492 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 39.1111% F1: 0.3257 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5677 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 0/10 iter: 1/2 loss: 2.0450 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2918 *
*********************************************************
Performing epoch 1 of 10
Fold 0 train - epoch: 1/10 iter: 0/15 loss: 0.9680 Acc: 53.1250% F1: 0.236 Time: 0.94s (0.00s)
Fold 0 train - epoch: 1/10 iter: 1/15 loss: 0.9068 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 2/15 loss: 1.2635 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 3/15 loss: 0.9587 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 4/15 loss: 1.0824 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.04s)
Fold 0 train - epoch: 1/10 iter: 5/15 loss: 0.9886 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 6/15 loss: 0.7432 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 7/15 loss: 1.0134 Acc: 46.8750% F1: 0.217 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/10 iter: 8/15 loss: 0.9385 Acc: 56.2500% F1: 0.303 Time: 0.94s (0.05s)
Fold 0 train - epoch: 1/10 iter: 9/15 loss: 0.9834 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.04s)
Fold 0 train - epoch: 1/10 iter: 10/15 loss: 0.9898 Acc: 62.5000% F1: 0.430 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 11/15 loss: 0.8770 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 12/15 loss: 1.0725 Acc: 37.5000% F1: 0.264 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 13/15 loss: 0.9412 Acc: 46.8750% F1: 0.325 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 14/15 loss: 0.5148 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 54.6667% F1: 0.3152 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7442 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3904 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3024 *
*********************************************************
Performing epoch 2 of 10
Fold 0 train - epoch: 2/10 iter: 0/15 loss: 0.9237 Acc: 53.1250% F1: 0.370 Time: 0.94s (0.00s)
Fold 0 train - epoch: 2/10 iter: 1/15 loss: 0.9129 Acc: 56.2500% F1: 0.389 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 2/15 loss: 0.8816 Acc: 59.3750% F1: 0.381 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 3/15 loss: 0.8621 Acc: 53.1250% F1: 0.271 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 4/15 loss: 0.8955 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 5/15 loss: 0.8731 Acc: 62.5000% F1: 0.361 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 6/15 loss: 0.7168 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 7/15 loss: 0.8821 Acc: 50.0000% F1: 0.259 Time: 0.93s (0.04s)
Fold 0 train - epoch: 2/10 iter: 8/15 loss: 0.8728 Acc: 56.2500% F1: 0.303 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 9/15 loss: 1.0270 Acc: 50.0000% F1: 0.267 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 10/15 loss: 1.0177 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 11/15 loss: 0.8491 Acc: 62.5000% F1: 0.256 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 12/15 loss: 0.9701 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 13/15 loss: 0.7831 Acc: 65.6250% F1: 0.370 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 14/15 loss: 0.1005 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 57.1111% F1: 0.3271 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6449 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 2/10 iter: 1/2 loss: 1.6214 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2586 *
*********************************************************
Performing epoch 3 of 10
Fold 0 train - epoch: 3/10 iter: 0/15 loss: 0.9012 Acc: 56.2500% F1: 0.294 Time: 0.94s (0.00s)
Fold 0 train - epoch: 3/10 iter: 1/15 loss: 0.7998 Acc: 71.8750% F1: 0.481 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 2/15 loss: 0.8207 Acc: 68.7500% F1: 0.478 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 3/15 loss: 0.7509 Acc: 68.7500% F1: 0.454 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 4/15 loss: 0.8538 Acc: 65.6250% F1: 0.455 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 5/15 loss: 0.8292 Acc: 65.6250% F1: 0.458 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 6/15 loss: 0.7639 Acc: 71.8750% F1: 0.447 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 7/15 loss: 0.7756 Acc: 68.7500% F1: 0.462 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 8/15 loss: 0.9005 Acc: 59.3750% F1: 0.381 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 9/15 loss: 0.9404 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 10/15 loss: 0.9178 Acc: 56.2500% F1: 0.367 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 11/15 loss: 0.7438 Acc: 78.1250% F1: 0.531 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 12/15 loss: 0.9118 Acc: 50.0000% F1: 0.269 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 13/15 loss: 0.7105 Acc: 68.7500% F1: 0.439 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 14/15 loss: 0.0368 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 65.1111% F1: 0.4308 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6513 Acc: 78.1250% F1: 0.439 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6894 Acc: 16.6667% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.3224 *
*********************************************************
Performing epoch 4 of 10
Fold 0 train - epoch: 4/10 iter: 0/15 loss: 0.8740 Acc: 53.1250% F1: 0.241 Time: 0.95s (0.00s)
Fold 0 train - epoch: 4/10 iter: 1/15 loss: 0.7815 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 2/15 loss: 0.8016 Acc: 65.6250% F1: 0.456 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 3/15 loss: 0.7280 Acc: 75.0000% F1: 0.484 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 4/15 loss: 0.7489 Acc: 78.1250% F1: 0.559 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 5/15 loss: 0.7828 Acc: 68.7500% F1: 0.603 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 6/15 loss: 0.6707 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 7/15 loss: 0.7460 Acc: 65.6250% F1: 0.434 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 8/15 loss: 0.7902 Acc: 59.3750% F1: 0.354 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 9/15 loss: 0.7853 Acc: 62.5000% F1: 0.430 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 10/15 loss: 0.7998 Acc: 59.3750% F1: 0.398 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 11/15 loss: 0.7428 Acc: 81.2500% F1: 0.562 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 12/15 loss: 0.8474 Acc: 62.5000% F1: 0.429 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 13/15 loss: 0.6547 Acc: 75.0000% F1: 0.508 Time: 0.94s (0.03s)
Fold 0 train - epoch: 4/10 iter: 14/15 loss: 0.0122 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 68.4444% F1: 0.4718 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7635 Acc: 68.7500% F1: 0.407 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 4/10 iter: 1/2 loss: 1.6169 Acc: 16.6667% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 50.0000% F1: 0.2926 *
*********************************************************
Performing epoch 5 of 10
Fold 0 train - epoch: 5/10 iter: 0/15 loss: 0.7457 Acc: 68.7500% F1: 0.464 Time: 0.95s (0.00s)
Fold 0 train - epoch: 5/10 iter: 1/15 loss: 0.6849 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 2/15 loss: 0.6502 Acc: 78.1250% F1: 0.548 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 3/15 loss: 0.6553 Acc: 84.3750% F1: 0.578 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 4/15 loss: 0.6503 Acc: 78.1250% F1: 0.559 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 5/15 loss: 0.5994 Acc: 78.1250% F1: 0.674 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 6/15 loss: 0.5816 Acc: 75.0000% F1: 0.490 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 7/15 loss: 0.6114 Acc: 78.1250% F1: 0.525 Time: 0.94s (0.03s)
Fold 0 train - epoch: 5/10 iter: 8/15 loss: 0.6525 Acc: 71.8750% F1: 0.512 Time: 0.94s (0.04s)
Fold 0 train - epoch: 5/10 iter: 9/15 loss: 0.6376 Acc: 75.0000% F1: 0.528 Time: 0.94s (0.04s)
Fold 0 train - epoch: 5/10 iter: 10/15 loss: 0.6136 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.04s)
Fold 0 train - epoch: 5/10 iter: 11/15 loss: 0.6288 Acc: 78.1250% F1: 0.538 Time: 0.93s (0.04s)
Fold 0 train - epoch: 5/10 iter: 12/15 loss: 0.7056 Acc: 78.1250% F1: 0.665 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 13/15 loss: 0.4893 Acc: 84.3750% F1: 0.600 Time: 0.94s (0.03s)
Fold 0 train - epoch: 5/10 iter: 14/15 loss: 0.0043 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 0 Epoch 5 train Avg acc: 76.8889% F1: 0.5591 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9005 Acc: 65.6250% F1: 0.322 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 5/10 iter: 1/2 loss: 1.6927 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3039 *
*********************************************************
Performing epoch 6 of 10
Fold 0 train - epoch: 6/10 iter: 0/15 loss: 0.6115 Acc: 75.0000% F1: 0.646 Time: 0.94s (0.00s)
Fold 0 train - epoch: 6/10 iter: 1/15 loss: 0.5712 Acc: 78.1250% F1: 0.557 Time: 0.93s (0.04s)
Fold 0 train - epoch: 6/10 iter: 2/15 loss: 0.4917 Acc: 87.5000% F1: 0.768 Time: 0.93s (0.04s)
Fold 0 train - epoch: 6/10 iter: 3/15 loss: 0.5947 Acc: 81.2500% F1: 0.552 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 4/15 loss: 0.5255 Acc: 87.5000% F1: 0.621 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 5/15 loss: 0.4862 Acc: 84.3750% F1: 0.768 Time: 0.94s (0.03s)
Fold 0 train - epoch: 6/10 iter: 6/15 loss: 0.4557 Acc: 81.2500% F1: 0.719 Time: 0.94s (0.05s)
Fold 0 train - epoch: 6/10 iter: 7/15 loss: 0.4962 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.05s)
Fold 0 train - epoch: 6/10 iter: 8/15 loss: 0.5069 Acc: 78.1250% F1: 0.724 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 9/15 loss: 0.5109 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 10/15 loss: 0.4432 Acc: 87.5000% F1: 0.819 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 11/15 loss: 0.5147 Acc: 81.2500% F1: 0.716 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 12/15 loss: 0.6153 Acc: 75.0000% F1: 0.622 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 13/15 loss: 0.3809 Acc: 90.6250% F1: 0.792 Time: 0.94s (0.03s)
Fold 0 train - epoch: 6/10 iter: 14/15 loss: 0.0018 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 6 train Avg acc: 82.4444% F1: 0.6991 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9584 Acc: 65.6250% F1: 0.269 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 6/10 iter: 1/2 loss: 2.0952 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 6 train-dev Avg acc: 48.0000% F1: 0.2890 *
*********************************************************
Performing epoch 7 of 10
Fold 0 train - epoch: 7/10 iter: 0/15 loss: 0.4558 Acc: 78.1250% F1: 0.557 Time: 0.95s (0.00s)
Fold 0 train - epoch: 7/10 iter: 1/15 loss: 0.3588 Acc: 84.3750% F1: 0.596 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 2/15 loss: 0.3643 Acc: 84.3750% F1: 0.745 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 3/15 loss: 0.3088 Acc: 93.7500% F1: 0.644 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 4/15 loss: 0.3948 Acc: 90.6250% F1: 0.846 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 5/15 loss: 0.3256 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 6/15 loss: 0.2773 Acc: 93.7500% F1: 0.854 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 7/15 loss: 0.3252 Acc: 87.5000% F1: 0.812 Time: 0.94s (0.03s)
Fold 0 train - epoch: 7/10 iter: 8/15 loss: 0.3698 Acc: 81.2500% F1: 0.762 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 9/15 loss: 0.4306 Acc: 78.1250% F1: 0.561 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 10/15 loss: 0.2971 Acc: 93.7500% F1: 0.893 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 11/15 loss: 0.2853 Acc: 93.7500% F1: 0.817 Time: 0.94s (0.03s)
Fold 0 train - epoch: 7/10 iter: 12/15 loss: 0.4161 Acc: 81.2500% F1: 0.744 Time: 0.94s (0.03s)
Fold 0 train - epoch: 7/10 iter: 13/15 loss: 0.2916 Acc: 84.3750% F1: 0.824 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 14/15 loss: 0.0005 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 7 train Avg acc: 87.1111% F1: 0.7717 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 7/10 iter: 0/2 loss: 1.4889 Acc: 43.7500% F1: 0.289 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 7/10 iter: 1/2 loss: 1.7590 Acc: 33.3333% F1: 0.200 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 7 train-dev Avg acc: 40.0000% F1: 0.3190 *
*********************************************************
Performing epoch 8 of 10
Fold 0 train - epoch: 8/10 iter: 0/15 loss: 0.3010 Acc: 84.3750% F1: 0.792 Time: 0.95s (0.00s)
Fold 0 train - epoch: 8/10 iter: 1/15 loss: 0.2758 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.04s)
Fold 0 train - epoch: 8/10 iter: 2/15 loss: 0.4014 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 3/15 loss: 0.2358 Acc: 93.7500% F1: 0.650 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 4/15 loss: 0.3828 Acc: 84.3750% F1: 0.796 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 5/15 loss: 0.2753 Acc: 90.6250% F1: 0.890 Time: 0.94s (0.03s)
Fold 0 train - epoch: 8/10 iter: 6/15 loss: 0.1465 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 7/15 loss: 0.1417 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 8/15 loss: 0.1677 Acc: 90.6250% F1: 0.871 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 9/15 loss: 0.1415 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 10/15 loss: 0.1078 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 11/15 loss: 0.2014 Acc: 96.8750% F1: 0.925 Time: 0.94s (0.03s)
Fold 0 train - epoch: 8/10 iter: 12/15 loss: 0.2311 Acc: 96.8750% F1: 0.945 Time: 0.94s (0.03s)
Fold 0 train - epoch: 8/10 iter: 13/15 loss: 0.0836 Acc: 100.0000% F1: 1.000 Time: 0.94s (0.03s)
Fold 0 train - epoch: 8/10 iter: 14/15 loss: 0.0001 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 8 train Avg acc: 94.0000% F1: 0.9040 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 8/10 iter: 0/2 loss: 1.3654 Acc: 68.7500% F1: 0.375 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 8/10 iter: 1/2 loss: 2.7747 Acc: 22.2222% F1: 0.157 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 8 train-dev Avg acc: 52.0000% F1: 0.3519 *
*********************************************************
Performing epoch 9 of 10
Fold 0 train - epoch: 9/10 iter: 0/15 loss: 0.1329 Acc: 96.8750% F1: 0.943 Time: 0.94s (0.00s)
Fold 0 train - epoch: 9/10 iter: 1/15 loss: 0.3094 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 2/15 loss: 0.1219 Acc: 96.8750% F1: 0.921 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 3/15 loss: 0.1339 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 4/15 loss: 0.2106 Acc: 96.8750% F1: 0.939 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 5/15 loss: 0.1047 Acc: 96.8750% F1: 0.953 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 6/15 loss: 0.1174 Acc: 93.7500% F1: 0.804 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 7/15 loss: 0.2328 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 8/15 loss: 0.0896 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 9/15 loss: 0.1319 Acc: 96.8750% F1: 0.950 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 10/15 loss: 0.0885 Acc: 100.0000% F1: 1.000 Time: 0.94s (0.03s)
Fold 0 train - epoch: 9/10 iter: 11/15 loss: 0.0998 Acc: 96.8750% F1: 0.933 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 12/15 loss: 0.1515 Acc: 93.7500% F1: 0.925 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 13/15 loss: 0.1927 Acc: 90.6250% F1: 0.894 Time: 0.94s (0.03s)
Fold 0 train - epoch: 9/10 iter: 14/15 loss: 0.0000 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 9 train Avg acc: 95.3333% F1: 0.9245 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 9/10 iter: 0/2 loss: 1.5931 Acc: 46.8750% F1: 0.260 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 9/10 iter: 1/2 loss: 3.0106 Acc: 22.2222% F1: 0.157 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 9 train-dev Avg acc: 38.0000% F1: 0.2742 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/10 iter: 0/15 loss: 3.8705 Acc: 15.6250% F1: 0.134 Time: 0.98s (0.00s)
Fold 1 train - epoch: 0/10 iter: 1/15 loss: 2.6615 Acc: 28.1250% F1: 0.303 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 2/15 loss: 2.1589 Acc: 15.6250% F1: 0.160 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 3/15 loss: 1.5973 Acc: 31.2500% F1: 0.262 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 4/15 loss: 1.4864 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 5/15 loss: 1.1431 Acc: 56.2500% F1: 0.326 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 6/15 loss: 0.9679 Acc: 56.2500% F1: 0.315 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 7/15 loss: 1.0290 Acc: 50.0000% F1: 0.326 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 8/15 loss: 1.1099 Acc: 53.1250% F1: 0.388 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 9/15 loss: 1.3034 Acc: 37.5000% F1: 0.267 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 10/15 loss: 1.0981 Acc: 46.8750% F1: 0.330 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 11/15 loss: 1.0667 Acc: 50.0000% F1: 0.344 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 12/15 loss: 1.2318 Acc: 37.5000% F1: 0.264 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 13/15 loss: 0.9153 Acc: 46.8750% F1: 0.319 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 14/15 loss: 0.6594 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.03s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 41.1111% F1: 0.3438 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6587 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/10 iter: 1/2 loss: 2.0510 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 52.0000% F1: 0.3022 *
*********************************************************
Performing epoch 1 of 10
Fold 1 train - epoch: 1/10 iter: 0/15 loss: 0.9628 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.00s)
Fold 1 train - epoch: 1/10 iter: 1/15 loss: 0.9675 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 2/15 loss: 1.0895 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 3/15 loss: 0.9281 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 4/15 loss: 1.0280 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 5/15 loss: 0.9831 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 6/15 loss: 0.7506 Acc: 68.7500% F1: 0.336 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 7/15 loss: 0.9291 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 8/15 loss: 0.9377 Acc: 62.5000% F1: 0.372 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 9/15 loss: 0.9740 Acc: 53.1250% F1: 0.333 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/10 iter: 10/15 loss: 0.9974 Acc: 56.2500% F1: 0.370 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/10 iter: 11/15 loss: 0.8020 Acc: 78.1250% F1: 0.522 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 12/15 loss: 1.0557 Acc: 46.8750% F1: 0.305 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/10 iter: 13/15 loss: 0.8902 Acc: 53.1250% F1: 0.365 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/10 iter: 14/15 loss: 0.2537 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 57.1111% F1: 0.3274 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/10 iter: 0/2 loss: 0.8029 Acc: 59.3750% F1: 0.434 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3492 Acc: 27.7778% F1: 0.175 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.3176 *
*********************************************************
Performing epoch 2 of 10
Fold 1 train - epoch: 2/10 iter: 0/15 loss: 0.9288 Acc: 50.0000% F1: 0.343 Time: 0.95s (0.00s)
Fold 1 train - epoch: 2/10 iter: 1/15 loss: 0.8795 Acc: 65.6250% F1: 0.453 Time: 0.93s (0.04s)
Fold 1 train - epoch: 2/10 iter: 2/15 loss: 0.8916 Acc: 59.3750% F1: 0.411 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 3/15 loss: 0.8277 Acc: 46.8750% F1: 0.290 Time: 0.93s (0.05s)
Fold 1 train - epoch: 2/10 iter: 4/15 loss: 0.8924 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 5/15 loss: 0.8888 Acc: 59.3750% F1: 0.344 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 6/15 loss: 0.7403 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 7/15 loss: 0.8640 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 8/15 loss: 0.8354 Acc: 62.5000% F1: 0.383 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 9/15 loss: 1.0105 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 10/15 loss: 1.0420 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 11/15 loss: 0.7967 Acc: 62.5000% F1: 0.314 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 12/15 loss: 0.9515 Acc: 56.2500% F1: 0.298 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 13/15 loss: 0.8141 Acc: 59.3750% F1: 0.300 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 14/15 loss: 0.1375 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 57.1111% F1: 0.3420 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/10 iter: 0/2 loss: 0.7079 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5508 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3123 *
*********************************************************
Performing epoch 3 of 10
Fold 1 train - epoch: 3/10 iter: 0/15 loss: 0.8960 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.00s)
Fold 1 train - epoch: 3/10 iter: 1/15 loss: 0.8310 Acc: 65.6250% F1: 0.447 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 2/15 loss: 0.8342 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 3/15 loss: 0.7915 Acc: 56.2500% F1: 0.315 Time: 0.92s (0.03s)
Fold 1 train - epoch: 3/10 iter: 4/15 loss: 0.7674 Acc: 71.8750% F1: 0.513 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 5/15 loss: 0.8608 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 6/15 loss: 0.6825 Acc: 68.7500% F1: 0.407 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 7/15 loss: 0.7962 Acc: 59.3750% F1: 0.378 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 8/15 loss: 0.8479 Acc: 62.5000% F1: 0.421 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 9/15 loss: 0.8933 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 10/15 loss: 0.9907 Acc: 59.3750% F1: 0.400 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 11/15 loss: 0.7482 Acc: 84.3750% F1: 0.588 Time: 0.94s (0.03s)
Fold 1 train - epoch: 3/10 iter: 12/15 loss: 0.8838 Acc: 56.2500% F1: 0.356 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 13/15 loss: 0.7123 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 14/15 loss: 0.0318 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 65.3333% F1: 0.4339 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7754 Acc: 56.2500% F1: 0.360 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 3/10 iter: 1/2 loss: 1.5165 Acc: 27.7778% F1: 0.175 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.2977 *
*********************************************************
Performing epoch 4 of 10
Fold 1 train - epoch: 4/10 iter: 0/15 loss: 0.8214 Acc: 59.3750% F1: 0.389 Time: 0.95s (0.00s)
Fold 1 train - epoch: 4/10 iter: 1/15 loss: 0.7166 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 2/15 loss: 0.7545 Acc: 71.8750% F1: 0.495 Time: 0.93s (0.04s)
Fold 1 train - epoch: 4/10 iter: 3/15 loss: 0.7220 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.05s)
Fold 1 train - epoch: 4/10 iter: 4/15 loss: 0.7467 Acc: 75.0000% F1: 0.527 Time: 0.93s (0.05s)
Fold 1 train - epoch: 4/10 iter: 5/15 loss: 0.7737 Acc: 71.8750% F1: 0.619 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 6/15 loss: 0.6279 Acc: 75.0000% F1: 0.654 Time: 0.93s (0.04s)
Fold 1 train - epoch: 4/10 iter: 7/15 loss: 0.6742 Acc: 62.5000% F1: 0.414 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 8/15 loss: 0.7227 Acc: 75.0000% F1: 0.551 Time: 0.93s (0.05s)
Fold 1 train - epoch: 4/10 iter: 9/15 loss: 0.8092 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.04s)
Fold 1 train - epoch: 4/10 iter: 10/15 loss: 0.7244 Acc: 68.7500% F1: 0.488 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 11/15 loss: 0.7488 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 12/15 loss: 0.8098 Acc: 65.6250% F1: 0.442 Time: 0.94s (0.03s)
Fold 1 train - epoch: 4/10 iter: 13/15 loss: 0.5847 Acc: 75.0000% F1: 0.498 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 14/15 loss: 0.0102 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 70.4444% F1: 0.5028 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8773 Acc: 56.2500% F1: 0.281 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 4/10 iter: 1/2 loss: 1.5321 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3287 *
*********************************************************
Performing epoch 5 of 10
Fold 1 train - epoch: 5/10 iter: 0/15 loss: 0.8223 Acc: 56.2500% F1: 0.367 Time: 0.95s (0.00s)
Fold 1 train - epoch: 5/10 iter: 1/15 loss: 0.6593 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 2/15 loss: 0.6620 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.04s)
Fold 1 train - epoch: 5/10 iter: 3/15 loss: 0.6334 Acc: 75.0000% F1: 0.509 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 4/15 loss: 0.7304 Acc: 75.0000% F1: 0.537 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 5/15 loss: 0.6113 Acc: 75.0000% F1: 0.636 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 6/15 loss: 0.4939 Acc: 84.3750% F1: 0.565 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 7/15 loss: 0.6020 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 8/15 loss: 0.6071 Acc: 68.7500% F1: 0.566 Time: 0.94s (0.03s)
Fold 1 train - epoch: 5/10 iter: 9/15 loss: 0.6813 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 10/15 loss: 0.5788 Acc: 65.6250% F1: 0.463 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 11/15 loss: 0.6301 Acc: 68.7500% F1: 0.483 Time: 0.94s (0.03s)
Fold 1 train - epoch: 5/10 iter: 12/15 loss: 0.7329 Acc: 68.7500% F1: 0.490 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 13/15 loss: 0.4507 Acc: 87.5000% F1: 0.617 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 14/15 loss: 0.0054 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 5 train Avg acc: 73.5556% F1: 0.5366 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 5/10 iter: 0/2 loss: 1.1131 Acc: 50.0000% F1: 0.282 Time: 0.35s (0.00s)
Fold 1 train-dev - epoch: 5/10 iter: 1/2 loss: 1.4695 Acc: 38.8889% F1: 0.306 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 5 train-dev Avg acc: 46.0000% F1: 0.3965 *
*********************************************************
Performing epoch 6 of 10
Fold 1 train - epoch: 6/10 iter: 0/15 loss: 0.5995 Acc: 71.8750% F1: 0.509 Time: 0.94s (0.00s)
Fold 1 train - epoch: 6/10 iter: 1/15 loss: 0.5123 Acc: 81.2500% F1: 0.688 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 2/15 loss: 0.4978 Acc: 84.3750% F1: 0.743 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 3/15 loss: 0.5252 Acc: 78.1250% F1: 0.530 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 4/15 loss: 0.5044 Acc: 78.1250% F1: 0.676 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 5/15 loss: 0.4831 Acc: 81.2500% F1: 0.691 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 6/15 loss: 0.3369 Acc: 90.6250% F1: 0.808 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 7/15 loss: 0.4381 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 8/15 loss: 0.4562 Acc: 81.2500% F1: 0.709 Time: 0.94s (0.03s)
Fold 1 train - epoch: 6/10 iter: 9/15 loss: 0.4941 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 10/15 loss: 0.4714 Acc: 87.5000% F1: 0.819 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 11/15 loss: 0.4451 Acc: 87.5000% F1: 0.761 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 12/15 loss: 0.4864 Acc: 84.3750% F1: 0.772 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 13/15 loss: 0.3075 Acc: 87.5000% F1: 0.767 Time: 0.94s (0.02s)
Fold 1 train - epoch: 6/10 iter: 14/15 loss: 0.0009 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 6 train Avg acc: 83.3333% F1: 0.7085 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 6/10 iter: 0/2 loss: 1.2803 Acc: 53.1250% F1: 0.311 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 6/10 iter: 1/2 loss: 1.6753 Acc: 38.8889% F1: 0.306 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 6 train-dev Avg acc: 48.0000% F1: 0.3982 *
*********************************************************
Performing epoch 7 of 10
Fold 1 train - epoch: 7/10 iter: 0/15 loss: 0.4555 Acc: 81.2500% F1: 0.761 Time: 0.95s (0.00s)
Fold 1 train - epoch: 7/10 iter: 1/15 loss: 0.3369 Acc: 93.7500% F1: 0.864 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 2/15 loss: 0.3428 Acc: 90.6250% F1: 0.875 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 3/15 loss: 0.3687 Acc: 87.5000% F1: 0.611 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 4/15 loss: 0.4501 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 5/15 loss: 0.3111 Acc: 90.6250% F1: 0.817 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 6/15 loss: 0.2583 Acc: 90.6250% F1: 0.808 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 7/15 loss: 0.2235 Acc: 96.8750% F1: 0.657 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 8/15 loss: 0.3625 Acc: 81.2500% F1: 0.730 Time: 0.94s (0.02s)
Fold 1 train - epoch: 7/10 iter: 9/15 loss: 0.3810 Acc: 87.5000% F1: 0.819 Time: 0.94s (0.05s)
Fold 1 train - epoch: 7/10 iter: 10/15 loss: 0.2549 Acc: 93.7500% F1: 0.866 Time: 0.94s (0.05s)
Fold 1 train - epoch: 7/10 iter: 11/15 loss: 0.2800 Acc: 90.6250% F1: 0.776 Time: 0.94s (0.04s)
Fold 1 train - epoch: 7/10 iter: 12/15 loss: 0.4479 Acc: 87.5000% F1: 0.791 Time: 0.94s (0.04s)
Fold 1 train - epoch: 7/10 iter: 13/15 loss: 0.1822 Acc: 96.8750% F1: 0.917 Time: 0.94s (0.04s)
Fold 1 train - epoch: 7/10 iter: 14/15 loss: 0.0003 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 1 Epoch 7 train Avg acc: 89.5556% F1: 0.8046 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 7/10 iter: 0/2 loss: 1.4863 Acc: 43.7500% F1: 0.243 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 7/10 iter: 1/2 loss: 2.0092 Acc: 33.3333% F1: 0.269 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 7 train-dev Avg acc: 40.0000% F1: 0.3325 *
*********************************************************
Performing epoch 8 of 10
Fold 1 train - epoch: 8/10 iter: 0/15 loss: 0.3032 Acc: 87.5000% F1: 0.814 Time: 0.94s (0.00s)
Fold 1 train - epoch: 8/10 iter: 1/15 loss: 0.2093 Acc: 93.7500% F1: 0.883 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 2/15 loss: 0.2277 Acc: 90.6250% F1: 0.844 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 3/15 loss: 0.1782 Acc: 93.7500% F1: 0.650 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 4/15 loss: 0.2194 Acc: 96.8750% F1: 0.939 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 5/15 loss: 0.2071 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 6/15 loss: 0.1007 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 7/15 loss: 0.1350 Acc: 96.8750% F1: 0.657 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 8/15 loss: 0.1611 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 9/15 loss: 0.1940 Acc: 93.7500% F1: 0.951 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 10/15 loss: 0.2303 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 11/15 loss: 0.1486 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 12/15 loss: 0.2215 Acc: 90.6250% F1: 0.865 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 13/15 loss: 0.1094 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 14/15 loss: 0.0001 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 8 train Avg acc: 94.4444% F1: 0.9032 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 8/10 iter: 0/2 loss: 2.2688 Acc: 37.5000% F1: 0.259 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 8/10 iter: 1/2 loss: 1.8243 Acc: 33.3333% F1: 0.241 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 8 train-dev Avg acc: 36.0000% F1: 0.3117 *
*********************************************************
Performing epoch 9 of 10
Fold 1 train - epoch: 9/10 iter: 0/15 loss: 0.2690 Acc: 90.6250% F1: 0.889 Time: 0.95s (0.00s)
Fold 1 train - epoch: 9/10 iter: 1/15 loss: 0.1135 Acc: 93.7500% F1: 0.950 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 2/15 loss: 0.1396 Acc: 96.8750% F1: 0.923 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 3/15 loss: 0.0946 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 4/15 loss: 0.1935 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 5/15 loss: 0.0726 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 6/15 loss: 0.0872 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 7/15 loss: 0.1897 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 8/15 loss: 0.1621 Acc: 96.8750% F1: 0.969 Time: 0.94s (0.03s)
Fold 1 train - epoch: 9/10 iter: 9/15 loss: 0.0972 Acc: 96.8750% F1: 0.950 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 10/15 loss: 0.0976 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 11/15 loss: 0.1147 Acc: 96.8750% F1: 0.916 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 12/15 loss: 0.1240 Acc: 96.8750% F1: 0.945 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 13/15 loss: 0.1101 Acc: 96.8750% F1: 0.917 Time: 0.94s (0.02s)
Fold 1 train - epoch: 9/10 iter: 14/15 loss: 0.0000 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 9 train Avg acc: 96.6667% F1: 0.9514 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 9/10 iter: 0/2 loss: 1.9502 Acc: 53.1250% F1: 0.335 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 9/10 iter: 1/2 loss: 2.7614 Acc: 33.3333% F1: 0.269 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 9 train-dev Avg acc: 46.0000% F1: 0.3840 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/10 iter: 0/15 loss: 3.9684 Acc: 12.5000% F1: 0.076 Time: 0.97s (0.00s)
Fold 2 train - epoch: 0/10 iter: 1/15 loss: 2.7127 Acc: 25.0000% F1: 0.269 Time: 0.93s (0.04s)
Fold 2 train - epoch: 0/10 iter: 2/15 loss: 2.1652 Acc: 25.0000% F1: 0.246 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 3/15 loss: 1.6330 Acc: 31.2500% F1: 0.233 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 4/15 loss: 1.4299 Acc: 50.0000% F1: 0.324 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 5/15 loss: 1.1685 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 6/15 loss: 1.0325 Acc: 56.2500% F1: 0.315 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 7/15 loss: 0.9478 Acc: 53.1250% F1: 0.352 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 8/15 loss: 1.3606 Acc: 46.8750% F1: 0.348 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 9/15 loss: 1.2660 Acc: 34.3750% F1: 0.240 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 10/15 loss: 1.1775 Acc: 40.6250% F1: 0.290 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 11/15 loss: 1.0183 Acc: 53.1250% F1: 0.369 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 12/15 loss: 1.2454 Acc: 34.3750% F1: 0.244 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 13/15 loss: 0.9708 Acc: 53.1250% F1: 0.371 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/10 iter: 14/15 loss: 0.4000 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 40.8889% F1: 0.3403 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5100 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 0/10 iter: 1/2 loss: 1.8019 Acc: 5.5556% F1: 0.048 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 52.0000% F1: 0.2570 *
*********************************************************
Performing epoch 1 of 10
Fold 2 train - epoch: 1/10 iter: 0/15 loss: 1.0548 Acc: 53.1250% F1: 0.278 Time: 0.95s (0.00s)
Fold 2 train - epoch: 1/10 iter: 1/15 loss: 0.9416 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 2/15 loss: 1.0985 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 3/15 loss: 0.9956 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 4/15 loss: 1.0732 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 5/15 loss: 1.0111 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 6/15 loss: 0.7585 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 7/15 loss: 0.8704 Acc: 50.0000% F1: 0.282 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 8/15 loss: 1.0657 Acc: 56.2500% F1: 0.308 Time: 0.94s (0.03s)
Fold 2 train - epoch: 1/10 iter: 9/15 loss: 0.9810 Acc: 53.1250% F1: 0.350 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 10/15 loss: 0.9714 Acc: 56.2500% F1: 0.380 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 11/15 loss: 0.8530 Acc: 68.7500% F1: 0.472 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 12/15 loss: 1.0392 Acc: 50.0000% F1: 0.356 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 13/15 loss: 0.9534 Acc: 56.2500% F1: 0.383 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 14/15 loss: 0.3275 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 56.6667% F1: 0.3384 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/10 iter: 0/2 loss: 0.6829 Acc: 84.3750% F1: 0.458 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3260 Acc: 11.1111% F1: 0.083 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.3067 *
*********************************************************
Performing epoch 2 of 10
Fold 2 train - epoch: 2/10 iter: 0/15 loss: 0.9423 Acc: 56.2500% F1: 0.367 Time: 0.94s (0.00s)
Fold 2 train - epoch: 2/10 iter: 1/15 loss: 0.8692 Acc: 62.5000% F1: 0.405 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 2/15 loss: 0.9199 Acc: 56.2500% F1: 0.363 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 3/15 loss: 0.8723 Acc: 59.3750% F1: 0.296 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 4/15 loss: 0.8832 Acc: 59.3750% F1: 0.370 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 5/15 loss: 0.8935 Acc: 59.3750% F1: 0.344 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 6/15 loss: 0.6922 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 7/15 loss: 0.8515 Acc: 53.1250% F1: 0.317 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 8/15 loss: 0.9476 Acc: 53.1250% F1: 0.294 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 9/15 loss: 1.0264 Acc: 46.8750% F1: 0.217 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 10/15 loss: 1.0441 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 11/15 loss: 0.7480 Acc: 65.6250% F1: 0.328 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 12/15 loss: 0.9740 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 13/15 loss: 0.8456 Acc: 65.6250% F1: 0.370 Time: 0.94s (0.03s)
Fold 2 train - epoch: 2/10 iter: 14/15 loss: 0.1028 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 58.0000% F1: 0.3281 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6286 Acc: 84.3750% F1: 0.458 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 2/10 iter: 1/2 loss: 1.4132 Acc: 11.1111% F1: 0.083 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.3067 *
*********************************************************
Performing epoch 3 of 10
Fold 2 train - epoch: 3/10 iter: 0/15 loss: 0.8866 Acc: 50.0000% F1: 0.295 Time: 0.94s (0.00s)
Fold 2 train - epoch: 3/10 iter: 1/15 loss: 0.8707 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 2/15 loss: 0.8423 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 3/15 loss: 0.8401 Acc: 46.8750% F1: 0.304 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 4/15 loss: 0.8609 Acc: 78.1250% F1: 0.556 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 5/15 loss: 0.8361 Acc: 62.5000% F1: 0.437 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 6/15 loss: 0.6773 Acc: 75.0000% F1: 0.447 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 7/15 loss: 0.7912 Acc: 62.5000% F1: 0.406 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 8/15 loss: 0.9874 Acc: 59.3750% F1: 0.374 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 9/15 loss: 0.9761 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 10/15 loss: 0.9132 Acc: 46.8750% F1: 0.280 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 11/15 loss: 0.8029 Acc: 68.7500% F1: 0.418 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 12/15 loss: 0.9467 Acc: 53.1250% F1: 0.283 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 13/15 loss: 0.7560 Acc: 75.0000% F1: 0.501 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/10 iter: 14/15 loss: 0.0357 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 62.0000% F1: 0.4065 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6075 Acc: 81.2500% F1: 0.448 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 3/10 iter: 1/2 loss: 1.4586 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 58.0000% F1: 0.3277 *
*********************************************************
Performing epoch 4 of 10
Fold 2 train - epoch: 4/10 iter: 0/15 loss: 0.8975 Acc: 53.1250% F1: 0.333 Time: 0.95s (0.00s)
Fold 2 train - epoch: 4/10 iter: 1/15 loss: 0.7190 Acc: 75.0000% F1: 0.517 Time: 0.92s (0.03s)
Fold 2 train - epoch: 4/10 iter: 2/15 loss: 0.7749 Acc: 71.8750% F1: 0.499 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 3/15 loss: 0.7572 Acc: 59.3750% F1: 0.373 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 4/15 loss: 0.7437 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 5/15 loss: 0.7455 Acc: 78.1250% F1: 0.669 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 6/15 loss: 0.6301 Acc: 75.0000% F1: 0.447 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 7/15 loss: 0.6768 Acc: 65.6250% F1: 0.438 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 8/15 loss: 0.8535 Acc: 56.2500% F1: 0.375 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 9/15 loss: 0.8148 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 10/15 loss: 0.8566 Acc: 53.1250% F1: 0.350 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 11/15 loss: 0.7387 Acc: 75.0000% F1: 0.483 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 12/15 loss: 0.8580 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.04s)
Fold 2 train - epoch: 4/10 iter: 13/15 loss: 0.6332 Acc: 81.2500% F1: 0.559 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 14/15 loss: 0.0134 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 67.5556% F1: 0.4665 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7046 Acc: 71.8750% F1: 0.284 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4588 Acc: 33.3333% F1: 0.200 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 58.0000% F1: 0.3738 *
*********************************************************
Performing epoch 5 of 10
Fold 2 train - epoch: 5/10 iter: 0/15 loss: 0.8471 Acc: 62.5000% F1: 0.430 Time: 0.95s (0.00s)
Fold 2 train - epoch: 5/10 iter: 1/15 loss: 0.7129 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 2/15 loss: 0.6957 Acc: 75.0000% F1: 0.527 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 3/15 loss: 0.5979 Acc: 75.0000% F1: 0.513 Time: 0.92s (0.03s)
Fold 2 train - epoch: 5/10 iter: 4/15 loss: 0.6992 Acc: 81.2500% F1: 0.583 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 5/15 loss: 0.6817 Acc: 68.7500% F1: 0.590 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 6/15 loss: 0.4656 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 7/15 loss: 0.5933 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 8/15 loss: 0.6445 Acc: 71.8750% F1: 0.603 Time: 0.94s (0.03s)
Fold 2 train - epoch: 5/10 iter: 9/15 loss: 0.7056 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 10/15 loss: 0.6911 Acc: 68.7500% F1: 0.484 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 11/15 loss: 0.6208 Acc: 81.2500% F1: 0.580 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 12/15 loss: 0.7217 Acc: 68.7500% F1: 0.490 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 13/15 loss: 0.5249 Acc: 90.6250% F1: 0.781 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 14/15 loss: 0.0030 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 5 train Avg acc: 74.8889% F1: 0.5572 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8756 Acc: 59.3750% F1: 0.307 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 5/10 iter: 1/2 loss: 1.3973 Acc: 33.3333% F1: 0.190 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 5 train-dev Avg acc: 50.0000% F1: 0.3478 *
*********************************************************
Performing epoch 6 of 10
Fold 2 train - epoch: 6/10 iter: 0/15 loss: 0.7695 Acc: 65.6250% F1: 0.465 Time: 0.95s (0.00s)
Fold 2 train - epoch: 6/10 iter: 1/15 loss: 0.5803 Acc: 81.2500% F1: 0.693 Time: 0.93s (0.05s)
Fold 2 train - epoch: 6/10 iter: 2/15 loss: 0.4897 Acc: 90.6250% F1: 0.791 Time: 0.93s (0.05s)
Fold 2 train - epoch: 6/10 iter: 3/15 loss: 0.4220 Acc: 93.7500% F1: 0.650 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 4/15 loss: 0.5776 Acc: 78.1250% F1: 0.559 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 5/15 loss: 0.4693 Acc: 87.5000% F1: 0.860 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 6/15 loss: 0.3920 Acc: 81.2500% F1: 0.719 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 7/15 loss: 0.4662 Acc: 75.0000% F1: 0.507 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 8/15 loss: 0.6182 Acc: 68.7500% F1: 0.611 Time: 0.94s (0.03s)
Fold 2 train - epoch: 6/10 iter: 9/15 loss: 0.6085 Acc: 78.1250% F1: 0.556 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 10/15 loss: 0.5076 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 11/15 loss: 0.4108 Acc: 93.7500% F1: 0.808 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 12/15 loss: 0.6222 Acc: 81.2500% F1: 0.685 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 13/15 loss: 0.3919 Acc: 93.7500% F1: 0.809 Time: 0.94s (0.03s)
Fold 2 train - epoch: 6/10 iter: 14/15 loss: 0.0016 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 6 train Avg acc: 81.7778% F1: 0.6821 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9105 Acc: 62.5000% F1: 0.267 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 6/10 iter: 1/2 loss: 1.8257 Acc: 33.3333% F1: 0.190 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 6 train-dev Avg acc: 52.0000% F1: 0.3513 *
*********************************************************
Performing epoch 7 of 10
Fold 2 train - epoch: 7/10 iter: 0/15 loss: 0.5022 Acc: 78.1250% F1: 0.550 Time: 0.94s (0.00s)
Fold 2 train - epoch: 7/10 iter: 1/15 loss: 0.3316 Acc: 90.6250% F1: 0.836 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 2/15 loss: 0.4357 Acc: 81.2500% F1: 0.697 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 3/15 loss: 0.3378 Acc: 87.5000% F1: 0.603 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 4/15 loss: 0.3935 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 5/15 loss: 0.2803 Acc: 90.6250% F1: 0.872 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 6/15 loss: 0.2250 Acc: 93.7500% F1: 0.652 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 7/15 loss: 0.2653 Acc: 96.8750% F1: 0.657 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 8/15 loss: 0.4386 Acc: 84.3750% F1: 0.806 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 9/15 loss: 0.4066 Acc: 84.3750% F1: 0.602 Time: 0.94s (0.03s)
Fold 2 train - epoch: 7/10 iter: 10/15 loss: 0.2579 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 11/15 loss: 0.3013 Acc: 93.7500% F1: 0.817 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 12/15 loss: 0.4227 Acc: 87.5000% F1: 0.787 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 13/15 loss: 0.2655 Acc: 93.7500% F1: 0.871 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 14/15 loss: 0.0002 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 2 Epoch 7 train Avg acc: 88.6667% F1: 0.7868 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 7/10 iter: 0/2 loss: 1.2182 Acc: 50.0000% F1: 0.232 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 7/10 iter: 1/2 loss: 1.8080 Acc: 33.3333% F1: 0.190 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 7 train-dev Avg acc: 44.0000% F1: 0.3152 *
*********************************************************
Performing epoch 8 of 10
Fold 2 train - epoch: 8/10 iter: 0/15 loss: 0.3447 Acc: 87.5000% F1: 0.787 Time: 0.94s (0.00s)
Fold 2 train - epoch: 8/10 iter: 1/15 loss: 0.2780 Acc: 93.7500% F1: 0.883 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 2/15 loss: 0.2513 Acc: 93.7500% F1: 0.900 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 3/15 loss: 0.2181 Acc: 93.7500% F1: 0.857 Time: 0.92s (0.03s)
Fold 2 train - epoch: 8/10 iter: 4/15 loss: 0.2258 Acc: 93.7500% F1: 0.865 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 5/15 loss: 0.2098 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 6/15 loss: 0.1139 Acc: 96.8750% F1: 0.881 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 7/15 loss: 0.1510 Acc: 96.8750% F1: 0.656 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 8/15 loss: 0.2110 Acc: 90.6250% F1: 0.872 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 9/15 loss: 0.2535 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 10/15 loss: 0.1539 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 11/15 loss: 0.2231 Acc: 90.6250% F1: 0.791 Time: 0.94s (0.03s)
Fold 2 train - epoch: 8/10 iter: 12/15 loss: 0.2175 Acc: 93.7500% F1: 0.883 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 13/15 loss: 0.2119 Acc: 96.8750% F1: 0.917 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 14/15 loss: 0.0002 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 8 train Avg acc: 93.7778% F1: 0.8789 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 8/10 iter: 0/2 loss: 1.3814 Acc: 46.8750% F1: 0.222 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 8/10 iter: 1/2 loss: 2.2420 Acc: 33.3333% F1: 0.190 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 8 train-dev Avg acc: 42.0000% F1: 0.3064 *
*********************************************************
Performing epoch 9 of 10
Fold 2 train - epoch: 9/10 iter: 0/15 loss: 0.2419 Acc: 90.6250% F1: 0.889 Time: 0.95s (0.00s)
Fold 2 train - epoch: 9/10 iter: 1/15 loss: 0.1831 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 2/15 loss: 0.1490 Acc: 93.7500% F1: 0.899 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 3/15 loss: 0.1005 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 4/15 loss: 0.1620 Acc: 96.8750% F1: 0.939 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 5/15 loss: 0.0866 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 6/15 loss: 0.0879 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 7/15 loss: 0.0744 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 8/15 loss: 0.0794 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 9/15 loss: 0.1664 Acc: 93.7500% F1: 0.906 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 10/15 loss: 0.1582 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 11/15 loss: 0.1184 Acc: 93.7500% F1: 0.817 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 12/15 loss: 0.1775 Acc: 93.7500% F1: 0.890 Time: 0.94s (0.03s)
Fold 2 train - epoch: 9/10 iter: 13/15 loss: 0.1922 Acc: 96.8750% F1: 0.925 Time: 0.94s (0.03s)
Fold 2 train - epoch: 9/10 iter: 14/15 loss: 0.0000 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 9 train Avg acc: 96.2222% F1: 0.9335 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 9/10 iter: 0/2 loss: 1.8814 Acc: 46.8750% F1: 0.222 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 9/10 iter: 1/2 loss: 2.1554 Acc: 38.8889% F1: 0.203 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 9 train-dev Avg acc: 44.0000% F1: 0.3219 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/10 iter: 0/15 loss: 4.0158 Acc: 15.6250% F1: 0.134 Time: 0.96s (0.00s)
Fold 3 train - epoch: 0/10 iter: 1/15 loss: 2.7642 Acc: 25.0000% F1: 0.271 Time: 0.93s (0.04s)
Fold 3 train - epoch: 0/10 iter: 2/15 loss: 1.9926 Acc: 28.1250% F1: 0.278 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 3/15 loss: 1.5216 Acc: 34.3750% F1: 0.237 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 4/15 loss: 1.3952 Acc: 46.8750% F1: 0.285 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 5/15 loss: 1.0467 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 6/15 loss: 0.9685 Acc: 53.1250% F1: 0.271 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 7/15 loss: 1.0097 Acc: 43.7500% F1: 0.270 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 8/15 loss: 1.3571 Acc: 40.6250% F1: 0.300 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 9/15 loss: 1.3637 Acc: 28.1250% F1: 0.198 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 10/15 loss: 1.1165 Acc: 40.6250% F1: 0.285 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 11/15 loss: 1.0575 Acc: 43.7500% F1: 0.306 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 12/15 loss: 1.1530 Acc: 43.7500% F1: 0.305 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 13/15 loss: 0.9588 Acc: 56.2500% F1: 0.382 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 14/15 loss: 0.6588 Acc: 100.0000% F1: 1.000 Time: 0.10s (0.03s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 40.0000% F1: 0.3326 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6003 Acc: 81.2500% F1: 0.571 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 0/10 iter: 1/2 loss: 1.9051 Acc: 5.5556% F1: 0.048 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2921 *
*********************************************************
Performing epoch 1 of 10
Fold 3 train - epoch: 1/10 iter: 0/15 loss: 0.9828 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.00s)
Fold 3 train - epoch: 1/10 iter: 1/15 loss: 0.9200 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 2/15 loss: 1.0105 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 3/15 loss: 0.9475 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 4/15 loss: 1.0745 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 5/15 loss: 0.8992 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 6/15 loss: 0.7028 Acc: 68.7500% F1: 0.336 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 7/15 loss: 0.8668 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 8/15 loss: 1.0498 Acc: 53.1250% F1: 0.292 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 9/15 loss: 1.0158 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 10/15 loss: 0.9759 Acc: 50.0000% F1: 0.317 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/10 iter: 11/15 loss: 0.8361 Acc: 65.6250% F1: 0.419 Time: 0.93s (0.11s)
Fold 3 train - epoch: 1/10 iter: 12/15 loss: 0.9616 Acc: 50.0000% F1: 0.317 Time: 0.94s (0.02s)
Fold 3 train - epoch: 1/10 iter: 13/15 loss: 0.9263 Acc: 56.2500% F1: 0.383 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 14/15 loss: 0.4927 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 55.7778% F1: 0.3145 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7144 Acc: 81.2500% F1: 0.644 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3710 Acc: 5.5556% F1: 0.042 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3122 *
*********************************************************
Performing epoch 2 of 10
Fold 3 train - epoch: 2/10 iter: 0/15 loss: 0.9041 Acc: 56.2500% F1: 0.380 Time: 0.95s (0.00s)
Fold 3 train - epoch: 2/10 iter: 1/15 loss: 0.8661 Acc: 65.6250% F1: 0.433 Time: 0.93s (0.04s)
Fold 3 train - epoch: 2/10 iter: 2/15 loss: 0.8482 Acc: 65.6250% F1: 0.451 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 3/15 loss: 0.8694 Acc: 46.8750% F1: 0.272 Time: 0.93s (0.04s)
Fold 3 train - epoch: 2/10 iter: 4/15 loss: 0.8925 Acc: 50.0000% F1: 0.333 Time: 0.93s (0.04s)
Fold 3 train - epoch: 2/10 iter: 5/15 loss: 0.8127 Acc: 71.8750% F1: 0.475 Time: 0.93s (0.04s)
Fold 3 train - epoch: 2/10 iter: 6/15 loss: 0.6806 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 7/15 loss: 0.8716 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.04s)
Fold 3 train - epoch: 2/10 iter: 8/15 loss: 0.9729 Acc: 56.2500% F1: 0.310 Time: 0.94s (0.03s)
Fold 3 train - epoch: 2/10 iter: 9/15 loss: 1.0069 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.04s)
Fold 3 train - epoch: 2/10 iter: 10/15 loss: 0.9588 Acc: 46.8750% F1: 0.252 Time: 0.94s (0.04s)
Fold 3 train - epoch: 2/10 iter: 11/15 loss: 0.7867 Acc: 65.6250% F1: 0.328 Time: 0.93s (0.04s)
Fold 3 train - epoch: 2/10 iter: 12/15 loss: 0.9566 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.04s)
Fold 3 train - epoch: 2/10 iter: 13/15 loss: 0.8266 Acc: 62.5000% F1: 0.314 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 14/15 loss: 0.1797 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 58.2222% F1: 0.3429 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6458 Acc: 81.2500% F1: 0.448 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5474 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2342 *
*********************************************************
Performing epoch 3 of 10
Fold 3 train - epoch: 3/10 iter: 0/15 loss: 0.9031 Acc: 62.5000% F1: 0.385 Time: 0.94s (0.00s)
Fold 3 train - epoch: 3/10 iter: 1/15 loss: 0.8219 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 2/15 loss: 0.7669 Acc: 68.7500% F1: 0.474 Time: 0.92s (0.03s)
Fold 3 train - epoch: 3/10 iter: 3/15 loss: 0.8104 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 4/15 loss: 0.8839 Acc: 65.6250% F1: 0.466 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 5/15 loss: 0.8133 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 6/15 loss: 0.7277 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 7/15 loss: 0.7527 Acc: 65.6250% F1: 0.426 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 8/15 loss: 0.9554 Acc: 59.3750% F1: 0.396 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 9/15 loss: 1.0013 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 10/15 loss: 0.9427 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 11/15 loss: 0.6792 Acc: 81.2500% F1: 0.546 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 12/15 loss: 0.9101 Acc: 59.3750% F1: 0.366 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 13/15 loss: 0.7227 Acc: 68.7500% F1: 0.449 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 14/15 loss: 0.1459 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 65.3333% F1: 0.4321 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6505 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6517 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.2283 *
*********************************************************
Performing epoch 4 of 10
Fold 3 train - epoch: 4/10 iter: 0/15 loss: 0.7821 Acc: 68.7500% F1: 0.460 Time: 0.95s (0.00s)
Fold 3 train - epoch: 4/10 iter: 1/15 loss: 0.7588 Acc: 71.8750% F1: 0.492 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 2/15 loss: 0.7259 Acc: 71.8750% F1: 0.499 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 3/15 loss: 0.7383 Acc: 65.6250% F1: 0.424 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 4/15 loss: 0.7955 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 5/15 loss: 0.6968 Acc: 78.1250% F1: 0.550 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 6/15 loss: 0.6445 Acc: 78.1250% F1: 0.506 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 7/15 loss: 0.6954 Acc: 71.8750% F1: 0.485 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 8/15 loss: 0.8442 Acc: 59.3750% F1: 0.416 Time: 0.93s (0.04s)
Fold 3 train - epoch: 4/10 iter: 9/15 loss: 0.8884 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 10/15 loss: 0.8153 Acc: 59.3750% F1: 0.400 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 11/15 loss: 0.6754 Acc: 78.1250% F1: 0.522 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 12/15 loss: 0.7334 Acc: 65.6250% F1: 0.436 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 13/15 loss: 0.7177 Acc: 68.7500% F1: 0.443 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 14/15 loss: 0.0721 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 69.7778% F1: 0.4733 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7770 Acc: 71.8750% F1: 0.396 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 4/10 iter: 1/2 loss: 1.5460 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.3637 *
*********************************************************
Performing epoch 5 of 10
Fold 3 train - epoch: 5/10 iter: 0/15 loss: 0.8257 Acc: 62.5000% F1: 0.427 Time: 0.94s (0.00s)
Fold 3 train - epoch: 5/10 iter: 1/15 loss: 0.6509 Acc: 78.1250% F1: 0.553 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 2/15 loss: 0.6189 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 3/15 loss: 0.6436 Acc: 78.1250% F1: 0.534 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 4/15 loss: 0.6507 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 5/15 loss: 0.5296 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 6/15 loss: 0.4856 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 7/15 loss: 0.5708 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 8/15 loss: 0.7355 Acc: 62.5000% F1: 0.452 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 9/15 loss: 0.7277 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 10/15 loss: 0.6754 Acc: 71.8750% F1: 0.518 Time: 0.94s (0.03s)
Fold 3 train - epoch: 5/10 iter: 11/15 loss: 0.5680 Acc: 84.3750% F1: 0.582 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 12/15 loss: 0.6295 Acc: 81.2500% F1: 0.574 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 13/15 loss: 0.6148 Acc: 81.2500% F1: 0.554 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 14/15 loss: 0.0114 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 5 train Avg acc: 76.8889% F1: 0.5492 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8167 Acc: 59.3750% F1: 0.362 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 5/10 iter: 1/2 loss: 1.7631 Acc: 27.7778% F1: 0.231 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3844 *
*********************************************************
Performing epoch 6 of 10
Fold 3 train - epoch: 6/10 iter: 0/15 loss: 0.6377 Acc: 71.8750% F1: 0.500 Time: 0.94s (0.00s)
Fold 3 train - epoch: 6/10 iter: 1/15 loss: 0.5048 Acc: 81.2500% F1: 0.571 Time: 0.92s (0.03s)
Fold 3 train - epoch: 6/10 iter: 2/15 loss: 0.5161 Acc: 78.1250% F1: 0.700 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 3/15 loss: 0.4660 Acc: 81.2500% F1: 0.767 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 4/15 loss: 0.5023 Acc: 84.3750% F1: 0.607 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 5/15 loss: 0.4481 Acc: 87.5000% F1: 0.749 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 6/15 loss: 0.3133 Acc: 90.6250% F1: 0.815 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 7/15 loss: 0.3552 Acc: 90.6250% F1: 0.614 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 8/15 loss: 0.6187 Acc: 68.7500% F1: 0.556 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 9/15 loss: 0.5320 Acc: 78.1250% F1: 0.678 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 10/15 loss: 0.4772 Acc: 81.2500% F1: 0.775 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 11/15 loss: 0.4049 Acc: 93.7500% F1: 0.817 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 12/15 loss: 0.5025 Acc: 84.3750% F1: 0.602 Time: 0.94s (0.03s)
Fold 3 train - epoch: 6/10 iter: 13/15 loss: 0.4058 Acc: 87.5000% F1: 0.755 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 14/15 loss: 0.0256 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 6 train Avg acc: 82.8889% F1: 0.6823 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 6/10 iter: 0/2 loss: 1.0005 Acc: 56.2500% F1: 0.343 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 6/10 iter: 1/2 loss: 1.8532 Acc: 27.7778% F1: 0.214 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 6 train-dev Avg acc: 46.0000% F1: 0.3710 *
*********************************************************
Performing epoch 7 of 10
Fold 3 train - epoch: 7/10 iter: 0/15 loss: 0.4932 Acc: 84.3750% F1: 0.719 Time: 0.95s (0.00s)
Fold 3 train - epoch: 7/10 iter: 1/15 loss: 0.3889 Acc: 81.2500% F1: 0.677 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 2/15 loss: 0.3503 Acc: 90.6250% F1: 0.791 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 3/15 loss: 0.2838 Acc: 96.8750% F1: 0.924 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 4/15 loss: 0.4376 Acc: 87.5000% F1: 0.822 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 5/15 loss: 0.2426 Acc: 93.7500% F1: 0.916 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 6/15 loss: 0.3249 Acc: 87.5000% F1: 0.784 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 7/15 loss: 0.2143 Acc: 96.8750% F1: 0.657 Time: 0.94s (0.03s)
Fold 3 train - epoch: 7/10 iter: 8/15 loss: 0.3727 Acc: 84.3750% F1: 0.777 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 9/15 loss: 0.3538 Acc: 84.3750% F1: 0.774 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 10/15 loss: 0.1942 Acc: 93.7500% F1: 0.893 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 11/15 loss: 0.2753 Acc: 90.6250% F1: 0.783 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 12/15 loss: 0.2711 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 13/15 loss: 0.2961 Acc: 90.6250% F1: 0.777 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 14/15 loss: 0.0019 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 7 train Avg acc: 89.7778% F1: 0.8145 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 7/10 iter: 0/2 loss: 1.1359 Acc: 56.2500% F1: 0.353 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 7/10 iter: 1/2 loss: 2.2915 Acc: 27.7778% F1: 0.214 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 7 train-dev Avg acc: 46.0000% F1: 0.3725 *
*********************************************************
Performing epoch 8 of 10
Fold 3 train - epoch: 8/10 iter: 0/15 loss: 0.3299 Acc: 84.3750% F1: 0.768 Time: 0.94s (0.00s)
Fold 3 train - epoch: 8/10 iter: 1/15 loss: 0.2126 Acc: 90.6250% F1: 0.836 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 2/15 loss: 0.2624 Acc: 93.7500% F1: 0.899 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 3/15 loss: 0.1684 Acc: 96.8750% F1: 0.874 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 4/15 loss: 0.3148 Acc: 87.5000% F1: 0.822 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 5/15 loss: 0.1671 Acc: 96.8750% F1: 0.973 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 6/15 loss: 0.1165 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 7/15 loss: 0.1816 Acc: 96.8750% F1: 0.878 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 8/15 loss: 0.1376 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 9/15 loss: 0.1642 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 10/15 loss: 0.2066 Acc: 93.7500% F1: 0.917 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 11/15 loss: 0.2508 Acc: 87.5000% F1: 0.741 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 12/15 loss: 0.1307 Acc: 93.7500% F1: 0.910 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 13/15 loss: 0.1564 Acc: 93.7500% F1: 0.891 Time: 0.94s (0.03s)
Fold 3 train - epoch: 8/10 iter: 14/15 loss: 0.0034 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 3 Epoch 8 train Avg acc: 93.7778% F1: 0.9035 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 8/10 iter: 0/2 loss: 1.3964 Acc: 53.1250% F1: 0.356 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 8/10 iter: 1/2 loss: 2.4906 Acc: 27.7778% F1: 0.207 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 8 train-dev Avg acc: 44.0000% F1: 0.3653 *
*********************************************************
Performing epoch 9 of 10
Fold 3 train - epoch: 9/10 iter: 0/15 loss: 0.1797 Acc: 93.7500% F1: 0.891 Time: 0.95s (0.00s)
Fold 3 train - epoch: 9/10 iter: 1/15 loss: 0.1410 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 2/15 loss: 0.0879 Acc: 96.8750% F1: 0.923 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 3/15 loss: 0.0552 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 4/15 loss: 0.0987 Acc: 96.8750% F1: 0.939 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 5/15 loss: 0.1154 Acc: 96.8750% F1: 0.935 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 6/15 loss: 0.0587 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 7/15 loss: 0.0472 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 8/15 loss: 0.1456 Acc: 96.8750% F1: 0.968 Time: 0.94s (0.03s)
Fold 3 train - epoch: 9/10 iter: 9/15 loss: 0.1819 Acc: 90.6250% F1: 0.894 Time: 0.94s (0.04s)
Fold 3 train - epoch: 9/10 iter: 10/15 loss: 0.1197 Acc: 96.8750% F1: 0.942 Time: 0.94s (0.04s)
Fold 3 train - epoch: 9/10 iter: 11/15 loss: 0.1022 Acc: 96.8750% F1: 0.974 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 12/15 loss: 0.1075 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 13/15 loss: 0.2293 Acc: 90.6250% F1: 0.846 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 14/15 loss: 0.0099 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 9 train Avg acc: 96.4444% F1: 0.9416 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 9/10 iter: 0/2 loss: 1.7962 Acc: 50.0000% F1: 0.373 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 9/10 iter: 1/2 loss: 2.6490 Acc: 27.7778% F1: 0.201 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 9 train-dev Avg acc: 42.0000% F1: 0.3628 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/10 iter: 0/15 loss: 3.9478 Acc: 15.6250% F1: 0.134 Time: 0.95s (0.00s)
Fold 4 train - epoch: 0/10 iter: 1/15 loss: 2.8097 Acc: 31.2500% F1: 0.346 Time: 0.93s (0.04s)
Fold 4 train - epoch: 0/10 iter: 2/15 loss: 2.0472 Acc: 28.1250% F1: 0.276 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 3/15 loss: 1.5141 Acc: 43.7500% F1: 0.310 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 4/15 loss: 1.2552 Acc: 59.3750% F1: 0.413 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 5/15 loss: 1.0131 Acc: 56.2500% F1: 0.327 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 6/15 loss: 0.8671 Acc: 65.6250% F1: 0.361 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 7/15 loss: 1.0308 Acc: 46.8750% F1: 0.308 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/10 iter: 8/15 loss: 1.4907 Acc: 34.3750% F1: 0.243 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 9/15 loss: 1.3631 Acc: 31.2500% F1: 0.220 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 10/15 loss: 1.1988 Acc: 46.8750% F1: 0.333 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 11/15 loss: 1.0732 Acc: 46.8750% F1: 0.328 Time: 0.94s (0.03s)
Fold 4 train - epoch: 0/10 iter: 12/15 loss: 1.1012 Acc: 43.7500% F1: 0.312 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 13/15 loss: 0.9783 Acc: 50.0000% F1: 0.344 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/10 iter: 14/15 loss: 0.5186 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 42.8889% F1: 0.3600 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5245 Acc: 71.8750% F1: 0.506 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 0/10 iter: 1/2 loss: 2.1501 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 52.0000% F1: 0.3177 *
*********************************************************
Performing epoch 1 of 10
Fold 4 train - epoch: 1/10 iter: 0/15 loss: 0.9629 Acc: 50.0000% F1: 0.267 Time: 0.95s (0.00s)
Fold 4 train - epoch: 1/10 iter: 1/15 loss: 0.9750 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.05s)
Fold 4 train - epoch: 1/10 iter: 2/15 loss: 1.1002 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.05s)
Fold 4 train - epoch: 1/10 iter: 3/15 loss: 0.9242 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.04s)
Fold 4 train - epoch: 1/10 iter: 4/15 loss: 1.0579 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.04s)
Fold 4 train - epoch: 1/10 iter: 5/15 loss: 0.9038 Acc: 62.5000% F1: 0.320 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 6/15 loss: 0.7112 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 7/15 loss: 0.8763 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 8/15 loss: 1.0652 Acc: 53.1250% F1: 0.292 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 9/15 loss: 0.9748 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 10/15 loss: 0.9736 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 11/15 loss: 0.8277 Acc: 53.1250% F1: 0.345 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 12/15 loss: 0.9811 Acc: 40.6250% F1: 0.225 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 13/15 loss: 0.9011 Acc: 56.2500% F1: 0.389 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 14/15 loss: 0.5295 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 54.0000% F1: 0.3040 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/10 iter: 0/2 loss: 0.6888 Acc: 71.8750% F1: 0.506 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 1/10 iter: 1/2 loss: 1.5713 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3177 *
*********************************************************
Performing epoch 2 of 10
Fold 4 train - epoch: 2/10 iter: 0/15 loss: 0.9081 Acc: 65.6250% F1: 0.447 Time: 0.94s (0.00s)
Fold 4 train - epoch: 2/10 iter: 1/15 loss: 0.8506 Acc: 65.6250% F1: 0.433 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 2/15 loss: 0.9123 Acc: 56.2500% F1: 0.345 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 3/15 loss: 0.8932 Acc: 53.1250% F1: 0.301 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 4/15 loss: 0.8688 Acc: 56.2500% F1: 0.327 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/10 iter: 5/15 loss: 0.8031 Acc: 68.7500% F1: 0.431 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 6/15 loss: 0.7356 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 7/15 loss: 0.8587 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.06s)
Fold 4 train - epoch: 2/10 iter: 8/15 loss: 0.9913 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 9/15 loss: 0.9535 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 10/15 loss: 1.0325 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 11/15 loss: 0.7473 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 12/15 loss: 0.8903 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 13/15 loss: 0.8605 Acc: 62.5000% F1: 0.377 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 14/15 loss: 0.2468 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 59.5556% F1: 0.3459 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6087 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 2/10 iter: 1/2 loss: 1.7172 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3281 *
*********************************************************
Performing epoch 3 of 10
Fold 4 train - epoch: 3/10 iter: 0/15 loss: 0.9126 Acc: 53.1250% F1: 0.333 Time: 0.94s (0.00s)
Fold 4 train - epoch: 3/10 iter: 1/15 loss: 0.8525 Acc: 68.7500% F1: 0.455 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 2/15 loss: 0.8604 Acc: 62.5000% F1: 0.424 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 3/15 loss: 0.8391 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 4/15 loss: 0.8414 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 5/15 loss: 0.8101 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 6/15 loss: 0.6920 Acc: 68.7500% F1: 0.407 Time: 0.94s (0.04s)
Fold 4 train - epoch: 3/10 iter: 7/15 loss: 0.7372 Acc: 65.6250% F1: 0.434 Time: 0.94s (0.04s)
Fold 4 train - epoch: 3/10 iter: 8/15 loss: 1.0170 Acc: 59.3750% F1: 0.368 Time: 0.94s (0.05s)
Fold 4 train - epoch: 3/10 iter: 9/15 loss: 0.9294 Acc: 56.2500% F1: 0.383 Time: 0.93s (0.04s)
Fold 4 train - epoch: 3/10 iter: 10/15 loss: 0.9216 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 11/15 loss: 0.7148 Acc: 68.7500% F1: 0.418 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 12/15 loss: 0.8673 Acc: 56.2500% F1: 0.294 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 13/15 loss: 0.8158 Acc: 65.6250% F1: 0.414 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 14/15 loss: 0.1328 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 62.4444% F1: 0.4048 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/10 iter: 0/2 loss: 0.5983 Acc: 78.1250% F1: 0.616 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 3/10 iter: 1/2 loss: 1.8299 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.3523 *
*********************************************************
Performing epoch 4 of 10
Fold 4 train - epoch: 4/10 iter: 0/15 loss: 0.8529 Acc: 59.3750% F1: 0.386 Time: 0.95s (0.00s)
Fold 4 train - epoch: 4/10 iter: 1/15 loss: 0.8614 Acc: 62.5000% F1: 0.435 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 2/15 loss: 0.7819 Acc: 62.5000% F1: 0.431 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 3/15 loss: 0.7820 Acc: 62.5000% F1: 0.415 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 4/15 loss: 0.8093 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 5/15 loss: 0.7316 Acc: 65.6250% F1: 0.439 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 6/15 loss: 0.6007 Acc: 78.1250% F1: 0.491 Time: 0.94s (0.03s)
Fold 4 train - epoch: 4/10 iter: 7/15 loss: 0.6539 Acc: 71.8750% F1: 0.485 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 8/15 loss: 0.8179 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 9/15 loss: 0.8139 Acc: 62.5000% F1: 0.433 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 10/15 loss: 0.8533 Acc: 59.3750% F1: 0.403 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 11/15 loss: 0.6644 Acc: 78.1250% F1: 0.531 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 12/15 loss: 0.7671 Acc: 59.3750% F1: 0.341 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 13/15 loss: 0.7284 Acc: 75.0000% F1: 0.513 Time: 0.94s (0.03s)
Fold 4 train - epoch: 4/10 iter: 14/15 loss: 0.0827 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 66.6667% F1: 0.4501 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7047 Acc: 75.0000% F1: 0.456 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 4/10 iter: 1/2 loss: 1.8246 Acc: 16.6667% F1: 0.118 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.3681 *
*********************************************************
Performing epoch 5 of 10
Fold 4 train - epoch: 5/10 iter: 0/15 loss: 0.7341 Acc: 65.6250% F1: 0.447 Time: 0.94s (0.00s)
Fold 4 train - epoch: 5/10 iter: 1/15 loss: 0.7214 Acc: 65.6250% F1: 0.581 Time: 0.94s (0.03s)
Fold 4 train - epoch: 5/10 iter: 2/15 loss: 0.6596 Acc: 75.0000% F1: 0.525 Time: 0.93s (0.03s)
Fold 4 train - epoch: 5/10 iter: 3/15 loss: 0.7337 Acc: 56.2500% F1: 0.394 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 4/15 loss: 0.7256 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 5/15 loss: 0.5867 Acc: 84.3750% F1: 0.722 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 6/15 loss: 0.4810 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 7/15 loss: 0.5339 Acc: 87.5000% F1: 0.593 Time: 0.94s (0.03s)
Fold 4 train - epoch: 5/10 iter: 8/15 loss: 0.7452 Acc: 68.7500% F1: 0.492 Time: 0.93s (0.04s)
Fold 4 train - epoch: 5/10 iter: 9/15 loss: 0.7508 Acc: 68.7500% F1: 0.483 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 10/15 loss: 0.7129 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.04s)
Fold 4 train - epoch: 5/10 iter: 11/15 loss: 0.4575 Acc: 87.5000% F1: 0.765 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 12/15 loss: 0.6164 Acc: 78.1250% F1: 0.546 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 13/15 loss: 0.6052 Acc: 78.1250% F1: 0.542 Time: 0.93s (0.03s)
Fold 4 train - epoch: 5/10 iter: 14/15 loss: 0.0212 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 5 train Avg acc: 75.5556% F1: 0.5737 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 5/10 iter: 0/2 loss: 0.7357 Acc: 71.8750% F1: 0.447 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 5/10 iter: 1/2 loss: 2.0700 Acc: 22.2222% F1: 0.148 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 5 train-dev Avg acc: 54.0000% F1: 0.3800 *
*********************************************************
Performing epoch 6 of 10
Fold 4 train - epoch: 6/10 iter: 0/15 loss: 0.6053 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.00s)
Fold 4 train - epoch: 6/10 iter: 1/15 loss: 0.5344 Acc: 78.1250% F1: 0.662 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 2/15 loss: 0.4936 Acc: 87.5000% F1: 0.766 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 3/15 loss: 0.5916 Acc: 75.0000% F1: 0.678 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 4/15 loss: 0.5570 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 5/15 loss: 0.4799 Acc: 87.5000% F1: 0.805 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 6/15 loss: 0.3243 Acc: 96.8750% F1: 0.881 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 7/15 loss: 0.3569 Acc: 90.6250% F1: 0.614 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 8/15 loss: 0.4927 Acc: 78.1250% F1: 0.658 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 9/15 loss: 0.5989 Acc: 81.2500% F1: 0.582 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 10/15 loss: 0.5926 Acc: 75.0000% F1: 0.643 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 11/15 loss: 0.4509 Acc: 87.5000% F1: 0.761 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 12/15 loss: 0.5522 Acc: 81.2500% F1: 0.681 Time: 0.94s (0.03s)
Fold 4 train - epoch: 6/10 iter: 13/15 loss: 0.4729 Acc: 87.5000% F1: 0.759 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 14/15 loss: 0.0251 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 6 train Avg acc: 82.6667% F1: 0.6886 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 6/10 iter: 0/2 loss: 0.8928 Acc: 56.2500% F1: 0.363 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 6/10 iter: 1/2 loss: 2.2619 Acc: 22.2222% F1: 0.148 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 6 train-dev Avg acc: 44.0000% F1: 0.3236 *
*********************************************************
Performing epoch 7 of 10
Fold 4 train - epoch: 7/10 iter: 0/15 loss: 0.5815 Acc: 75.0000% F1: 0.533 Time: 0.95s (0.00s)
Fold 4 train - epoch: 7/10 iter: 1/15 loss: 0.3460 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 2/15 loss: 0.3744 Acc: 90.6250% F1: 0.789 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 3/15 loss: 0.4321 Acc: 81.2500% F1: 0.574 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 4/15 loss: 0.4158 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 5/15 loss: 0.2924 Acc: 96.8750% F1: 0.944 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 6/15 loss: 0.2437 Acc: 90.6250% F1: 0.623 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 7/15 loss: 0.3619 Acc: 87.5000% F1: 0.601 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 8/15 loss: 0.3846 Acc: 87.5000% F1: 0.811 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 9/15 loss: 0.2873 Acc: 90.6250% F1: 0.843 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 10/15 loss: 0.3375 Acc: 87.5000% F1: 0.742 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 11/15 loss: 0.2536 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 12/15 loss: 0.2723 Acc: 96.8750% F1: 0.937 Time: 0.94s (0.03s)
Fold 4 train - epoch: 7/10 iter: 13/15 loss: 0.2096 Acc: 96.8750% F1: 0.924 Time: 0.94s (0.03s)
Fold 4 train - epoch: 7/10 iter: 14/15 loss: 0.0097 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 7 train Avg acc: 89.3333% F1: 0.7981 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 7/10 iter: 0/2 loss: 0.9682 Acc: 53.1250% F1: 0.352 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 7/10 iter: 1/2 loss: 2.7766 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 7 train-dev Avg acc: 40.0000% F1: 0.3000 *
*********************************************************
Performing epoch 8 of 10
Fold 4 train - epoch: 8/10 iter: 0/15 loss: 0.2962 Acc: 93.7500% F1: 0.918 Time: 0.95s (0.00s)
Fold 4 train - epoch: 8/10 iter: 1/15 loss: 0.2672 Acc: 90.6250% F1: 0.860 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 2/15 loss: 0.3089 Acc: 87.5000% F1: 0.767 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 3/15 loss: 0.2766 Acc: 87.5000% F1: 0.765 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 4/15 loss: 0.2442 Acc: 87.5000% F1: 0.822 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 5/15 loss: 0.2268 Acc: 93.7500% F1: 0.916 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 6/15 loss: 0.1620 Acc: 96.8750% F1: 0.881 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 7/15 loss: 0.0944 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 8/15 loss: 0.2693 Acc: 87.5000% F1: 0.862 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 9/15 loss: 0.1654 Acc: 93.7500% F1: 0.868 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 10/15 loss: 0.2892 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 11/15 loss: 0.1116 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 12/15 loss: 0.1897 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 13/15 loss: 0.2072 Acc: 90.6250% F1: 0.784 Time: 0.94s (0.03s)
Fold 4 train - epoch: 8/10 iter: 14/15 loss: 0.0040 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 8 train Avg acc: 92.6667% F1: 0.8817 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 8/10 iter: 0/2 loss: 1.1637 Acc: 53.1250% F1: 0.352 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 8/10 iter: 1/2 loss: 3.3557 Acc: 16.6667% F1: 0.163 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 8 train-dev Avg acc: 40.0000% F1: 0.3325 *
*********************************************************
Performing epoch 9 of 10
Fold 4 train - epoch: 9/10 iter: 0/15 loss: 0.2582 Acc: 87.5000% F1: 0.743 Time: 0.95s (0.00s)
Fold 4 train - epoch: 9/10 iter: 1/15 loss: 0.0707 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 2/15 loss: 0.0960 Acc: 96.8750% F1: 0.977 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 3/15 loss: 0.0988 Acc: 96.8750% F1: 0.924 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 4/15 loss: 0.1755 Acc: 93.7500% F1: 0.865 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 5/15 loss: 0.1260 Acc: 96.8750% F1: 0.944 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 6/15 loss: 0.0895 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 7/15 loss: 0.0976 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 8/15 loss: 0.1077 Acc: 96.8750% F1: 0.968 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 9/15 loss: 0.1739 Acc: 90.6250% F1: 0.899 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 10/15 loss: 0.1712 Acc: 90.6250% F1: 0.842 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 11/15 loss: 0.0667 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 12/15 loss: 0.1278 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 13/15 loss: 0.1503 Acc: 96.8750% F1: 0.943 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 14/15 loss: 0.0039 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 9 train Avg acc: 95.5556% F1: 0.9274 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 9/10 iter: 0/2 loss: 1.3589 Acc: 56.2500% F1: 0.363 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 9/10 iter: 1/2 loss: 4.0090 Acc: 16.6667% F1: 0.156 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 9 train-dev Avg acc: 42.0000% F1: 0.3450 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/10 iter: 0/15 loss: 3.8376 Acc: 15.6250% F1: 0.132 Time: 0.97s (0.00s)
Fold 5 train - epoch: 0/10 iter: 1/15 loss: 2.9337 Acc: 21.8750% F1: 0.219 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 2/15 loss: 2.0476 Acc: 34.3750% F1: 0.342 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 3/15 loss: 1.4519 Acc: 37.5000% F1: 0.277 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 4/15 loss: 1.0847 Acc: 59.3750% F1: 0.422 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 5/15 loss: 1.0543 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 6/15 loss: 0.9300 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 7/15 loss: 1.0930 Acc: 46.8750% F1: 0.308 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 8/15 loss: 1.4836 Acc: 37.5000% F1: 0.249 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 9/15 loss: 1.3342 Acc: 31.2500% F1: 0.223 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/10 iter: 10/15 loss: 1.2117 Acc: 37.5000% F1: 0.265 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 11/15 loss: 1.0967 Acc: 43.7500% F1: 0.304 Time: 0.94s (0.02s)
Fold 5 train - epoch: 0/10 iter: 12/15 loss: 1.1139 Acc: 50.0000% F1: 0.357 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 13/15 loss: 1.0465 Acc: 40.6250% F1: 0.284 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 14/15 loss: 0.5165 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.03s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 41.1111% F1: 0.3504 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6370 Acc: 68.7500% F1: 0.330 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 0/10 iter: 1/2 loss: 1.8410 Acc: 16.6667% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 50.0000% F1: 0.3115 *
*********************************************************
Performing epoch 1 of 10
Fold 5 train - epoch: 1/10 iter: 0/15 loss: 1.0021 Acc: 59.3750% F1: 0.400 Time: 0.94s (0.00s)
Fold 5 train - epoch: 1/10 iter: 1/15 loss: 0.9490 Acc: 62.5000% F1: 0.402 Time: 0.92s (0.03s)
Fold 5 train - epoch: 1/10 iter: 2/15 loss: 0.9679 Acc: 50.0000% F1: 0.262 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 3/15 loss: 0.9160 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 4/15 loss: 0.9855 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 5/15 loss: 0.9264 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 6/15 loss: 0.6942 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 7/15 loss: 1.0034 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/10 iter: 8/15 loss: 1.0862 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 9/15 loss: 1.0122 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 10/15 loss: 1.0139 Acc: 40.6250% F1: 0.225 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/10 iter: 11/15 loss: 0.8384 Acc: 65.6250% F1: 0.394 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 12/15 loss: 0.9950 Acc: 53.1250% F1: 0.333 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/10 iter: 13/15 loss: 0.9520 Acc: 46.8750% F1: 0.325 Time: 0.94s (0.02s)
Fold 5 train - epoch: 1/10 iter: 14/15 loss: 0.6874 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 54.4444% F1: 0.3081 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7854 Acc: 65.6250% F1: 0.317 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3701 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.3004 *
*********************************************************
Performing epoch 2 of 10
Fold 5 train - epoch: 2/10 iter: 0/15 loss: 0.8799 Acc: 59.3750% F1: 0.421 Time: 0.94s (0.00s)
Fold 5 train - epoch: 2/10 iter: 1/15 loss: 0.9325 Acc: 59.3750% F1: 0.415 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 2/15 loss: 0.8888 Acc: 59.3750% F1: 0.411 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 3/15 loss: 0.8897 Acc: 40.6250% F1: 0.242 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 4/15 loss: 0.8702 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 5/15 loss: 0.8509 Acc: 62.5000% F1: 0.319 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 6/15 loss: 0.7467 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 7/15 loss: 0.8668 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.04s)
Fold 5 train - epoch: 2/10 iter: 8/15 loss: 1.0032 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 9/15 loss: 1.0098 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 10/15 loss: 0.9636 Acc: 50.0000% F1: 0.267 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 11/15 loss: 0.7754 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 12/15 loss: 0.8911 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 13/15 loss: 0.8824 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 14/15 loss: 0.1905 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 56.6667% F1: 0.3364 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6522 Acc: 78.1250% F1: 0.292 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5861 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.2950 *
*********************************************************
Performing epoch 3 of 10
Fold 5 train - epoch: 3/10 iter: 0/15 loss: 0.8773 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.00s)
Fold 5 train - epoch: 3/10 iter: 1/15 loss: 0.8354 Acc: 62.5000% F1: 0.401 Time: 0.93s (0.04s)
Fold 5 train - epoch: 3/10 iter: 2/15 loss: 0.8293 Acc: 56.2500% F1: 0.375 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 3/15 loss: 0.8465 Acc: 46.8750% F1: 0.272 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 4/15 loss: 0.8786 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 5/15 loss: 0.8224 Acc: 68.7500% F1: 0.451 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 6/15 loss: 0.6833 Acc: 75.0000% F1: 0.461 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 7/15 loss: 0.7946 Acc: 75.0000% F1: 0.501 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 8/15 loss: 0.9958 Acc: 59.3750% F1: 0.368 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 9/15 loss: 0.8841 Acc: 62.5000% F1: 0.433 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 10/15 loss: 0.9174 Acc: 46.8750% F1: 0.280 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 11/15 loss: 0.8062 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 12/15 loss: 0.8513 Acc: 56.2500% F1: 0.326 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 13/15 loss: 0.7712 Acc: 75.0000% F1: 0.501 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 14/15 loss: 0.1502 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 62.4444% F1: 0.4008 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6653 Acc: 75.0000% F1: 0.286 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6261 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.3445 *
*********************************************************
Performing epoch 4 of 10
Fold 5 train - epoch: 4/10 iter: 0/15 loss: 0.8221 Acc: 62.5000% F1: 0.409 Time: 0.94s (0.00s)
Fold 5 train - epoch: 4/10 iter: 1/15 loss: 0.8375 Acc: 59.3750% F1: 0.366 Time: 0.93s (0.04s)
Fold 5 train - epoch: 4/10 iter: 2/15 loss: 0.7380 Acc: 65.6250% F1: 0.451 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 3/15 loss: 0.7653 Acc: 56.2500% F1: 0.356 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 4/15 loss: 0.7885 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 5/15 loss: 0.6967 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 6/15 loss: 0.6213 Acc: 71.8750% F1: 0.426 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 7/15 loss: 0.6877 Acc: 68.7500% F1: 0.459 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 8/15 loss: 0.9244 Acc: 59.3750% F1: 0.411 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 9/15 loss: 0.7683 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 10/15 loss: 0.8431 Acc: 65.6250% F1: 0.458 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 11/15 loss: 0.6793 Acc: 71.8750% F1: 0.475 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 12/15 loss: 0.7874 Acc: 65.6250% F1: 0.458 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 13/15 loss: 0.6732 Acc: 78.1250% F1: 0.537 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 14/15 loss: 0.0891 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 68.0000% F1: 0.4738 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8532 Acc: 56.2500% F1: 0.240 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 4/10 iter: 1/2 loss: 1.5180 Acc: 27.7778% F1: 0.196 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3016 *
*********************************************************
Performing epoch 5 of 10
Fold 5 train - epoch: 5/10 iter: 0/15 loss: 0.7541 Acc: 68.7500% F1: 0.481 Time: 0.94s (0.00s)
Fold 5 train - epoch: 5/10 iter: 1/15 loss: 0.6339 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 2/15 loss: 0.6142 Acc: 84.3750% F1: 0.593 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 3/15 loss: 0.7441 Acc: 62.5000% F1: 0.434 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 4/15 loss: 0.6461 Acc: 68.7500% F1: 0.488 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 5/15 loss: 0.5126 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 6/15 loss: 0.4641 Acc: 81.2500% F1: 0.697 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 7/15 loss: 0.6141 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 8/15 loss: 0.7706 Acc: 62.5000% F1: 0.432 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 9/15 loss: 0.6400 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 10/15 loss: 0.6814 Acc: 71.8750% F1: 0.513 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 11/15 loss: 0.5873 Acc: 75.0000% F1: 0.659 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 12/15 loss: 0.5670 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 13/15 loss: 0.6126 Acc: 87.5000% F1: 0.759 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 14/15 loss: 0.0227 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 5 train Avg acc: 76.0000% F1: 0.5745 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 5/10 iter: 0/2 loss: 1.0405 Acc: 34.3750% F1: 0.171 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 5/10 iter: 1/2 loss: 1.5826 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 5 train-dev Avg acc: 34.0000% F1: 0.2527 *
*********************************************************
Performing epoch 6 of 10
Fold 5 train - epoch: 6/10 iter: 0/15 loss: 0.6908 Acc: 71.8750% F1: 0.520 Time: 0.94s (0.00s)
Fold 5 train - epoch: 6/10 iter: 1/15 loss: 0.5007 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 2/15 loss: 0.3847 Acc: 90.6250% F1: 0.791 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 3/15 loss: 0.6331 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 4/15 loss: 0.6304 Acc: 68.7500% F1: 0.488 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 5/15 loss: 0.5159 Acc: 81.2500% F1: 0.770 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 6/15 loss: 0.3735 Acc: 84.3750% F1: 0.758 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 7/15 loss: 0.5062 Acc: 81.2500% F1: 0.548 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 8/15 loss: 0.6316 Acc: 75.0000% F1: 0.664 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 9/15 loss: 0.5206 Acc: 81.2500% F1: 0.583 Time: 0.94s (0.03s)
Fold 5 train - epoch: 6/10 iter: 10/15 loss: 0.5648 Acc: 81.2500% F1: 0.775 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 11/15 loss: 0.4160 Acc: 84.3750% F1: 0.730 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 12/15 loss: 0.3942 Acc: 90.6250% F1: 0.761 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 13/15 loss: 0.4449 Acc: 87.5000% F1: 0.606 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 14/15 loss: 0.0140 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 6 train Avg acc: 80.6667% F1: 0.6739 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 6/10 iter: 0/2 loss: 1.3615 Acc: 31.2500% F1: 0.193 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 6/10 iter: 1/2 loss: 1.5169 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 6 train-dev Avg acc: 32.0000% F1: 0.2486 *
*********************************************************
Performing epoch 7 of 10
Fold 5 train - epoch: 7/10 iter: 0/15 loss: 0.4496 Acc: 84.3750% F1: 0.714 Time: 0.95s (0.00s)
Fold 5 train - epoch: 7/10 iter: 1/15 loss: 0.4803 Acc: 84.3750% F1: 0.792 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 2/15 loss: 0.3270 Acc: 90.6250% F1: 0.877 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 3/15 loss: 0.4428 Acc: 84.3750% F1: 0.587 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 4/15 loss: 0.3724 Acc: 90.6250% F1: 0.846 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 5/15 loss: 0.3391 Acc: 90.6250% F1: 0.846 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 6/15 loss: 0.2324 Acc: 87.5000% F1: 0.793 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 7/15 loss: 0.4420 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 8/15 loss: 0.4559 Acc: 84.3750% F1: 0.781 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 9/15 loss: 0.4750 Acc: 75.0000% F1: 0.650 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 10/15 loss: 0.5207 Acc: 75.0000% F1: 0.643 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 11/15 loss: 0.2973 Acc: 87.5000% F1: 0.761 Time: 0.94s (0.03s)
Fold 5 train - epoch: 7/10 iter: 12/15 loss: 0.3708 Acc: 93.7500% F1: 0.918 Time: 0.94s (0.03s)
Fold 5 train - epoch: 7/10 iter: 13/15 loss: 0.2852 Acc: 93.7500% F1: 0.816 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 14/15 loss: 0.0085 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 7 train Avg acc: 86.0000% F1: 0.7755 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 7/10 iter: 0/2 loss: 1.5954 Acc: 37.5000% F1: 0.263 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 7/10 iter: 1/2 loss: 1.5936 Acc: 38.8889% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 7 train-dev Avg acc: 38.0000% F1: 0.2943 *
*********************************************************
Performing epoch 8 of 10
Fold 5 train - epoch: 8/10 iter: 0/15 loss: 0.3785 Acc: 84.3750% F1: 0.794 Time: 0.94s (0.00s)
Fold 5 train - epoch: 8/10 iter: 1/15 loss: 0.4446 Acc: 81.2500% F1: 0.770 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 2/15 loss: 0.2924 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 3/15 loss: 0.4124 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 4/15 loss: 0.2930 Acc: 93.7500% F1: 0.869 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 5/15 loss: 0.2961 Acc: 87.5000% F1: 0.846 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 6/15 loss: 0.1960 Acc: 90.6250% F1: 0.825 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 7/15 loss: 0.3722 Acc: 87.5000% F1: 0.592 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 8/15 loss: 0.3049 Acc: 87.5000% F1: 0.840 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 9/15 loss: 0.3261 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 10/15 loss: 0.3805 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 11/15 loss: 0.2958 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 12/15 loss: 0.2626 Acc: 93.7500% F1: 0.945 Time: 0.94s (0.02s)
Fold 5 train - epoch: 8/10 iter: 13/15 loss: 0.3340 Acc: 87.5000% F1: 0.847 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 14/15 loss: 0.0053 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 8 train Avg acc: 88.4444% F1: 0.8374 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 8/10 iter: 0/2 loss: 1.3784 Acc: 40.6250% F1: 0.197 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 8/10 iter: 1/2 loss: 2.4168 Acc: 33.3333% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 8 train-dev Avg acc: 38.0000% F1: 0.3004 *
*********************************************************
Performing epoch 9 of 10
Fold 5 train - epoch: 9/10 iter: 0/15 loss: 0.1831 Acc: 96.8750% F1: 0.943 Time: 0.94s (0.00s)
Fold 5 train - epoch: 9/10 iter: 1/15 loss: 0.1776 Acc: 96.8750% F1: 0.943 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 2/15 loss: 0.2427 Acc: 93.7500% F1: 0.894 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 3/15 loss: 0.3798 Acc: 84.3750% F1: 0.798 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 4/15 loss: 0.3873 Acc: 84.3750% F1: 0.797 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 5/15 loss: 0.4076 Acc: 81.2500% F1: 0.794 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 6/15 loss: 0.2823 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 7/15 loss: 0.2356 Acc: 90.6250% F1: 0.613 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 8/15 loss: 0.2680 Acc: 93.7500% F1: 0.907 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 9/15 loss: 0.1371 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 10/15 loss: 0.2564 Acc: 90.6250% F1: 0.825 Time: 0.94s (0.03s)
Fold 5 train - epoch: 9/10 iter: 11/15 loss: 0.1690 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 12/15 loss: 0.4702 Acc: 84.3750% F1: 0.815 Time: 0.94s (0.03s)
Fold 5 train - epoch: 9/10 iter: 13/15 loss: 0.4741 Acc: 81.2500% F1: 0.788 Time: 0.94s (0.03s)
Fold 5 train - epoch: 9/10 iter: 14/15 loss: 0.0012 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 9 train Avg acc: 90.2222% F1: 0.8672 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 9/10 iter: 0/2 loss: 0.8468 Acc: 81.2500% F1: 0.299 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 9/10 iter: 1/2 loss: 4.4105 Acc: 11.1111% F1: 0.095 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 9 train-dev Avg acc: 56.0000% F1: 0.3083 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/10 iter: 0/15 loss: 3.6992 Acc: 12.5000% F1: 0.078 Time: 0.96s (0.00s)
Fold 6 train - epoch: 0/10 iter: 1/15 loss: 2.9075 Acc: 21.8750% F1: 0.235 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 2/15 loss: 1.9830 Acc: 28.1250% F1: 0.271 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 3/15 loss: 1.3842 Acc: 46.8750% F1: 0.411 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 4/15 loss: 1.3806 Acc: 40.6250% F1: 0.283 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 5/15 loss: 1.1901 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 6/15 loss: 0.9903 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 7/15 loss: 1.1301 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 8/15 loss: 1.4110 Acc: 50.0000% F1: 0.361 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 9/15 loss: 1.1896 Acc: 40.6250% F1: 0.288 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 10/15 loss: 1.1617 Acc: 34.3750% F1: 0.232 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 11/15 loss: 1.0356 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 12/15 loss: 1.0837 Acc: 43.7500% F1: 0.311 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 13/15 loss: 1.0774 Acc: 50.0000% F1: 0.338 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 14/15 loss: 0.6227 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 42.0000% F1: 0.3538 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6489 Acc: 68.7500% F1: 0.487 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 0/10 iter: 1/2 loss: 1.8257 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 52.0000% F1: 0.3312 *
*********************************************************
Performing epoch 1 of 10
Fold 6 train - epoch: 1/10 iter: 0/15 loss: 1.0198 Acc: 56.2500% F1: 0.349 Time: 0.95s (0.00s)
Fold 6 train - epoch: 1/10 iter: 1/15 loss: 0.9309 Acc: 59.3750% F1: 0.341 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 2/15 loss: 0.9658 Acc: 53.1250% F1: 0.306 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 3/15 loss: 0.9264 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 4/15 loss: 1.0107 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 5/15 loss: 0.9624 Acc: 62.5000% F1: 0.320 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 6/15 loss: 0.7132 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 7/15 loss: 0.9559 Acc: 53.1250% F1: 0.296 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/10 iter: 8/15 loss: 1.0531 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 9/15 loss: 1.0851 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 10/15 loss: 0.9670 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 11/15 loss: 0.8012 Acc: 62.5000% F1: 0.380 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 12/15 loss: 0.9302 Acc: 50.0000% F1: 0.317 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/10 iter: 13/15 loss: 0.9349 Acc: 56.2500% F1: 0.382 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 14/15 loss: 0.5688 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 56.2222% F1: 0.3208 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7396 Acc: 59.3750% F1: 0.434 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3946 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 44.0000% F1: 0.2764 *
*********************************************************
Performing epoch 2 of 10
Fold 6 train - epoch: 2/10 iter: 0/15 loss: 0.9238 Acc: 53.1250% F1: 0.352 Time: 0.94s (0.00s)
Fold 6 train - epoch: 2/10 iter: 1/15 loss: 0.8710 Acc: 62.5000% F1: 0.401 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 2/15 loss: 0.8941 Acc: 56.2500% F1: 0.375 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 3/15 loss: 0.9025 Acc: 56.2500% F1: 0.315 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 4/15 loss: 0.8832 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 5/15 loss: 0.8643 Acc: 68.7500% F1: 0.418 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 6/15 loss: 0.7141 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 7/15 loss: 0.8947 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 8/15 loss: 1.0771 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 9/15 loss: 0.9611 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 10/15 loss: 0.9519 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 11/15 loss: 0.8170 Acc: 62.5000% F1: 0.256 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 12/15 loss: 0.9508 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 13/15 loss: 0.8308 Acc: 68.7500% F1: 0.433 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 14/15 loss: 0.2220 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 58.6667% F1: 0.3412 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6731 Acc: 75.0000% F1: 0.526 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5308 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3289 *
*********************************************************
Performing epoch 3 of 10
Fold 6 train - epoch: 3/10 iter: 0/15 loss: 0.8945 Acc: 62.5000% F1: 0.405 Time: 0.94s (0.00s)
Fold 6 train - epoch: 3/10 iter: 1/15 loss: 0.8397 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 2/15 loss: 0.7965 Acc: 71.8750% F1: 0.499 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 3/15 loss: 0.8078 Acc: 65.6250% F1: 0.424 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 4/15 loss: 0.8612 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 5/15 loss: 0.8450 Acc: 68.7500% F1: 0.476 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 6/15 loss: 0.6865 Acc: 75.0000% F1: 0.447 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 7/15 loss: 0.8613 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 8/15 loss: 1.0227 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 9/15 loss: 0.9748 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 10/15 loss: 0.9256 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 11/15 loss: 0.7261 Acc: 71.8750% F1: 0.460 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 12/15 loss: 0.8136 Acc: 62.5000% F1: 0.381 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 13/15 loss: 0.8601 Acc: 53.1250% F1: 0.276 Time: 0.94s (0.03s)
Fold 6 train - epoch: 3/10 iter: 14/15 loss: 0.1476 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 62.4444% F1: 0.3945 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7317 Acc: 59.3750% F1: 0.434 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 3/10 iter: 1/2 loss: 1.5395 Acc: 22.2222% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.2980 *
*********************************************************
Performing epoch 4 of 10
Fold 6 train - epoch: 4/10 iter: 0/15 loss: 0.7976 Acc: 71.8750% F1: 0.500 Time: 0.94s (0.00s)
Fold 6 train - epoch: 4/10 iter: 1/15 loss: 0.7892 Acc: 71.8750% F1: 0.496 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 2/15 loss: 0.7328 Acc: 78.1250% F1: 0.548 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 3/15 loss: 0.7855 Acc: 59.3750% F1: 0.373 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 4/15 loss: 0.7581 Acc: 75.0000% F1: 0.537 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 5/15 loss: 0.7596 Acc: 68.7500% F1: 0.564 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 6/15 loss: 0.6124 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 7/15 loss: 0.7759 Acc: 68.7500% F1: 0.451 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 8/15 loss: 0.9176 Acc: 59.3750% F1: 0.368 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 9/15 loss: 0.8518 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 10/15 loss: 0.8864 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 11/15 loss: 0.6767 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 12/15 loss: 0.7373 Acc: 65.6250% F1: 0.439 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 13/15 loss: 0.7423 Acc: 68.7500% F1: 0.445 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 14/15 loss: 0.0555 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 68.6667% F1: 0.4743 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/10 iter: 0/2 loss: 0.9175 Acc: 46.8750% F1: 0.364 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4225 Acc: 33.3333% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 42.0000% F1: 0.2899 *
*********************************************************
Performing epoch 5 of 10
Fold 6 train - epoch: 5/10 iter: 0/15 loss: 0.7556 Acc: 75.0000% F1: 0.526 Time: 0.95s (0.00s)
Fold 6 train - epoch: 5/10 iter: 1/15 loss: 0.6818 Acc: 84.3750% F1: 0.593 Time: 0.92s (0.02s)
Fold 6 train - epoch: 5/10 iter: 2/15 loss: 0.6219 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 3/15 loss: 0.7328 Acc: 65.6250% F1: 0.452 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 4/15 loss: 0.7061 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 5/15 loss: 0.6733 Acc: 75.0000% F1: 0.646 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 6/15 loss: 0.5140 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 7/15 loss: 0.7051 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 8/15 loss: 0.8212 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 9/15 loss: 0.7410 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 10/15 loss: 0.7155 Acc: 78.1250% F1: 0.556 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 11/15 loss: 0.5949 Acc: 78.1250% F1: 0.689 Time: 0.94s (0.03s)
Fold 6 train - epoch: 5/10 iter: 12/15 loss: 0.6406 Acc: 84.3750% F1: 0.712 Time: 0.94s (0.02s)
Fold 6 train - epoch: 5/10 iter: 13/15 loss: 0.5807 Acc: 84.3750% F1: 0.588 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 14/15 loss: 0.0375 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 5 train Avg acc: 76.2222% F1: 0.5671 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 5/10 iter: 0/2 loss: 1.1494 Acc: 40.6250% F1: 0.327 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 5/10 iter: 1/2 loss: 1.3702 Acc: 33.3333% F1: 0.200 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 5 train-dev Avg acc: 38.0000% F1: 0.2648 *
*********************************************************
Performing epoch 6 of 10
Fold 6 train - epoch: 6/10 iter: 0/15 loss: 0.6464 Acc: 75.0000% F1: 0.528 Time: 0.95s (0.00s)
Fold 6 train - epoch: 6/10 iter: 1/15 loss: 0.6455 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 2/15 loss: 0.5006 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 3/15 loss: 0.6115 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 4/15 loss: 0.5419 Acc: 81.2500% F1: 0.583 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 5/15 loss: 0.4819 Acc: 81.2500% F1: 0.688 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 6/15 loss: 0.4563 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 7/15 loss: 0.5988 Acc: 68.7500% F1: 0.459 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 8/15 loss: 0.6220 Acc: 68.7500% F1: 0.551 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 9/15 loss: 0.5013 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 10/15 loss: 0.5372 Acc: 87.5000% F1: 0.624 Time: 0.94s (0.02s)
Fold 6 train - epoch: 6/10 iter: 11/15 loss: 0.4004 Acc: 87.5000% F1: 0.843 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 12/15 loss: 0.4973 Acc: 87.5000% F1: 0.798 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 13/15 loss: 0.4533 Acc: 84.3750% F1: 0.588 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 14/15 loss: 0.0199 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 6 Epoch 6 train Avg acc: 80.0000% F1: 0.6427 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 6/10 iter: 0/2 loss: 1.3297 Acc: 37.5000% F1: 0.206 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 6/10 iter: 1/2 loss: 1.5737 Acc: 33.3333% F1: 0.211 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 6 train-dev Avg acc: 36.0000% F1: 0.2550 *
*********************************************************
Performing epoch 7 of 10
Fold 6 train - epoch: 7/10 iter: 0/15 loss: 0.4776 Acc: 84.3750% F1: 0.596 Time: 0.94s (0.00s)
Fold 6 train - epoch: 7/10 iter: 1/15 loss: 0.4158 Acc: 90.6250% F1: 0.832 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 2/15 loss: 0.3187 Acc: 93.7500% F1: 0.900 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 3/15 loss: 0.4081 Acc: 87.5000% F1: 0.608 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 4/15 loss: 0.4399 Acc: 87.5000% F1: 0.819 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 5/15 loss: 0.3726 Acc: 87.5000% F1: 0.806 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 6/15 loss: 0.3619 Acc: 87.5000% F1: 0.591 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 7/15 loss: 0.4530 Acc: 78.1250% F1: 0.525 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 8/15 loss: 0.5248 Acc: 81.2500% F1: 0.730 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 9/15 loss: 0.3454 Acc: 90.6250% F1: 0.770 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 10/15 loss: 0.5143 Acc: 84.3750% F1: 0.774 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 11/15 loss: 0.3102 Acc: 93.7500% F1: 0.898 Time: 0.94s (0.02s)
Fold 6 train - epoch: 7/10 iter: 12/15 loss: 0.3330 Acc: 90.6250% F1: 0.817 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 13/15 loss: 0.3490 Acc: 90.6250% F1: 0.877 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 14/15 loss: 0.0152 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 7 train Avg acc: 87.7778% F1: 0.7798 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 7/10 iter: 0/2 loss: 1.7095 Acc: 37.5000% F1: 0.210 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 7/10 iter: 1/2 loss: 1.6252 Acc: 38.8889% F1: 0.306 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 7 train-dev Avg acc: 38.0000% F1: 0.3305 *
*********************************************************
Performing epoch 8 of 10
Fold 6 train - epoch: 8/10 iter: 0/15 loss: 0.3628 Acc: 90.6250% F1: 0.836 Time: 0.95s (0.00s)
Fold 6 train - epoch: 8/10 iter: 1/15 loss: 0.3090 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 2/15 loss: 0.2311 Acc: 93.7500% F1: 0.954 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 3/15 loss: 0.3135 Acc: 87.5000% F1: 0.614 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 4/15 loss: 0.2609 Acc: 90.6250% F1: 0.892 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 5/15 loss: 0.2609 Acc: 93.7500% F1: 0.917 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 6/15 loss: 0.1932 Acc: 93.7500% F1: 0.652 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 7/15 loss: 0.3625 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 8/15 loss: 0.2984 Acc: 87.5000% F1: 0.820 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 9/15 loss: 0.4225 Acc: 84.3750% F1: 0.606 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 10/15 loss: 0.6912 Acc: 71.8750% F1: 0.623 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 11/15 loss: 0.2278 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 12/15 loss: 0.2950 Acc: 90.6250% F1: 0.767 Time: 0.94s (0.02s)
Fold 6 train - epoch: 8/10 iter: 13/15 loss: 0.1742 Acc: 93.7500% F1: 0.816 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 14/15 loss: 0.0050 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 8 train Avg acc: 89.5556% F1: 0.8081 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 8/10 iter: 0/2 loss: 2.3323 Acc: 37.5000% F1: 0.208 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 8/10 iter: 1/2 loss: 1.4815 Acc: 44.4444% F1: 0.232 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 8 train-dev Avg acc: 40.0000% F1: 0.2892 *
*********************************************************
Performing epoch 9 of 10
Fold 6 train - epoch: 9/10 iter: 0/15 loss: 0.3726 Acc: 90.6250% F1: 0.836 Time: 0.95s (0.00s)
Fold 6 train - epoch: 9/10 iter: 1/15 loss: 0.4298 Acc: 87.5000% F1: 0.867 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 2/15 loss: 0.3244 Acc: 90.6250% F1: 0.931 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 3/15 loss: 0.6825 Acc: 75.0000% F1: 0.678 Time: 0.92s (0.02s)
Fold 6 train - epoch: 9/10 iter: 4/15 loss: 0.2782 Acc: 87.5000% F1: 0.878 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 5/15 loss: 0.3983 Acc: 84.3750% F1: 0.852 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 6/15 loss: 0.2293 Acc: 87.5000% F1: 0.591 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 7/15 loss: 0.2299 Acc: 93.7500% F1: 0.857 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 8/15 loss: 0.1693 Acc: 96.8750% F1: 0.968 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 9/15 loss: 0.2721 Acc: 84.3750% F1: 0.844 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 10/15 loss: 0.7401 Acc: 68.7500% F1: 0.657 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 11/15 loss: 0.5730 Acc: 78.1250% F1: 0.655 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 12/15 loss: 0.9328 Acc: 62.5000% F1: 0.444 Time: 0.94s (0.02s)
Fold 6 train - epoch: 9/10 iter: 13/15 loss: 0.6885 Acc: 75.0000% F1: 0.717 Time: 0.94s (0.02s)
Fold 6 train - epoch: 9/10 iter: 14/15 loss: 0.0040 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 9 train Avg acc: 83.1111% F1: 0.8116 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 9/10 iter: 0/2 loss: 1.3285 Acc: 53.1250% F1: 0.283 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 9/10 iter: 1/2 loss: 3.4394 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 9 train-dev Avg acc: 40.0000% F1: 0.2708 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/10 iter: 0/15 loss: 3.9517 Acc: 9.3750% F1: 0.057 Time: 0.96s (0.00s)
Fold 7 train - epoch: 0/10 iter: 1/15 loss: 2.9138 Acc: 18.7500% F1: 0.183 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 2/15 loss: 1.8356 Acc: 28.1250% F1: 0.278 Time: 0.93s (0.04s)
Fold 7 train - epoch: 0/10 iter: 3/15 loss: 1.4501 Acc: 34.3750% F1: 0.229 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 4/15 loss: 1.4275 Acc: 43.7500% F1: 0.309 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 5/15 loss: 1.2425 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 6/15 loss: 0.9895 Acc: 65.6250% F1: 0.322 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 7/15 loss: 1.0488 Acc: 50.0000% F1: 0.302 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 8/15 loss: 1.5261 Acc: 37.5000% F1: 0.249 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 9/15 loss: 1.3205 Acc: 28.1250% F1: 0.187 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 10/15 loss: 1.3243 Acc: 34.3750% F1: 0.231 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 11/15 loss: 1.0484 Acc: 34.3750% F1: 0.237 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 12/15 loss: 1.1070 Acc: 50.0000% F1: 0.356 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 13/15 loss: 1.0787 Acc: 50.0000% F1: 0.349 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 14/15 loss: 0.6225 Acc: 50.0000% F1: 0.333 Time: 0.09s (0.02s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 38.6667% F1: 0.3250 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6113 Acc: 65.6250% F1: 0.396 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 0/10 iter: 1/2 loss: 1.7016 Acc: 33.3333% F1: 0.211 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.3521 *
*********************************************************
Performing epoch 1 of 10
Fold 7 train - epoch: 1/10 iter: 0/15 loss: 0.9663 Acc: 50.0000% F1: 0.317 Time: 0.94s (0.00s)
Fold 7 train - epoch: 1/10 iter: 1/15 loss: 0.9160 Acc: 62.5000% F1: 0.402 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 2/15 loss: 0.9521 Acc: 53.1250% F1: 0.328 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 3/15 loss: 0.8861 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 4/15 loss: 1.0272 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 5/15 loss: 0.9553 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 6/15 loss: 0.7529 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 7/15 loss: 0.9848 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 8/15 loss: 1.1051 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 9/15 loss: 1.0517 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 10/15 loss: 1.0929 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 11/15 loss: 0.7990 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 12/15 loss: 0.9444 Acc: 59.3750% F1: 0.367 Time: 0.94s (0.02s)
Fold 7 train - epoch: 1/10 iter: 13/15 loss: 0.9684 Acc: 40.6250% F1: 0.247 Time: 0.93s (0.04s)
Fold 7 train - epoch: 1/10 iter: 14/15 loss: 0.3642 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 54.8889% F1: 0.2967 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7399 Acc: 56.2500% F1: 0.360 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3589 Acc: 33.3333% F1: 0.211 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.3179 *
*********************************************************
Performing epoch 2 of 10
Fold 7 train - epoch: 2/10 iter: 0/15 loss: 0.8719 Acc: 68.7500% F1: 0.475 Time: 0.94s (0.00s)
Fold 7 train - epoch: 2/10 iter: 1/15 loss: 0.9631 Acc: 56.2500% F1: 0.379 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 2/15 loss: 0.8467 Acc: 68.7500% F1: 0.482 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 3/15 loss: 0.8989 Acc: 43.7500% F1: 0.287 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 4/15 loss: 0.9062 Acc: 56.2500% F1: 0.381 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 5/15 loss: 0.9082 Acc: 59.3750% F1: 0.389 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 6/15 loss: 0.7543 Acc: 68.7500% F1: 0.378 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 7/15 loss: 0.8678 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 8/15 loss: 1.0080 Acc: 56.2500% F1: 0.308 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 9/15 loss: 0.9522 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 10/15 loss: 1.0070 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 11/15 loss: 0.6986 Acc: 68.7500% F1: 0.343 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 12/15 loss: 0.9196 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 13/15 loss: 0.9212 Acc: 62.5000% F1: 0.351 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 14/15 loss: 0.2593 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 58.8889% F1: 0.3701 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6024 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 2/10 iter: 1/2 loss: 1.6728 Acc: 0.0000% F1: 0.000 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 3 of 10
Fold 7 train - epoch: 3/10 iter: 0/15 loss: 0.8361 Acc: 65.6250% F1: 0.409 Time: 0.95s (0.00s)
Fold 7 train - epoch: 3/10 iter: 1/15 loss: 0.8346 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 2/15 loss: 0.8401 Acc: 65.6250% F1: 0.433 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 3/15 loss: 0.8010 Acc: 56.2500% F1: 0.284 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 4/15 loss: 0.8485 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 5/15 loss: 0.7957 Acc: 71.8750% F1: 0.489 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 6/15 loss: 0.7059 Acc: 71.8750% F1: 0.396 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 7/15 loss: 0.7834 Acc: 65.6250% F1: 0.426 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 8/15 loss: 1.0321 Acc: 53.1250% F1: 0.292 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 9/15 loss: 0.9569 Acc: 59.3750% F1: 0.389 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 10/15 loss: 0.9537 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 11/15 loss: 0.7270 Acc: 75.0000% F1: 0.462 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 12/15 loss: 0.8658 Acc: 59.3750% F1: 0.367 Time: 0.94s (0.02s)
Fold 7 train - epoch: 3/10 iter: 13/15 loss: 0.8263 Acc: 65.6250% F1: 0.429 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 14/15 loss: 0.1796 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 63.5556% F1: 0.3953 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6247 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6605 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 52.0000% F1: 0.3022 *
*********************************************************
Performing epoch 4 of 10
Fold 7 train - epoch: 4/10 iter: 0/15 loss: 0.8213 Acc: 65.6250% F1: 0.430 Time: 0.94s (0.00s)
Fold 7 train - epoch: 4/10 iter: 1/15 loss: 0.7721 Acc: 75.0000% F1: 0.514 Time: 0.93s (0.04s)
Fold 7 train - epoch: 4/10 iter: 2/15 loss: 0.7040 Acc: 65.6250% F1: 0.451 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 3/15 loss: 0.7954 Acc: 53.1250% F1: 0.322 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 4/15 loss: 0.7768 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 5/15 loss: 0.7721 Acc: 71.8750% F1: 0.590 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 6/15 loss: 0.6718 Acc: 71.8750% F1: 0.391 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 7/15 loss: 0.7078 Acc: 65.6250% F1: 0.434 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 8/15 loss: 0.9125 Acc: 56.2500% F1: 0.345 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 9/15 loss: 0.8243 Acc: 65.6250% F1: 0.462 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 10/15 loss: 0.8990 Acc: 65.6250% F1: 0.458 Time: 0.94s (0.03s)
Fold 7 train - epoch: 4/10 iter: 11/15 loss: 0.6347 Acc: 75.0000% F1: 0.498 Time: 0.94s (0.02s)
Fold 7 train - epoch: 4/10 iter: 12/15 loss: 0.8029 Acc: 62.5000% F1: 0.417 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 13/15 loss: 0.7450 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 14/15 loss: 0.0682 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 67.5556% F1: 0.4632 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7331 Acc: 65.6250% F1: 0.469 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 4/10 iter: 1/2 loss: 1.6144 Acc: 27.7778% F1: 0.175 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 52.0000% F1: 0.3401 *
*********************************************************
Performing epoch 5 of 10
Fold 7 train - epoch: 5/10 iter: 0/15 loss: 0.7640 Acc: 68.7500% F1: 0.455 Time: 0.95s (0.00s)
Fold 7 train - epoch: 5/10 iter: 1/15 loss: 0.7307 Acc: 75.0000% F1: 0.513 Time: 0.92s (0.03s)
Fold 7 train - epoch: 5/10 iter: 2/15 loss: 0.6188 Acc: 75.0000% F1: 0.527 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 3/15 loss: 0.7111 Acc: 65.6250% F1: 0.451 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 4/15 loss: 0.6968 Acc: 78.1250% F1: 0.553 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 5/15 loss: 0.6079 Acc: 78.1250% F1: 0.662 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 6/15 loss: 0.6023 Acc: 78.1250% F1: 0.506 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 7/15 loss: 0.6173 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 8/15 loss: 0.8395 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 9/15 loss: 0.7480 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 10/15 loss: 0.7552 Acc: 65.6250% F1: 0.462 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 11/15 loss: 0.5603 Acc: 75.0000% F1: 0.648 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 12/15 loss: 0.6595 Acc: 75.0000% F1: 0.526 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 13/15 loss: 0.6206 Acc: 81.2500% F1: 0.566 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 14/15 loss: 0.0398 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 5 train Avg acc: 74.2222% F1: 0.5383 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8300 Acc: 62.5000% F1: 0.451 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 5/10 iter: 1/2 loss: 1.7864 Acc: 33.3333% F1: 0.200 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 5 train-dev Avg acc: 52.0000% F1: 0.3484 *
*********************************************************
Performing epoch 6 of 10
Fold 7 train - epoch: 6/10 iter: 0/15 loss: 0.5418 Acc: 84.3750% F1: 0.600 Time: 0.95s (0.00s)
Fold 7 train - epoch: 6/10 iter: 1/15 loss: 0.5651 Acc: 81.2500% F1: 0.562 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 2/15 loss: 0.4646 Acc: 87.5000% F1: 0.768 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 3/15 loss: 0.5128 Acc: 84.3750% F1: 0.586 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 4/15 loss: 0.5967 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 5/15 loss: 0.4597 Acc: 87.5000% F1: 0.749 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 6/15 loss: 0.3622 Acc: 90.6250% F1: 0.623 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 7/15 loss: 0.4420 Acc: 90.6250% F1: 0.614 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 8/15 loss: 0.7123 Acc: 68.7500% F1: 0.544 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 9/15 loss: 0.6101 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 10/15 loss: 0.4941 Acc: 84.3750% F1: 0.599 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 11/15 loss: 0.4894 Acc: 84.3750% F1: 0.722 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 12/15 loss: 0.5083 Acc: 84.3750% F1: 0.792 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 13/15 loss: 0.3996 Acc: 90.6250% F1: 0.781 Time: 0.94s (0.03s)
Fold 7 train - epoch: 6/10 iter: 14/15 loss: 0.0213 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 6 train Avg acc: 84.0000% F1: 0.6657 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9736 Acc: 62.5000% F1: 0.451 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 6/10 iter: 1/2 loss: 2.2901 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 6 train-dev Avg acc: 48.0000% F1: 0.3085 *
*********************************************************
Performing epoch 7 of 10
Fold 7 train - epoch: 7/10 iter: 0/15 loss: 0.4265 Acc: 84.3750% F1: 0.593 Time: 0.94s (0.00s)
Fold 7 train - epoch: 7/10 iter: 1/15 loss: 0.4779 Acc: 81.2500% F1: 0.676 Time: 0.92s (0.03s)
Fold 7 train - epoch: 7/10 iter: 2/15 loss: 0.3002 Acc: 90.6250% F1: 0.791 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 3/15 loss: 0.4052 Acc: 90.6250% F1: 0.792 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 4/15 loss: 0.4646 Acc: 78.1250% F1: 0.675 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 5/15 loss: 0.3993 Acc: 87.5000% F1: 0.749 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 6/15 loss: 0.3862 Acc: 84.3750% F1: 0.565 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 7/15 loss: 0.2545 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 8/15 loss: 0.4814 Acc: 75.0000% F1: 0.626 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 9/15 loss: 0.3527 Acc: 87.5000% F1: 0.624 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 10/15 loss: 0.3798 Acc: 90.6250% F1: 0.844 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 11/15 loss: 0.2689 Acc: 93.7500% F1: 0.896 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 12/15 loss: 0.4046 Acc: 84.3750% F1: 0.714 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 13/15 loss: 0.3471 Acc: 87.5000% F1: 0.765 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 14/15 loss: 0.0064 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 7 train Avg acc: 86.6667% F1: 0.7357 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 7/10 iter: 0/2 loss: 1.2283 Acc: 46.8750% F1: 0.252 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 7/10 iter: 1/2 loss: 2.5339 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 7 train-dev Avg acc: 42.0000% F1: 0.3062 *
*********************************************************
Performing epoch 8 of 10
Fold 7 train - epoch: 8/10 iter: 0/15 loss: 0.2149 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.00s)
Fold 7 train - epoch: 8/10 iter: 1/15 loss: 0.2740 Acc: 93.7500% F1: 0.864 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 2/15 loss: 0.2076 Acc: 96.8750% F1: 0.977 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 3/15 loss: 0.3454 Acc: 87.5000% F1: 0.763 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 4/15 loss: 0.1507 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 5/15 loss: 0.2791 Acc: 93.7500% F1: 0.872 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 6/15 loss: 0.2358 Acc: 90.6250% F1: 0.623 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 7/15 loss: 0.1650 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 8/15 loss: 0.2638 Acc: 93.7500% F1: 0.907 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 9/15 loss: 0.1788 Acc: 93.7500% F1: 0.866 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 10/15 loss: 0.3549 Acc: 84.3750% F1: 0.707 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 11/15 loss: 0.1955 Acc: 93.7500% F1: 0.896 Time: 0.94s (0.02s)
Fold 7 train - epoch: 8/10 iter: 12/15 loss: 0.2625 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 13/15 loss: 0.2143 Acc: 93.7500% F1: 0.806 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 14/15 loss: 0.0034 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 8 train Avg acc: 93.5556% F1: 0.8768 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 8/10 iter: 0/2 loss: 1.5798 Acc: 37.5000% F1: 0.214 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 8/10 iter: 1/2 loss: 2.7395 Acc: 38.8889% F1: 0.324 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 8 train-dev Avg acc: 38.0000% F1: 0.3530 *
*********************************************************
Performing epoch 9 of 10
Fold 7 train - epoch: 9/10 iter: 0/15 loss: 0.1287 Acc: 96.8750% F1: 0.943 Time: 0.94s (0.00s)
Fold 7 train - epoch: 9/10 iter: 1/15 loss: 0.1814 Acc: 96.8750% F1: 0.937 Time: 0.92s (0.03s)
Fold 7 train - epoch: 9/10 iter: 2/15 loss: 0.1466 Acc: 93.7500% F1: 0.954 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 3/15 loss: 0.2808 Acc: 90.6250% F1: 0.931 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 4/15 loss: 0.1769 Acc: 90.6250% F1: 0.892 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 5/15 loss: 0.1773 Acc: 96.8750% F1: 0.944 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 6/15 loss: 0.1551 Acc: 93.7500% F1: 0.652 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 7/15 loss: 0.1458 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 8/15 loss: 0.1480 Acc: 93.7500% F1: 0.926 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 9/15 loss: 0.1138 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 10/15 loss: 0.3591 Acc: 90.6250% F1: 0.894 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 11/15 loss: 0.2605 Acc: 87.5000% F1: 0.827 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 12/15 loss: 0.1190 Acc: 96.8750% F1: 0.943 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 13/15 loss: 0.1651 Acc: 96.8750% F1: 0.924 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 14/15 loss: 0.0020 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 9 train Avg acc: 94.4444% F1: 0.9220 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 9/10 iter: 0/2 loss: 2.1993 Acc: 34.3750% F1: 0.226 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 9/10 iter: 1/2 loss: 2.5806 Acc: 44.4444% F1: 0.333 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 9 train-dev Avg acc: 38.0000% F1: 0.3529 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/10 iter: 0/15 loss: 4.0820 Acc: 12.5000% F1: 0.076 Time: 0.97s (0.00s)
Fold 8 train - epoch: 0/10 iter: 1/15 loss: 2.8856 Acc: 18.7500% F1: 0.169 Time: 0.92s (0.03s)
Fold 8 train - epoch: 0/10 iter: 2/15 loss: 1.8314 Acc: 28.1250% F1: 0.284 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 3/15 loss: 1.2976 Acc: 50.0000% F1: 0.436 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 4/15 loss: 1.3479 Acc: 50.0000% F1: 0.348 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 5/15 loss: 1.3953 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 6/15 loss: 0.8585 Acc: 68.7500% F1: 0.378 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 7/15 loss: 1.0551 Acc: 62.5000% F1: 0.407 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 8/15 loss: 1.5044 Acc: 34.3750% F1: 0.211 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 9/15 loss: 1.2453 Acc: 28.1250% F1: 0.189 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 10/15 loss: 1.2981 Acc: 34.3750% F1: 0.220 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 11/15 loss: 1.1294 Acc: 40.6250% F1: 0.284 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 12/15 loss: 1.1206 Acc: 50.0000% F1: 0.352 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 13/15 loss: 1.1865 Acc: 37.5000% F1: 0.256 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 14/15 loss: 0.6567 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 40.8889% F1: 0.3530 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6419 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/10 iter: 1/2 loss: 1.7187 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 50.0000% F1: 0.2746 *
*********************************************************
Performing epoch 1 of 10
Fold 8 train - epoch: 1/10 iter: 0/15 loss: 0.9704 Acc: 59.3750% F1: 0.367 Time: 0.94s (0.00s)
Fold 8 train - epoch: 1/10 iter: 1/15 loss: 0.8645 Acc: 65.6250% F1: 0.424 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 2/15 loss: 0.9708 Acc: 53.1250% F1: 0.306 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 3/15 loss: 0.9870 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 4/15 loss: 1.0922 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 5/15 loss: 1.0112 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 6/15 loss: 0.7455 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 7/15 loss: 1.0142 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 8/15 loss: 1.1542 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 9/15 loss: 1.0636 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 10/15 loss: 1.0310 Acc: 40.6250% F1: 0.193 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 11/15 loss: 0.7950 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 12/15 loss: 0.9848 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 13/15 loss: 0.9525 Acc: 46.8750% F1: 0.310 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 14/15 loss: 0.4508 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 55.1111% F1: 0.2978 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7994 Acc: 59.3750% F1: 0.434 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 1/10 iter: 1/2 loss: 1.2517 Acc: 16.6667% F1: 0.111 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 44.0000% F1: 0.2775 *
*********************************************************
Performing epoch 2 of 10
Fold 8 train - epoch: 2/10 iter: 0/15 loss: 0.9154 Acc: 68.7500% F1: 0.485 Time: 0.94s (0.00s)
Fold 8 train - epoch: 2/10 iter: 1/15 loss: 0.9289 Acc: 53.1250% F1: 0.375 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 2/15 loss: 0.8658 Acc: 62.5000% F1: 0.437 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 3/15 loss: 0.8999 Acc: 46.8750% F1: 0.304 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 4/15 loss: 0.8696 Acc: 68.7500% F1: 0.484 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 5/15 loss: 0.9319 Acc: 65.6250% F1: 0.375 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 6/15 loss: 0.7629 Acc: 65.6250% F1: 0.322 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 7/15 loss: 0.8484 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 8/15 loss: 0.9928 Acc: 50.0000% F1: 0.227 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 9/15 loss: 1.0168 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 10/15 loss: 1.0575 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 11/15 loss: 0.7708 Acc: 65.6250% F1: 0.264 Time: 0.94s (0.03s)
Fold 8 train - epoch: 2/10 iter: 12/15 loss: 0.9181 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 13/15 loss: 0.8859 Acc: 59.3750% F1: 0.300 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 14/15 loss: 0.1912 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 57.3333% F1: 0.3513 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6271 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5239 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.2918 *
*********************************************************
Performing epoch 3 of 10
Fold 8 train - epoch: 3/10 iter: 0/15 loss: 0.8642 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.00s)
Fold 8 train - epoch: 3/10 iter: 1/15 loss: 0.8442 Acc: 62.5000% F1: 0.356 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 2/15 loss: 0.8354 Acc: 62.5000% F1: 0.424 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 3/15 loss: 0.8205 Acc: 62.5000% F1: 0.345 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 4/15 loss: 0.8289 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 5/15 loss: 0.8359 Acc: 62.5000% F1: 0.389 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 6/15 loss: 0.7180 Acc: 78.1250% F1: 0.498 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 7/15 loss: 0.7812 Acc: 65.6250% F1: 0.426 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 8/15 loss: 1.0036 Acc: 50.0000% F1: 0.311 Time: 0.93s (0.04s)
Fold 8 train - epoch: 3/10 iter: 9/15 loss: 0.9785 Acc: 56.2500% F1: 0.352 Time: 0.94s (0.03s)
Fold 8 train - epoch: 3/10 iter: 10/15 loss: 0.9755 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 11/15 loss: 0.7147 Acc: 78.1250% F1: 0.522 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 12/15 loss: 0.8765 Acc: 53.1250% F1: 0.284 Time: 0.94s (0.03s)
Fold 8 train - epoch: 3/10 iter: 13/15 loss: 0.9089 Acc: 59.3750% F1: 0.359 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 14/15 loss: 0.1995 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 62.2222% F1: 0.3870 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6411 Acc: 81.2500% F1: 0.571 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 3/10 iter: 1/2 loss: 1.4600 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.3224 *
*********************************************************
Performing epoch 4 of 10
Fold 8 train - epoch: 4/10 iter: 0/15 loss: 0.8716 Acc: 62.5000% F1: 0.385 Time: 0.95s (0.00s)
Fold 8 train - epoch: 4/10 iter: 1/15 loss: 0.7986 Acc: 68.7500% F1: 0.444 Time: 0.92s (0.03s)
Fold 8 train - epoch: 4/10 iter: 2/15 loss: 0.7853 Acc: 59.3750% F1: 0.404 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 3/15 loss: 0.7411 Acc: 68.7500% F1: 0.443 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 4/15 loss: 0.8061 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 5/15 loss: 0.8546 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 6/15 loss: 0.6489 Acc: 68.7500% F1: 0.378 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 7/15 loss: 0.7673 Acc: 68.7500% F1: 0.453 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 8/15 loss: 0.9336 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 9/15 loss: 0.8097 Acc: 62.5000% F1: 0.444 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 10/15 loss: 0.8674 Acc: 56.2500% F1: 0.383 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 11/15 loss: 0.6383 Acc: 81.2500% F1: 0.562 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 12/15 loss: 0.7762 Acc: 68.7500% F1: 0.472 Time: 0.94s (0.02s)
Fold 8 train - epoch: 4/10 iter: 13/15 loss: 0.8138 Acc: 65.6250% F1: 0.412 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 14/15 loss: 0.0872 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 66.4444% F1: 0.4407 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7021 Acc: 78.1250% F1: 0.374 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 4/10 iter: 1/2 loss: 1.3184 Acc: 22.2222% F1: 0.220 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 58.0000% F1: 0.4297 *
*********************************************************
Performing epoch 5 of 10
Fold 8 train - epoch: 5/10 iter: 0/15 loss: 0.7561 Acc: 62.5000% F1: 0.417 Time: 0.95s (0.00s)
Fold 8 train - epoch: 5/10 iter: 1/15 loss: 0.7049 Acc: 75.0000% F1: 0.517 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 2/15 loss: 0.6941 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 3/15 loss: 0.6962 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 4/15 loss: 0.7565 Acc: 68.7500% F1: 0.488 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 5/15 loss: 0.7101 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 6/15 loss: 0.5639 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 7/15 loss: 0.6687 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 8/15 loss: 0.8188 Acc: 71.8750% F1: 0.526 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 9/15 loss: 0.6977 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 10/15 loss: 0.7372 Acc: 75.0000% F1: 0.650 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 11/15 loss: 0.5291 Acc: 81.2500% F1: 0.796 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 12/15 loss: 0.6421 Acc: 81.2500% F1: 0.565 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 13/15 loss: 0.5815 Acc: 84.3750% F1: 0.728 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 14/15 loss: 0.0219 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 5 train Avg acc: 74.4444% F1: 0.5602 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8238 Acc: 62.5000% F1: 0.305 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 5/10 iter: 1/2 loss: 1.1671 Acc: 44.4444% F1: 0.317 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 5 train-dev Avg acc: 56.0000% F1: 0.4586 *
*********************************************************
Performing epoch 6 of 10
Fold 8 train - epoch: 6/10 iter: 0/15 loss: 0.6008 Acc: 87.5000% F1: 0.614 Time: 0.94s (0.00s)
Fold 8 train - epoch: 6/10 iter: 1/15 loss: 0.6307 Acc: 68.7500% F1: 0.484 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 2/15 loss: 0.5563 Acc: 78.1250% F1: 0.547 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 3/15 loss: 0.6552 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 4/15 loss: 0.6051 Acc: 81.2500% F1: 0.584 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 5/15 loss: 0.5860 Acc: 81.2500% F1: 0.580 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 6/15 loss: 0.4226 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.03s)
Fold 8 train - epoch: 6/10 iter: 7/15 loss: 0.5382 Acc: 78.1250% F1: 0.844 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 8/15 loss: 0.6637 Acc: 68.7500% F1: 0.502 Time: 0.93s (0.03s)
Fold 8 train - epoch: 6/10 iter: 9/15 loss: 0.5869 Acc: 78.1250% F1: 0.557 Time: 0.94s (0.02s)
Fold 8 train - epoch: 6/10 iter: 10/15 loss: 0.5696 Acc: 78.1250% F1: 0.676 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 11/15 loss: 0.4443 Acc: 84.3750% F1: 0.824 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 12/15 loss: 0.5737 Acc: 84.3750% F1: 0.590 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 13/15 loss: 0.4895 Acc: 84.3750% F1: 0.734 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 14/15 loss: 0.0551 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 6 train Avg acc: 79.3333% F1: 0.6115 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 6/10 iter: 0/2 loss: 0.8937 Acc: 68.7500% F1: 0.428 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 6/10 iter: 1/2 loss: 1.1771 Acc: 44.4444% F1: 0.317 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 6 train-dev Avg acc: 60.0000% F1: 0.4945 *
*********************************************************
Performing epoch 7 of 10
Fold 8 train - epoch: 7/10 iter: 0/15 loss: 0.4383 Acc: 84.3750% F1: 0.708 Time: 0.94s (0.00s)
Fold 8 train - epoch: 7/10 iter: 1/15 loss: 0.4988 Acc: 81.2500% F1: 0.680 Time: 0.92s (0.03s)
Fold 8 train - epoch: 7/10 iter: 2/15 loss: 0.3610 Acc: 90.6250% F1: 0.931 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 3/15 loss: 0.3905 Acc: 93.7500% F1: 0.816 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 4/15 loss: 0.3840 Acc: 93.7500% F1: 0.869 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 5/15 loss: 0.4211 Acc: 84.3750% F1: 0.709 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 6/15 loss: 0.3066 Acc: 87.5000% F1: 0.579 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 7/15 loss: 0.3451 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 8/15 loss: 0.4332 Acc: 81.2500% F1: 0.730 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 9/15 loss: 0.4117 Acc: 81.2500% F1: 0.701 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 10/15 loss: 0.3213 Acc: 90.6250% F1: 0.844 Time: 0.94s (0.03s)
Fold 8 train - epoch: 7/10 iter: 11/15 loss: 0.3638 Acc: 87.5000% F1: 0.843 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 12/15 loss: 0.4263 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 13/15 loss: 0.3015 Acc: 90.6250% F1: 0.763 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 14/15 loss: 0.0047 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 7 train Avg acc: 88.0000% F1: 0.7825 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 7/10 iter: 0/2 loss: 1.1340 Acc: 56.2500% F1: 0.379 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 7/10 iter: 1/2 loss: 1.1105 Acc: 61.1111% F1: 0.465 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 7 train-dev Avg acc: 58.0000% F1: 0.5466 *
*********************************************************
Performing epoch 8 of 10
Fold 8 train - epoch: 8/10 iter: 0/15 loss: 0.3992 Acc: 81.2500% F1: 0.574 Time: 0.95s (0.00s)
Fold 8 train - epoch: 8/10 iter: 1/15 loss: 0.2852 Acc: 90.6250% F1: 0.840 Time: 0.93s (0.04s)
Fold 8 train - epoch: 8/10 iter: 2/15 loss: 0.2875 Acc: 93.7500% F1: 0.954 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 3/15 loss: 0.3234 Acc: 84.3750% F1: 0.741 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 4/15 loss: 0.2066 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 5/15 loss: 0.3853 Acc: 81.2500% F1: 0.743 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 6/15 loss: 0.2478 Acc: 90.6250% F1: 0.623 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 7/15 loss: 0.2912 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 8/15 loss: 0.3247 Acc: 84.3750% F1: 0.790 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 9/15 loss: 0.2925 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 10/15 loss: 0.2886 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 11/15 loss: 0.2349 Acc: 90.6250% F1: 0.843 Time: 0.94s (0.02s)
Fold 8 train - epoch: 8/10 iter: 12/15 loss: 0.1940 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 13/15 loss: 0.2590 Acc: 87.5000% F1: 0.729 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 14/15 loss: 0.0038 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 8 train Avg acc: 89.7778% F1: 0.8300 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 8/10 iter: 0/2 loss: 1.8277 Acc: 40.6250% F1: 0.273 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 8/10 iter: 1/2 loss: 1.0638 Acc: 66.6667% F1: 0.433 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 8 train-dev Avg acc: 50.0000% F1: 0.4617 *
*********************************************************
Performing epoch 9 of 10
Fold 8 train - epoch: 9/10 iter: 0/15 loss: 0.3283 Acc: 90.6250% F1: 0.836 Time: 0.94s (0.00s)
Fold 8 train - epoch: 9/10 iter: 1/15 loss: 0.3501 Acc: 84.3750% F1: 0.792 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 2/15 loss: 0.1575 Acc: 96.8750% F1: 0.977 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 3/15 loss: 0.1634 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 4/15 loss: 0.1106 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 5/15 loss: 0.2667 Acc: 93.7500% F1: 0.872 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 6/15 loss: 0.1766 Acc: 87.5000% F1: 0.591 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 7/15 loss: 0.1786 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 8/15 loss: 0.1996 Acc: 96.8750% F1: 0.965 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 9/15 loss: 0.3400 Acc: 84.3750% F1: 0.798 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 10/15 loss: 0.4342 Acc: 81.2500% F1: 0.817 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 11/15 loss: 0.0978 Acc: 96.8750% F1: 0.926 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 12/15 loss: 0.1826 Acc: 90.6250% F1: 0.830 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 13/15 loss: 0.1155 Acc: 96.8750% F1: 0.924 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 14/15 loss: 0.0045 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 8 Epoch 9 train Avg acc: 92.2222% F1: 0.8874 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 9/10 iter: 0/2 loss: 2.4323 Acc: 25.0000% F1: 0.197 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 9/10 iter: 1/2 loss: 0.9424 Acc: 77.7778% F1: 0.544 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 9 train-dev Avg acc: 44.0000% F1: 0.4156 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/10 iter: 0/15 loss: 3.9400 Acc: 12.5000% F1: 0.076 Time: 0.97s (0.00s)
Fold 9 train - epoch: 0/10 iter: 1/15 loss: 2.8075 Acc: 21.8750% F1: 0.212 Time: 0.93s (0.03s)
Fold 9 train - epoch: 0/10 iter: 2/15 loss: 1.7014 Acc: 40.6250% F1: 0.395 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 3/15 loss: 1.2034 Acc: 50.0000% F1: 0.337 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 4/15 loss: 1.6375 Acc: 34.3750% F1: 0.236 Time: 0.93s (0.03s)
Fold 9 train - epoch: 0/10 iter: 5/15 loss: 1.4850 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 6/15 loss: 0.8934 Acc: 62.5000% F1: 0.305 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 7/15 loss: 1.1374 Acc: 40.6250% F1: 0.242 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 8/15 loss: 1.4378 Acc: 37.5000% F1: 0.247 Time: 0.93s (0.03s)
Fold 9 train - epoch: 0/10 iter: 9/15 loss: 1.4152 Acc: 25.0000% F1: 0.179 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 10/15 loss: 1.3391 Acc: 40.6250% F1: 0.283 Time: 0.93s (0.03s)
Fold 9 train - epoch: 0/10 iter: 11/15 loss: 1.0102 Acc: 40.6250% F1: 0.279 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 12/15 loss: 1.1665 Acc: 40.6250% F1: 0.290 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 13/15 loss: 1.1532 Acc: 40.6250% F1: 0.274 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 14/15 loss: 0.5213 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 38.6667% F1: 0.3302 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/10 iter: 0/2 loss: 0.7253 Acc: 62.5000% F1: 0.451 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 0/10 iter: 1/2 loss: 1.1832 Acc: 38.8889% F1: 0.274 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.3142 *
*********************************************************
Performing epoch 1 of 10
Fold 9 train - epoch: 1/10 iter: 0/15 loss: 1.0587 Acc: 56.2500% F1: 0.387 Time: 0.94s (0.00s)
Fold 9 train - epoch: 1/10 iter: 1/15 loss: 0.9695 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 2/15 loss: 0.9762 Acc: 59.3750% F1: 0.389 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 3/15 loss: 0.9142 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 4/15 loss: 1.1063 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.04s)
Fold 9 train - epoch: 1/10 iter: 5/15 loss: 0.9602 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 6/15 loss: 0.7436 Acc: 68.7500% F1: 0.330 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 7/15 loss: 1.0124 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 8/15 loss: 1.0747 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.04s)
Fold 9 train - epoch: 1/10 iter: 9/15 loss: 1.0910 Acc: 46.8750% F1: 0.255 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 10/15 loss: 1.0316 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 11/15 loss: 0.7429 Acc: 68.7500% F1: 0.378 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 12/15 loss: 0.9499 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 13/15 loss: 0.9344 Acc: 56.2500% F1: 0.360 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 14/15 loss: 0.4924 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 55.5556% F1: 0.3059 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7811 Acc: 59.3750% F1: 0.479 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 1/10 iter: 1/2 loss: 1.0625 Acc: 44.4444% F1: 0.315 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3366 *
*********************************************************
Performing epoch 2 of 10
Fold 9 train - epoch: 2/10 iter: 0/15 loss: 0.9107 Acc: 56.2500% F1: 0.386 Time: 0.95s (0.00s)
Fold 9 train - epoch: 2/10 iter: 1/15 loss: 0.8728 Acc: 65.6250% F1: 0.455 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 2/15 loss: 0.8987 Acc: 62.5000% F1: 0.440 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 3/15 loss: 0.8798 Acc: 65.6250% F1: 0.434 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 4/15 loss: 0.9487 Acc: 46.8750% F1: 0.285 Time: 0.93s (0.04s)
Fold 9 train - epoch: 2/10 iter: 5/15 loss: 0.8716 Acc: 68.7500% F1: 0.451 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 6/15 loss: 0.7761 Acc: 65.6250% F1: 0.354 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 7/15 loss: 0.8784 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 8/15 loss: 1.0239 Acc: 53.1250% F1: 0.292 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 9/15 loss: 1.0692 Acc: 46.8750% F1: 0.255 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 10/15 loss: 1.0230 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 11/15 loss: 0.7092 Acc: 75.0000% F1: 0.415 Time: 0.94s (0.02s)
Fold 9 train - epoch: 2/10 iter: 12/15 loss: 0.8943 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.02s)
Fold 9 train - epoch: 2/10 iter: 13/15 loss: 0.8687 Acc: 65.6250% F1: 0.398 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 14/15 loss: 0.2686 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 59.7778% F1: 0.3749 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6978 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 2/10 iter: 1/2 loss: 1.1994 Acc: 27.7778% F1: 0.178 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.2980 *
*********************************************************
Performing epoch 3 of 10
Fold 9 train - epoch: 3/10 iter: 0/15 loss: 0.9001 Acc: 62.5000% F1: 0.391 Time: 0.94s (0.00s)
Fold 9 train - epoch: 3/10 iter: 1/15 loss: 0.8559 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 2/15 loss: 0.8264 Acc: 59.3750% F1: 0.403 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 3/15 loss: 0.7918 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 4/15 loss: 0.9137 Acc: 53.1250% F1: 0.283 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 5/15 loss: 0.8097 Acc: 75.0000% F1: 0.513 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 6/15 loss: 0.6763 Acc: 71.8750% F1: 0.415 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 7/15 loss: 0.8425 Acc: 56.2500% F1: 0.355 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 8/15 loss: 1.0306 Acc: 53.1250% F1: 0.292 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 9/15 loss: 0.9330 Acc: 65.6250% F1: 0.466 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 10/15 loss: 0.9386 Acc: 53.1250% F1: 0.342 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 11/15 loss: 0.6959 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 12/15 loss: 0.8273 Acc: 68.7500% F1: 0.450 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 13/15 loss: 0.8515 Acc: 62.5000% F1: 0.351 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 14/15 loss: 0.1258 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 63.5556% F1: 0.4005 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7272 Acc: 62.5000% F1: 0.500 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 3/10 iter: 1/2 loss: 1.1861 Acc: 38.8889% F1: 0.274 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.3264 *
*********************************************************
Performing epoch 4 of 10
Fold 9 train - epoch: 4/10 iter: 0/15 loss: 0.8564 Acc: 62.5000% F1: 0.436 Time: 0.94s (0.00s)
Fold 9 train - epoch: 4/10 iter: 1/15 loss: 0.8201 Acc: 59.3750% F1: 0.366 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 2/15 loss: 0.7801 Acc: 65.6250% F1: 0.466 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 3/15 loss: 0.7247 Acc: 59.3750% F1: 0.355 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 4/15 loss: 0.8689 Acc: 59.3750% F1: 0.395 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 5/15 loss: 0.7452 Acc: 71.8750% F1: 0.481 Time: 0.93s (0.04s)
Fold 9 train - epoch: 4/10 iter: 6/15 loss: 0.5374 Acc: 81.2500% F1: 0.498 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 7/15 loss: 0.7947 Acc: 65.6250% F1: 0.442 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 8/15 loss: 0.8458 Acc: 68.7500% F1: 0.492 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 9/15 loss: 0.9716 Acc: 50.0000% F1: 0.301 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 10/15 loss: 0.9543 Acc: 56.2500% F1: 0.376 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 11/15 loss: 0.6321 Acc: 71.8750% F1: 0.463 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 12/15 loss: 0.7990 Acc: 65.6250% F1: 0.453 Time: 0.94s (0.02s)
Fold 9 train - epoch: 4/10 iter: 13/15 loss: 0.7068 Acc: 75.0000% F1: 0.501 Time: 0.94s (0.02s)
Fold 9 train - epoch: 4/10 iter: 14/15 loss: 0.1062 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 65.3333% F1: 0.4338 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8082 Acc: 59.3750% F1: 0.513 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 4/10 iter: 1/2 loss: 1.2663 Acc: 44.4444% F1: 0.315 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.3450 *
*********************************************************
Performing epoch 5 of 10
Fold 9 train - epoch: 5/10 iter: 0/15 loss: 0.7042 Acc: 75.0000% F1: 0.630 Time: 0.94s (0.00s)
Fold 9 train - epoch: 5/10 iter: 1/15 loss: 0.6991 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 2/15 loss: 0.5764 Acc: 81.2500% F1: 0.582 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 3/15 loss: 0.7396 Acc: 71.8750% F1: 0.474 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 4/15 loss: 0.7784 Acc: 62.5000% F1: 0.439 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 5/15 loss: 0.6235 Acc: 75.0000% F1: 0.504 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 6/15 loss: 0.4624 Acc: 81.2500% F1: 0.525 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 7/15 loss: 0.6738 Acc: 75.0000% F1: 0.514 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 8/15 loss: 0.7889 Acc: 65.6250% F1: 0.468 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 9/15 loss: 0.7593 Acc: 68.7500% F1: 0.485 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 10/15 loss: 0.7023 Acc: 71.8750% F1: 0.616 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 11/15 loss: 0.4769 Acc: 81.2500% F1: 0.547 Time: 0.94s (0.02s)
Fold 9 train - epoch: 5/10 iter: 12/15 loss: 0.6532 Acc: 78.1250% F1: 0.538 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 13/15 loss: 0.6264 Acc: 75.0000% F1: 0.511 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 14/15 loss: 0.0391 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 5 train Avg acc: 73.7778% F1: 0.5337 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8895 Acc: 53.1250% F1: 0.317 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 5/10 iter: 1/2 loss: 1.3965 Acc: 44.4444% F1: 0.324 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 5 train-dev Avg acc: 50.0000% F1: 0.3296 *
*********************************************************
Performing epoch 6 of 10
Fold 9 train - epoch: 6/10 iter: 0/15 loss: 0.6381 Acc: 75.0000% F1: 0.636 Time: 0.95s (0.00s)
Fold 9 train - epoch: 6/10 iter: 1/15 loss: 0.6251 Acc: 75.0000% F1: 0.532 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 2/15 loss: 0.4326 Acc: 87.5000% F1: 0.822 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 3/15 loss: 0.6453 Acc: 78.1250% F1: 0.559 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 4/15 loss: 0.6076 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 5/15 loss: 0.4870 Acc: 87.5000% F1: 0.739 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 6/15 loss: 0.4404 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 7/15 loss: 0.5427 Acc: 87.5000% F1: 0.601 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 8/15 loss: 0.6081 Acc: 78.1250% F1: 0.689 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 9/15 loss: 0.5491 Acc: 78.1250% F1: 0.568 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 10/15 loss: 0.4932 Acc: 84.3750% F1: 0.823 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 11/15 loss: 0.3562 Acc: 84.3750% F1: 0.782 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 12/15 loss: 0.5594 Acc: 84.3750% F1: 0.593 Time: 0.94s (0.02s)
Fold 9 train - epoch: 6/10 iter: 13/15 loss: 0.4476 Acc: 81.2500% F1: 0.700 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 14/15 loss: 0.0072 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 6 train Avg acc: 81.3333% F1: 0.6719 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 6/10 iter: 0/2 loss: 1.0294 Acc: 50.0000% F1: 0.302 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 6/10 iter: 1/2 loss: 1.6195 Acc: 38.8889% F1: 0.281 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 6 train-dev Avg acc: 46.0000% F1: 0.3000 *
*********************************************************
Performing epoch 7 of 10
Fold 9 train - epoch: 7/10 iter: 0/15 loss: 0.3930 Acc: 87.5000% F1: 0.735 Time: 0.94s (0.00s)
Fold 9 train - epoch: 7/10 iter: 1/15 loss: 0.4545 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 2/15 loss: 0.2815 Acc: 93.7500% F1: 0.952 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 3/15 loss: 0.3568 Acc: 87.5000% F1: 0.768 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 4/15 loss: 0.4496 Acc: 87.5000% F1: 0.798 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 5/15 loss: 0.4435 Acc: 81.2500% F1: 0.682 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 6/15 loss: 0.2731 Acc: 93.7500% F1: 0.633 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 7/15 loss: 0.4119 Acc: 84.3750% F1: 0.830 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 8/15 loss: 0.4990 Acc: 71.8750% F1: 0.623 Time: 0.96s (0.09s)
Fold 9 train - epoch: 7/10 iter: 9/15 loss: 0.4737 Acc: 78.1250% F1: 0.727 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 10/15 loss: 0.4306 Acc: 84.3750% F1: 0.823 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 11/15 loss: 0.2604 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 12/15 loss: 0.4039 Acc: 84.3750% F1: 0.844 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 13/15 loss: 0.3209 Acc: 93.7500% F1: 0.810 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 14/15 loss: 0.0082 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 7 train Avg acc: 86.0000% F1: 0.7868 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 7/10 iter: 0/2 loss: 1.2327 Acc: 46.8750% F1: 0.290 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 7/10 iter: 1/2 loss: 1.7346 Acc: 50.0000% F1: 0.363 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 7 train-dev Avg acc: 48.0000% F1: 0.3275 *
*********************************************************
Performing epoch 8 of 10
Fold 9 train - epoch: 8/10 iter: 0/15 loss: 0.3681 Acc: 81.2500% F1: 0.736 Time: 0.95s (0.00s)
Fold 9 train - epoch: 8/10 iter: 1/15 loss: 0.2540 Acc: 93.7500% F1: 0.870 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 2/15 loss: 0.2203 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 3/15 loss: 0.2725 Acc: 87.5000% F1: 0.595 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 4/15 loss: 0.2739 Acc: 93.7500% F1: 0.928 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 5/15 loss: 0.3403 Acc: 93.7500% F1: 0.872 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 6/15 loss: 0.2270 Acc: 90.6250% F1: 0.604 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 7/15 loss: 0.2579 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 8/15 loss: 0.2043 Acc: 96.8750% F1: 0.955 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 9/15 loss: 0.2415 Acc: 90.6250% F1: 0.823 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 10/15 loss: 0.2401 Acc: 93.7500% F1: 0.893 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 11/15 loss: 0.1141 Acc: 96.8750% F1: 0.926 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 12/15 loss: 0.2338 Acc: 90.6250% F1: 0.767 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 13/15 loss: 0.1330 Acc: 96.8750% F1: 0.924 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 14/15 loss: 0.0026 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 8 train Avg acc: 92.6667% F1: 0.8725 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 8/10 iter: 0/2 loss: 1.5039 Acc: 43.7500% F1: 0.288 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 8/10 iter: 1/2 loss: 2.0021 Acc: 44.4444% F1: 0.324 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 8 train-dev Avg acc: 44.0000% F1: 0.3082 *
*********************************************************
Performing epoch 9 of 10
Fold 9 train - epoch: 9/10 iter: 0/15 loss: 0.1701 Acc: 100.0000% F1: 1.000 Time: 0.94s (0.00s)
Fold 9 train - epoch: 9/10 iter: 1/15 loss: 0.2286 Acc: 93.7500% F1: 0.950 Time: 0.93s (0.03s)
Fold 9 train - epoch: 9/10 iter: 2/15 loss: 0.1329 Acc: 93.7500% F1: 0.917 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 3/15 loss: 0.2523 Acc: 90.6250% F1: 0.846 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 4/15 loss: 0.2415 Acc: 90.6250% F1: 0.903 Time: 0.93s (0.03s)
Fold 9 train - epoch: 9/10 iter: 5/15 loss: 0.1886 Acc: 93.7500% F1: 0.899 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 6/15 loss: 0.0943 Acc: 96.8750% F1: 0.659 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 7/15 loss: 0.2139 Acc: 93.7500% F1: 0.956 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 8/15 loss: 0.3580 Acc: 87.5000% F1: 0.862 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 9/15 loss: 0.1995 Acc: 90.6250% F1: 0.872 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 10/15 loss: 0.3825 Acc: 81.2500% F1: 0.794 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 11/15 loss: 0.1089 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 12/15 loss: 0.0953 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 13/15 loss: 0.1323 Acc: 93.7500% F1: 0.897 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 14/15 loss: 0.0040 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 9 Epoch 9 train Avg acc: 93.3333% F1: 0.9158 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 9/10 iter: 0/2 loss: 2.2097 Acc: 34.3750% F1: 0.244 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 9/10 iter: 1/2 loss: 1.5961 Acc: 55.5556% F1: 0.364 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 9 train-dev Avg acc: 42.0000% F1: 0.2933 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 52.4000% F1: 0.3044
	Epoch 1 Accuracy: 50.2000% F1: 0.3065
	Epoch 2 Accuracy: 54.4000% F1: 0.2887
	Epoch 3 Accuracy: 53.0000% F1: 0.3122
	Epoch 4 Accuracy: 51.6000% F1: 0.3433
	Epoch 5 Accuracy: 47.6000% F1: 0.3467
	Epoch 6 Accuracy: 46.0000% F1: 0.3340
	Epoch 7 Accuracy: 43.4000% F1: 0.3444
	Epoch 8 Accuracy: 42.4000% F1: 0.3380
	Epoch 9 Accuracy: 43.2000% F1: 0.3329
all done :)
************************************
** MODEL TIME ID: 20220225-100222 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/10 iter: 0/15 loss: 3.8440 Acc: 15.6250% F1: 0.116 Time: 0.96s (0.00s)
Fold 0 train - epoch: 0/10 iter: 1/15 loss: 2.8002 Acc: 31.2500% F1: 0.323 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 2/15 loss: 2.3566 Acc: 9.3750% F1: 0.095 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 3/15 loss: 1.5689 Acc: 34.3750% F1: 0.252 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 4/15 loss: 1.5058 Acc: 40.6250% F1: 0.266 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 5/15 loss: 1.1865 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 6/15 loss: 1.0270 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 7/15 loss: 0.9905 Acc: 50.0000% F1: 0.316 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 8/15 loss: 1.0821 Acc: 59.3750% F1: 0.422 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 9/15 loss: 1.1763 Acc: 43.7500% F1: 0.310 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 10/15 loss: 1.1568 Acc: 46.8750% F1: 0.325 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 11/15 loss: 1.0721 Acc: 40.6250% F1: 0.280 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 12/15 loss: 1.2408 Acc: 40.6250% F1: 0.290 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 13/15 loss: 0.9517 Acc: 50.0000% F1: 0.345 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 14/15 loss: 0.5774 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 40.4444% F1: 0.3385 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5799 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 0/10 iter: 1/2 loss: 2.0642 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2918 *
*********************************************************
Performing epoch 1 of 10
Fold 0 train - epoch: 1/10 iter: 0/15 loss: 1.0011 Acc: 46.8750% F1: 0.256 Time: 0.94s (0.00s)
Fold 0 train - epoch: 1/10 iter: 1/15 loss: 0.9158 Acc: 65.6250% F1: 0.404 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 2/15 loss: 1.1555 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 3/15 loss: 1.0548 Acc: 59.3750% F1: 0.248 Time: 0.92s (0.02s)
Fold 0 train - epoch: 1/10 iter: 4/15 loss: 1.0338 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 5/15 loss: 1.0361 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 6/15 loss: 0.7347 Acc: 65.6250% F1: 0.264 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 7/15 loss: 0.9245 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 8/15 loss: 0.9291 Acc: 59.3750% F1: 0.319 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 9/15 loss: 0.9461 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 10/15 loss: 1.0386 Acc: 43.7500% F1: 0.265 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 11/15 loss: 0.8720 Acc: 59.3750% F1: 0.381 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 12/15 loss: 1.0294 Acc: 50.0000% F1: 0.356 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 13/15 loss: 0.8916 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 14/15 loss: 0.5002 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 53.5556% F1: 0.3039 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7895 Acc: 62.5000% F1: 0.500 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3586 Acc: 16.6667% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 46.0000% F1: 0.2980 *
*********************************************************
Performing epoch 2 of 10
Fold 0 train - epoch: 2/10 iter: 0/15 loss: 0.9508 Acc: 46.8750% F1: 0.324 Time: 0.95s (0.00s)
Fold 0 train - epoch: 2/10 iter: 1/15 loss: 0.8979 Acc: 62.5000% F1: 0.440 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 2/15 loss: 0.9127 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 3/15 loss: 0.8744 Acc: 50.0000% F1: 0.227 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 4/15 loss: 0.9361 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 5/15 loss: 0.8677 Acc: 62.5000% F1: 0.361 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 6/15 loss: 0.6908 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 7/15 loss: 0.8600 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/10 iter: 8/15 loss: 0.9057 Acc: 59.3750% F1: 0.319 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 9/15 loss: 0.9630 Acc: 46.8750% F1: 0.217 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 10/15 loss: 1.0533 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 11/15 loss: 0.8498 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 12/15 loss: 0.9314 Acc: 59.3750% F1: 0.349 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 13/15 loss: 0.8544 Acc: 59.3750% F1: 0.300 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/10 iter: 14/15 loss: 0.1044 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 56.8889% F1: 0.3318 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6833 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5723 Acc: 16.6667% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 56.0000% F1: 0.3224 *
*********************************************************
Performing epoch 3 of 10
Fold 0 train - epoch: 3/10 iter: 0/15 loss: 0.8959 Acc: 59.3750% F1: 0.342 Time: 0.94s (0.00s)
Fold 0 train - epoch: 3/10 iter: 1/15 loss: 0.8236 Acc: 71.8750% F1: 0.500 Time: 0.92s (0.03s)
Fold 0 train - epoch: 3/10 iter: 2/15 loss: 0.8189 Acc: 68.7500% F1: 0.478 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 3/15 loss: 0.8084 Acc: 65.6250% F1: 0.406 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 4/15 loss: 0.8122 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 5/15 loss: 0.8003 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 6/15 loss: 0.7445 Acc: 62.5000% F1: 0.405 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 7/15 loss: 0.7916 Acc: 59.3750% F1: 0.388 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 8/15 loss: 0.8435 Acc: 68.7500% F1: 0.496 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 9/15 loss: 0.8748 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 10/15 loss: 0.9437 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 11/15 loss: 0.7666 Acc: 65.6250% F1: 0.394 Time: 0.94s (0.02s)
Fold 0 train - epoch: 3/10 iter: 12/15 loss: 0.8795 Acc: 53.1250% F1: 0.317 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 13/15 loss: 0.8193 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 14/15 loss: 0.0198 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 64.2222% F1: 0.4247 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7264 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6251 Acc: 16.6667% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.3123 *
*********************************************************
Performing epoch 4 of 10
Fold 0 train - epoch: 4/10 iter: 0/15 loss: 0.8451 Acc: 59.3750% F1: 0.384 Time: 0.95s (0.00s)
Fold 0 train - epoch: 4/10 iter: 1/15 loss: 0.7648 Acc: 75.0000% F1: 0.522 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 2/15 loss: 0.7884 Acc: 71.8750% F1: 0.497 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 3/15 loss: 0.7982 Acc: 65.6250% F1: 0.421 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 4/15 loss: 0.7618 Acc: 78.1250% F1: 0.553 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 5/15 loss: 0.7516 Acc: 68.7500% F1: 0.475 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 6/15 loss: 0.6365 Acc: 78.1250% F1: 0.525 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 7/15 loss: 0.7336 Acc: 65.6250% F1: 0.442 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 8/15 loss: 0.8018 Acc: 68.7500% F1: 0.472 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 9/15 loss: 0.8194 Acc: 65.6250% F1: 0.450 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 10/15 loss: 0.7624 Acc: 62.5000% F1: 0.417 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 11/15 loss: 0.7150 Acc: 75.0000% F1: 0.501 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 12/15 loss: 0.8528 Acc: 59.3750% F1: 0.380 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 13/15 loss: 0.6384 Acc: 78.1250% F1: 0.537 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 14/15 loss: 0.0138 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 69.5556% F1: 0.4729 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8427 Acc: 71.8750% F1: 0.566 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 4/10 iter: 1/2 loss: 1.5671 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 52.0000% F1: 0.3303 *
*********************************************************
Performing epoch 5 of 10
Fold 0 train - epoch: 5/10 iter: 0/15 loss: 0.8082 Acc: 65.6250% F1: 0.444 Time: 0.94s (0.00s)
Fold 0 train - epoch: 5/10 iter: 1/15 loss: 0.6682 Acc: 81.2500% F1: 0.576 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 2/15 loss: 0.6366 Acc: 81.2500% F1: 0.568 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 3/15 loss: 0.6355 Acc: 78.1250% F1: 0.519 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 4/15 loss: 0.6860 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 5/15 loss: 0.6314 Acc: 65.6250% F1: 0.424 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 6/15 loss: 0.5514 Acc: 78.1250% F1: 0.531 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 7/15 loss: 0.6622 Acc: 68.7500% F1: 0.457 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 8/15 loss: 0.6384 Acc: 75.0000% F1: 0.562 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 9/15 loss: 0.6585 Acc: 78.1250% F1: 0.552 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 10/15 loss: 0.6537 Acc: 75.0000% F1: 0.650 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 11/15 loss: 0.6675 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 12/15 loss: 0.7726 Acc: 71.8750% F1: 0.616 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 13/15 loss: 0.5084 Acc: 78.1250% F1: 0.542 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 14/15 loss: 0.0023 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 5 train Avg acc: 74.2222% F1: 0.5392 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8987 Acc: 65.6250% F1: 0.354 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 5/10 iter: 1/2 loss: 1.7827 Acc: 16.6667% F1: 0.125 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3122 *
*********************************************************
Performing epoch 6 of 10
Fold 0 train - epoch: 6/10 iter: 0/15 loss: 0.7249 Acc: 68.7500% F1: 0.479 Time: 0.94s (0.00s)
Fold 0 train - epoch: 6/10 iter: 1/15 loss: 0.4903 Acc: 81.2500% F1: 0.576 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 2/15 loss: 0.5512 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 3/15 loss: 0.4916 Acc: 81.2500% F1: 0.552 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 4/15 loss: 0.5951 Acc: 78.1250% F1: 0.549 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 5/15 loss: 0.5288 Acc: 81.2500% F1: 0.749 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 6/15 loss: 0.4397 Acc: 87.5000% F1: 0.793 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 7/15 loss: 0.4538 Acc: 81.2500% F1: 0.548 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 8/15 loss: 0.4510 Acc: 87.5000% F1: 0.813 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 9/15 loss: 0.5234 Acc: 81.2500% F1: 0.576 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 10/15 loss: 0.4649 Acc: 87.5000% F1: 0.819 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 11/15 loss: 0.4458 Acc: 87.5000% F1: 0.617 Time: 0.94s (0.02s)
Fold 0 train - epoch: 6/10 iter: 12/15 loss: 0.6477 Acc: 71.8750% F1: 0.595 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 13/15 loss: 0.3465 Acc: 87.5000% F1: 0.731 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 14/15 loss: 0.0007 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 6 train Avg acc: 81.7778% F1: 0.6719 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 6/10 iter: 0/2 loss: 1.1138 Acc: 46.8750% F1: 0.280 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 6/10 iter: 1/2 loss: 1.8200 Acc: 27.7778% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 6 train-dev Avg acc: 40.0000% F1: 0.2896 *
*********************************************************
Performing epoch 7 of 10
Fold 0 train - epoch: 7/10 iter: 0/15 loss: 0.4687 Acc: 75.0000% F1: 0.648 Time: 0.95s (0.00s)
Fold 0 train - epoch: 7/10 iter: 1/15 loss: 0.4195 Acc: 84.3750% F1: 0.699 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 2/15 loss: 0.3922 Acc: 87.5000% F1: 0.768 Time: 0.93s (0.02s)
Fold 0 train - epoch: 7/10 iter: 3/15 loss: 0.3748 Acc: 87.5000% F1: 0.602 Time: 0.93s (0.02s)
Fold 0 train - epoch: 7/10 iter: 4/15 loss: 0.4307 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.02s)
Fold 0 train - epoch: 7/10 iter: 5/15 loss: 0.4088 Acc: 87.5000% F1: 0.797 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 6/15 loss: 0.2756 Acc: 93.7500% F1: 0.854 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 7/15 loss: 0.3410 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 8/15 loss: 0.3589 Acc: 87.5000% F1: 0.813 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 9/15 loss: 0.3484 Acc: 87.5000% F1: 0.731 Time: 0.94s (0.03s)
Fold 0 train - epoch: 7/10 iter: 10/15 loss: 0.3260 Acc: 93.7500% F1: 0.917 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 11/15 loss: 0.2673 Acc: 93.7500% F1: 0.808 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 12/15 loss: 0.5360 Acc: 75.0000% F1: 0.627 Time: 0.94s (0.03s)
Fold 0 train - epoch: 7/10 iter: 13/15 loss: 0.1944 Acc: 93.7500% F1: 0.907 Time: 0.94s (0.03s)
Fold 0 train - epoch: 7/10 iter: 14/15 loss: 0.0003 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 7 train Avg acc: 87.1111% F1: 0.7639 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 7/10 iter: 0/2 loss: 1.1355 Acc: 62.5000% F1: 0.320 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 7/10 iter: 1/2 loss: 2.7671 Acc: 22.2222% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 7 train-dev Avg acc: 48.0000% F1: 0.3261 *
*********************************************************
Performing epoch 8 of 10
Fold 0 train - epoch: 8/10 iter: 0/15 loss: 0.3141 Acc: 87.5000% F1: 0.730 Time: 0.94s (0.00s)
Fold 0 train - epoch: 8/10 iter: 1/15 loss: 0.2726 Acc: 87.5000% F1: 0.614 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 2/15 loss: 0.1905 Acc: 93.7500% F1: 0.899 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 3/15 loss: 0.1936 Acc: 90.6250% F1: 0.615 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 4/15 loss: 0.2902 Acc: 87.5000% F1: 0.820 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 5/15 loss: 0.2634 Acc: 90.6250% F1: 0.853 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 6/15 loss: 0.1444 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 7/15 loss: 0.2128 Acc: 96.8750% F1: 0.878 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 8/15 loss: 0.2476 Acc: 90.6250% F1: 0.901 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 9/15 loss: 0.3020 Acc: 87.5000% F1: 0.747 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 10/15 loss: 0.2048 Acc: 90.6250% F1: 0.893 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 11/15 loss: 0.2004 Acc: 93.7500% F1: 0.808 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 12/15 loss: 0.1840 Acc: 93.7500% F1: 0.906 Time: 0.93s (0.02s)
Fold 0 train - epoch: 8/10 iter: 13/15 loss: 0.2530 Acc: 90.6250% F1: 0.871 Time: 0.94s (0.02s)
Fold 0 train - epoch: 8/10 iter: 14/15 loss: 0.0001 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 8 train Avg acc: 91.5556% F1: 0.8478 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 8/10 iter: 0/2 loss: 1.6331 Acc: 43.7500% F1: 0.289 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 8/10 iter: 1/2 loss: 2.3701 Acc: 27.7778% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 8 train-dev Avg acc: 38.0000% F1: 0.3035 *
*********************************************************
Performing epoch 9 of 10
Fold 0 train - epoch: 9/10 iter: 0/15 loss: 0.1357 Acc: 96.8750% F1: 0.943 Time: 0.94s (0.00s)
Fold 0 train - epoch: 9/10 iter: 1/15 loss: 0.1695 Acc: 93.7500% F1: 0.913 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 2/15 loss: 0.1275 Acc: 96.8750% F1: 0.977 Time: 0.93s (0.04s)
Fold 0 train - epoch: 9/10 iter: 3/15 loss: 0.0969 Acc: 96.8750% F1: 0.917 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 4/15 loss: 0.2239 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 5/15 loss: 0.1352 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 6/15 loss: 0.1293 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 7/15 loss: 0.1099 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 8/15 loss: 0.1444 Acc: 93.7500% F1: 0.905 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 9/15 loss: 0.1249 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 10/15 loss: 0.1358 Acc: 93.7500% F1: 0.868 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 11/15 loss: 0.1226 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 12/15 loss: 0.2321 Acc: 90.6250% F1: 0.881 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 13/15 loss: 0.1877 Acc: 90.6250% F1: 0.822 Time: 0.93s (0.02s)
Fold 0 train - epoch: 9/10 iter: 14/15 loss: 0.0000 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 9 train Avg acc: 95.5556% F1: 0.9218 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 9/10 iter: 0/2 loss: 2.2045 Acc: 28.1250% F1: 0.204 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 9/10 iter: 1/2 loss: 2.3573 Acc: 38.8889% F1: 0.212 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 9 train-dev Avg acc: 32.0000% F1: 0.2630 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/10 iter: 0/15 loss: 3.8491 Acc: 9.3750% F1: 0.061 Time: 0.97s (0.00s)
Fold 1 train - epoch: 0/10 iter: 1/15 loss: 2.7003 Acc: 28.1250% F1: 0.280 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 2/15 loss: 2.2915 Acc: 18.7500% F1: 0.188 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 3/15 loss: 1.5819 Acc: 34.3750% F1: 0.267 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 4/15 loss: 1.5756 Acc: 43.7500% F1: 0.265 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 5/15 loss: 1.2791 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 6/15 loss: 0.9109 Acc: 50.0000% F1: 0.259 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 7/15 loss: 1.0089 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 8/15 loss: 1.1315 Acc: 50.0000% F1: 0.362 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 9/15 loss: 1.2473 Acc: 40.6250% F1: 0.286 Time: 0.94s (0.02s)
Fold 1 train - epoch: 0/10 iter: 10/15 loss: 1.2211 Acc: 31.2500% F1: 0.220 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 11/15 loss: 0.9670 Acc: 50.0000% F1: 0.344 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 12/15 loss: 1.2481 Acc: 46.8750% F1: 0.330 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 13/15 loss: 0.9720 Acc: 50.0000% F1: 0.345 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/10 iter: 14/15 loss: 0.3399 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 40.0000% F1: 0.3306 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6341 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/10 iter: 1/2 loss: 2.1214 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.3217 *
*********************************************************
Performing epoch 1 of 10
Fold 1 train - epoch: 1/10 iter: 0/15 loss: 1.0361 Acc: 56.2500% F1: 0.327 Time: 0.95s (0.00s)
Fold 1 train - epoch: 1/10 iter: 1/15 loss: 1.0328 Acc: 62.5000% F1: 0.361 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 2/15 loss: 1.0259 Acc: 50.0000% F1: 0.262 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 3/15 loss: 0.8926 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 4/15 loss: 1.0303 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 5/15 loss: 0.9719 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 6/15 loss: 0.6947 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 7/15 loss: 0.8948 Acc: 43.7500% F1: 0.254 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 8/15 loss: 0.9887 Acc: 53.1250% F1: 0.347 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 9/15 loss: 0.9433 Acc: 43.7500% F1: 0.265 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 10/15 loss: 1.0209 Acc: 50.0000% F1: 0.332 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 11/15 loss: 0.9140 Acc: 59.3750% F1: 0.402 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 12/15 loss: 0.9943 Acc: 56.2500% F1: 0.403 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 13/15 loss: 0.8697 Acc: 62.5000% F1: 0.429 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 14/15 loss: 0.4041 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 55.5556% F1: 0.3353 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7506 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 1/10 iter: 1/2 loss: 1.4760 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3276 *
*********************************************************
Performing epoch 2 of 10
Fold 1 train - epoch: 2/10 iter: 0/15 loss: 0.9556 Acc: 53.1250% F1: 0.370 Time: 0.95s (0.00s)
Fold 1 train - epoch: 2/10 iter: 1/15 loss: 0.8695 Acc: 68.7500% F1: 0.471 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 2/15 loss: 0.9295 Acc: 56.2500% F1: 0.345 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 3/15 loss: 0.8488 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 4/15 loss: 0.8920 Acc: 56.2500% F1: 0.327 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 5/15 loss: 0.9009 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 6/15 loss: 0.7171 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 7/15 loss: 0.8895 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 8/15 loss: 0.8939 Acc: 59.3750% F1: 0.319 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 9/15 loss: 0.9911 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 10/15 loss: 0.9805 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 11/15 loss: 0.8645 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 12/15 loss: 0.8945 Acc: 62.5000% F1: 0.395 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 13/15 loss: 0.8354 Acc: 65.6250% F1: 0.398 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/10 iter: 14/15 loss: 0.1864 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 58.6667% F1: 0.3396 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/10 iter: 0/2 loss: 0.7382 Acc: 71.8750% F1: 0.418 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5319 Acc: 22.2222% F1: 0.148 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3276 *
*********************************************************
Performing epoch 3 of 10
Fold 1 train - epoch: 3/10 iter: 0/15 loss: 0.8882 Acc: 56.2500% F1: 0.380 Time: 0.94s (0.00s)
Fold 1 train - epoch: 3/10 iter: 1/15 loss: 0.8592 Acc: 65.6250% F1: 0.455 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 2/15 loss: 0.8362 Acc: 65.6250% F1: 0.456 Time: 0.92s (0.02s)
Fold 1 train - epoch: 3/10 iter: 3/15 loss: 0.7168 Acc: 71.8750% F1: 0.482 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 4/15 loss: 0.8372 Acc: 65.6250% F1: 0.458 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 5/15 loss: 0.8951 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 6/15 loss: 0.6883 Acc: 71.8750% F1: 0.426 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 7/15 loss: 0.8216 Acc: 59.3750% F1: 0.366 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 8/15 loss: 0.8810 Acc: 59.3750% F1: 0.391 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 9/15 loss: 0.9072 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 10/15 loss: 0.9503 Acc: 59.3750% F1: 0.389 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 11/15 loss: 0.7747 Acc: 68.7500% F1: 0.418 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 12/15 loss: 0.8773 Acc: 65.6250% F1: 0.456 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 13/15 loss: 0.7603 Acc: 65.6250% F1: 0.419 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 14/15 loss: 0.0227 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 63.3333% F1: 0.4177 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/10 iter: 0/2 loss: 0.8152 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 3/10 iter: 1/2 loss: 1.5448 Acc: 33.3333% F1: 0.190 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3288 *
*********************************************************
Performing epoch 4 of 10
Fold 1 train - epoch: 4/10 iter: 0/15 loss: 0.8639 Acc: 50.0000% F1: 0.332 Time: 0.94s (0.00s)
Fold 1 train - epoch: 4/10 iter: 1/15 loss: 0.7657 Acc: 68.7500% F1: 0.475 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 2/15 loss: 0.7346 Acc: 71.8750% F1: 0.503 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 3/15 loss: 0.7131 Acc: 71.8750% F1: 0.482 Time: 0.92s (0.02s)
Fold 1 train - epoch: 4/10 iter: 4/15 loss: 0.7879 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 5/15 loss: 0.7805 Acc: 59.3750% F1: 0.428 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 6/15 loss: 0.6886 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 7/15 loss: 0.7115 Acc: 75.0000% F1: 0.501 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 8/15 loss: 0.7790 Acc: 65.6250% F1: 0.448 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 9/15 loss: 0.8207 Acc: 62.5000% F1: 0.417 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 10/15 loss: 0.8278 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 11/15 loss: 0.7043 Acc: 75.0000% F1: 0.513 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 12/15 loss: 0.7900 Acc: 59.3750% F1: 0.392 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 13/15 loss: 0.6645 Acc: 81.2500% F1: 0.554 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 14/15 loss: 0.0135 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 67.3333% F1: 0.4578 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/10 iter: 0/2 loss: 0.9100 Acc: 46.8750% F1: 0.364 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 4/10 iter: 1/2 loss: 1.5362 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 42.0000% F1: 0.2894 *
*********************************************************
Performing epoch 5 of 10
Fold 1 train - epoch: 5/10 iter: 0/15 loss: 0.8230 Acc: 65.6250% F1: 0.462 Time: 0.94s (0.00s)
Fold 1 train - epoch: 5/10 iter: 1/15 loss: 0.7142 Acc: 68.7500% F1: 0.480 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 2/15 loss: 0.6613 Acc: 81.2500% F1: 0.568 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 3/15 loss: 0.6373 Acc: 68.7500% F1: 0.462 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 4/15 loss: 0.6999 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 5/15 loss: 0.5958 Acc: 78.1250% F1: 0.674 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 6/15 loss: 0.4994 Acc: 81.2500% F1: 0.530 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 7/15 loss: 0.6331 Acc: 71.8750% F1: 0.479 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 8/15 loss: 0.6546 Acc: 65.6250% F1: 0.448 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 9/15 loss: 0.6798 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 10/15 loss: 0.7050 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 11/15 loss: 0.6179 Acc: 78.1250% F1: 0.538 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 12/15 loss: 0.7239 Acc: 68.7500% F1: 0.564 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 13/15 loss: 0.5469 Acc: 87.5000% F1: 0.765 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 14/15 loss: 0.0086 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 5 train Avg acc: 74.0000% F1: 0.5487 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 5/10 iter: 0/2 loss: 1.0672 Acc: 46.8750% F1: 0.364 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 5/10 iter: 1/2 loss: 1.6009 Acc: 33.3333% F1: 0.190 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 5 train-dev Avg acc: 42.0000% F1: 0.2895 *
*********************************************************
Performing epoch 6 of 10
Fold 1 train - epoch: 6/10 iter: 0/15 loss: 0.6821 Acc: 65.6250% F1: 0.463 Time: 0.95s (0.00s)
Fold 1 train - epoch: 6/10 iter: 1/15 loss: 0.5613 Acc: 78.1250% F1: 0.541 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 2/15 loss: 0.5194 Acc: 81.2500% F1: 0.568 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 3/15 loss: 0.5103 Acc: 84.3750% F1: 0.569 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 4/15 loss: 0.5934 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 5/15 loss: 0.5115 Acc: 81.2500% F1: 0.764 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 6/15 loss: 0.4227 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 7/15 loss: 0.4721 Acc: 78.1250% F1: 0.525 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 8/15 loss: 0.6259 Acc: 71.8750% F1: 0.591 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 9/15 loss: 0.5648 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 10/15 loss: 0.5148 Acc: 78.1250% F1: 0.675 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 11/15 loss: 0.4531 Acc: 87.5000% F1: 0.606 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 12/15 loss: 0.5348 Acc: 78.1250% F1: 0.653 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 13/15 loss: 0.3227 Acc: 96.8750% F1: 0.917 Time: 0.93s (0.02s)
Fold 1 train - epoch: 6/10 iter: 14/15 loss: 0.0018 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 6 train Avg acc: 80.6667% F1: 0.6469 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 6/10 iter: 0/2 loss: 1.2552 Acc: 40.6250% F1: 0.228 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 6/10 iter: 1/2 loss: 1.6952 Acc: 33.3333% F1: 0.190 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 6 train-dev Avg acc: 38.0000% F1: 0.2765 *
*********************************************************
Performing epoch 7 of 10
Fold 1 train - epoch: 7/10 iter: 0/15 loss: 0.5696 Acc: 81.2500% F1: 0.701 Time: 0.95s (0.00s)
Fold 1 train - epoch: 7/10 iter: 1/15 loss: 0.3906 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 2/15 loss: 0.4039 Acc: 93.7500% F1: 0.899 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 3/15 loss: 0.4000 Acc: 84.3750% F1: 0.582 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 4/15 loss: 0.4361 Acc: 84.3750% F1: 0.719 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 5/15 loss: 0.3208 Acc: 93.7500% F1: 0.927 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 6/15 loss: 0.2779 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 7/15 loss: 0.3254 Acc: 87.5000% F1: 0.601 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 8/15 loss: 0.3098 Acc: 90.6250% F1: 0.852 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 9/15 loss: 0.4376 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 10/15 loss: 0.3709 Acc: 84.3750% F1: 0.721 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 11/15 loss: 0.2916 Acc: 87.5000% F1: 0.606 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 12/15 loss: 0.3858 Acc: 90.6250% F1: 0.815 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 13/15 loss: 0.2497 Acc: 90.6250% F1: 0.792 Time: 0.94s (0.02s)
Fold 1 train - epoch: 7/10 iter: 14/15 loss: 0.0009 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 7 train Avg acc: 88.2222% F1: 0.7737 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 7/10 iter: 0/2 loss: 1.7075 Acc: 34.3750% F1: 0.200 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 7/10 iter: 1/2 loss: 1.8511 Acc: 38.8889% F1: 0.203 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 7 train-dev Avg acc: 36.0000% F1: 0.2659 *
*********************************************************
Performing epoch 8 of 10
Fold 1 train - epoch: 8/10 iter: 0/15 loss: 0.3774 Acc: 87.5000% F1: 0.814 Time: 0.95s (0.00s)
Fold 1 train - epoch: 8/10 iter: 1/15 loss: 0.2590 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 2/15 loss: 0.3169 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 3/15 loss: 0.2181 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 4/15 loss: 0.2451 Acc: 93.7500% F1: 0.865 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 5/15 loss: 0.1951 Acc: 96.8750% F1: 0.943 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 6/15 loss: 0.1910 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 7/15 loss: 0.3328 Acc: 87.5000% F1: 0.913 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 8/15 loss: 0.2675 Acc: 87.5000% F1: 0.813 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 9/15 loss: 0.3511 Acc: 81.2500% F1: 0.683 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 10/15 loss: 0.2842 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 11/15 loss: 0.2227 Acc: 93.7500% F1: 0.817 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 12/15 loss: 0.4503 Acc: 81.2500% F1: 0.673 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 13/15 loss: 0.2035 Acc: 96.8750% F1: 0.917 Time: 0.93s (0.02s)
Fold 1 train - epoch: 8/10 iter: 14/15 loss: 0.0002 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 8 train Avg acc: 90.8889% F1: 0.8420 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 8/10 iter: 0/2 loss: 1.6953 Acc: 40.6250% F1: 0.232 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 8/10 iter: 1/2 loss: 2.2342 Acc: 33.3333% F1: 0.259 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 8 train-dev Avg acc: 38.0000% F1: 0.3188 *
*********************************************************
Performing epoch 9 of 10
Fold 1 train - epoch: 9/10 iter: 0/15 loss: 0.2802 Acc: 87.5000% F1: 0.863 Time: 0.95s (0.00s)
Fold 1 train - epoch: 9/10 iter: 1/15 loss: 0.1747 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 2/15 loss: 0.1946 Acc: 90.6250% F1: 0.877 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 3/15 loss: 0.1436 Acc: 96.8750% F1: 0.880 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 4/15 loss: 0.1783 Acc: 90.6250% F1: 0.767 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 5/15 loss: 0.2166 Acc: 90.6250% F1: 0.889 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 6/15 loss: 0.1340 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 7/15 loss: 0.0999 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 8/15 loss: 0.2379 Acc: 90.6250% F1: 0.885 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 9/15 loss: 0.1435 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 10/15 loss: 0.2002 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 11/15 loss: 0.2618 Acc: 87.5000% F1: 0.761 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 12/15 loss: 0.2177 Acc: 90.6250% F1: 0.852 Time: 0.94s (0.02s)
Fold 1 train - epoch: 9/10 iter: 13/15 loss: 0.1328 Acc: 93.7500% F1: 0.946 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 14/15 loss: 0.0003 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 9 train Avg acc: 93.3333% F1: 0.8954 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 9/10 iter: 0/2 loss: 2.4585 Acc: 28.1250% F1: 0.181 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 9/10 iter: 1/2 loss: 2.0058 Acc: 44.4444% F1: 0.348 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 9 train-dev Avg acc: 34.0000% F1: 0.3191 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/10 iter: 0/15 loss: 3.9283 Acc: 9.3750% F1: 0.059 Time: 0.96s (0.00s)
Fold 2 train - epoch: 0/10 iter: 1/15 loss: 2.7941 Acc: 25.0000% F1: 0.263 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 2/15 loss: 1.9868 Acc: 34.3750% F1: 0.335 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 3/15 loss: 1.5083 Acc: 40.6250% F1: 0.285 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 4/15 loss: 1.6086 Acc: 46.8750% F1: 0.285 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 5/15 loss: 1.1735 Acc: 56.2500% F1: 0.327 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 6/15 loss: 0.9039 Acc: 53.1250% F1: 0.271 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 7/15 loss: 1.0212 Acc: 43.7500% F1: 0.283 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 8/15 loss: 1.4402 Acc: 28.1250% F1: 0.210 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 9/15 loss: 1.4126 Acc: 31.2500% F1: 0.221 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 10/15 loss: 1.2550 Acc: 40.6250% F1: 0.278 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 11/15 loss: 1.0246 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 12/15 loss: 1.2393 Acc: 34.3750% F1: 0.244 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 13/15 loss: 1.0152 Acc: 46.8750% F1: 0.325 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 14/15 loss: 0.5897 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 38.4444% F1: 0.3255 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/10 iter: 0/2 loss: 0.4797 Acc: 84.3750% F1: 0.458 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 0/10 iter: 1/2 loss: 1.8786 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 10
Fold 2 train - epoch: 1/10 iter: 0/15 loss: 1.0061 Acc: 59.3750% F1: 0.306 Time: 0.95s (0.00s)
Fold 2 train - epoch: 1/10 iter: 1/15 loss: 0.9573 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 2/15 loss: 1.0939 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 3/15 loss: 1.0491 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 4/15 loss: 1.0913 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 5/15 loss: 1.0329 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 6/15 loss: 0.6844 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 7/15 loss: 0.9373 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 8/15 loss: 0.9887 Acc: 56.2500% F1: 0.310 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 9/15 loss: 0.9873 Acc: 50.0000% F1: 0.267 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 10/15 loss: 1.0266 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 11/15 loss: 0.8146 Acc: 62.5000% F1: 0.399 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 12/15 loss: 1.1110 Acc: 37.5000% F1: 0.255 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 13/15 loss: 0.9408 Acc: 53.1250% F1: 0.363 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 14/15 loss: 0.5041 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 54.4444% F1: 0.2975 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7407 Acc: 78.1250% F1: 0.547 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 1/10 iter: 1/2 loss: 1.1931 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 62.0000% F1: 0.4033 *
*********************************************************
Performing epoch 2 of 10
Fold 2 train - epoch: 2/10 iter: 0/15 loss: 1.0067 Acc: 53.1250% F1: 0.370 Time: 0.94s (0.00s)
Fold 2 train - epoch: 2/10 iter: 1/15 loss: 0.9370 Acc: 53.1250% F1: 0.370 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 2/15 loss: 0.9037 Acc: 59.3750% F1: 0.415 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 3/15 loss: 0.8634 Acc: 56.2500% F1: 0.367 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 4/15 loss: 0.8770 Acc: 59.3750% F1: 0.403 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 5/15 loss: 0.8867 Acc: 59.3750% F1: 0.370 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 6/15 loss: 0.7075 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 7/15 loss: 0.8751 Acc: 59.3750% F1: 0.379 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 8/15 loss: 0.9607 Acc: 59.3750% F1: 0.374 Time: 0.94s (0.03s)
Fold 2 train - epoch: 2/10 iter: 9/15 loss: 1.0046 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 10/15 loss: 1.0040 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 11/15 loss: 0.8289 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 12/15 loss: 0.9181 Acc: 59.3750% F1: 0.349 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 13/15 loss: 0.8487 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 14/15 loss: 0.0914 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 58.2222% F1: 0.3667 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/10 iter: 0/2 loss: 0.5564 Acc: 87.5000% F1: 0.467 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5613 Acc: 5.5556% F1: 0.048 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.2795 *
*********************************************************
Performing epoch 3 of 10
Fold 2 train - epoch: 3/10 iter: 0/15 loss: 0.8592 Acc: 56.2500% F1: 0.292 Time: 0.95s (0.00s)
Fold 2 train - epoch: 3/10 iter: 1/15 loss: 0.8604 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 2/15 loss: 0.8128 Acc: 65.6250% F1: 0.433 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 3/15 loss: 0.7701 Acc: 65.6250% F1: 0.361 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 4/15 loss: 0.9709 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 5/15 loss: 0.8888 Acc: 56.2500% F1: 0.383 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 6/15 loss: 0.7205 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 7/15 loss: 0.7629 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 8/15 loss: 0.9973 Acc: 59.3750% F1: 0.416 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 9/15 loss: 0.8859 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 10/15 loss: 0.9291 Acc: 65.6250% F1: 0.450 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 11/15 loss: 0.7720 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 12/15 loss: 0.9262 Acc: 59.3750% F1: 0.372 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 13/15 loss: 0.7434 Acc: 68.7500% F1: 0.449 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/10 iter: 14/15 loss: 0.0565 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 63.7778% F1: 0.4156 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6010 Acc: 81.2500% F1: 0.304 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 3/10 iter: 1/2 loss: 1.4804 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 60.0000% F1: 0.3587 *
*********************************************************
Performing epoch 4 of 10
Fold 2 train - epoch: 4/10 iter: 0/15 loss: 0.8865 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.00s)
Fold 2 train - epoch: 4/10 iter: 1/15 loss: 0.8113 Acc: 65.6250% F1: 0.420 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 2/15 loss: 0.8114 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 3/15 loss: 0.7214 Acc: 68.7500% F1: 0.443 Time: 0.92s (0.02s)
Fold 2 train - epoch: 4/10 iter: 4/15 loss: 0.7767 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 5/15 loss: 0.7358 Acc: 71.8750% F1: 0.499 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 6/15 loss: 0.6104 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 7/15 loss: 0.7505 Acc: 62.5000% F1: 0.407 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 8/15 loss: 0.9310 Acc: 56.2500% F1: 0.369 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 9/15 loss: 0.8278 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 10/15 loss: 0.8458 Acc: 62.5000% F1: 0.434 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 11/15 loss: 0.6159 Acc: 84.3750% F1: 0.588 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 12/15 loss: 0.8613 Acc: 62.5000% F1: 0.439 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/10 iter: 13/15 loss: 0.6727 Acc: 71.8750% F1: 0.480 Time: 0.94s (0.02s)
Fold 2 train - epoch: 4/10 iter: 14/15 loss: 0.0104 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 67.5556% F1: 0.4528 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7358 Acc: 65.6250% F1: 0.269 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4461 Acc: 22.2222% F1: 0.140 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 50.0000% F1: 0.3107 *
*********************************************************
Performing epoch 5 of 10
Fold 2 train - epoch: 5/10 iter: 0/15 loss: 0.8831 Acc: 50.0000% F1: 0.333 Time: 0.94s (0.00s)
Fold 2 train - epoch: 5/10 iter: 1/15 loss: 0.6755 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 2/15 loss: 0.6704 Acc: 84.3750% F1: 0.593 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 3/15 loss: 0.6456 Acc: 71.8750% F1: 0.489 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 4/15 loss: 0.7248 Acc: 78.1250% F1: 0.556 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 5/15 loss: 0.6399 Acc: 78.1250% F1: 0.723 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 6/15 loss: 0.5305 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 7/15 loss: 0.5701 Acc: 84.3750% F1: 0.571 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 8/15 loss: 0.7880 Acc: 62.5000% F1: 0.427 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 9/15 loss: 0.6624 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 10/15 loss: 0.7346 Acc: 71.8750% F1: 0.624 Time: 0.94s (0.02s)
Fold 2 train - epoch: 5/10 iter: 11/15 loss: 0.5571 Acc: 75.0000% F1: 0.490 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 12/15 loss: 0.7045 Acc: 75.0000% F1: 0.631 Time: 0.93s (0.02s)
Fold 2 train - epoch: 5/10 iter: 13/15 loss: 0.5569 Acc: 81.2500% F1: 0.716 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 14/15 loss: 0.0039 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 2 Epoch 5 train Avg acc: 74.2222% F1: 0.5690 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9153 Acc: 56.2500% F1: 0.294 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 5/10 iter: 1/2 loss: 1.3427 Acc: 44.4444% F1: 0.222 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 5 train-dev Avg acc: 52.0000% F1: 0.3717 *
*********************************************************
Performing epoch 6 of 10
Fold 2 train - epoch: 6/10 iter: 0/15 loss: 0.6994 Acc: 71.8750% F1: 0.507 Time: 0.94s (0.00s)
Fold 2 train - epoch: 6/10 iter: 1/15 loss: 0.6377 Acc: 68.7500% F1: 0.485 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 2/15 loss: 0.5251 Acc: 84.3750% F1: 0.721 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 3/15 loss: 0.5633 Acc: 78.1250% F1: 0.526 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 4/15 loss: 0.5210 Acc: 81.2500% F1: 0.584 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 5/15 loss: 0.5506 Acc: 81.2500% F1: 0.748 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 6/15 loss: 0.4100 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 7/15 loss: 0.4520 Acc: 90.6250% F1: 0.614 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 8/15 loss: 0.6615 Acc: 71.8750% F1: 0.591 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 9/15 loss: 0.5026 Acc: 78.1250% F1: 0.561 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 10/15 loss: 0.4920 Acc: 84.3750% F1: 0.847 Time: 0.94s (0.02s)
Fold 2 train - epoch: 6/10 iter: 11/15 loss: 0.4250 Acc: 87.5000% F1: 0.756 Time: 0.94s (0.02s)
Fold 2 train - epoch: 6/10 iter: 12/15 loss: 0.6057 Acc: 78.1250% F1: 0.654 Time: 0.93s (0.02s)
Fold 2 train - epoch: 6/10 iter: 13/15 loss: 0.5404 Acc: 71.8750% F1: 0.487 Time: 0.94s (0.02s)
Fold 2 train - epoch: 6/10 iter: 14/15 loss: 0.0029 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 2 Epoch 6 train Avg acc: 80.0000% F1: 0.6556 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 6/10 iter: 0/2 loss: 1.0228 Acc: 53.1250% F1: 0.288 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 6/10 iter: 1/2 loss: 1.6137 Acc: 38.8889% F1: 0.212 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 6 train-dev Avg acc: 48.0000% F1: 0.3499 *
*********************************************************
Performing epoch 7 of 10
Fold 2 train - epoch: 7/10 iter: 0/15 loss: 0.4815 Acc: 78.1250% F1: 0.672 Time: 0.95s (0.00s)
Fold 2 train - epoch: 7/10 iter: 1/15 loss: 0.4363 Acc: 84.3750% F1: 0.709 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 2/15 loss: 0.3178 Acc: 90.6250% F1: 0.789 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 3/15 loss: 0.4727 Acc: 87.5000% F1: 0.617 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 4/15 loss: 0.5188 Acc: 81.2500% F1: 0.582 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 5/15 loss: 0.3260 Acc: 81.2500% F1: 0.745 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 6/15 loss: 0.2918 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 7/15 loss: 0.3906 Acc: 84.3750% F1: 0.571 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 8/15 loss: 0.4424 Acc: 81.2500% F1: 0.771 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 9/15 loss: 0.4079 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 10/15 loss: 0.3312 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 11/15 loss: 0.2799 Acc: 90.6250% F1: 0.783 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 12/15 loss: 0.5479 Acc: 84.3750% F1: 0.753 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 13/15 loss: 0.3813 Acc: 84.3750% F1: 0.738 Time: 0.94s (0.02s)
Fold 2 train - epoch: 7/10 iter: 14/15 loss: 0.0007 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 7 train Avg acc: 85.3333% F1: 0.7488 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 7/10 iter: 0/2 loss: 1.0458 Acc: 50.0000% F1: 0.232 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 7/10 iter: 1/2 loss: 1.8409 Acc: 38.8889% F1: 0.212 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 7 train-dev Avg acc: 46.0000% F1: 0.3398 *
*********************************************************
Performing epoch 8 of 10
Fold 2 train - epoch: 8/10 iter: 0/15 loss: 0.4264 Acc: 81.2500% F1: 0.769 Time: 0.95s (0.00s)
Fold 2 train - epoch: 8/10 iter: 1/15 loss: 0.2958 Acc: 87.5000% F1: 0.797 Time: 0.92s (0.02s)
Fold 2 train - epoch: 8/10 iter: 2/15 loss: 0.3414 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 3/15 loss: 0.3573 Acc: 87.5000% F1: 0.769 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 4/15 loss: 0.3725 Acc: 87.5000% F1: 0.621 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 5/15 loss: 0.2204 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 6/15 loss: 0.2413 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 7/15 loss: 0.1972 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 8/15 loss: 0.2621 Acc: 90.6250% F1: 0.895 Time: 0.94s (0.02s)
Fold 2 train - epoch: 8/10 iter: 9/15 loss: 0.3830 Acc: 84.3750% F1: 0.711 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 10/15 loss: 0.2596 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 11/15 loss: 0.2295 Acc: 87.5000% F1: 0.761 Time: 0.94s (0.02s)
Fold 2 train - epoch: 8/10 iter: 12/15 loss: 0.3452 Acc: 87.5000% F1: 0.721 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 13/15 loss: 0.1771 Acc: 96.8750% F1: 0.917 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 14/15 loss: 0.0002 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 8 train Avg acc: 90.4444% F1: 0.8315 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 8/10 iter: 0/2 loss: 1.3478 Acc: 46.8750% F1: 0.222 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 8/10 iter: 1/2 loss: 1.5791 Acc: 50.0000% F1: 0.333 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 8 train-dev Avg acc: 48.0000% F1: 0.3958 *
*********************************************************
Performing epoch 9 of 10
Fold 2 train - epoch: 9/10 iter: 0/15 loss: 0.2870 Acc: 87.5000% F1: 0.814 Time: 0.94s (0.00s)
Fold 2 train - epoch: 9/10 iter: 1/15 loss: 0.2622 Acc: 93.7500% F1: 0.913 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 2/15 loss: 0.3067 Acc: 87.5000% F1: 0.870 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 3/15 loss: 0.3874 Acc: 78.1250% F1: 0.699 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 4/15 loss: 0.2626 Acc: 90.6250% F1: 0.767 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 5/15 loss: 0.1154 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 6/15 loss: 0.0961 Acc: 96.8750% F1: 0.881 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 7/15 loss: 0.2094 Acc: 87.5000% F1: 0.913 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 8/15 loss: 0.2415 Acc: 87.5000% F1: 0.840 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 9/15 loss: 0.4616 Acc: 84.3750% F1: 0.844 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 10/15 loss: 0.4019 Acc: 87.5000% F1: 0.802 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 11/15 loss: 0.1715 Acc: 90.6250% F1: 0.790 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 12/15 loss: 0.1792 Acc: 90.6250% F1: 0.853 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 13/15 loss: 0.2216 Acc: 93.7500% F1: 0.809 Time: 0.93s (0.02s)
Fold 2 train - epoch: 9/10 iter: 14/15 loss: 0.0003 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 2 Epoch 9 train Avg acc: 89.7778% F1: 0.8492 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 9/10 iter: 0/2 loss: 1.7689 Acc: 43.7500% F1: 0.248 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 9/10 iter: 1/2 loss: 1.6641 Acc: 50.0000% F1: 0.333 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 9 train-dev Avg acc: 46.0000% F1: 0.3861 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/10 iter: 0/15 loss: 4.1908 Acc: 12.5000% F1: 0.112 Time: 0.96s (0.00s)
Fold 3 train - epoch: 0/10 iter: 1/15 loss: 2.9083 Acc: 25.0000% F1: 0.263 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 2/15 loss: 1.9593 Acc: 21.8750% F1: 0.217 Time: 0.92s (0.02s)
Fold 3 train - epoch: 0/10 iter: 3/15 loss: 1.5227 Acc: 46.8750% F1: 0.339 Time: 0.92s (0.02s)
Fold 3 train - epoch: 0/10 iter: 4/15 loss: 1.6051 Acc: 50.0000% F1: 0.301 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 5/15 loss: 0.9799 Acc: 59.3750% F1: 0.342 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 6/15 loss: 0.9361 Acc: 56.2500% F1: 0.284 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 7/15 loss: 0.9122 Acc: 59.3750% F1: 0.400 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 8/15 loss: 1.4102 Acc: 37.5000% F1: 0.280 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 9/15 loss: 1.3960 Acc: 37.5000% F1: 0.267 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 10/15 loss: 1.2174 Acc: 46.8750% F1: 0.334 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 11/15 loss: 1.0279 Acc: 50.0000% F1: 0.344 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 12/15 loss: 1.0619 Acc: 40.6250% F1: 0.287 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 13/15 loss: 0.9512 Acc: 53.1250% F1: 0.355 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 14/15 loss: 0.4744 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 42.8889% F1: 0.3502 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5822 Acc: 81.2500% F1: 0.448 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 0/10 iter: 1/2 loss: 1.9693 Acc: 5.5556% F1: 0.048 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2676 *
*********************************************************
Performing epoch 1 of 10
Fold 3 train - epoch: 1/10 iter: 0/15 loss: 1.0576 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.00s)
Fold 3 train - epoch: 1/10 iter: 1/15 loss: 0.9661 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 2/15 loss: 1.0505 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 3/15 loss: 0.9923 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 4/15 loss: 1.0254 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 5/15 loss: 0.8641 Acc: 62.5000% F1: 0.320 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 6/15 loss: 0.6802 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 7/15 loss: 0.8465 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 8/15 loss: 1.0416 Acc: 59.3750% F1: 0.396 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 9/15 loss: 1.0352 Acc: 53.1250% F1: 0.335 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 10/15 loss: 1.0446 Acc: 40.6250% F1: 0.281 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 11/15 loss: 0.8827 Acc: 53.1250% F1: 0.364 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 12/15 loss: 1.0297 Acc: 50.0000% F1: 0.356 Time: 0.94s (0.02s)
Fold 3 train - epoch: 1/10 iter: 13/15 loss: 1.0015 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 14/15 loss: 0.6063 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 54.6667% F1: 0.3309 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/10 iter: 0/2 loss: 0.6954 Acc: 84.3750% F1: 0.677 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 1/10 iter: 1/2 loss: 1.2387 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 58.0000% F1: 0.3477 *
*********************************************************
Performing epoch 2 of 10
Fold 3 train - epoch: 2/10 iter: 0/15 loss: 0.9257 Acc: 53.1250% F1: 0.350 Time: 0.94s (0.00s)
Fold 3 train - epoch: 2/10 iter: 1/15 loss: 0.8629 Acc: 62.5000% F1: 0.415 Time: 0.92s (0.02s)
Fold 3 train - epoch: 2/10 iter: 2/15 loss: 0.9102 Acc: 43.7500% F1: 0.279 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 3/15 loss: 0.8733 Acc: 50.0000% F1: 0.287 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 4/15 loss: 0.9436 Acc: 56.2500% F1: 0.327 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 5/15 loss: 0.8487 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 6/15 loss: 0.7160 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 7/15 loss: 0.8629 Acc: 56.2500% F1: 0.333 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 8/15 loss: 1.0123 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 9/15 loss: 0.9967 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 10/15 loss: 1.0352 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 11/15 loss: 0.7866 Acc: 65.6250% F1: 0.370 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 12/15 loss: 0.9483 Acc: 53.1250% F1: 0.278 Time: 0.94s (0.02s)
Fold 3 train - epoch: 2/10 iter: 13/15 loss: 0.8579 Acc: 56.2500% F1: 0.321 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 14/15 loss: 0.2151 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 55.3333% F1: 0.3144 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6565 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/10 iter: 1/2 loss: 1.3551 Acc: 5.5556% F1: 0.042 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2637 *
*********************************************************
Performing epoch 3 of 10
Fold 3 train - epoch: 3/10 iter: 0/15 loss: 0.8916 Acc: 53.1250% F1: 0.364 Time: 0.95s (0.00s)
Fold 3 train - epoch: 3/10 iter: 1/15 loss: 0.8251 Acc: 68.7500% F1: 0.464 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 2/15 loss: 0.8309 Acc: 68.7500% F1: 0.474 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 3/15 loss: 0.8343 Acc: 59.3750% F1: 0.385 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 4/15 loss: 0.9116 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 5/15 loss: 0.8483 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 6/15 loss: 0.6704 Acc: 78.1250% F1: 0.506 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 7/15 loss: 0.7918 Acc: 59.3750% F1: 0.379 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 8/15 loss: 0.9792 Acc: 62.5000% F1: 0.428 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 9/15 loss: 0.9529 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 10/15 loss: 0.9882 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 11/15 loss: 0.7177 Acc: 71.8750% F1: 0.439 Time: 0.94s (0.02s)
Fold 3 train - epoch: 3/10 iter: 12/15 loss: 0.8779 Acc: 62.5000% F1: 0.409 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 13/15 loss: 0.7869 Acc: 62.5000% F1: 0.375 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 14/15 loss: 0.0968 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 62.6667% F1: 0.4078 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6842 Acc: 78.1250% F1: 0.292 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/10 iter: 1/2 loss: 1.4002 Acc: 5.5556% F1: 0.042 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 52.0000% F1: 0.2650 *
*********************************************************
Performing epoch 4 of 10
Fold 3 train - epoch: 4/10 iter: 0/15 loss: 0.9074 Acc: 53.1250% F1: 0.362 Time: 0.94s (0.00s)
Fold 3 train - epoch: 4/10 iter: 1/15 loss: 0.7816 Acc: 65.6250% F1: 0.450 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 2/15 loss: 0.7243 Acc: 68.7500% F1: 0.478 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 3/15 loss: 0.7563 Acc: 65.6250% F1: 0.446 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 4/15 loss: 0.8340 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 5/15 loss: 0.7284 Acc: 75.0000% F1: 0.514 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 6/15 loss: 0.6323 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.05s)
Fold 3 train - epoch: 4/10 iter: 7/15 loss: 0.7245 Acc: 62.5000% F1: 0.415 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/10 iter: 8/15 loss: 0.9176 Acc: 59.3750% F1: 0.396 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 9/15 loss: 0.9236 Acc: 65.6250% F1: 0.462 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 10/15 loss: 0.8640 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 11/15 loss: 0.6347 Acc: 78.1250% F1: 0.531 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 12/15 loss: 0.7329 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 13/15 loss: 0.6589 Acc: 78.1250% F1: 0.526 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 14/15 loss: 0.0742 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 68.4444% F1: 0.4651 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7837 Acc: 68.7500% F1: 0.378 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 4/10 iter: 1/2 loss: 1.2814 Acc: 27.7778% F1: 0.224 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.4217 *
*********************************************************
Performing epoch 5 of 10
Fold 3 train - epoch: 5/10 iter: 0/15 loss: 0.8611 Acc: 56.2500% F1: 0.389 Time: 0.94s (0.00s)
Fold 3 train - epoch: 5/10 iter: 1/15 loss: 0.7693 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 2/15 loss: 0.6596 Acc: 81.2500% F1: 0.569 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 3/15 loss: 0.6325 Acc: 78.1250% F1: 0.534 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 4/15 loss: 0.7516 Acc: 65.6250% F1: 0.462 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 5/15 loss: 0.5892 Acc: 75.0000% F1: 0.628 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 6/15 loss: 0.5172 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 7/15 loss: 0.5967 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 8/15 loss: 0.8060 Acc: 62.5000% F1: 0.460 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 9/15 loss: 0.7993 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 10/15 loss: 0.7231 Acc: 65.6250% F1: 0.569 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 11/15 loss: 0.5411 Acc: 81.2500% F1: 0.562 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 12/15 loss: 0.6555 Acc: 75.0000% F1: 0.526 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 13/15 loss: 0.6236 Acc: 78.1250% F1: 0.538 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 14/15 loss: 0.0271 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 3 Epoch 5 train Avg acc: 72.6667% F1: 0.5269 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8542 Acc: 59.3750% F1: 0.336 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 5/10 iter: 1/2 loss: 1.2918 Acc: 27.7778% F1: 0.214 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3828 *
*********************************************************
Performing epoch 6 of 10
Fold 3 train - epoch: 6/10 iter: 0/15 loss: 0.6961 Acc: 65.6250% F1: 0.463 Time: 0.95s (0.00s)
Fold 3 train - epoch: 6/10 iter: 1/15 loss: 0.5537 Acc: 78.1250% F1: 0.549 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 2/15 loss: 0.5654 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 3/15 loss: 0.5549 Acc: 81.2500% F1: 0.553 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 4/15 loss: 0.6208 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 5/15 loss: 0.4266 Acc: 84.3750% F1: 0.709 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 6/15 loss: 0.4100 Acc: 81.2500% F1: 0.530 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 7/15 loss: 0.5066 Acc: 84.3750% F1: 0.571 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 8/15 loss: 0.6545 Acc: 71.8750% F1: 0.629 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 9/15 loss: 0.7252 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 10/15 loss: 0.5287 Acc: 81.2500% F1: 0.775 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 11/15 loss: 0.5131 Acc: 81.2500% F1: 0.705 Time: 0.94s (0.02s)
Fold 3 train - epoch: 6/10 iter: 12/15 loss: 0.4649 Acc: 84.3750% F1: 0.703 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 13/15 loss: 0.4745 Acc: 84.3750% F1: 0.728 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 14/15 loss: 0.0110 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 6 train Avg acc: 78.8889% F1: 0.6341 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 6/10 iter: 0/2 loss: 1.0289 Acc: 50.0000% F1: 0.307 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 6/10 iter: 1/2 loss: 1.3245 Acc: 22.2222% F1: 0.178 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 6 train-dev Avg acc: 40.0000% F1: 0.3253 *
*********************************************************
Performing epoch 7 of 10
Fold 3 train - epoch: 7/10 iter: 0/15 loss: 0.5598 Acc: 78.1250% F1: 0.740 Time: 0.94s (0.00s)
Fold 3 train - epoch: 7/10 iter: 1/15 loss: 0.4574 Acc: 81.2500% F1: 0.688 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 2/15 loss: 0.3508 Acc: 93.7500% F1: 0.900 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 3/15 loss: 0.3132 Acc: 90.6250% F1: 0.787 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 4/15 loss: 0.5138 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 5/15 loss: 0.2596 Acc: 93.7500% F1: 0.856 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 6/15 loss: 0.3178 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 7/15 loss: 0.3683 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 8/15 loss: 0.4237 Acc: 78.1250% F1: 0.690 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 9/15 loss: 0.4610 Acc: 81.2500% F1: 0.701 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 10/15 loss: 0.3986 Acc: 81.2500% F1: 0.741 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 11/15 loss: 0.2038 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 12/15 loss: 0.3841 Acc: 90.6250% F1: 0.823 Time: 0.94s (0.02s)
Fold 3 train - epoch: 7/10 iter: 13/15 loss: 0.2459 Acc: 93.7500% F1: 0.809 Time: 0.94s (0.02s)
Fold 3 train - epoch: 7/10 iter: 14/15 loss: 0.0185 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 3 Epoch 7 train Avg acc: 87.1111% F1: 0.7739 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 7/10 iter: 0/2 loss: 1.3222 Acc: 43.7500% F1: 0.279 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 7/10 iter: 1/2 loss: 1.4458 Acc: 38.8889% F1: 0.251 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 7 train-dev Avg acc: 42.0000% F1: 0.3591 *
*********************************************************
Performing epoch 8 of 10
Fold 3 train - epoch: 8/10 iter: 0/15 loss: 0.4280 Acc: 81.2500% F1: 0.761 Time: 0.94s (0.00s)
Fold 3 train - epoch: 8/10 iter: 1/15 loss: 0.3592 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 2/15 loss: 0.2857 Acc: 90.6250% F1: 0.877 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 3/15 loss: 0.2487 Acc: 90.6250% F1: 0.787 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 4/15 loss: 0.2394 Acc: 90.6250% F1: 0.767 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 5/15 loss: 0.2333 Acc: 90.6250% F1: 0.882 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 6/15 loss: 0.1409 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 7/15 loss: 0.3205 Acc: 87.5000% F1: 0.592 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 8/15 loss: 0.3248 Acc: 87.5000% F1: 0.805 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 9/15 loss: 0.3773 Acc: 84.3750% F1: 0.798 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 10/15 loss: 0.2442 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 11/15 loss: 0.2103 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 12/15 loss: 0.2083 Acc: 93.7500% F1: 0.864 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 13/15 loss: 0.3035 Acc: 87.5000% F1: 0.759 Time: 0.94s (0.02s)
Fold 3 train - epoch: 8/10 iter: 14/15 loss: 0.0013 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 3 Epoch 8 train Avg acc: 90.2222% F1: 0.8339 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 8/10 iter: 0/2 loss: 1.6294 Acc: 37.5000% F1: 0.256 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 8/10 iter: 1/2 loss: 1.5948 Acc: 33.3333% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 8 train-dev Avg acc: 36.0000% F1: 0.3166 *
*********************************************************
Performing epoch 9 of 10
Fold 3 train - epoch: 9/10 iter: 0/15 loss: 0.2581 Acc: 87.5000% F1: 0.870 Time: 0.95s (0.00s)
Fold 3 train - epoch: 9/10 iter: 1/15 loss: 0.2477 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 2/15 loss: 0.1050 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 3/15 loss: 0.1289 Acc: 96.8750% F1: 0.924 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 4/15 loss: 0.2091 Acc: 93.7500% F1: 0.865 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 5/15 loss: 0.0831 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 6/15 loss: 0.1176 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 7/15 loss: 0.0749 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 8/15 loss: 0.2598 Acc: 90.6250% F1: 0.895 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 9/15 loss: 0.2307 Acc: 93.7500% F1: 0.924 Time: 0.94s (0.02s)
Fold 3 train - epoch: 9/10 iter: 10/15 loss: 0.1299 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 11/15 loss: 0.1583 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 12/15 loss: 0.2155 Acc: 93.7500% F1: 0.910 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 13/15 loss: 0.1060 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 14/15 loss: 0.0009 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 9 train Avg acc: 95.5556% F1: 0.9247 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 9/10 iter: 0/2 loss: 1.7094 Acc: 40.6250% F1: 0.281 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 9/10 iter: 1/2 loss: 2.0638 Acc: 16.6667% F1: 0.139 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 9 train-dev Avg acc: 32.0000% F1: 0.2703 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/10 iter: 0/15 loss: 3.9746 Acc: 15.6250% F1: 0.134 Time: 0.98s (0.00s)
Fold 4 train - epoch: 0/10 iter: 1/15 loss: 2.8513 Acc: 18.7500% F1: 0.200 Time: 0.92s (0.03s)
Fold 4 train - epoch: 0/10 iter: 2/15 loss: 2.0963 Acc: 31.2500% F1: 0.306 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 3/15 loss: 1.6401 Acc: 37.5000% F1: 0.272 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 4/15 loss: 1.3963 Acc: 50.0000% F1: 0.324 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 5/15 loss: 1.0258 Acc: 62.5000% F1: 0.385 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 6/15 loss: 0.9082 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/10 iter: 7/15 loss: 1.0228 Acc: 53.1250% F1: 0.357 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 8/15 loss: 1.4296 Acc: 40.6250% F1: 0.281 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 9/15 loss: 1.3490 Acc: 37.5000% F1: 0.268 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 10/15 loss: 1.1217 Acc: 43.7500% F1: 0.312 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 11/15 loss: 0.9149 Acc: 50.0000% F1: 0.348 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/10 iter: 12/15 loss: 1.1028 Acc: 43.7500% F1: 0.310 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/10 iter: 13/15 loss: 0.9837 Acc: 50.0000% F1: 0.344 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 14/15 loss: 0.5437 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 42.2222% F1: 0.3510 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/10 iter: 0/2 loss: 0.4951 Acc: 84.3750% F1: 0.599 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 0/10 iter: 1/2 loss: 2.2442 Acc: 5.5556% F1: 0.044 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.3013 *
*********************************************************
Performing epoch 1 of 10
Fold 4 train - epoch: 1/10 iter: 0/15 loss: 1.0359 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.00s)
Fold 4 train - epoch: 1/10 iter: 1/15 loss: 0.9787 Acc: 62.5000% F1: 0.361 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 2/15 loss: 1.0504 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 3/15 loss: 0.9471 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 4/15 loss: 1.0660 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 5/15 loss: 0.8763 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 6/15 loss: 0.6855 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 7/15 loss: 0.8541 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 8/15 loss: 1.0075 Acc: 56.2500% F1: 0.310 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 9/15 loss: 1.0085 Acc: 46.8750% F1: 0.280 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 10/15 loss: 0.9974 Acc: 50.0000% F1: 0.343 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 11/15 loss: 0.8477 Acc: 71.8750% F1: 0.469 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 12/15 loss: 0.9919 Acc: 46.8750% F1: 0.314 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 13/15 loss: 0.9175 Acc: 56.2500% F1: 0.392 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 14/15 loss: 0.5671 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 56.0000% F1: 0.3238 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7480 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 1/10 iter: 1/2 loss: 1.5497 Acc: 27.7778% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3508 *
*********************************************************
Performing epoch 2 of 10
Fold 4 train - epoch: 2/10 iter: 0/15 loss: 1.0038 Acc: 53.1250% F1: 0.361 Time: 0.95s (0.00s)
Fold 4 train - epoch: 2/10 iter: 1/15 loss: 0.8725 Acc: 62.5000% F1: 0.415 Time: 0.92s (0.03s)
Fold 4 train - epoch: 2/10 iter: 2/15 loss: 0.8958 Acc: 50.0000% F1: 0.291 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 3/15 loss: 0.8363 Acc: 59.3750% F1: 0.355 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 4/15 loss: 0.9403 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 5/15 loss: 0.8727 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 6/15 loss: 0.7922 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 7/15 loss: 0.8892 Acc: 46.8750% F1: 0.244 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 8/15 loss: 0.9797 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 9/15 loss: 0.9965 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 10/15 loss: 0.9984 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 11/15 loss: 0.7401 Acc: 65.6250% F1: 0.328 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 12/15 loss: 0.8865 Acc: 50.0000% F1: 0.269 Time: 0.94s (0.02s)
Fold 4 train - epoch: 2/10 iter: 13/15 loss: 0.8599 Acc: 56.2500% F1: 0.287 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 14/15 loss: 0.2379 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 56.0000% F1: 0.3176 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6450 Acc: 78.1250% F1: 0.547 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 2/10 iter: 1/2 loss: 1.7274 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.3623 *
*********************************************************
Performing epoch 3 of 10
Fold 4 train - epoch: 3/10 iter: 0/15 loss: 0.9371 Acc: 56.2500% F1: 0.381 Time: 0.95s (0.00s)
Fold 4 train - epoch: 3/10 iter: 1/15 loss: 0.9009 Acc: 68.7500% F1: 0.455 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 2/15 loss: 0.8091 Acc: 75.0000% F1: 0.526 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 3/15 loss: 0.7965 Acc: 68.7500% F1: 0.462 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 4/15 loss: 0.9008 Acc: 62.5000% F1: 0.430 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 5/15 loss: 0.8340 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 6/15 loss: 0.6723 Acc: 71.8750% F1: 0.426 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 7/15 loss: 0.7849 Acc: 62.5000% F1: 0.415 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 8/15 loss: 1.0044 Acc: 59.3750% F1: 0.396 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 9/15 loss: 0.9492 Acc: 59.3750% F1: 0.389 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 10/15 loss: 0.9072 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 11/15 loss: 0.7020 Acc: 71.8750% F1: 0.453 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 12/15 loss: 0.8944 Acc: 53.1250% F1: 0.311 Time: 0.94s (0.02s)
Fold 4 train - epoch: 3/10 iter: 13/15 loss: 0.8440 Acc: 59.3750% F1: 0.360 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 14/15 loss: 0.0925 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 63.5556% F1: 0.4199 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6771 Acc: 71.8750% F1: 0.386 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 3/10 iter: 1/2 loss: 1.7882 Acc: 27.7778% F1: 0.175 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.3763 *
*********************************************************
Performing epoch 4 of 10
Fold 4 train - epoch: 4/10 iter: 0/15 loss: 0.8313 Acc: 65.6250% F1: 0.458 Time: 0.94s (0.00s)
Fold 4 train - epoch: 4/10 iter: 1/15 loss: 0.7968 Acc: 75.0000% F1: 0.520 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 2/15 loss: 0.7407 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 3/15 loss: 0.8302 Acc: 56.2500% F1: 0.382 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 4/15 loss: 0.7901 Acc: 65.6250% F1: 0.463 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 5/15 loss: 0.7310 Acc: 65.6250% F1: 0.424 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 6/15 loss: 0.6151 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 7/15 loss: 0.6492 Acc: 78.1250% F1: 0.527 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 8/15 loss: 0.9111 Acc: 56.2500% F1: 0.353 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 9/15 loss: 0.7947 Acc: 68.7500% F1: 0.483 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 10/15 loss: 0.8376 Acc: 62.5000% F1: 0.423 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/10 iter: 11/15 loss: 0.6203 Acc: 71.8750% F1: 0.439 Time: 0.94s (0.08s)
Fold 4 train - epoch: 4/10 iter: 12/15 loss: 0.8133 Acc: 62.5000% F1: 0.401 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 13/15 loss: 0.7487 Acc: 68.7500% F1: 0.466 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 14/15 loss: 0.0590 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 67.5556% F1: 0.4591 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7640 Acc: 62.5000% F1: 0.364 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 4/10 iter: 1/2 loss: 1.8464 Acc: 27.7778% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 50.0000% F1: 0.3465 *
*********************************************************
Performing epoch 5 of 10
Fold 4 train - epoch: 5/10 iter: 0/15 loss: 0.9120 Acc: 53.1250% F1: 0.370 Time: 0.95s (0.00s)
Fold 4 train - epoch: 5/10 iter: 1/15 loss: 0.7516 Acc: 71.8750% F1: 0.505 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 2/15 loss: 0.5863 Acc: 81.2500% F1: 0.568 Time: 0.93s (0.03s)
Fold 4 train - epoch: 5/10 iter: 3/15 loss: 0.6524 Acc: 75.0000% F1: 0.505 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 4/15 loss: 0.7099 Acc: 78.1250% F1: 0.549 Time: 0.93s (0.03s)
Fold 4 train - epoch: 5/10 iter: 5/15 loss: 0.6015 Acc: 75.0000% F1: 0.638 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 6/15 loss: 0.4850 Acc: 81.2500% F1: 0.530 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 7/15 loss: 0.6337 Acc: 81.2500% F1: 0.548 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 8/15 loss: 0.8333 Acc: 56.2500% F1: 0.348 Time: 0.93s (0.03s)
Fold 4 train - epoch: 5/10 iter: 9/15 loss: 0.8784 Acc: 59.3750% F1: 0.403 Time: 0.94s (0.02s)
Fold 4 train - epoch: 5/10 iter: 10/15 loss: 0.7279 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 11/15 loss: 0.5553 Acc: 81.2500% F1: 0.705 Time: 0.94s (0.02s)
Fold 4 train - epoch: 5/10 iter: 12/15 loss: 0.6403 Acc: 81.2500% F1: 0.688 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 13/15 loss: 0.6232 Acc: 84.3750% F1: 0.584 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 14/15 loss: 0.0499 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 5 train Avg acc: 73.7778% F1: 0.5452 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9168 Acc: 56.2500% F1: 0.356 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 5/10 iter: 1/2 loss: 1.9248 Acc: 27.7778% F1: 0.159 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 5 train-dev Avg acc: 46.0000% F1: 0.3330 *
*********************************************************
Performing epoch 6 of 10
Fold 4 train - epoch: 6/10 iter: 0/15 loss: 0.7142 Acc: 65.6250% F1: 0.461 Time: 0.95s (0.00s)
Fold 4 train - epoch: 6/10 iter: 1/15 loss: 0.5576 Acc: 81.2500% F1: 0.693 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 2/15 loss: 0.4909 Acc: 84.3750% F1: 0.599 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 3/15 loss: 0.6352 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 4/15 loss: 0.6702 Acc: 78.1250% F1: 0.552 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 5/15 loss: 0.5198 Acc: 87.5000% F1: 0.815 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 6/15 loss: 0.4243 Acc: 87.5000% F1: 0.784 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 7/15 loss: 0.5878 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 8/15 loss: 0.6783 Acc: 71.8750% F1: 0.519 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 9/15 loss: 0.6271 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 10/15 loss: 0.5257 Acc: 81.2500% F1: 0.583 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 11/15 loss: 0.3989 Acc: 93.7500% F1: 0.808 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 12/15 loss: 0.5176 Acc: 87.5000% F1: 0.794 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 13/15 loss: 0.4500 Acc: 84.3750% F1: 0.580 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 14/15 loss: 0.0414 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 6 train Avg acc: 80.8889% F1: 0.6392 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9645 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 6/10 iter: 1/2 loss: 2.2555 Acc: 27.7778% F1: 0.159 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 6 train-dev Avg acc: 48.0000% F1: 0.3461 *
*********************************************************
Performing epoch 7 of 10
Fold 4 train - epoch: 7/10 iter: 0/15 loss: 0.5910 Acc: 71.8750% F1: 0.510 Time: 0.94s (0.00s)
Fold 4 train - epoch: 7/10 iter: 1/15 loss: 0.4847 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 2/15 loss: 0.3752 Acc: 87.5000% F1: 0.767 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 3/15 loss: 0.5450 Acc: 75.0000% F1: 0.520 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 4/15 loss: 0.4632 Acc: 87.5000% F1: 0.748 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 5/15 loss: 0.3357 Acc: 87.5000% F1: 0.819 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 6/15 loss: 0.3087 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 7/15 loss: 0.3733 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 8/15 loss: 0.4706 Acc: 75.0000% F1: 0.642 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 9/15 loss: 0.4794 Acc: 81.2500% F1: 0.583 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 10/15 loss: 0.3698 Acc: 90.6250% F1: 0.894 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 11/15 loss: 0.2653 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 12/15 loss: 0.3517 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 13/15 loss: 0.3459 Acc: 90.6250% F1: 0.874 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 14/15 loss: 0.0077 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 7 train Avg acc: 86.4444% F1: 0.7631 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 7/10 iter: 0/2 loss: 1.3570 Acc: 37.5000% F1: 0.269 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 7/10 iter: 1/2 loss: 2.4168 Acc: 27.7778% F1: 0.152 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 7 train-dev Avg acc: 34.0000% F1: 0.2604 *
*********************************************************
Performing epoch 8 of 10
Fold 4 train - epoch: 8/10 iter: 0/15 loss: 0.4362 Acc: 90.6250% F1: 0.889 Time: 0.95s (0.00s)
Fold 4 train - epoch: 8/10 iter: 1/15 loss: 0.2669 Acc: 90.6250% F1: 0.836 Time: 0.94s (0.03s)
Fold 4 train - epoch: 8/10 iter: 2/15 loss: 0.2727 Acc: 90.6250% F1: 0.877 Time: 0.93s (0.04s)
Fold 4 train - epoch: 8/10 iter: 3/15 loss: 0.4243 Acc: 78.1250% F1: 0.701 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 4/15 loss: 0.3228 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 5/15 loss: 0.1800 Acc: 96.8750% F1: 0.944 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 6/15 loss: 0.1835 Acc: 96.8750% F1: 0.881 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 7/15 loss: 0.1543 Acc: 96.8750% F1: 0.657 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 8/15 loss: 0.3720 Acc: 84.3750% F1: 0.781 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 9/15 loss: 0.3938 Acc: 87.5000% F1: 0.822 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 10/15 loss: 0.5148 Acc: 75.0000% F1: 0.694 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 11/15 loss: 0.1909 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 12/15 loss: 0.3221 Acc: 87.5000% F1: 0.832 Time: 0.94s (0.02s)
Fold 4 train - epoch: 8/10 iter: 13/15 loss: 0.2690 Acc: 93.7500% F1: 0.894 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 14/15 loss: 0.0087 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 8 train Avg acc: 89.5556% F1: 0.8357 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 8/10 iter: 0/2 loss: 1.6776 Acc: 37.5000% F1: 0.269 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 8/10 iter: 1/2 loss: 2.6968 Acc: 27.7778% F1: 0.152 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 8 train-dev Avg acc: 34.0000% F1: 0.2604 *
*********************************************************
Performing epoch 9 of 10
Fold 4 train - epoch: 9/10 iter: 0/15 loss: 0.4003 Acc: 84.3750% F1: 0.792 Time: 0.94s (0.00s)
Fold 4 train - epoch: 9/10 iter: 1/15 loss: 0.3213 Acc: 93.7500% F1: 0.913 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 2/15 loss: 0.2324 Acc: 90.6250% F1: 0.877 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 3/15 loss: 0.3228 Acc: 87.5000% F1: 0.597 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 4/15 loss: 0.3410 Acc: 87.5000% F1: 0.870 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 5/15 loss: 0.2654 Acc: 87.5000% F1: 0.863 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 6/15 loss: 0.1334 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 7/15 loss: 0.1369 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 8/15 loss: 0.3362 Acc: 84.3750% F1: 0.819 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 9/15 loss: 0.3238 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 10/15 loss: 0.4729 Acc: 75.0000% F1: 0.711 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 11/15 loss: 0.1953 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 12/15 loss: 0.3473 Acc: 84.3750% F1: 0.714 Time: 0.94s (0.03s)
Fold 4 train - epoch: 9/10 iter: 13/15 loss: 0.2728 Acc: 90.6250% F1: 0.792 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 14/15 loss: 0.0024 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 9 train Avg acc: 88.4444% F1: 0.8309 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 9/10 iter: 0/2 loss: 1.2322 Acc: 53.1250% F1: 0.333 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 9/10 iter: 1/2 loss: 3.5583 Acc: 16.6667% F1: 0.145 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 9 train-dev Avg acc: 40.0000% F1: 0.3164 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/10 iter: 0/15 loss: 3.8698 Acc: 12.5000% F1: 0.110 Time: 1.00s (0.00s)
Fold 5 train - epoch: 0/10 iter: 1/15 loss: 2.8623 Acc: 25.0000% F1: 0.271 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 2/15 loss: 1.8452 Acc: 34.3750% F1: 0.320 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 3/15 loss: 1.4273 Acc: 40.6250% F1: 0.287 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 4/15 loss: 1.3839 Acc: 53.1250% F1: 0.365 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 5/15 loss: 1.0147 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 6/15 loss: 0.8469 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 7/15 loss: 1.0752 Acc: 56.2500% F1: 0.362 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 8/15 loss: 1.4628 Acc: 40.6250% F1: 0.294 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 9/15 loss: 1.3696 Acc: 25.0000% F1: 0.178 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 10/15 loss: 1.3286 Acc: 25.0000% F1: 0.167 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 11/15 loss: 1.0595 Acc: 40.6250% F1: 0.273 Time: 0.94s (0.02s)
Fold 5 train - epoch: 0/10 iter: 12/15 loss: 1.0622 Acc: 40.6250% F1: 0.290 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 13/15 loss: 1.0206 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 14/15 loss: 0.5499 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 39.7778% F1: 0.3284 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5227 Acc: 81.2500% F1: 0.393 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 0/10 iter: 1/2 loss: 1.9648 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.3267 *
*********************************************************
Performing epoch 1 of 10
Fold 5 train - epoch: 1/10 iter: 0/15 loss: 0.9913 Acc: 59.3750% F1: 0.389 Time: 0.94s (0.00s)
Fold 5 train - epoch: 1/10 iter: 1/15 loss: 0.9795 Acc: 56.2500% F1: 0.245 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 2/15 loss: 1.0513 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 3/15 loss: 0.9605 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 4/15 loss: 1.0600 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 5/15 loss: 0.8988 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 6/15 loss: 0.7224 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 7/15 loss: 0.9280 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 8/15 loss: 1.0930 Acc: 56.2500% F1: 0.310 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 9/15 loss: 1.0330 Acc: 43.7500% F1: 0.239 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 10/15 loss: 0.9633 Acc: 46.8750% F1: 0.300 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 11/15 loss: 0.8667 Acc: 53.1250% F1: 0.306 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 12/15 loss: 0.9756 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 13/15 loss: 0.9846 Acc: 46.8750% F1: 0.319 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/10 iter: 14/15 loss: 0.7858 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 53.5556% F1: 0.2909 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7310 Acc: 71.8750% F1: 0.418 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3683 Acc: 22.2222% F1: 0.157 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3281 *
*********************************************************
Performing epoch 2 of 10
Fold 5 train - epoch: 2/10 iter: 0/15 loss: 0.9491 Acc: 56.2500% F1: 0.395 Time: 0.95s (0.00s)
Fold 5 train - epoch: 2/10 iter: 1/15 loss: 0.9630 Acc: 53.1250% F1: 0.370 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 2/15 loss: 0.9026 Acc: 62.5000% F1: 0.421 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 3/15 loss: 0.8756 Acc: 53.1250% F1: 0.301 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 4/15 loss: 0.9122 Acc: 53.1250% F1: 0.352 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 5/15 loss: 0.8395 Acc: 62.5000% F1: 0.319 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 6/15 loss: 0.7020 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 7/15 loss: 0.8944 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 8/15 loss: 1.0357 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 9/15 loss: 1.0178 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 10/15 loss: 0.9922 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 11/15 loss: 0.8178 Acc: 65.6250% F1: 0.328 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 12/15 loss: 0.9216 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 13/15 loss: 0.8752 Acc: 65.6250% F1: 0.419 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 14/15 loss: 0.2704 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 56.2222% F1: 0.3287 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/10 iter: 0/2 loss: 0.5973 Acc: 78.1250% F1: 0.439 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5620 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.2918 *
*********************************************************
Performing epoch 3 of 10
Fold 5 train - epoch: 3/10 iter: 0/15 loss: 0.9643 Acc: 46.8750% F1: 0.254 Time: 0.95s (0.00s)
Fold 5 train - epoch: 3/10 iter: 1/15 loss: 0.8709 Acc: 68.7500% F1: 0.436 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 2/15 loss: 0.8060 Acc: 75.0000% F1: 0.524 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 3/15 loss: 0.7767 Acc: 68.7500% F1: 0.439 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 4/15 loss: 0.8756 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 5/15 loss: 0.7987 Acc: 75.0000% F1: 0.529 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 6/15 loss: 0.6975 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 7/15 loss: 0.7921 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 8/15 loss: 1.0610 Acc: 59.3750% F1: 0.368 Time: 0.94s (0.02s)
Fold 5 train - epoch: 3/10 iter: 9/15 loss: 0.9815 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 10/15 loss: 0.9703 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.05s)
Fold 5 train - epoch: 3/10 iter: 11/15 loss: 0.7130 Acc: 71.8750% F1: 0.432 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 12/15 loss: 0.8599 Acc: 56.2500% F1: 0.326 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 13/15 loss: 0.7948 Acc: 71.8750% F1: 0.469 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/10 iter: 14/15 loss: 0.1430 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 65.7778% F1: 0.4286 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6109 Acc: 81.2500% F1: 0.571 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 3/10 iter: 1/2 loss: 1.5572 Acc: 22.2222% F1: 0.157 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 60.0000% F1: 0.3749 *
*********************************************************
Performing epoch 4 of 10
Fold 5 train - epoch: 4/10 iter: 0/15 loss: 0.8534 Acc: 59.3750% F1: 0.400 Time: 0.95s (0.00s)
Fold 5 train - epoch: 4/10 iter: 1/15 loss: 0.7992 Acc: 68.7500% F1: 0.452 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 2/15 loss: 0.7764 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 3/15 loss: 0.8071 Acc: 53.1250% F1: 0.358 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 4/15 loss: 0.8150 Acc: 71.8750% F1: 0.509 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 5/15 loss: 0.6914 Acc: 71.8750% F1: 0.499 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 6/15 loss: 0.5801 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 7/15 loss: 0.7887 Acc: 68.7500% F1: 0.453 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 8/15 loss: 0.9918 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 9/15 loss: 0.8534 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 10/15 loss: 0.8371 Acc: 56.2500% F1: 0.356 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/10 iter: 11/15 loss: 0.6679 Acc: 71.8750% F1: 0.460 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 12/15 loss: 0.7417 Acc: 65.6250% F1: 0.418 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 13/15 loss: 0.7462 Acc: 75.0000% F1: 0.515 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 14/15 loss: 0.0864 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 66.6667% F1: 0.4440 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7554 Acc: 71.8750% F1: 0.415 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4414 Acc: 27.7778% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 56.0000% F1: 0.3829 *
*********************************************************
Performing epoch 5 of 10
Fold 5 train - epoch: 5/10 iter: 0/15 loss: 0.8102 Acc: 62.5000% F1: 0.440 Time: 0.95s (0.00s)
Fold 5 train - epoch: 5/10 iter: 1/15 loss: 0.7039 Acc: 71.8750% F1: 0.502 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 2/15 loss: 0.6058 Acc: 84.3750% F1: 0.590 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 3/15 loss: 0.8032 Acc: 65.6250% F1: 0.452 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 4/15 loss: 0.7567 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 5/15 loss: 0.5982 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 6/15 loss: 0.5911 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 7/15 loss: 0.7880 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 8/15 loss: 0.8940 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 9/15 loss: 0.8086 Acc: 65.6250% F1: 0.462 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 10/15 loss: 0.7778 Acc: 62.5000% F1: 0.435 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 11/15 loss: 0.5156 Acc: 87.5000% F1: 0.751 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 12/15 loss: 0.6673 Acc: 75.0000% F1: 0.520 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 13/15 loss: 0.6375 Acc: 84.3750% F1: 0.584 Time: 0.93s (0.02s)
Fold 5 train - epoch: 5/10 iter: 14/15 loss: 0.0200 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 5 train Avg acc: 72.2222% F1: 0.5214 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9255 Acc: 53.1250% F1: 0.317 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 5/10 iter: 1/2 loss: 1.4383 Acc: 27.7778% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 5 train-dev Avg acc: 44.0000% F1: 0.3094 *
*********************************************************
Performing epoch 6 of 10
Fold 5 train - epoch: 6/10 iter: 0/15 loss: 0.7574 Acc: 59.3750% F1: 0.431 Time: 0.94s (0.00s)
Fold 5 train - epoch: 6/10 iter: 1/15 loss: 0.6984 Acc: 71.8750% F1: 0.509 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 2/15 loss: 0.5230 Acc: 78.1250% F1: 0.547 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 3/15 loss: 0.6537 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 4/15 loss: 0.6378 Acc: 75.0000% F1: 0.528 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 5/15 loss: 0.4779 Acc: 87.5000% F1: 0.819 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 6/15 loss: 0.4204 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 7/15 loss: 0.5928 Acc: 78.1250% F1: 0.527 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 8/15 loss: 0.7245 Acc: 68.7500% F1: 0.492 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 9/15 loss: 0.7482 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 10/15 loss: 0.6663 Acc: 68.7500% F1: 0.590 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 11/15 loss: 0.4752 Acc: 84.3750% F1: 0.811 Time: 0.94s (0.02s)
Fold 5 train - epoch: 6/10 iter: 12/15 loss: 0.5940 Acc: 71.8750% F1: 0.595 Time: 0.93s (0.04s)
Fold 5 train - epoch: 6/10 iter: 13/15 loss: 0.5312 Acc: 87.5000% F1: 0.763 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 14/15 loss: 0.0377 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 6 train Avg acc: 75.1111% F1: 0.5965 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 6/10 iter: 0/2 loss: 1.1934 Acc: 34.3750% F1: 0.231 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 6/10 iter: 1/2 loss: 1.3969 Acc: 33.3333% F1: 0.190 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 6 train-dev Avg acc: 34.0000% F1: 0.2523 *
*********************************************************
Performing epoch 7 of 10
Fold 5 train - epoch: 7/10 iter: 0/15 loss: 0.7066 Acc: 75.0000% F1: 0.651 Time: 0.94s (0.00s)
Fold 5 train - epoch: 7/10 iter: 1/15 loss: 0.6125 Acc: 65.6250% F1: 0.467 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 2/15 loss: 0.4694 Acc: 84.3750% F1: 0.832 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 3/15 loss: 0.6158 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 4/15 loss: 0.5095 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 5/15 loss: 0.3656 Acc: 87.5000% F1: 0.814 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 6/15 loss: 0.3398 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 7/15 loss: 0.4206 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 8/15 loss: 0.5463 Acc: 75.0000% F1: 0.663 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 9/15 loss: 0.5412 Acc: 71.8750% F1: 0.623 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 10/15 loss: 0.6911 Acc: 62.5000% F1: 0.530 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 11/15 loss: 0.4352 Acc: 81.2500% F1: 0.762 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 12/15 loss: 0.5218 Acc: 75.0000% F1: 0.700 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 13/15 loss: 0.4063 Acc: 84.3750% F1: 0.787 Time: 0.93s (0.02s)
Fold 5 train - epoch: 7/10 iter: 14/15 loss: 0.0056 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 7 train Avg acc: 77.5556% F1: 0.6838 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 7/10 iter: 0/2 loss: 1.0276 Acc: 53.1250% F1: 0.343 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 7/10 iter: 1/2 loss: 1.8285 Acc: 22.2222% F1: 0.140 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 7 train-dev Avg acc: 42.0000% F1: 0.3119 *
*********************************************************
Performing epoch 8 of 10
Fold 5 train - epoch: 8/10 iter: 0/15 loss: 0.5373 Acc: 71.8750% F1: 0.614 Time: 0.94s (0.00s)
Fold 5 train - epoch: 8/10 iter: 1/15 loss: 0.5082 Acc: 81.2500% F1: 0.688 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 2/15 loss: 0.2220 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 3/15 loss: 0.5571 Acc: 75.0000% F1: 0.655 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 4/15 loss: 0.6328 Acc: 71.8750% F1: 0.512 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 5/15 loss: 0.4521 Acc: 81.2500% F1: 0.766 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 6/15 loss: 0.3330 Acc: 93.7500% F1: 0.844 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 7/15 loss: 0.3309 Acc: 87.5000% F1: 0.601 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 8/15 loss: 0.4785 Acc: 78.1250% F1: 0.725 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 9/15 loss: 0.3721 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 10/15 loss: 0.4297 Acc: 81.2500% F1: 0.773 Time: 0.94s (0.02s)
Fold 5 train - epoch: 8/10 iter: 11/15 loss: 0.4203 Acc: 81.2500% F1: 0.762 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 12/15 loss: 0.4612 Acc: 78.1250% F1: 0.716 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 13/15 loss: 0.5991 Acc: 78.1250% F1: 0.681 Time: 0.93s (0.02s)
Fold 5 train - epoch: 8/10 iter: 14/15 loss: 0.0025 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 5 Epoch 8 train Avg acc: 82.0000% F1: 0.7440 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 8/10 iter: 0/2 loss: 0.7909 Acc: 71.8750% F1: 0.279 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 8/10 iter: 1/2 loss: 3.2834 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 8 train-dev Avg acc: 50.0000% F1: 0.2825 *
*********************************************************
Performing epoch 9 of 10
Fold 5 train - epoch: 9/10 iter: 0/15 loss: 0.4129 Acc: 78.1250% F1: 0.724 Time: 0.94s (0.00s)
Fold 5 train - epoch: 9/10 iter: 1/15 loss: 0.3187 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 2/15 loss: 0.2238 Acc: 96.8750% F1: 0.977 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 3/15 loss: 0.2729 Acc: 93.7500% F1: 0.810 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 4/15 loss: 0.4314 Acc: 81.2500% F1: 0.698 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 5/15 loss: 0.4522 Acc: 75.0000% F1: 0.688 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 6/15 loss: 0.3214 Acc: 87.5000% F1: 0.798 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 7/15 loss: 0.4029 Acc: 87.5000% F1: 0.912 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 8/15 loss: 0.4916 Acc: 78.1250% F1: 0.763 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 9/15 loss: 0.3804 Acc: 81.2500% F1: 0.738 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 10/15 loss: 0.4248 Acc: 84.3750% F1: 0.776 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 11/15 loss: 0.2871 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 12/15 loss: 0.3018 Acc: 90.6250% F1: 0.838 Time: 0.94s (0.02s)
Fold 5 train - epoch: 9/10 iter: 13/15 loss: 0.3054 Acc: 90.6250% F1: 0.792 Time: 0.93s (0.02s)
Fold 5 train - epoch: 9/10 iter: 14/15 loss: 0.0052 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 5 Epoch 9 train Avg acc: 85.5556% F1: 0.7893 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 9/10 iter: 0/2 loss: 0.7809 Acc: 78.1250% F1: 0.292 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 9/10 iter: 1/2 loss: 4.2936 Acc: 5.5556% F1: 0.051 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 9 train-dev Avg acc: 52.0000% F1: 0.2623 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/10 iter: 0/15 loss: 4.0106 Acc: 12.5000% F1: 0.081 Time: 0.96s (0.00s)
Fold 6 train - epoch: 0/10 iter: 1/15 loss: 2.9950 Acc: 25.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 2/15 loss: 1.9962 Acc: 37.5000% F1: 0.353 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 3/15 loss: 1.4840 Acc: 37.5000% F1: 0.258 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 4/15 loss: 1.4681 Acc: 43.7500% F1: 0.278 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 5/15 loss: 1.2160 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 6/15 loss: 0.9412 Acc: 65.6250% F1: 0.322 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 7/15 loss: 1.0980 Acc: 50.0000% F1: 0.326 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 8/15 loss: 1.4746 Acc: 37.5000% F1: 0.249 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 9/15 loss: 1.2002 Acc: 25.0000% F1: 0.178 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 10/15 loss: 1.2073 Acc: 43.7500% F1: 0.296 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 11/15 loss: 1.0656 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 12/15 loss: 1.1072 Acc: 43.7500% F1: 0.311 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/10 iter: 13/15 loss: 1.0116 Acc: 53.1250% F1: 0.369 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/10 iter: 14/15 loss: 0.6856 Acc: 50.0000% F1: 0.333 Time: 0.10s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 41.1111% F1: 0.3462 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6735 Acc: 62.5000% F1: 0.451 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 0/10 iter: 1/2 loss: 1.7206 Acc: 22.2222% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 48.0000% F1: 0.3090 *
*********************************************************
Performing epoch 1 of 10
Fold 6 train - epoch: 1/10 iter: 0/15 loss: 0.9812 Acc: 56.2500% F1: 0.350 Time: 0.94s (0.00s)
Fold 6 train - epoch: 1/10 iter: 1/15 loss: 0.9213 Acc: 62.5000% F1: 0.385 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 2/15 loss: 1.0195 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 3/15 loss: 0.9959 Acc: 59.3750% F1: 0.296 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 4/15 loss: 1.0215 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 5/15 loss: 0.9409 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 6/15 loss: 0.6996 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 7/15 loss: 1.0071 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 8/15 loss: 1.1166 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 9/15 loss: 0.9929 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 10/15 loss: 1.0208 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 11/15 loss: 0.8045 Acc: 68.7500% F1: 0.418 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 12/15 loss: 0.9617 Acc: 53.1250% F1: 0.312 Time: 0.94s (0.02s)
Fold 6 train - epoch: 1/10 iter: 13/15 loss: 0.9495 Acc: 62.5000% F1: 0.421 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/10 iter: 14/15 loss: 0.6005 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 56.2222% F1: 0.3076 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7909 Acc: 53.1250% F1: 0.399 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3235 Acc: 33.3333% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 46.0000% F1: 0.3141 *
*********************************************************
Performing epoch 2 of 10
Fold 6 train - epoch: 2/10 iter: 0/15 loss: 1.0050 Acc: 56.2500% F1: 0.390 Time: 0.95s (0.00s)
Fold 6 train - epoch: 2/10 iter: 1/15 loss: 0.9433 Acc: 62.5000% F1: 0.427 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 2/15 loss: 0.8742 Acc: 62.5000% F1: 0.434 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 3/15 loss: 0.8928 Acc: 53.1250% F1: 0.338 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 4/15 loss: 0.9115 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 5/15 loss: 0.9225 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 6/15 loss: 0.6992 Acc: 68.7500% F1: 0.378 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 7/15 loss: 0.8838 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 8/15 loss: 1.0139 Acc: 56.2500% F1: 0.310 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 9/15 loss: 1.0035 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 10/15 loss: 1.0218 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 11/15 loss: 0.7483 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 12/15 loss: 0.8826 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.02s)
Fold 6 train - epoch: 2/10 iter: 13/15 loss: 0.8610 Acc: 59.3750% F1: 0.301 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 14/15 loss: 0.2218 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 58.4444% F1: 0.3577 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6800 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5547 Acc: 22.2222% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 56.0000% F1: 0.3537 *
*********************************************************
Performing epoch 3 of 10
Fold 6 train - epoch: 3/10 iter: 0/15 loss: 0.8669 Acc: 65.6250% F1: 0.444 Time: 0.94s (0.00s)
Fold 6 train - epoch: 3/10 iter: 1/15 loss: 0.8545 Acc: 68.7500% F1: 0.439 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 2/15 loss: 0.7969 Acc: 68.7500% F1: 0.477 Time: 0.93s (0.04s)
Fold 6 train - epoch: 3/10 iter: 3/15 loss: 0.8497 Acc: 56.2500% F1: 0.286 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 4/15 loss: 0.8462 Acc: 68.7500% F1: 0.476 Time: 0.93s (0.04s)
Fold 6 train - epoch: 3/10 iter: 5/15 loss: 0.8795 Acc: 68.7500% F1: 0.451 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 6/15 loss: 0.6735 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.04s)
Fold 6 train - epoch: 3/10 iter: 7/15 loss: 0.7932 Acc: 59.3750% F1: 0.396 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 8/15 loss: 1.0643 Acc: 56.2500% F1: 0.375 Time: 0.93s (0.04s)
Fold 6 train - epoch: 3/10 iter: 9/15 loss: 0.8807 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.04s)
Fold 6 train - epoch: 3/10 iter: 10/15 loss: 0.9092 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 11/15 loss: 0.7210 Acc: 75.0000% F1: 0.490 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 12/15 loss: 0.8095 Acc: 65.6250% F1: 0.397 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 13/15 loss: 0.7835 Acc: 62.5000% F1: 0.377 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/10 iter: 14/15 loss: 0.1239 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 64.8889% F1: 0.4201 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7805 Acc: 59.3750% F1: 0.434 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 3/10 iter: 1/2 loss: 1.4881 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.2980 *
*********************************************************
Performing epoch 4 of 10
Fold 6 train - epoch: 4/10 iter: 0/15 loss: 0.8143 Acc: 71.8750% F1: 0.506 Time: 0.94s (0.00s)
Fold 6 train - epoch: 4/10 iter: 1/15 loss: 0.7485 Acc: 68.7500% F1: 0.439 Time: 0.92s (0.03s)
Fold 6 train - epoch: 4/10 iter: 2/15 loss: 0.7368 Acc: 78.1250% F1: 0.549 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 3/15 loss: 0.7622 Acc: 65.6250% F1: 0.421 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 4/15 loss: 0.8209 Acc: 62.5000% F1: 0.437 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 5/15 loss: 0.7443 Acc: 68.7500% F1: 0.466 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 6/15 loss: 0.6206 Acc: 75.0000% F1: 0.447 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 7/15 loss: 0.8229 Acc: 68.7500% F1: 0.457 Time: 0.94s (0.02s)
Fold 6 train - epoch: 4/10 iter: 8/15 loss: 0.9041 Acc: 50.0000% F1: 0.315 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 9/15 loss: 0.8026 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 10/15 loss: 0.8355 Acc: 59.3750% F1: 0.417 Time: 0.94s (0.02s)
Fold 6 train - epoch: 4/10 iter: 11/15 loss: 0.5990 Acc: 78.1250% F1: 0.522 Time: 0.94s (0.04s)
Fold 6 train - epoch: 4/10 iter: 12/15 loss: 0.7564 Acc: 65.6250% F1: 0.445 Time: 0.93s (0.04s)
Fold 6 train - epoch: 4/10 iter: 13/15 loss: 0.7310 Acc: 68.7500% F1: 0.466 Time: 0.93s (0.02s)
Fold 6 train - epoch: 4/10 iter: 14/15 loss: 0.0432 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 67.7778% F1: 0.4587 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/10 iter: 0/2 loss: 1.0045 Acc: 40.6250% F1: 0.327 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 4/10 iter: 1/2 loss: 1.3564 Acc: 44.4444% F1: 0.254 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 42.0000% F1: 0.2967 *
*********************************************************
Performing epoch 5 of 10
Fold 6 train - epoch: 5/10 iter: 0/15 loss: 0.8362 Acc: 62.5000% F1: 0.442 Time: 0.95s (0.00s)
Fold 6 train - epoch: 5/10 iter: 1/15 loss: 0.6612 Acc: 78.1250% F1: 0.541 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 2/15 loss: 0.6137 Acc: 81.2500% F1: 0.569 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 3/15 loss: 0.6946 Acc: 78.1250% F1: 0.534 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 4/15 loss: 0.7798 Acc: 65.6250% F1: 0.463 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 5/15 loss: 0.5854 Acc: 81.2500% F1: 0.678 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 6/15 loss: 0.5072 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 7/15 loss: 0.7266 Acc: 71.8750% F1: 0.479 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 8/15 loss: 0.8548 Acc: 65.6250% F1: 0.463 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 9/15 loss: 0.7014 Acc: 68.7500% F1: 0.483 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 10/15 loss: 0.6642 Acc: 68.7500% F1: 0.493 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 11/15 loss: 0.5211 Acc: 81.2500% F1: 0.711 Time: 0.93s (0.02s)
Fold 6 train - epoch: 5/10 iter: 12/15 loss: 0.6065 Acc: 78.1250% F1: 0.546 Time: 0.94s (0.02s)
Fold 6 train - epoch: 5/10 iter: 13/15 loss: 0.6048 Acc: 87.5000% F1: 0.611 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 14/15 loss: 0.0427 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 5 train Avg acc: 75.1111% F1: 0.5451 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 5/10 iter: 0/2 loss: 1.2267 Acc: 37.5000% F1: 0.206 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 5/10 iter: 1/2 loss: 1.3330 Acc: 44.4444% F1: 0.254 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 5 train-dev Avg acc: 40.0000% F1: 0.2866 *
*********************************************************
Performing epoch 6 of 10
Fold 6 train - epoch: 6/10 iter: 0/15 loss: 0.7214 Acc: 71.8750% F1: 0.507 Time: 0.94s (0.00s)
Fold 6 train - epoch: 6/10 iter: 1/15 loss: 0.6067 Acc: 81.2500% F1: 0.562 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 2/15 loss: 0.4721 Acc: 87.5000% F1: 0.613 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 3/15 loss: 0.6053 Acc: 71.8750% F1: 0.495 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 4/15 loss: 0.5401 Acc: 84.3750% F1: 0.598 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 5/15 loss: 0.4232 Acc: 84.3750% F1: 0.714 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 6/15 loss: 0.4405 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 7/15 loss: 0.5286 Acc: 81.2500% F1: 0.548 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 8/15 loss: 0.6606 Acc: 71.8750% F1: 0.519 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 9/15 loss: 0.6354 Acc: 68.7500% F1: 0.483 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 10/15 loss: 0.5993 Acc: 75.0000% F1: 0.539 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 11/15 loss: 0.3707 Acc: 87.5000% F1: 0.751 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 12/15 loss: 0.5401 Acc: 78.1250% F1: 0.545 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 13/15 loss: 0.5527 Acc: 78.1250% F1: 0.542 Time: 0.93s (0.02s)
Fold 6 train - epoch: 6/10 iter: 14/15 loss: 0.0306 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 6 train Avg acc: 79.1111% F1: 0.5755 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 6/10 iter: 0/2 loss: 1.5151 Acc: 34.3750% F1: 0.193 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 6/10 iter: 1/2 loss: 1.3417 Acc: 44.4444% F1: 0.242 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 6 train-dev Avg acc: 38.0000% F1: 0.2724 *
*********************************************************
Performing epoch 7 of 10
Fold 6 train - epoch: 7/10 iter: 0/15 loss: 0.5158 Acc: 84.3750% F1: 0.716 Time: 0.95s (0.00s)
Fold 6 train - epoch: 7/10 iter: 1/15 loss: 0.5049 Acc: 78.1250% F1: 0.551 Time: 0.92s (0.03s)
Fold 6 train - epoch: 7/10 iter: 2/15 loss: 0.3931 Acc: 90.6250% F1: 0.789 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 3/15 loss: 0.4705 Acc: 81.2500% F1: 0.566 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 4/15 loss: 0.4244 Acc: 87.5000% F1: 0.740 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 5/15 loss: 0.4006 Acc: 84.3750% F1: 0.783 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 6/15 loss: 0.3108 Acc: 87.5000% F1: 0.596 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 7/15 loss: 0.3744 Acc: 84.3750% F1: 0.570 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 8/15 loss: 0.5774 Acc: 75.0000% F1: 0.617 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 9/15 loss: 0.4068 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 10/15 loss: 0.4497 Acc: 84.3750% F1: 0.798 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 11/15 loss: 0.2673 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 12/15 loss: 0.3580 Acc: 87.5000% F1: 0.738 Time: 0.94s (0.02s)
Fold 6 train - epoch: 7/10 iter: 13/15 loss: 0.3409 Acc: 93.7500% F1: 0.806 Time: 0.93s (0.02s)
Fold 6 train - epoch: 7/10 iter: 14/15 loss: 0.0132 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 7 train Avg acc: 85.3333% F1: 0.7167 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 7/10 iter: 0/2 loss: 1.8449 Acc: 34.3750% F1: 0.196 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 7/10 iter: 1/2 loss: 1.4685 Acc: 38.8889% F1: 0.222 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 7 train-dev Avg acc: 36.0000% F1: 0.2634 *
*********************************************************
Performing epoch 8 of 10
Fold 6 train - epoch: 8/10 iter: 0/15 loss: 0.4760 Acc: 81.2500% F1: 0.755 Time: 0.95s (0.00s)
Fold 6 train - epoch: 8/10 iter: 1/15 loss: 0.4214 Acc: 87.5000% F1: 0.814 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 2/15 loss: 0.2534 Acc: 87.5000% F1: 0.767 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 3/15 loss: 0.3806 Acc: 84.3750% F1: 0.587 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 4/15 loss: 0.3409 Acc: 90.6250% F1: 0.841 Time: 0.94s (0.02s)
Fold 6 train - epoch: 8/10 iter: 5/15 loss: 0.2342 Acc: 96.8750% F1: 0.973 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 6/15 loss: 0.2455 Acc: 93.7500% F1: 0.652 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 7/15 loss: 0.3440 Acc: 90.6250% F1: 0.834 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 8/15 loss: 0.4075 Acc: 81.2500% F1: 0.769 Time: 0.94s (0.02s)
Fold 6 train - epoch: 8/10 iter: 9/15 loss: 0.3870 Acc: 81.2500% F1: 0.701 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 10/15 loss: 0.4336 Acc: 84.3750% F1: 0.798 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 11/15 loss: 0.2741 Acc: 90.6250% F1: 0.868 Time: 0.94s (0.02s)
Fold 6 train - epoch: 8/10 iter: 12/15 loss: 0.2827 Acc: 87.5000% F1: 0.730 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 13/15 loss: 0.2394 Acc: 90.6250% F1: 0.792 Time: 0.93s (0.02s)
Fold 6 train - epoch: 8/10 iter: 14/15 loss: 0.0026 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 8 train Avg acc: 87.7778% F1: 0.8006 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 8/10 iter: 0/2 loss: 2.2406 Acc: 31.2500% F1: 0.180 Time: 0.35s (0.00s)
Fold 6 train-dev - epoch: 8/10 iter: 1/2 loss: 1.5942 Acc: 44.4444% F1: 0.232 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 8 train-dev Avg acc: 36.0000% F1: 0.2610 *
*********************************************************
Performing epoch 9 of 10
Fold 6 train - epoch: 9/10 iter: 0/15 loss: 0.3720 Acc: 84.3750% F1: 0.714 Time: 0.95s (0.00s)
Fold 6 train - epoch: 9/10 iter: 1/15 loss: 0.3336 Acc: 90.6250% F1: 0.757 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 2/15 loss: 0.2421 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 3/15 loss: 0.5091 Acc: 75.0000% F1: 0.515 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 4/15 loss: 0.3778 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 5/15 loss: 0.3108 Acc: 90.6250% F1: 0.886 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 6/15 loss: 0.2118 Acc: 93.7500% F1: 0.854 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 7/15 loss: 0.1938 Acc: 93.7500% F1: 0.646 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 8/15 loss: 0.1582 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 9/15 loss: 0.2580 Acc: 93.7500% F1: 0.868 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 10/15 loss: 0.4792 Acc: 71.8750% F1: 0.664 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 11/15 loss: 0.2384 Acc: 87.5000% F1: 0.799 Time: 0.93s (0.02s)
Fold 6 train - epoch: 9/10 iter: 12/15 loss: 0.4987 Acc: 81.2500% F1: 0.794 Time: 0.94s (0.03s)
Fold 6 train - epoch: 9/10 iter: 13/15 loss: 0.6216 Acc: 78.1250% F1: 0.681 Time: 0.94s (0.02s)
Fold 6 train - epoch: 9/10 iter: 14/15 loss: 0.0016 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 6 Epoch 9 train Avg acc: 86.0000% F1: 0.7968 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 9/10 iter: 0/2 loss: 1.6563 Acc: 46.8750% F1: 0.217 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 9/10 iter: 1/2 loss: 2.8217 Acc: 33.3333% F1: 0.211 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 9 train-dev Avg acc: 42.0000% F1: 0.3004 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/10 iter: 0/15 loss: 3.8399 Acc: 9.3750% F1: 0.059 Time: 0.95s (0.00s)
Fold 7 train - epoch: 0/10 iter: 1/15 loss: 2.9444 Acc: 25.0000% F1: 0.272 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 2/15 loss: 1.6362 Acc: 40.6250% F1: 0.384 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 3/15 loss: 1.4221 Acc: 43.7500% F1: 0.306 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 4/15 loss: 1.4995 Acc: 40.6250% F1: 0.262 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 5/15 loss: 1.2716 Acc: 59.3750% F1: 0.253 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 6/15 loss: 0.9588 Acc: 56.2500% F1: 0.315 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 7/15 loss: 1.0162 Acc: 68.7500% F1: 0.459 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 8/15 loss: 1.5077 Acc: 31.2500% F1: 0.213 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 9/15 loss: 1.3355 Acc: 28.1250% F1: 0.187 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 10/15 loss: 1.3355 Acc: 37.5000% F1: 0.249 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 11/15 loss: 1.0121 Acc: 31.2500% F1: 0.219 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 12/15 loss: 1.1288 Acc: 46.8750% F1: 0.325 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 13/15 loss: 0.9759 Acc: 56.2500% F1: 0.373 Time: 0.93s (0.03s)
Fold 7 train - epoch: 0/10 iter: 14/15 loss: 0.5354 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 41.3333% F1: 0.3473 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5518 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 0/10 iter: 1/2 loss: 1.9256 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.3119 *
*********************************************************
Performing epoch 1 of 10
Fold 7 train - epoch: 1/10 iter: 0/15 loss: 0.9679 Acc: 62.5000% F1: 0.361 Time: 0.94s (0.00s)
Fold 7 train - epoch: 1/10 iter: 1/15 loss: 0.9848 Acc: 56.2500% F1: 0.245 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 2/15 loss: 1.0064 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 3/15 loss: 0.9701 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 4/15 loss: 1.1591 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 5/15 loss: 0.9645 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 6/15 loss: 0.7594 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 7/15 loss: 0.9211 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 8/15 loss: 1.0600 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.03s)
Fold 7 train - epoch: 1/10 iter: 9/15 loss: 1.0458 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 10/15 loss: 1.0365 Acc: 43.7500% F1: 0.265 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 11/15 loss: 0.7882 Acc: 75.0000% F1: 0.498 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 12/15 loss: 0.9934 Acc: 50.0000% F1: 0.332 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 13/15 loss: 0.9068 Acc: 59.3750% F1: 0.391 Time: 0.94s (0.02s)
Fold 7 train - epoch: 1/10 iter: 14/15 loss: 0.5593 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 56.0000% F1: 0.3017 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7532 Acc: 50.0000% F1: 0.333 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3756 Acc: 38.8889% F1: 0.233 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 46.0000% F1: 0.3135 *
*********************************************************
Performing epoch 2 of 10
Fold 7 train - epoch: 2/10 iter: 0/15 loss: 0.9674 Acc: 53.1250% F1: 0.375 Time: 0.94s (0.00s)
Fold 7 train - epoch: 2/10 iter: 1/15 loss: 0.9319 Acc: 59.3750% F1: 0.415 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 2/15 loss: 0.8548 Acc: 59.3750% F1: 0.415 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 3/15 loss: 0.8453 Acc: 59.3750% F1: 0.373 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 4/15 loss: 0.9120 Acc: 59.3750% F1: 0.403 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 5/15 loss: 0.9094 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 6/15 loss: 0.6982 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 7/15 loss: 0.8842 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 8/15 loss: 1.0545 Acc: 56.2500% F1: 0.310 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 9/15 loss: 1.0159 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 10/15 loss: 1.0191 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 11/15 loss: 0.7218 Acc: 71.8750% F1: 0.408 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/10 iter: 12/15 loss: 0.9084 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 13/15 loss: 0.8705 Acc: 59.3750% F1: 0.336 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 14/15 loss: 0.2233 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 58.8889% F1: 0.3648 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6227 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 2/10 iter: 1/2 loss: 1.6693 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2586 *
*********************************************************
Performing epoch 3 of 10
Fold 7 train - epoch: 3/10 iter: 0/15 loss: 0.8615 Acc: 68.7500% F1: 0.451 Time: 0.94s (0.00s)
Fold 7 train - epoch: 3/10 iter: 1/15 loss: 0.8395 Acc: 62.5000% F1: 0.383 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 2/15 loss: 0.8512 Acc: 59.3750% F1: 0.404 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 3/15 loss: 0.8351 Acc: 62.5000% F1: 0.391 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 4/15 loss: 0.8526 Acc: 65.6250% F1: 0.450 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 5/15 loss: 0.8238 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 6/15 loss: 0.6946 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 7/15 loss: 0.8266 Acc: 59.3750% F1: 0.379 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 8/15 loss: 1.0044 Acc: 56.2500% F1: 0.345 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 9/15 loss: 0.9562 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 10/15 loss: 0.9038 Acc: 56.2500% F1: 0.370 Time: 0.94s (0.02s)
Fold 7 train - epoch: 3/10 iter: 11/15 loss: 0.6429 Acc: 75.0000% F1: 0.452 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 12/15 loss: 0.8467 Acc: 62.5000% F1: 0.385 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 13/15 loss: 0.8414 Acc: 65.6250% F1: 0.414 Time: 0.93s (0.03s)
Fold 7 train - epoch: 3/10 iter: 14/15 loss: 0.1941 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 63.7778% F1: 0.4091 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6830 Acc: 68.7500% F1: 0.407 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6258 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.2926 *
*********************************************************
Performing epoch 4 of 10
Fold 7 train - epoch: 4/10 iter: 0/15 loss: 0.7935 Acc: 65.6250% F1: 0.444 Time: 0.94s (0.00s)
Fold 7 train - epoch: 4/10 iter: 1/15 loss: 0.8225 Acc: 68.7500% F1: 0.452 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 2/15 loss: 0.6917 Acc: 78.1250% F1: 0.549 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 3/15 loss: 0.7324 Acc: 62.5000% F1: 0.403 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 4/15 loss: 0.8333 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 5/15 loss: 0.6750 Acc: 75.0000% F1: 0.513 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 6/15 loss: 0.6853 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 7/15 loss: 0.8176 Acc: 59.3750% F1: 0.365 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 8/15 loss: 0.9375 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 9/15 loss: 0.8788 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 10/15 loss: 0.9139 Acc: 59.3750% F1: 0.408 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 11/15 loss: 0.5832 Acc: 81.2500% F1: 0.557 Time: 0.93s (0.03s)
Fold 7 train - epoch: 4/10 iter: 12/15 loss: 0.8116 Acc: 62.5000% F1: 0.426 Time: 0.94s (0.02s)
Fold 7 train - epoch: 4/10 iter: 13/15 loss: 0.7267 Acc: 75.0000% F1: 0.513 Time: 0.94s (0.02s)
Fold 7 train - epoch: 4/10 iter: 14/15 loss: 0.0675 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 68.4444% F1: 0.4585 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8535 Acc: 50.0000% F1: 0.382 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4818 Acc: 38.8889% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3186 *
*********************************************************
Performing epoch 5 of 10
Fold 7 train - epoch: 5/10 iter: 0/15 loss: 0.7575 Acc: 75.0000% F1: 0.527 Time: 0.94s (0.00s)
Fold 7 train - epoch: 5/10 iter: 1/15 loss: 0.7744 Acc: 75.0000% F1: 0.513 Time: 0.92s (0.03s)
Fold 7 train - epoch: 5/10 iter: 2/15 loss: 0.6470 Acc: 75.0000% F1: 0.526 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 3/15 loss: 0.6983 Acc: 68.7500% F1: 0.454 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 4/15 loss: 0.7388 Acc: 68.7500% F1: 0.488 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 5/15 loss: 0.5884 Acc: 71.8750% F1: 0.459 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 6/15 loss: 0.5141 Acc: 81.2500% F1: 0.504 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 7/15 loss: 0.6392 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 8/15 loss: 0.8093 Acc: 65.6250% F1: 0.463 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 9/15 loss: 0.7806 Acc: 62.5000% F1: 0.433 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 10/15 loss: 0.7685 Acc: 56.2500% F1: 0.402 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 11/15 loss: 0.5279 Acc: 81.2500% F1: 0.697 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 12/15 loss: 0.6746 Acc: 78.1250% F1: 0.667 Time: 0.94s (0.03s)
Fold 7 train - epoch: 5/10 iter: 13/15 loss: 0.5503 Acc: 81.2500% F1: 0.554 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 14/15 loss: 0.0481 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 5 train Avg acc: 72.6667% F1: 0.5230 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 5/10 iter: 0/2 loss: 1.0273 Acc: 46.8750% F1: 0.244 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 5/10 iter: 1/2 loss: 1.5098 Acc: 38.8889% F1: 0.222 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 5 train-dev Avg acc: 44.0000% F1: 0.3096 *
*********************************************************
Performing epoch 6 of 10
Fold 7 train - epoch: 6/10 iter: 0/15 loss: 0.6606 Acc: 68.7500% F1: 0.481 Time: 0.95s (0.00s)
Fold 7 train - epoch: 6/10 iter: 1/15 loss: 0.6986 Acc: 65.6250% F1: 0.454 Time: 0.92s (0.03s)
Fold 7 train - epoch: 6/10 iter: 2/15 loss: 0.4576 Acc: 84.3750% F1: 0.745 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 3/15 loss: 0.5957 Acc: 71.8750% F1: 0.496 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 4/15 loss: 0.6709 Acc: 68.7500% F1: 0.486 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 5/15 loss: 0.4628 Acc: 87.5000% F1: 0.815 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 6/15 loss: 0.3791 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 7/15 loss: 0.5515 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 8/15 loss: 0.7187 Acc: 68.7500% F1: 0.485 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 9/15 loss: 0.4594 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.03s)
Fold 7 train - epoch: 6/10 iter: 10/15 loss: 0.5733 Acc: 78.1250% F1: 0.751 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 11/15 loss: 0.4212 Acc: 87.5000% F1: 0.756 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 12/15 loss: 0.6293 Acc: 75.0000% F1: 0.634 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 13/15 loss: 0.5100 Acc: 84.3750% F1: 0.734 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 14/15 loss: 0.0300 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 6 train Avg acc: 78.4444% F1: 0.6300 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 6/10 iter: 0/2 loss: 1.0927 Acc: 50.0000% F1: 0.262 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 6/10 iter: 1/2 loss: 1.9266 Acc: 27.7778% F1: 0.175 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 6 train-dev Avg acc: 42.0000% F1: 0.2931 *
*********************************************************
Performing epoch 7 of 10
Fold 7 train - epoch: 7/10 iter: 0/15 loss: 0.5015 Acc: 81.2500% F1: 0.567 Time: 0.95s (0.00s)
Fold 7 train - epoch: 7/10 iter: 1/15 loss: 0.4298 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 2/15 loss: 0.3110 Acc: 90.6250% F1: 0.789 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 3/15 loss: 0.4106 Acc: 81.2500% F1: 0.571 Time: 0.95s (0.02s)
Fold 7 train - epoch: 7/10 iter: 4/15 loss: 0.4470 Acc: 81.2500% F1: 0.575 Time: 0.93s (0.05s)
Fold 7 train - epoch: 7/10 iter: 5/15 loss: 0.3896 Acc: 84.3750% F1: 0.709 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 6/15 loss: 0.3400 Acc: 87.5000% F1: 0.579 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 7/15 loss: 0.2872 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 8/15 loss: 0.6055 Acc: 71.8750% F1: 0.519 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 9/15 loss: 0.4244 Acc: 84.3750% F1: 0.602 Time: 0.94s (0.02s)
Fold 7 train - epoch: 7/10 iter: 10/15 loss: 0.3931 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 11/15 loss: 0.3000 Acc: 90.6250% F1: 0.868 Time: 0.94s (0.02s)
Fold 7 train - epoch: 7/10 iter: 12/15 loss: 0.3288 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 13/15 loss: 0.3174 Acc: 90.6250% F1: 0.781 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 14/15 loss: 0.0178 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 7 Epoch 7 train Avg acc: 86.4444% F1: 0.7168 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 7/10 iter: 0/2 loss: 1.4479 Acc: 40.6250% F1: 0.197 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 7/10 iter: 1/2 loss: 2.0251 Acc: 38.8889% F1: 0.283 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 7 train-dev Avg acc: 40.0000% F1: 0.3319 *
*********************************************************
Performing epoch 8 of 10
Fold 7 train - epoch: 8/10 iter: 0/15 loss: 0.3789 Acc: 84.3750% F1: 0.717 Time: 0.95s (0.00s)
Fold 7 train - epoch: 8/10 iter: 1/15 loss: 0.3774 Acc: 87.5000% F1: 0.738 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 2/15 loss: 0.2324 Acc: 90.6250% F1: 0.791 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 3/15 loss: 0.2691 Acc: 93.7500% F1: 0.816 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 4/15 loss: 0.2798 Acc: 93.7500% F1: 0.892 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 5/15 loss: 0.3916 Acc: 87.5000% F1: 0.815 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 6/15 loss: 0.2246 Acc: 90.6250% F1: 0.611 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 7/15 loss: 0.2275 Acc: 96.8750% F1: 0.878 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 8/15 loss: 0.2253 Acc: 93.7500% F1: 0.915 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 9/15 loss: 0.2497 Acc: 90.6250% F1: 0.765 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 10/15 loss: 0.2272 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 11/15 loss: 0.2059 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 7 train - epoch: 8/10 iter: 12/15 loss: 0.2456 Acc: 93.7500% F1: 0.859 Time: 0.94s (0.03s)
Fold 7 train - epoch: 8/10 iter: 13/15 loss: 0.1808 Acc: 93.7500% F1: 0.816 Time: 0.94s (0.03s)
Fold 7 train - epoch: 8/10 iter: 14/15 loss: 0.0030 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 7 Epoch 8 train Avg acc: 91.5556% F1: 0.8321 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 8/10 iter: 0/2 loss: 1.8275 Acc: 43.7500% F1: 0.254 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 8/10 iter: 1/2 loss: 2.2205 Acc: 38.8889% F1: 0.283 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 8 train-dev Avg acc: 42.0000% F1: 0.3523 *
*********************************************************
Performing epoch 9 of 10
Fold 7 train - epoch: 9/10 iter: 0/15 loss: 0.3130 Acc: 87.5000% F1: 0.735 Time: 0.95s (0.00s)
Fold 7 train - epoch: 9/10 iter: 1/15 loss: 0.2814 Acc: 93.7500% F1: 0.913 Time: 0.93s (0.03s)
Fold 7 train - epoch: 9/10 iter: 2/15 loss: 0.0888 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 3/15 loss: 0.2077 Acc: 93.7500% F1: 0.900 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 4/15 loss: 0.1306 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 7 train - epoch: 9/10 iter: 5/15 loss: 0.2041 Acc: 90.6250% F1: 0.812 Time: 0.93s (0.03s)
Fold 7 train - epoch: 9/10 iter: 6/15 loss: 0.2930 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.03s)
Fold 7 train - epoch: 9/10 iter: 7/15 loss: 0.1389 Acc: 96.8750% F1: 0.878 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 8/15 loss: 0.1450 Acc: 96.8750% F1: 0.968 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 9/15 loss: 0.2372 Acc: 87.5000% F1: 0.747 Time: 0.93s (0.03s)
Fold 7 train - epoch: 9/10 iter: 10/15 loss: 0.1791 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 11/15 loss: 0.0792 Acc: 100.0000% F1: 1.000 Time: 0.94s (0.02s)
Fold 7 train - epoch: 9/10 iter: 12/15 loss: 0.1156 Acc: 96.8750% F1: 0.937 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 13/15 loss: 0.2187 Acc: 93.7500% F1: 0.896 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 14/15 loss: 0.0013 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 9 train Avg acc: 94.6667% F1: 0.9112 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 9/10 iter: 0/2 loss: 2.0903 Acc: 43.7500% F1: 0.274 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 9/10 iter: 1/2 loss: 2.6145 Acc: 38.8889% F1: 0.283 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 9 train-dev Avg acc: 42.0000% F1: 0.3550 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/10 iter: 0/15 loss: 4.1280 Acc: 12.5000% F1: 0.076 Time: 0.96s (0.00s)
Fold 8 train - epoch: 0/10 iter: 1/15 loss: 2.9939 Acc: 21.8750% F1: 0.210 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 2/15 loss: 1.7512 Acc: 34.3750% F1: 0.324 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 3/15 loss: 1.4352 Acc: 34.3750% F1: 0.235 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 4/15 loss: 1.5054 Acc: 46.8750% F1: 0.313 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 5/15 loss: 1.4076 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 6/15 loss: 0.8421 Acc: 62.5000% F1: 0.345 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 7/15 loss: 1.0296 Acc: 53.1250% F1: 0.344 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 8/15 loss: 1.5431 Acc: 37.5000% F1: 0.273 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 9/15 loss: 1.2960 Acc: 37.5000% F1: 0.265 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 10/15 loss: 1.3153 Acc: 34.3750% F1: 0.219 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 11/15 loss: 0.9997 Acc: 40.6250% F1: 0.284 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 12/15 loss: 1.0311 Acc: 56.2500% F1: 0.401 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 13/15 loss: 1.0350 Acc: 53.1250% F1: 0.355 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 14/15 loss: 0.5151 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 41.3333% F1: 0.3509 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6152 Acc: 78.1250% F1: 0.547 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/10 iter: 1/2 loss: 1.7538 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.3119 *
*********************************************************
Performing epoch 1 of 10
Fold 8 train - epoch: 1/10 iter: 0/15 loss: 0.9150 Acc: 59.3750% F1: 0.344 Time: 0.95s (0.00s)
Fold 8 train - epoch: 1/10 iter: 1/15 loss: 0.9401 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.04s)
Fold 8 train - epoch: 1/10 iter: 2/15 loss: 0.9195 Acc: 50.0000% F1: 0.262 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 3/15 loss: 0.9232 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.04s)
Fold 8 train - epoch: 1/10 iter: 4/15 loss: 1.0765 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 5/15 loss: 0.8918 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 6/15 loss: 0.7297 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 7/15 loss: 0.9369 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 8/15 loss: 1.1189 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.04s)
Fold 8 train - epoch: 1/10 iter: 9/15 loss: 1.0633 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 10/15 loss: 0.9594 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 11/15 loss: 0.8199 Acc: 65.6250% F1: 0.398 Time: 0.94s (0.03s)
Fold 8 train - epoch: 1/10 iter: 12/15 loss: 0.9745 Acc: 43.7500% F1: 0.285 Time: 0.93s (0.04s)
Fold 8 train - epoch: 1/10 iter: 13/15 loss: 0.9270 Acc: 56.2500% F1: 0.382 Time: 0.93s (0.03s)
Fold 8 train - epoch: 1/10 iter: 14/15 loss: 0.7187 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 54.6667% F1: 0.2934 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7881 Acc: 46.8750% F1: 0.398 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 1/10 iter: 1/2 loss: 1.2709 Acc: 33.3333% F1: 0.182 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 42.0000% F1: 0.2940 *
*********************************************************
Performing epoch 2 of 10
Fold 8 train - epoch: 2/10 iter: 0/15 loss: 0.9375 Acc: 59.3750% F1: 0.415 Time: 0.94s (0.00s)
Fold 8 train - epoch: 2/10 iter: 1/15 loss: 0.8942 Acc: 59.3750% F1: 0.408 Time: 0.93s (0.05s)
Fold 8 train - epoch: 2/10 iter: 2/15 loss: 0.8649 Acc: 62.5000% F1: 0.431 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 3/15 loss: 0.8518 Acc: 62.5000% F1: 0.369 Time: 0.94s (0.03s)
Fold 8 train - epoch: 2/10 iter: 4/15 loss: 0.9043 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.05s)
Fold 8 train - epoch: 2/10 iter: 5/15 loss: 0.8438 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 6/15 loss: 0.7296 Acc: 71.8750% F1: 0.396 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 7/15 loss: 0.8660 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 8/15 loss: 1.0059 Acc: 56.2500% F1: 0.310 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 9/15 loss: 1.0524 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 10/15 loss: 1.0430 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 11/15 loss: 0.7032 Acc: 68.7500% F1: 0.343 Time: 0.93s (0.04s)
Fold 8 train - epoch: 2/10 iter: 12/15 loss: 0.9020 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 13/15 loss: 0.9625 Acc: 56.2500% F1: 0.245 Time: 0.93s (0.04s)
Fold 8 train - epoch: 2/10 iter: 14/15 loss: 0.2993 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 58.6667% F1: 0.3468 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6495 Acc: 84.3750% F1: 0.599 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 2/10 iter: 1/2 loss: 1.4619 Acc: 22.2222% F1: 0.157 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 62.0000% F1: 0.3865 *
*********************************************************
Performing epoch 3 of 10
Fold 8 train - epoch: 3/10 iter: 0/15 loss: 0.8975 Acc: 65.6250% F1: 0.444 Time: 0.95s (0.00s)
Fold 8 train - epoch: 3/10 iter: 1/15 loss: 0.8220 Acc: 71.8750% F1: 0.475 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 2/15 loss: 0.8259 Acc: 68.7500% F1: 0.472 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 3/15 loss: 0.8037 Acc: 68.7500% F1: 0.462 Time: 0.93s (0.04s)
Fold 8 train - epoch: 3/10 iter: 4/15 loss: 0.8524 Acc: 68.7500% F1: 0.464 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 5/15 loss: 0.8028 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 6/15 loss: 0.7192 Acc: 68.7500% F1: 0.403 Time: 0.93s (0.04s)
Fold 8 train - epoch: 3/10 iter: 7/15 loss: 0.7760 Acc: 62.5000% F1: 0.407 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 8/15 loss: 1.0101 Acc: 59.3750% F1: 0.363 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 9/15 loss: 0.9661 Acc: 56.2500% F1: 0.352 Time: 0.94s (0.04s)
Fold 8 train - epoch: 3/10 iter: 10/15 loss: 0.9387 Acc: 53.1250% F1: 0.333 Time: 0.94s (0.04s)
Fold 8 train - epoch: 3/10 iter: 11/15 loss: 0.7065 Acc: 75.0000% F1: 0.452 Time: 0.94s (0.03s)
Fold 8 train - epoch: 3/10 iter: 12/15 loss: 0.8145 Acc: 65.6250% F1: 0.417 Time: 0.94s (0.03s)
Fold 8 train - epoch: 3/10 iter: 13/15 loss: 0.8512 Acc: 62.5000% F1: 0.377 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 14/15 loss: 0.1154 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 66.0000% F1: 0.4289 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6856 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 3/10 iter: 1/2 loss: 1.3600 Acc: 33.3333% F1: 0.200 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 58.0000% F1: 0.3833 *
*********************************************************
Performing epoch 4 of 10
Fold 8 train - epoch: 4/10 iter: 0/15 loss: 0.8493 Acc: 62.5000% F1: 0.402 Time: 0.94s (0.00s)
Fold 8 train - epoch: 4/10 iter: 1/15 loss: 0.7888 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.04s)
Fold 8 train - epoch: 4/10 iter: 2/15 loss: 0.7702 Acc: 71.8750% F1: 0.504 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 3/15 loss: 0.7740 Acc: 65.6250% F1: 0.434 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 4/15 loss: 0.8172 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 5/15 loss: 0.7387 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.04s)
Fold 8 train - epoch: 4/10 iter: 6/15 loss: 0.6780 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 7/15 loss: 0.7151 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 8/15 loss: 0.9535 Acc: 59.3750% F1: 0.368 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 9/15 loss: 0.9630 Acc: 56.2500% F1: 0.352 Time: 0.94s (0.04s)
Fold 8 train - epoch: 4/10 iter: 10/15 loss: 0.8118 Acc: 62.5000% F1: 0.430 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 11/15 loss: 0.5745 Acc: 78.1250% F1: 0.512 Time: 0.94s (0.03s)
Fold 8 train - epoch: 4/10 iter: 12/15 loss: 0.7430 Acc: 78.1250% F1: 0.545 Time: 0.94s (0.04s)
Fold 8 train - epoch: 4/10 iter: 13/15 loss: 0.7243 Acc: 71.8750% F1: 0.486 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 14/15 loss: 0.0933 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 70.2222% F1: 0.4753 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8408 Acc: 59.3750% F1: 0.513 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 4/10 iter: 1/2 loss: 1.1330 Acc: 50.0000% F1: 0.240 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 56.0000% F1: 0.3952 *
*********************************************************
Performing epoch 5 of 10
Fold 8 train - epoch: 5/10 iter: 0/15 loss: 0.7844 Acc: 65.6250% F1: 0.455 Time: 0.94s (0.00s)
Fold 8 train - epoch: 5/10 iter: 1/15 loss: 0.7567 Acc: 68.7500% F1: 0.480 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 2/15 loss: 0.7028 Acc: 71.8750% F1: 0.503 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 3/15 loss: 0.6912 Acc: 75.0000% F1: 0.510 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 4/15 loss: 0.7505 Acc: 75.0000% F1: 0.528 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 5/15 loss: 0.6813 Acc: 71.8750% F1: 0.489 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 6/15 loss: 0.5414 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 7/15 loss: 0.7180 Acc: 71.8750% F1: 0.479 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 8/15 loss: 0.8583 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 9/15 loss: 0.8128 Acc: 56.2500% F1: 0.350 Time: 0.94s (0.03s)
Fold 8 train - epoch: 5/10 iter: 10/15 loss: 0.7326 Acc: 78.1250% F1: 0.560 Time: 0.94s (0.03s)
Fold 8 train - epoch: 5/10 iter: 11/15 loss: 0.6129 Acc: 81.2500% F1: 0.685 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 12/15 loss: 0.7228 Acc: 78.1250% F1: 0.550 Time: 0.94s (0.03s)
Fold 8 train - epoch: 5/10 iter: 13/15 loss: 0.6193 Acc: 81.2500% F1: 0.550 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 14/15 loss: 0.0390 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 8 Epoch 5 train Avg acc: 72.6667% F1: 0.5113 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9979 Acc: 43.7500% F1: 0.417 Time: 0.35s (0.00s)
Fold 8 train-dev - epoch: 5/10 iter: 1/2 loss: 1.0650 Acc: 50.0000% F1: 0.240 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 5 train-dev Avg acc: 46.0000% F1: 0.3250 *
*********************************************************
Performing epoch 6 of 10
Fold 8 train - epoch: 6/10 iter: 0/15 loss: 0.6385 Acc: 78.1250% F1: 0.551 Time: 0.94s (0.00s)
Fold 8 train - epoch: 6/10 iter: 1/15 loss: 0.6265 Acc: 78.1250% F1: 0.546 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 2/15 loss: 0.6210 Acc: 78.1250% F1: 0.547 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 3/15 loss: 0.6071 Acc: 81.2500% F1: 0.558 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 4/15 loss: 0.5820 Acc: 78.1250% F1: 0.556 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 5/15 loss: 0.5457 Acc: 81.2500% F1: 0.569 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 6/15 loss: 0.3824 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 7/15 loss: 0.6278 Acc: 78.1250% F1: 0.534 Time: 0.94s (0.04s)
Fold 8 train - epoch: 6/10 iter: 8/15 loss: 0.7442 Acc: 65.6250% F1: 0.448 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 9/15 loss: 0.6868 Acc: 68.7500% F1: 0.479 Time: 0.94s (0.04s)
Fold 8 train - epoch: 6/10 iter: 10/15 loss: 0.6773 Acc: 78.1250% F1: 0.750 Time: 0.94s (0.04s)
Fold 8 train - epoch: 6/10 iter: 11/15 loss: 0.4887 Acc: 81.2500% F1: 0.712 Time: 0.93s (0.03s)
Fold 8 train - epoch: 6/10 iter: 12/15 loss: 0.5827 Acc: 81.2500% F1: 0.565 Time: 0.94s (0.03s)
Fold 8 train - epoch: 6/10 iter: 13/15 loss: 0.4694 Acc: 87.5000% F1: 0.760 Time: 0.94s (0.03s)
Fold 8 train - epoch: 6/10 iter: 14/15 loss: 0.0199 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 8 Epoch 6 train Avg acc: 78.6667% F1: 0.5938 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 6/10 iter: 0/2 loss: 1.1506 Acc: 46.8750% F1: 0.442 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 6/10 iter: 1/2 loss: 1.0688 Acc: 61.1111% F1: 0.428 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 6 train-dev Avg acc: 52.0000% F1: 0.5139 *
*********************************************************
Performing epoch 7 of 10
Fold 8 train - epoch: 7/10 iter: 0/15 loss: 0.5525 Acc: 78.1250% F1: 0.661 Time: 0.95s (0.00s)
Fold 8 train - epoch: 7/10 iter: 1/15 loss: 0.5077 Acc: 81.2500% F1: 0.694 Time: 0.92s (0.03s)
Fold 8 train - epoch: 7/10 iter: 2/15 loss: 0.4549 Acc: 84.3750% F1: 0.744 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 3/15 loss: 0.4855 Acc: 78.1250% F1: 0.545 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 4/15 loss: 0.5858 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 5/15 loss: 0.4604 Acc: 81.2500% F1: 0.679 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 6/15 loss: 0.4582 Acc: 71.8750% F1: 0.426 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 7/15 loss: 0.4562 Acc: 87.5000% F1: 0.913 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 8/15 loss: 0.5294 Acc: 78.1250% F1: 0.671 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 9/15 loss: 0.6602 Acc: 71.8750% F1: 0.623 Time: 0.94s (0.03s)
Fold 8 train - epoch: 7/10 iter: 10/15 loss: 0.5357 Acc: 78.1250% F1: 0.676 Time: 0.94s (0.03s)
Fold 8 train - epoch: 7/10 iter: 11/15 loss: 0.2949 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 12/15 loss: 0.4896 Acc: 81.2500% F1: 0.571 Time: 0.94s (0.02s)
Fold 8 train - epoch: 7/10 iter: 13/15 loss: 0.3731 Acc: 90.6250% F1: 0.761 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 14/15 loss: 0.0067 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 8 Epoch 7 train Avg acc: 80.6667% F1: 0.6723 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 7/10 iter: 0/2 loss: 1.3764 Acc: 40.6250% F1: 0.269 Time: 0.35s (0.00s)
Fold 8 train-dev - epoch: 7/10 iter: 1/2 loss: 1.0854 Acc: 55.5556% F1: 0.345 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 7 train-dev Avg acc: 46.0000% F1: 0.3938 *
*********************************************************
Performing epoch 8 of 10
Fold 8 train - epoch: 8/10 iter: 0/15 loss: 0.4496 Acc: 81.2500% F1: 0.571 Time: 0.94s (0.00s)
Fold 8 train - epoch: 8/10 iter: 1/15 loss: 0.4297 Acc: 81.2500% F1: 0.567 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 2/15 loss: 0.5439 Acc: 81.2500% F1: 0.723 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 3/15 loss: 0.5330 Acc: 81.2500% F1: 0.809 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 4/15 loss: 0.4081 Acc: 81.2500% F1: 0.680 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 5/15 loss: 0.4174 Acc: 84.3750% F1: 0.703 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 6/15 loss: 0.2231 Acc: 96.8750% F1: 0.881 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 7/15 loss: 0.3031 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 8/15 loss: 0.3982 Acc: 84.3750% F1: 0.821 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 9/15 loss: 0.6307 Acc: 65.6250% F1: 0.450 Time: 0.94s (0.03s)
Fold 8 train - epoch: 8/10 iter: 10/15 loss: 0.4991 Acc: 78.1250% F1: 0.722 Time: 0.94s (0.03s)
Fold 8 train - epoch: 8/10 iter: 11/15 loss: 0.2977 Acc: 90.6250% F1: 0.863 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 12/15 loss: 0.3721 Acc: 84.3750% F1: 0.781 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 13/15 loss: 0.4624 Acc: 75.0000% F1: 0.621 Time: 0.94s (0.03s)
Fold 8 train - epoch: 8/10 iter: 14/15 loss: 0.0088 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 8 Epoch 8 train Avg acc: 82.8889% F1: 0.7412 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 8/10 iter: 0/2 loss: 1.2852 Acc: 40.6250% F1: 0.310 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 8/10 iter: 1/2 loss: 1.2393 Acc: 61.1111% F1: 0.424 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 8 train-dev Avg acc: 48.0000% F1: 0.4500 *
*********************************************************
Performing epoch 9 of 10
Fold 8 train - epoch: 9/10 iter: 0/15 loss: 0.3327 Acc: 81.2500% F1: 0.749 Time: 0.95s (0.00s)
Fold 8 train - epoch: 9/10 iter: 1/15 loss: 0.2763 Acc: 93.7500% F1: 0.913 Time: 0.93s (0.04s)
Fold 8 train - epoch: 9/10 iter: 2/15 loss: 0.3386 Acc: 87.5000% F1: 0.908 Time: 0.94s (0.04s)
Fold 8 train - epoch: 9/10 iter: 3/15 loss: 0.4500 Acc: 81.2500% F1: 0.698 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 4/15 loss: 0.5382 Acc: 68.7500% F1: 0.585 Time: 0.93s (0.04s)
Fold 8 train - epoch: 9/10 iter: 5/15 loss: 0.4586 Acc: 87.5000% F1: 0.810 Time: 0.93s (0.04s)
Fold 8 train - epoch: 9/10 iter: 6/15 loss: 0.4897 Acc: 71.8750% F1: 0.513 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 7/15 loss: 0.4272 Acc: 84.3750% F1: 0.785 Time: 0.94s (0.03s)
Fold 8 train - epoch: 9/10 iter: 8/15 loss: 0.3094 Acc: 84.3750% F1: 0.809 Time: 0.94s (0.04s)
Fold 8 train - epoch: 9/10 iter: 9/15 loss: 0.2860 Acc: 87.5000% F1: 0.822 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 10/15 loss: 0.2758 Acc: 90.6250% F1: 0.844 Time: 0.94s (0.04s)
Fold 8 train - epoch: 9/10 iter: 11/15 loss: 0.2343 Acc: 93.7500% F1: 0.896 Time: 0.94s (0.02s)
Fold 8 train - epoch: 9/10 iter: 12/15 loss: 0.5530 Acc: 68.7500% F1: 0.451 Time: 0.93s (0.04s)
Fold 8 train - epoch: 9/10 iter: 13/15 loss: 0.5386 Acc: 75.0000% F1: 0.508 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 14/15 loss: 0.0014 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 9 train Avg acc: 82.6667% F1: 0.7551 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 9/10 iter: 0/2 loss: 0.6531 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 9/10 iter: 1/2 loss: 2.6894 Acc: 11.1111% F1: 0.089 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 9 train-dev Avg acc: 54.0000% F1: 0.2953 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-1.3427e-01, -5.2043e-02, -3.1730e-01,  7.7185e-03,  2.2718e-01,
         -1.2485e-02, -8.7573e-01,  8.7754e-02,  3.1903e-01, -7.1417e-01,
         -1.0531e-01,  3.8222e-01,  6.7937e-01, -3.8726e-01,  4.7699e-01,
          1.4205e-01,  2.5994e-01,  8.3718e-02, -4.1070e-02,  8.9625e-01,
          3.4981e-01,  9.9909e-01, -2.8823e-02,  1.0305e-01, -1.1855e-02,
          1.6405e-01, -2.3254e-01,  8.8272e-01,  7.9386e-01,  3.9150e-01,
         -5.2010e-02,  4.3826e-02, -9.3425e-01,  3.9029e-02, -6.8694e-01,
         -9.0047e-01,  1.8324e-01,  4.5297e-01,  1.2854e-01, -1.3574e-01,
         -4.8877e-01,  2.9836e-02,  9.7575e-01, -1.6667e-01,  5.8254e-02,
          1.6298e-01, -4.4672e-01, -8.4298e-02, -3.0374e-01, -1.5000e-01,
         -5.9653e-02,  1.9811e-01,  1.6561e-01,  2.3411e-01,  8.5286e-02,
         -5.6858e-01, -1.4357e-01, -1.0175e-01, -8.9844e-02, -1.3578e-01,
         -2.5966e-02,  6.3280e-03, -1.6611e-01,  3.3488e-01, -1.0166e-01,
         -6.0971e-01,  9.3515e-02, -1.4272e-01,  1.3597e-01, -1.6784e-01,
         -6.3107e-01,  1.6805e-02, -1.2910e-02, -4.1386e-01, -1.4811e-01,
          7.4433e-02, -1.3688e-01,  9.8218e-01,  1.3731e-01, -9.3218e-01,
          2.6519e-01, -2.7857e-01, -1.1158e-01,  5.5284e-01,  9.5148e-02,
         -8.4937e-01,  2.5100e-01,  1.0778e-01, -9.8135e-01,  1.4275e-02,
          1.3438e-01,  2.0289e-02,  5.8889e-01, -7.7190e-02,  3.3984e-01,
         -4.2519e-01,  1.9229e-01, -3.0741e-01, -7.8635e-02, -6.3769e-02,
          1.2060e-01,  2.4521e-02,  2.4581e-01,  1.4983e-01,  1.1399e-01,
         -1.2498e-01,  5.1792e-01,  6.0336e-01, -5.0772e-01, -3.0754e-02,
          7.5591e-02, -6.3567e-03,  1.8606e-01, -4.6118e-01,  1.6056e-01,
          1.2801e-02, -9.6367e-01,  1.6059e-02, -9.1491e-01, -4.1800e-02,
          3.6182e-02, -1.7334e-01, -1.1488e-02, -5.2373e-01,  1.2484e-01,
         -5.1054e-02,  3.8911e-01, -9.8725e-01,  8.1092e-03, -2.2386e-01,
          1.8410e-01, -7.3818e-02, -8.8557e-01, -9.4751e-01, -3.1224e-02,
          5.8935e-01, -2.5691e-02,  9.3858e-01, -9.2931e-03,  7.5998e-01,
          5.0982e-01, -2.9885e-01, -2.1517e-01, -5.5710e-02,  2.1749e-01,
         -7.5390e-01,  6.4417e-02, -1.2383e-01, -2.7376e-01, -2.3859e-01,
         -4.4036e-01, -1.9280e-02,  2.3078e-01, -3.9127e-01, -7.7679e-02,
          9.1648e-01,  1.7104e-01,  4.8854e-01,  6.0223e-01,  8.7919e-02,
         -9.1622e-02,  4.0255e-01,  4.6382e-01, -1.1486e-03,  4.0938e-01,
          9.5949e-02, -5.6327e-01, -2.1093e-01, -9.2682e-02,  2.6223e-01,
         -7.2934e-02,  2.1425e-02,  1.3119e-01, -8.5896e-01, -1.7235e-01,
          2.0050e-02,  9.1581e-01, -3.7307e-01, -6.4677e-02, -4.4224e-01,
         -8.9332e-02, -2.8622e-01, -4.5301e-01,  8.9443e-01,  3.2461e-02,
         -5.8442e-02, -9.2699e-01,  1.4812e-01,  5.6077e-02, -6.2798e-02,
         -6.7927e-01,  1.9792e-01, -6.5947e-02,  4.0267e-02, -4.2839e-02,
          3.8495e-02, -4.6833e-01,  2.9401e-01,  9.2071e-02,  6.3615e-02,
         -1.1654e-01,  6.9556e-01, -5.9316e-01,  9.5253e-02, -1.3256e-01,
          3.3466e-01, -1.7752e-01,  2.1078e-01, -1.0137e-01,  1.0123e-01,
         -8.9425e-02,  8.2857e-01, -8.5620e-01,  1.5171e-02,  3.6696e-01,
         -9.1042e-01,  6.5279e-02,  5.1839e-01,  2.1656e-01, -3.3174e-01,
          3.0229e-02, -5.4227e-01, -7.1983e-01, -4.5853e-02,  9.7421e-01,
         -1.0693e-02, -1.5177e-01, -1.2869e-01, -4.0551e-02,  8.0544e-02,
          8.6966e-01,  5.4973e-01,  1.3580e-01, -6.7628e-01,  9.0486e-01,
         -3.2078e-01, -7.7394e-01, -4.4947e-01,  1.4387e-01, -8.2006e-01,
          7.0439e-02,  4.3553e-01,  2.5146e-01, -8.2801e-02, -6.1538e-01,
         -5.7652e-02,  8.9027e-01,  5.0808e-01, -1.0141e-01, -5.2030e-01,
         -3.1903e-01, -8.9989e-02, -3.6716e-02,  6.5007e-01, -9.3585e-02,
         -9.0144e-01, -9.0111e-01, -6.6763e-01, -8.9144e-01,  2.0476e-01,
         -6.9712e-01,  4.0178e-01,  3.0558e-02,  3.3950e-01,  1.2099e-01,
          2.3233e-01, -7.9690e-01, -7.2448e-01, -9.7378e-02, -9.4773e-02,
         -4.7357e-01,  2.6715e-01, -7.3083e-02, -5.5435e-01,  3.2868e-01,
          1.1664e-01,  6.7809e-01,  4.5797e-02, -2.2899e-02,  9.6351e-02,
         -4.2516e-02,  4.0657e-01, -1.2937e-01, -3.5707e-01,  9.9965e-01,
          9.0658e-01,  1.8074e-01, -6.5191e-01, -9.7356e-01, -4.0589e-01,
          9.8508e-01, -7.5611e-01, -9.8195e-01, -4.5773e-01,  1.1232e-02,
          1.1484e-01, -9.7510e-01,  7.2990e-02,  8.4135e-02, -6.1178e-01,
         -3.6072e-01,  7.8716e-01, -7.3167e-01, -9.9808e-01,  2.7757e-01,
          1.3333e-01, -1.9620e-01,  1.3586e-01, -1.1636e-01,  8.2076e-01,
         -1.0438e-01,  1.0493e-01, -1.9179e-03,  8.5354e-03, -5.6592e-01,
          2.0262e-01,  2.2786e-01, -8.2484e-01,  9.8559e-01, -3.2001e-01,
         -4.9279e-02, -3.9323e-01,  5.0811e-01, -2.2473e-02,  2.6917e-01,
         -9.4584e-01,  8.7543e-02, -4.0029e-01,  9.2407e-02,  1.5674e-01,
          4.8820e-03,  4.1651e-01, -5.6495e-02,  1.8783e-01, -4.6611e-01,
          1.8194e-01, -5.7284e-01,  3.6836e-01, -3.2850e-01, -1.0817e-01,
          4.0539e-01, -8.0227e-01,  6.4076e-01,  1.1207e-01, -4.3299e-01,
          9.9307e-01,  8.8671e-01, -1.8837e-01,  2.7371e-01,  7.2890e-02,
         -6.3018e-01,  6.7949e-01,  2.7161e-01, -6.2758e-01,  4.7690e-02,
          3.1111e-02, -4.1889e-02, -4.4851e-02,  8.7454e-01,  8.0477e-02,
         -1.6779e-02, -1.3831e-01,  9.8137e-01, -8.0492e-01,  9.5639e-01,
          2.6154e-01, -9.4402e-01,  7.1486e-01,  8.2278e-01,  2.2274e-02,
         -8.1385e-02, -1.5312e-01,  2.7719e-01,  1.3479e-01,  9.6957e-01,
          3.3174e-02, -9.7816e-02, -5.0278e-02,  3.8879e-01,  7.1143e-01,
          4.7529e-02,  4.5967e-02, -3.6066e-01,  2.9707e-01,  3.0058e-01,
          1.3892e-02,  1.4539e-01, -7.8588e-02,  1.6275e-01, -6.6840e-01,
         -3.0341e-01,  3.1895e-01,  9.9726e-01,  8.3435e-02,  9.3892e-01,
          3.8654e-01,  5.1785e-02, -7.6627e-02,  2.5456e-03,  3.3430e-01,
          2.1425e-01, -1.2925e-01,  1.8334e-01,  8.0447e-01, -9.8653e-01,
         -8.3225e-01, -5.9040e-02,  3.6195e-02,  8.6011e-01, -7.6509e-02,
          1.5642e-01, -4.7409e-02,  2.4019e-01, -5.4211e-02, -5.4465e-01,
         -3.1373e-01,  9.7410e-01,  7.2489e-02, -8.9020e-02, -8.2236e-01,
          4.1510e-01,  2.3057e-02, -7.2885e-02, -1.7547e-01, -8.3923e-01,
          9.4862e-02, -9.0575e-01,  9.2958e-01,  2.0379e-01,  1.7974e-01,
         -1.3047e-01,  5.2406e-01,  9.9604e-01, -9.5139e-01,  5.5762e-02,
          9.4959e-01, -2.9899e-01, -9.7916e-01,  5.6665e-01,  3.3742e-02,
          9.6123e-02,  2.4789e-01,  7.0040e-02,  2.9284e-02, -8.6313e-01,
         -4.2480e-01,  2.9647e-01,  8.9290e-01, -8.9400e-01, -1.1781e-01,
         -3.0807e-01,  1.9543e-01, -9.6012e-01,  3.5346e-01, -2.5217e-01,
         -8.1517e-02,  5.5710e-02, -3.7736e-01,  7.7057e-01, -6.9428e-05,
          3.4188e-02, -6.3282e-02,  2.5441e-01, -8.7786e-02,  8.0087e-01,
         -9.4251e-01, -1.3142e-01, -6.1199e-02,  3.2102e-01,  2.5144e-01,
         -7.1274e-03, -2.8895e-01, -5.5067e-02,  9.8432e-01, -2.9477e-01,
          2.3754e-01,  2.4172e-01, -1.1777e-01,  5.6783e-02,  3.6705e-02,
          8.4541e-01, -5.1400e-02,  2.6646e-01,  5.1277e-01,  8.9927e-01,
          4.7737e-02,  4.5388e-02,  1.9932e-01, -1.2695e-01,  2.0733e-01,
          3.2787e-01,  6.2285e-02,  6.5995e-02, -6.7281e-02, -8.1984e-01,
         -7.7740e-02, -1.4502e-01,  3.4060e-02,  2.6113e-02, -2.5315e-01,
          6.0701e-01,  9.9428e-01,  6.3714e-02,  6.2111e-01, -8.5917e-01,
          1.0094e-02,  1.1149e-01,  9.9666e-01,  8.7680e-01,  9.8309e-02,
          1.9829e-02,  1.5113e-01, -7.9213e-02, -6.0664e-01,  1.7958e-02,
         -1.5478e-01, -4.1217e-02, -2.6358e-01,  8.4456e-01,  3.4749e-02,
         -9.7591e-01, -4.4002e-01,  4.7369e-02, -7.2416e-01,  9.9621e-01,
         -2.5242e-01, -2.2555e-01, -3.7821e-02, -1.8827e-01, -6.4712e-01,
          1.9067e-01, -6.5881e-01, -2.7225e-02,  2.5950e-02,  9.6894e-01,
         -9.9293e-02, -5.0053e-02, -5.1598e-01,  7.1138e-02, -1.4756e-01,
          3.7213e-01, -6.6887e-01,  5.3074e-01, -5.0778e-01,  1.0794e-01,
          6.9640e-01,  1.7083e-02, -2.1878e-01, -7.9784e-02,  2.0812e-01,
          2.0685e-02, -6.2582e-01,  1.3915e-01, -6.7623e-01, -8.5377e-03,
         -1.0978e-02,  1.4896e-01,  3.1727e-02, -3.5585e-01, -1.4532e-01,
          1.4551e-01, -6.4696e-02, -9.3726e-02, -1.1334e-01, -6.9375e-02,
          2.3105e-02, -2.9064e-01,  3.5482e-02,  5.7998e-03,  1.6320e-01,
         -4.1259e-01, -1.1099e-01,  8.9094e-03, -9.9504e-01, -2.5540e-01,
         -9.9601e-01,  5.0103e-01, -6.2714e-01,  4.9916e-02,  7.2164e-01,
          4.2512e-01,  5.7043e-01, -7.6260e-03,  4.8316e-01,  5.1785e-01,
          3.1847e-01, -2.2416e-02,  1.1883e-01, -1.9114e-01,  3.5212e-02,
          7.0610e-04,  4.2732e-03, -3.7339e-01, -1.3702e-01,  1.8476e-01,
          9.7560e-01, -1.5243e-02, -8.1933e-02,  9.4307e-01,  2.1963e-02,
          5.4762e-03,  9.9880e-01,  7.1631e-01, -8.9611e-01, -2.7607e-02,
         -3.5607e-01, -2.3171e-01,  3.0498e-02, -2.1867e-02, -3.5444e-02,
         -2.3526e-01, -2.2705e-01, -3.6003e-01, -2.3812e-01,  3.2395e-01,
          1.1860e-01, -8.1572e-02, -1.6813e-02,  2.2448e-01,  8.8438e-01,
          6.8441e-01, -2.8539e-01, -7.5782e-01, -4.2669e-01,  4.4648e-01,
          3.7609e-02, -1.7402e-01,  1.0848e-01,  9.9008e-01,  4.5924e-03,
         -5.6084e-01,  1.5495e-01,  4.1591e-02,  1.6083e-01, -1.2128e-01,
          2.6179e-02, -7.1915e-02,  7.1236e-01,  5.4161e-02,  8.4390e-01,
          5.6323e-01, -2.8990e-01,  2.8292e-02,  5.9627e-01, -8.7986e-02,
         -8.7848e-01, -7.0290e-01, -5.7020e-01,  3.5764e-01,  3.6087e-02,
          1.3017e-01, -2.9577e-02, -7.1493e-03,  8.0007e-03,  2.0222e-02,
         -9.9937e-01,  2.5840e-01, -4.4161e-03, -8.2031e-02,  9.2151e-01,
          2.0482e-01,  4.6413e-01,  3.4105e-02, -9.4859e-01,  5.2726e-01,
          9.7268e-03, -2.6314e-02,  2.7845e-01, -3.2989e-02, -5.6437e-01,
         -1.4855e-01, -9.5652e-02, -4.5713e-01, -5.9856e-01, -8.0677e-01,
         -8.9816e-01,  1.0932e-01,  3.1341e-01,  7.8731e-01,  7.5258e-01,
         -3.4000e-01,  1.6122e-01,  5.4578e-01, -6.2733e-01, -9.4142e-01,
          9.9668e-02, -5.6999e-01, -2.1131e-02,  6.1775e-02,  5.4290e-01,
         -1.7949e-01,  6.6996e-01,  4.5220e-02, -7.4237e-01, -1.8440e-01,
          7.8385e-03,  8.2786e-01, -7.3192e-01,  2.3231e-02,  5.5883e-01,
          1.2214e-01, -1.2049e-01,  7.1703e-02,  9.3623e-01,  7.9565e-01,
         -3.6844e-02,  2.8034e-01, -8.8042e-02,  3.2275e-02, -2.1069e-01,
          2.4181e-01, -3.3027e-02, -9.8697e-02, -2.8074e-01, -2.1025e-01,
          6.7348e-01,  8.4511e-02, -1.4119e-01,  1.7837e-01,  1.5804e-01,
          4.1178e-01, -3.0503e-01, -9.0585e-01,  7.9839e-02,  5.5570e-01,
          7.2816e-01, -3.1553e-01,  8.7513e-01,  1.3038e-01,  4.8535e-02,
         -9.4911e-02,  5.3337e-03, -3.0556e-01, -5.6780e-01,  3.2776e-02,
         -1.7691e-01,  7.6386e-03, -6.1914e-03,  9.9642e-01,  3.6717e-01,
          5.2910e-01, -3.6041e-01,  4.5756e-02, -2.7532e-02, -1.1277e-01,
         -9.9934e-01, -4.0972e-02,  1.5896e-01,  4.7324e-02,  6.7879e-03,
         -3.3822e-02,  5.9479e-03,  6.4044e-01, -1.3847e-01,  5.4714e-01,
          2.3041e-01,  1.5688e-01, -1.6587e-02,  2.6973e-01,  1.7407e-01,
         -1.1315e-01, -1.3388e-01, -5.8620e-01,  5.1988e-01,  3.5590e-04,
         -5.4150e-01, -1.2651e-02, -1.1893e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/10 iter: 0/15 loss: 4.0096 Acc: 12.5000% F1: 0.076 Time: 0.96s (0.00s)
Fold 9 train - epoch: 0/10 iter: 1/15 loss: 2.8884 Acc: 18.7500% F1: 0.165 Time: 0.93s (0.03s)
Fold 9 train - epoch: 0/10 iter: 2/15 loss: 1.6145 Acc: 34.3750% F1: 0.345 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 3/15 loss: 1.2801 Acc: 43.7500% F1: 0.314 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 4/15 loss: 1.8123 Acc: 34.3750% F1: 0.175 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 5/15 loss: 1.4889 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.03s)
Fold 9 train - epoch: 0/10 iter: 6/15 loss: 0.8005 Acc: 65.6250% F1: 0.354 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 7/15 loss: 1.0661 Acc: 43.7500% F1: 0.275 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 8/15 loss: 1.5107 Acc: 34.3750% F1: 0.244 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 9/15 loss: 1.4604 Acc: 21.8750% F1: 0.158 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 10/15 loss: 1.2922 Acc: 34.3750% F1: 0.235 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/10 iter: 11/15 loss: 0.9421 Acc: 43.7500% F1: 0.301 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 12/15 loss: 1.0465 Acc: 46.8750% F1: 0.331 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 13/15 loss: 1.0417 Acc: 46.8750% F1: 0.309 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 14/15 loss: 0.4967 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 38.6667% F1: 0.3289 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6454 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 0/10 iter: 1/2 loss: 1.3408 Acc: 38.8889% F1: 0.274 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 60.0000% F1: 0.3433 *
*********************************************************
Performing epoch 1 of 10
Fold 9 train - epoch: 1/10 iter: 0/15 loss: 1.0671 Acc: 53.1250% F1: 0.340 Time: 0.95s (0.00s)
Fold 9 train - epoch: 1/10 iter: 1/15 loss: 0.9694 Acc: 59.3750% F1: 0.342 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 2/15 loss: 1.0523 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 3/15 loss: 0.8421 Acc: 65.6250% F1: 0.322 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 4/15 loss: 1.2551 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 5/15 loss: 0.8902 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 6/15 loss: 0.6952 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 7/15 loss: 0.9324 Acc: 50.0000% F1: 0.286 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 8/15 loss: 1.0445 Acc: 53.1250% F1: 0.241 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 9/15 loss: 1.0936 Acc: 40.6250% F1: 0.193 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 10/15 loss: 1.0053 Acc: 50.0000% F1: 0.322 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 11/15 loss: 0.7419 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 12/15 loss: 0.9344 Acc: 50.0000% F1: 0.296 Time: 0.94s (0.02s)
Fold 9 train - epoch: 1/10 iter: 13/15 loss: 0.8976 Acc: 53.1250% F1: 0.329 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 14/15 loss: 0.4469 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 55.7778% F1: 0.3046 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/10 iter: 0/2 loss: 0.8003 Acc: 56.2500% F1: 0.491 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 1/10 iter: 1/2 loss: 1.0578 Acc: 44.4444% F1: 0.315 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3339 *
*********************************************************
Performing epoch 2 of 10
Fold 9 train - epoch: 2/10 iter: 0/15 loss: 0.9500 Acc: 56.2500% F1: 0.395 Time: 0.94s (0.00s)
Fold 9 train - epoch: 2/10 iter: 1/15 loss: 0.9384 Acc: 56.2500% F1: 0.389 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 2/15 loss: 0.8282 Acc: 65.6250% F1: 0.466 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 3/15 loss: 0.8322 Acc: 65.6250% F1: 0.440 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 4/15 loss: 0.9534 Acc: 53.1250% F1: 0.339 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 5/15 loss: 0.8634 Acc: 59.3750% F1: 0.367 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 6/15 loss: 0.7278 Acc: 75.0000% F1: 0.403 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 7/15 loss: 0.9014 Acc: 46.8750% F1: 0.272 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 8/15 loss: 1.0058 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 9/15 loss: 1.0677 Acc: 53.1250% F1: 0.317 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 10/15 loss: 1.0246 Acc: 50.0000% F1: 0.269 Time: 0.94s (0.02s)
Fold 9 train - epoch: 2/10 iter: 11/15 loss: 0.6654 Acc: 71.8750% F1: 0.351 Time: 0.94s (0.03s)
Fold 9 train - epoch: 2/10 iter: 12/15 loss: 0.8986 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.02s)
Fold 9 train - epoch: 2/10 iter: 13/15 loss: 0.9050 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 14/15 loss: 0.2078 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 59.1111% F1: 0.3672 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6993 Acc: 78.1250% F1: 0.616 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 2/10 iter: 1/2 loss: 1.2422 Acc: 38.8889% F1: 0.274 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 64.0000% F1: 0.3782 *
*********************************************************
Performing epoch 3 of 10
Fold 9 train - epoch: 3/10 iter: 0/15 loss: 0.8937 Acc: 59.3750% F1: 0.380 Time: 0.95s (0.00s)
Fold 9 train - epoch: 3/10 iter: 1/15 loss: 0.8793 Acc: 56.2500% F1: 0.367 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 2/15 loss: 0.8372 Acc: 62.5000% F1: 0.433 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 3/15 loss: 0.8072 Acc: 56.2500% F1: 0.338 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 4/15 loss: 0.9023 Acc: 59.3750% F1: 0.404 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 5/15 loss: 0.7840 Acc: 71.8750% F1: 0.492 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 6/15 loss: 0.6787 Acc: 75.0000% F1: 0.434 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 7/15 loss: 0.8115 Acc: 71.8750% F1: 0.490 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 8/15 loss: 0.9733 Acc: 50.0000% F1: 0.311 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 9/15 loss: 0.9597 Acc: 56.2500% F1: 0.376 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 10/15 loss: 0.8666 Acc: 59.3750% F1: 0.422 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 11/15 loss: 0.6106 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 12/15 loss: 0.8169 Acc: 62.5000% F1: 0.402 Time: 0.94s (0.02s)
Fold 9 train - epoch: 3/10 iter: 13/15 loss: 0.8574 Acc: 71.8750% F1: 0.464 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 14/15 loss: 0.1092 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 63.5556% F1: 0.4178 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7287 Acc: 56.2500% F1: 0.459 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 3/10 iter: 1/2 loss: 1.3021 Acc: 44.4444% F1: 0.315 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 52.0000% F1: 0.3260 *
*********************************************************
Performing epoch 4 of 10
Fold 9 train - epoch: 4/10 iter: 0/15 loss: 0.8478 Acc: 68.7500% F1: 0.485 Time: 0.94s (0.00s)
Fold 9 train - epoch: 4/10 iter: 1/15 loss: 0.7865 Acc: 71.8750% F1: 0.499 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 2/15 loss: 0.7686 Acc: 68.7500% F1: 0.491 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 3/15 loss: 0.7071 Acc: 75.0000% F1: 0.499 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 4/15 loss: 0.8198 Acc: 65.6250% F1: 0.450 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 5/15 loss: 0.7375 Acc: 71.8750% F1: 0.500 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 6/15 loss: 0.5860 Acc: 84.3750% F1: 0.548 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 7/15 loss: 0.7424 Acc: 68.7500% F1: 0.472 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 8/15 loss: 0.8968 Acc: 53.1250% F1: 0.328 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 9/15 loss: 0.8582 Acc: 62.5000% F1: 0.425 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 10/15 loss: 0.8172 Acc: 65.6250% F1: 0.477 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/10 iter: 11/15 loss: 0.5659 Acc: 81.2500% F1: 0.520 Time: 0.94s (0.03s)
Fold 9 train - epoch: 4/10 iter: 12/15 loss: 0.7611 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 13/15 loss: 0.8524 Acc: 62.5000% F1: 0.393 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 14/15 loss: 0.0562 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 69.1111% F1: 0.4717 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8430 Acc: 43.7500% F1: 0.400 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 4/10 iter: 1/2 loss: 1.3815 Acc: 44.4444% F1: 0.315 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 44.0000% F1: 0.2891 *
*********************************************************
Performing epoch 5 of 10
Fold 9 train - epoch: 5/10 iter: 0/15 loss: 0.7323 Acc: 78.1250% F1: 0.653 Time: 0.94s (0.00s)
Fold 9 train - epoch: 5/10 iter: 1/15 loss: 0.8035 Acc: 65.6250% F1: 0.461 Time: 0.92s (0.03s)
Fold 9 train - epoch: 5/10 iter: 2/15 loss: 0.6097 Acc: 75.0000% F1: 0.534 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 3/15 loss: 0.6959 Acc: 68.7500% F1: 0.466 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 4/15 loss: 0.8232 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 5/15 loss: 0.6514 Acc: 75.0000% F1: 0.491 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 6/15 loss: 0.4858 Acc: 87.5000% F1: 0.572 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 7/15 loss: 0.6691 Acc: 68.7500% F1: 0.466 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 8/15 loss: 0.7920 Acc: 65.6250% F1: 0.508 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 9/15 loss: 0.7890 Acc: 68.7500% F1: 0.590 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 10/15 loss: 0.6835 Acc: 68.7500% F1: 0.596 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 11/15 loss: 0.4510 Acc: 90.6250% F1: 0.829 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 12/15 loss: 0.6736 Acc: 75.0000% F1: 0.647 Time: 0.94s (0.03s)
Fold 9 train - epoch: 5/10 iter: 13/15 loss: 0.5740 Acc: 84.3750% F1: 0.586 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 14/15 loss: 0.0359 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 5 train Avg acc: 74.2222% F1: 0.5761 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9718 Acc: 50.0000% F1: 0.326 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 5/10 iter: 1/2 loss: 1.5224 Acc: 44.4444% F1: 0.324 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3282 *
*********************************************************
Performing epoch 6 of 10
Fold 9 train - epoch: 6/10 iter: 0/15 loss: 0.6696 Acc: 71.8750% F1: 0.610 Time: 0.95s (0.00s)
Fold 9 train - epoch: 6/10 iter: 1/15 loss: 0.6775 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 2/15 loss: 0.4152 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 3/15 loss: 0.4979 Acc: 78.1250% F1: 0.542 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 4/15 loss: 0.6407 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 5/15 loss: 0.5340 Acc: 81.2500% F1: 0.561 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 6/15 loss: 0.3345 Acc: 87.5000% F1: 0.579 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 7/15 loss: 0.5871 Acc: 78.1250% F1: 0.547 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 8/15 loss: 0.5267 Acc: 75.0000% F1: 0.609 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 9/15 loss: 0.6446 Acc: 68.7500% F1: 0.496 Time: 0.93s (0.03s)
Fold 9 train - epoch: 6/10 iter: 10/15 loss: 0.6435 Acc: 75.0000% F1: 0.643 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 11/15 loss: 0.2622 Acc: 93.7500% F1: 0.943 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 12/15 loss: 0.5798 Acc: 78.1250% F1: 0.555 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 13/15 loss: 0.4615 Acc: 84.3750% F1: 0.588 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 14/15 loss: 0.0227 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 6 train Avg acc: 79.3333% F1: 0.6169 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 6/10 iter: 0/2 loss: 1.1483 Acc: 46.8750% F1: 0.308 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 6/10 iter: 1/2 loss: 1.6483 Acc: 55.5556% F1: 0.376 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 6 train-dev Avg acc: 50.0000% F1: 0.3438 *
*********************************************************
Performing epoch 7 of 10
Fold 9 train - epoch: 7/10 iter: 0/15 loss: 0.5277 Acc: 75.0000% F1: 0.701 Time: 0.95s (0.00s)
Fold 9 train - epoch: 7/10 iter: 1/15 loss: 0.5694 Acc: 81.2500% F1: 0.576 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 2/15 loss: 0.3503 Acc: 90.6250% F1: 0.902 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 3/15 loss: 0.5035 Acc: 81.2500% F1: 0.574 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 4/15 loss: 0.4666 Acc: 84.3750% F1: 0.703 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 5/15 loss: 0.3878 Acc: 93.7500% F1: 0.856 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 6/15 loss: 0.2455 Acc: 90.6250% F1: 0.604 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 7/15 loss: 0.4988 Acc: 81.2500% F1: 0.767 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 8/15 loss: 0.4279 Acc: 81.2500% F1: 0.782 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 9/15 loss: 0.5862 Acc: 75.0000% F1: 0.699 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 10/15 loss: 0.4079 Acc: 84.3750% F1: 0.779 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 11/15 loss: 0.2403 Acc: 93.7500% F1: 0.878 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 12/15 loss: 0.2810 Acc: 90.6250% F1: 0.761 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 13/15 loss: 0.4127 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 14/15 loss: 0.0084 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 7 train Avg acc: 84.8889% F1: 0.7634 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 7/10 iter: 0/2 loss: 1.5384 Acc: 37.5000% F1: 0.252 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 7/10 iter: 1/2 loss: 1.7117 Acc: 50.0000% F1: 0.336 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 7 train-dev Avg acc: 42.0000% F1: 0.2909 *
*********************************************************
Performing epoch 8 of 10
Fold 9 train - epoch: 8/10 iter: 0/15 loss: 0.4108 Acc: 78.1250% F1: 0.739 Time: 0.95s (0.00s)
Fold 9 train - epoch: 8/10 iter: 1/15 loss: 0.5752 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 2/15 loss: 0.2188 Acc: 93.7500% F1: 0.952 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 3/15 loss: 0.2968 Acc: 90.6250% F1: 0.774 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 4/15 loss: 0.3501 Acc: 84.3750% F1: 0.706 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 5/15 loss: 0.2979 Acc: 87.5000% F1: 0.791 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 6/15 loss: 0.1959 Acc: 96.8750% F1: 0.659 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 7/15 loss: 0.3125 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 8/15 loss: 0.2885 Acc: 87.5000% F1: 0.862 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 9/15 loss: 0.4560 Acc: 81.2500% F1: 0.794 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 10/15 loss: 0.4269 Acc: 78.1250% F1: 0.711 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 11/15 loss: 0.1218 Acc: 96.8750% F1: 0.970 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 12/15 loss: 0.3388 Acc: 84.3750% F1: 0.722 Time: 0.94s (0.02s)
Fold 9 train - epoch: 8/10 iter: 13/15 loss: 0.3089 Acc: 87.5000% F1: 0.749 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 14/15 loss: 0.0099 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 8 train Avg acc: 86.2222% F1: 0.7983 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 8/10 iter: 0/2 loss: 1.5909 Acc: 37.5000% F1: 0.259 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 8/10 iter: 1/2 loss: 2.0937 Acc: 22.2222% F1: 0.172 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 8 train-dev Avg acc: 32.0000% F1: 0.2308 *
*********************************************************
Performing epoch 9 of 10
Fold 9 train - epoch: 9/10 iter: 0/15 loss: 0.2916 Acc: 87.5000% F1: 0.794 Time: 0.95s (0.00s)
Fold 9 train - epoch: 9/10 iter: 1/15 loss: 0.1971 Acc: 93.7500% F1: 0.897 Time: 0.93s (0.04s)
Fold 9 train - epoch: 9/10 iter: 2/15 loss: 0.2196 Acc: 93.7500% F1: 0.952 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 3/15 loss: 0.4436 Acc: 75.0000% F1: 0.729 Time: 0.93s (0.04s)
Fold 9 train - epoch: 9/10 iter: 4/15 loss: 0.3432 Acc: 78.1250% F1: 0.705 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 5/15 loss: 0.4800 Acc: 84.3750% F1: 0.802 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 6/15 loss: 0.2424 Acc: 93.7500% F1: 0.626 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 7/15 loss: 0.2108 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 8/15 loss: 0.1455 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 9/15 loss: 0.2039 Acc: 93.7500% F1: 0.928 Time: 0.94s (0.02s)
Fold 9 train - epoch: 9/10 iter: 10/15 loss: 0.3504 Acc: 87.5000% F1: 0.847 Time: 0.94s (0.02s)
Fold 9 train - epoch: 9/10 iter: 11/15 loss: 0.2900 Acc: 81.2500% F1: 0.648 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 12/15 loss: 0.8275 Acc: 68.7500% F1: 0.527 Time: 0.94s (0.02s)
Fold 9 train - epoch: 9/10 iter: 13/15 loss: 0.9669 Acc: 71.8750% F1: 0.584 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 14/15 loss: 0.0030 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 9 train Avg acc: 86.0000% F1: 0.8179 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 9/10 iter: 0/2 loss: 1.3163 Acc: 59.3750% F1: 0.300 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 9/10 iter: 1/2 loss: 3.5611 Acc: 27.7778% F1: 0.192 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 9 train-dev Avg acc: 48.0000% F1: 0.2690 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 54.6000% F1: 0.3019
	Epoch 1 Accuracy: 51.4000% F1: 0.3311
	Epoch 2 Accuracy: 56.6000% F1: 0.3224
	Epoch 3 Accuracy: 53.8000% F1: 0.3316
	Epoch 4 Accuracy: 49.2000% F1: 0.3381
	Epoch 5 Accuracy: 45.8000% F1: 0.3248
	Epoch 6 Accuracy: 43.0000% F1: 0.3263
	Epoch 7 Accuracy: 41.2000% F1: 0.3143
	Epoch 8 Accuracy: 40.2000% F1: 0.3172
	Epoch 9 Accuracy: 42.2000% F1: 0.3037
all done :)
************************************
** MODEL TIME ID: 20220225-131551 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/10 iter: 0/15 loss: 3.7794 Acc: 15.6250% F1: 0.134 Time: 0.96s (0.00s)
Fold 0 train - epoch: 0/10 iter: 1/15 loss: 2.8394 Acc: 18.7500% F1: 0.206 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 2/15 loss: 2.2930 Acc: 12.5000% F1: 0.125 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 3/15 loss: 1.5559 Acc: 37.5000% F1: 0.276 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 4/15 loss: 1.5851 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 5/15 loss: 1.0896 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 6/15 loss: 1.0062 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 7/15 loss: 0.9745 Acc: 53.1250% F1: 0.352 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 8/15 loss: 1.0991 Acc: 46.8750% F1: 0.335 Time: 0.93s (0.02s)
Fold 0 train - epoch: 0/10 iter: 9/15 loss: 1.1753 Acc: 40.6250% F1: 0.288 Time: 0.93s (0.02s)
Fold 0 train - epoch: 0/10 iter: 10/15 loss: 1.2116 Acc: 40.6250% F1: 0.286 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 11/15 loss: 1.1147 Acc: 34.3750% F1: 0.240 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 12/15 loss: 1.1916 Acc: 43.7500% F1: 0.311 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/10 iter: 13/15 loss: 0.9236 Acc: 59.3750% F1: 0.415 Time: 0.94s (0.02s)
Fold 0 train - epoch: 0/10 iter: 14/15 loss: 0.5665 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 39.7778% F1: 0.3248 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5842 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 0/10 iter: 1/2 loss: 2.0552 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2918 *
*********************************************************
Performing epoch 1 of 10
Fold 0 train - epoch: 1/10 iter: 0/15 loss: 0.9963 Acc: 53.1250% F1: 0.236 Time: 0.95s (0.00s)
Fold 0 train - epoch: 1/10 iter: 1/15 loss: 0.9221 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 2/15 loss: 1.0769 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 3/15 loss: 0.9948 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 4/15 loss: 1.0133 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 5/15 loss: 0.9376 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/10 iter: 6/15 loss: 0.7414 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 7/15 loss: 0.9369 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 8/15 loss: 0.9465 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 9/15 loss: 0.9569 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 10/15 loss: 0.9870 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 11/15 loss: 0.8880 Acc: 62.5000% F1: 0.380 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/10 iter: 12/15 loss: 1.0619 Acc: 40.6250% F1: 0.271 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 13/15 loss: 0.8940 Acc: 56.2500% F1: 0.374 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/10 iter: 14/15 loss: 0.5689 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 54.6667% F1: 0.2934 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7693 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3686 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3312 *
*********************************************************
Performing epoch 2 of 10
Fold 0 train - epoch: 2/10 iter: 0/15 loss: 0.9535 Acc: 50.0000% F1: 0.354 Time: 0.94s (0.00s)
Fold 0 train - epoch: 2/10 iter: 1/15 loss: 0.9211 Acc: 65.6250% F1: 0.453 Time: 0.92s (0.03s)
Fold 0 train - epoch: 2/10 iter: 2/15 loss: 0.8673 Acc: 56.2500% F1: 0.389 Time: 0.93s (0.02s)
Fold 0 train - epoch: 2/10 iter: 3/15 loss: 0.8668 Acc: 59.3750% F1: 0.355 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 4/15 loss: 0.9353 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 5/15 loss: 0.8934 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 6/15 loss: 0.7516 Acc: 65.6250% F1: 0.264 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 7/15 loss: 0.9140 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.02s)
Fold 0 train - epoch: 2/10 iter: 8/15 loss: 0.8864 Acc: 59.3750% F1: 0.319 Time: 0.94s (0.02s)
Fold 0 train - epoch: 2/10 iter: 9/15 loss: 0.9618 Acc: 50.0000% F1: 0.265 Time: 0.94s (0.04s)
Fold 0 train - epoch: 2/10 iter: 10/15 loss: 1.0293 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/10 iter: 11/15 loss: 0.8362 Acc: 59.3750% F1: 0.300 Time: 0.93s (0.02s)
Fold 0 train - epoch: 2/10 iter: 12/15 loss: 0.9924 Acc: 53.1250% F1: 0.283 Time: 0.93s (0.02s)
Fold 0 train - epoch: 2/10 iter: 13/15 loss: 0.7964 Acc: 65.6250% F1: 0.398 Time: 0.94s (0.02s)
Fold 0 train - epoch: 2/10 iter: 14/15 loss: 0.0785 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 57.3333% F1: 0.3463 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6750 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5767 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 56.0000% F1: 0.3224 *
*********************************************************
Performing epoch 3 of 10
Fold 0 train - epoch: 3/10 iter: 0/15 loss: 0.8767 Acc: 56.2500% F1: 0.350 Time: 0.94s (0.00s)
Fold 0 train - epoch: 3/10 iter: 1/15 loss: 0.8329 Acc: 71.8750% F1: 0.492 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 2/15 loss: 0.8918 Acc: 68.7500% F1: 0.477 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 3/15 loss: 0.8660 Acc: 56.2500% F1: 0.338 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 4/15 loss: 0.8110 Acc: 68.7500% F1: 0.484 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 5/15 loss: 0.8631 Acc: 62.5000% F1: 0.437 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 6/15 loss: 0.7491 Acc: 75.0000% F1: 0.495 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 7/15 loss: 0.8116 Acc: 65.6250% F1: 0.426 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 8/15 loss: 0.8276 Acc: 65.6250% F1: 0.470 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 9/15 loss: 0.8937 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 10/15 loss: 0.8903 Acc: 59.3750% F1: 0.400 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/10 iter: 11/15 loss: 0.7596 Acc: 68.7500% F1: 0.418 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 12/15 loss: 0.8785 Acc: 65.6250% F1: 0.435 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 13/15 loss: 0.7382 Acc: 65.6250% F1: 0.370 Time: 0.93s (0.02s)
Fold 0 train - epoch: 3/10 iter: 14/15 loss: 0.0203 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 65.3333% F1: 0.4345 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7126 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6041 Acc: 16.6667% F1: 0.133 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.3123 *
*********************************************************
Performing epoch 4 of 10
Fold 0 train - epoch: 4/10 iter: 0/15 loss: 0.8208 Acc: 65.6250% F1: 0.433 Time: 0.94s (0.00s)
Fold 0 train - epoch: 4/10 iter: 1/15 loss: 0.7614 Acc: 71.8750% F1: 0.500 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 2/15 loss: 0.8018 Acc: 71.8750% F1: 0.499 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 3/15 loss: 0.7040 Acc: 75.0000% F1: 0.495 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 4/15 loss: 0.7569 Acc: 81.2500% F1: 0.578 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 5/15 loss: 0.7622 Acc: 71.8750% F1: 0.611 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 6/15 loss: 0.6522 Acc: 78.1250% F1: 0.506 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 7/15 loss: 0.7019 Acc: 65.6250% F1: 0.424 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 8/15 loss: 0.8404 Acc: 65.6250% F1: 0.448 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 9/15 loss: 0.8159 Acc: 65.6250% F1: 0.450 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 10/15 loss: 0.8420 Acc: 62.5000% F1: 0.426 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/10 iter: 11/15 loss: 0.7467 Acc: 71.8750% F1: 0.475 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 12/15 loss: 0.8511 Acc: 59.3750% F1: 0.412 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 13/15 loss: 0.6934 Acc: 71.8750% F1: 0.486 Time: 0.93s (0.02s)
Fold 0 train - epoch: 4/10 iter: 14/15 loss: 0.0117 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 70.0000% F1: 0.4867 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8525 Acc: 68.7500% F1: 0.543 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 4/10 iter: 1/2 loss: 1.5290 Acc: 22.2222% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 52.0000% F1: 0.3417 *
*********************************************************
Performing epoch 5 of 10
Fold 0 train - epoch: 5/10 iter: 0/15 loss: 0.8279 Acc: 59.3750% F1: 0.397 Time: 0.94s (0.00s)
Fold 0 train - epoch: 5/10 iter: 1/15 loss: 0.6636 Acc: 81.2500% F1: 0.584 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 2/15 loss: 0.7283 Acc: 68.7500% F1: 0.491 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 3/15 loss: 0.7169 Acc: 71.8750% F1: 0.479 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 4/15 loss: 0.7310 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 5/15 loss: 0.6199 Acc: 75.0000% F1: 0.640 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 6/15 loss: 0.5274 Acc: 75.0000% F1: 0.447 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 7/15 loss: 0.6897 Acc: 75.0000% F1: 0.501 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 8/15 loss: 0.6249 Acc: 75.0000% F1: 0.614 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 9/15 loss: 0.6256 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 10/15 loss: 0.6957 Acc: 71.8750% F1: 0.507 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 11/15 loss: 0.6432 Acc: 81.2500% F1: 0.566 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 12/15 loss: 0.8047 Acc: 68.7500% F1: 0.496 Time: 0.93s (0.02s)
Fold 0 train - epoch: 5/10 iter: 13/15 loss: 0.5915 Acc: 78.1250% F1: 0.533 Time: 0.93s (0.03s)
Fold 0 train - epoch: 5/10 iter: 14/15 loss: 0.0051 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 0 Epoch 5 train Avg acc: 73.5556% F1: 0.5321 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9188 Acc: 59.3750% F1: 0.330 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 5/10 iter: 1/2 loss: 1.6063 Acc: 22.2222% F1: 0.148 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 5 train-dev Avg acc: 46.0000% F1: 0.3133 *
*********************************************************
Performing epoch 6 of 10
Fold 0 train - epoch: 6/10 iter: 0/15 loss: 0.6032 Acc: 75.0000% F1: 0.526 Time: 0.95s (0.00s)
Fold 0 train - epoch: 6/10 iter: 1/15 loss: 0.5035 Acc: 84.3750% F1: 0.593 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 2/15 loss: 0.4875 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 3/15 loss: 0.5311 Acc: 81.2500% F1: 0.540 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 4/15 loss: 0.6180 Acc: 75.0000% F1: 0.527 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 5/15 loss: 0.5271 Acc: 75.0000% F1: 0.630 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 6/15 loss: 0.5022 Acc: 81.2500% F1: 0.540 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 7/15 loss: 0.5396 Acc: 78.1250% F1: 0.529 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 8/15 loss: 0.5288 Acc: 75.0000% F1: 0.655 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 9/15 loss: 0.4851 Acc: 87.5000% F1: 0.624 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 10/15 loss: 0.4940 Acc: 81.2500% F1: 0.697 Time: 0.93s (0.03s)
Fold 0 train - epoch: 6/10 iter: 11/15 loss: 0.5486 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 12/15 loss: 0.6223 Acc: 75.0000% F1: 0.540 Time: 0.93s (0.02s)
Fold 0 train - epoch: 6/10 iter: 13/15 loss: 0.4153 Acc: 87.5000% F1: 0.767 Time: 0.94s (0.02s)
Fold 0 train - epoch: 6/10 iter: 14/15 loss: 0.0018 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 0 Epoch 6 train Avg acc: 79.5556% F1: 0.6116 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9793 Acc: 56.2500% F1: 0.327 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 6/10 iter: 1/2 loss: 1.8786 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 6 train-dev Avg acc: 44.0000% F1: 0.3089 *
*********************************************************
Performing epoch 7 of 10
Fold 0 train - epoch: 7/10 iter: 0/15 loss: 0.5537 Acc: 71.8750% F1: 0.622 Time: 0.94s (0.00s)
Fold 0 train - epoch: 7/10 iter: 1/15 loss: 0.3785 Acc: 87.5000% F1: 0.614 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 2/15 loss: 0.3717 Acc: 87.5000% F1: 0.611 Time: 0.93s (0.02s)
Fold 0 train - epoch: 7/10 iter: 3/15 loss: 0.3739 Acc: 90.6250% F1: 0.626 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 4/15 loss: 0.4535 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 5/15 loss: 0.3348 Acc: 90.6250% F1: 0.872 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 6/15 loss: 0.2886 Acc: 96.8750% F1: 0.869 Time: 0.93s (0.02s)
Fold 0 train - epoch: 7/10 iter: 7/15 loss: 0.3486 Acc: 84.3750% F1: 0.571 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 8/15 loss: 0.3525 Acc: 87.5000% F1: 0.813 Time: 0.93s (0.02s)
Fold 0 train - epoch: 7/10 iter: 9/15 loss: 0.3895 Acc: 84.3750% F1: 0.700 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 10/15 loss: 0.3822 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 11/15 loss: 0.3093 Acc: 90.6250% F1: 0.780 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 12/15 loss: 0.4824 Acc: 75.0000% F1: 0.665 Time: 0.93s (0.03s)
Fold 0 train - epoch: 7/10 iter: 13/15 loss: 0.2441 Acc: 93.7500% F1: 0.951 Time: 0.93s (0.02s)
Fold 0 train - epoch: 7/10 iter: 14/15 loss: 0.0002 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 0 Epoch 7 train Avg acc: 86.6667% F1: 0.7476 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 7/10 iter: 0/2 loss: 1.2380 Acc: 50.0000% F1: 0.295 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 7/10 iter: 1/2 loss: 2.0115 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 0 Epoch 7 train-dev Avg acc: 40.0000% F1: 0.2871 *
*********************************************************
Performing epoch 8 of 10
Fold 0 train - epoch: 8/10 iter: 0/15 loss: 0.2794 Acc: 87.5000% F1: 0.735 Time: 0.95s (0.00s)
Fold 0 train - epoch: 8/10 iter: 1/15 loss: 0.2416 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 2/15 loss: 0.2504 Acc: 96.8750% F1: 0.921 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 3/15 loss: 0.1893 Acc: 90.6250% F1: 0.620 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 4/15 loss: 0.2674 Acc: 90.6250% F1: 0.767 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 5/15 loss: 0.2518 Acc: 90.6250% F1: 0.823 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 6/15 loss: 0.2460 Acc: 90.6250% F1: 0.789 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 7/15 loss: 0.1679 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 8/15 loss: 0.1979 Acc: 93.7500% F1: 0.896 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 9/15 loss: 0.3090 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 10/15 loss: 0.1871 Acc: 93.7500% F1: 0.891 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 11/15 loss: 0.1713 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 12/15 loss: 0.1802 Acc: 93.7500% F1: 0.927 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 13/15 loss: 0.1377 Acc: 96.8750% F1: 0.974 Time: 0.93s (0.03s)
Fold 0 train - epoch: 8/10 iter: 14/15 loss: 0.0001 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 8 train Avg acc: 92.6667% F1: 0.8587 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 8/10 iter: 0/2 loss: 1.8891 Acc: 25.0000% F1: 0.192 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 8/10 iter: 1/2 loss: 2.0143 Acc: 38.8889% F1: 0.267 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 8 train-dev Avg acc: 30.0000% F1: 0.2707 *
*********************************************************
Performing epoch 9 of 10
Fold 0 train - epoch: 9/10 iter: 0/15 loss: 0.3607 Acc: 87.5000% F1: 0.814 Time: 0.94s (0.00s)
Fold 0 train - epoch: 9/10 iter: 1/15 loss: 0.3423 Acc: 87.5000% F1: 0.814 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 2/15 loss: 0.2004 Acc: 90.6250% F1: 0.841 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 3/15 loss: 0.1858 Acc: 87.5000% F1: 0.598 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 4/15 loss: 0.2167 Acc: 90.6250% F1: 0.867 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 5/15 loss: 0.1328 Acc: 93.7500% F1: 0.910 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 6/15 loss: 0.2646 Acc: 93.7500% F1: 0.938 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 7/15 loss: 0.1430 Acc: 96.8750% F1: 0.878 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 8/15 loss: 0.1345 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 9/15 loss: 0.1410 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 10/15 loss: 0.1171 Acc: 96.8750% F1: 0.940 Time: 0.94s (0.03s)
Fold 0 train - epoch: 9/10 iter: 11/15 loss: 0.1020 Acc: 96.8750% F1: 0.916 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 12/15 loss: 0.1799 Acc: 93.7500% F1: 0.927 Time: 0.93s (0.03s)
Fold 0 train - epoch: 9/10 iter: 13/15 loss: 0.0969 Acc: 100.0000% F1: 1.000 Time: 0.94s (0.02s)
Fold 0 train - epoch: 9/10 iter: 14/15 loss: 0.0000 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 0 Epoch 9 train Avg acc: 93.7778% F1: 0.9064 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 9/10 iter: 0/2 loss: 1.6153 Acc: 37.5000% F1: 0.235 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 9/10 iter: 1/2 loss: 2.5652 Acc: 33.3333% F1: 0.200 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 9 train-dev Avg acc: 36.0000% F1: 0.2942 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/10 iter: 0/15 loss: 3.6720 Acc: 15.6250% F1: 0.134 Time: 1.00s (0.00s)
Fold 1 train - epoch: 0/10 iter: 1/15 loss: 2.6613 Acc: 31.2500% F1: 0.329 Time: 0.93s (0.04s)
Fold 1 train - epoch: 0/10 iter: 2/15 loss: 2.0109 Acc: 34.3750% F1: 0.311 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 3/15 loss: 1.5424 Acc: 37.5000% F1: 0.278 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 4/15 loss: 1.6281 Acc: 43.7500% F1: 0.269 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 5/15 loss: 1.2356 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 6/15 loss: 0.8715 Acc: 59.3750% F1: 0.296 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 7/15 loss: 1.0348 Acc: 40.6250% F1: 0.265 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 8/15 loss: 1.2234 Acc: 46.8750% F1: 0.345 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 9/15 loss: 1.2302 Acc: 46.8750% F1: 0.333 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 10/15 loss: 1.2051 Acc: 40.6250% F1: 0.290 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 11/15 loss: 1.0388 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 12/15 loss: 1.2402 Acc: 28.1250% F1: 0.204 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 13/15 loss: 0.9914 Acc: 46.8750% F1: 0.295 Time: 0.93s (0.03s)
Fold 1 train - epoch: 0/10 iter: 14/15 loss: 0.4024 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 40.6667% F1: 0.3407 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6313 Acc: 78.1250% F1: 0.439 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 0/10 iter: 1/2 loss: 2.1636 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2921 *
*********************************************************
Performing epoch 1 of 10
Fold 1 train - epoch: 1/10 iter: 0/15 loss: 0.9949 Acc: 56.2500% F1: 0.327 Time: 0.94s (0.00s)
Fold 1 train - epoch: 1/10 iter: 1/15 loss: 1.0009 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 2/15 loss: 1.1238 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 3/15 loss: 0.9758 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 4/15 loss: 1.1181 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 5/15 loss: 1.0036 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/10 iter: 6/15 loss: 0.7582 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 7/15 loss: 0.9772 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 1 train - epoch: 1/10 iter: 8/15 loss: 0.9525 Acc: 56.2500% F1: 0.302 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 9/15 loss: 0.9153 Acc: 50.0000% F1: 0.267 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 10/15 loss: 1.0229 Acc: 50.0000% F1: 0.332 Time: 0.93s (0.04s)
Fold 1 train - epoch: 1/10 iter: 11/15 loss: 0.8521 Acc: 65.6250% F1: 0.444 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 12/15 loss: 1.0605 Acc: 46.8750% F1: 0.339 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 13/15 loss: 0.9648 Acc: 43.7500% F1: 0.306 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/10 iter: 14/15 loss: 0.4194 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 54.6667% F1: 0.3207 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/10 iter: 0/2 loss: 0.8152 Acc: 59.3750% F1: 0.373 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3995 Acc: 27.7778% F1: 0.175 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.3085 *
*********************************************************
Performing epoch 2 of 10
Fold 1 train - epoch: 2/10 iter: 0/15 loss: 0.9658 Acc: 40.6250% F1: 0.278 Time: 0.95s (0.00s)
Fold 1 train - epoch: 2/10 iter: 1/15 loss: 0.8811 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 2/15 loss: 0.9195 Acc: 62.5000% F1: 0.424 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 3/15 loss: 0.8444 Acc: 59.3750% F1: 0.330 Time: 0.92s (0.03s)
Fold 1 train - epoch: 2/10 iter: 4/15 loss: 0.9038 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 5/15 loss: 0.8998 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 6/15 loss: 0.7079 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 7/15 loss: 0.8745 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 8/15 loss: 0.8891 Acc: 62.5000% F1: 0.383 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 9/15 loss: 0.9120 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 10/15 loss: 0.9754 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 11/15 loss: 0.7896 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 12/15 loss: 0.9470 Acc: 59.3750% F1: 0.352 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 13/15 loss: 0.8464 Acc: 56.2500% F1: 0.287 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/10 iter: 14/15 loss: 0.0651 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 57.5556% F1: 0.3396 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/10 iter: 0/2 loss: 0.7210 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 2/10 iter: 1/2 loss: 1.6070 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3119 *
*********************************************************
Performing epoch 3 of 10
Fold 1 train - epoch: 3/10 iter: 0/15 loss: 0.9370 Acc: 59.3750% F1: 0.342 Time: 0.94s (0.00s)
Fold 1 train - epoch: 3/10 iter: 1/15 loss: 0.8692 Acc: 62.5000% F1: 0.405 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 2/15 loss: 0.8374 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 3/15 loss: 0.7805 Acc: 56.2500% F1: 0.338 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 4/15 loss: 0.8241 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 5/15 loss: 0.8227 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 6/15 loss: 0.6820 Acc: 75.0000% F1: 0.484 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 7/15 loss: 0.7874 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 8/15 loss: 0.8437 Acc: 59.3750% F1: 0.362 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 9/15 loss: 0.8690 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/10 iter: 10/15 loss: 0.9391 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 11/15 loss: 0.7395 Acc: 78.1250% F1: 0.522 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 12/15 loss: 0.9037 Acc: 62.5000% F1: 0.440 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 13/15 loss: 0.7456 Acc: 65.6250% F1: 0.394 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/10 iter: 14/15 loss: 0.0245 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 64.8889% F1: 0.4290 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/10 iter: 0/2 loss: 0.8214 Acc: 59.3750% F1: 0.373 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 3/10 iter: 1/2 loss: 1.5667 Acc: 27.7778% F1: 0.175 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 48.0000% F1: 0.3085 *
*********************************************************
Performing epoch 4 of 10
Fold 1 train - epoch: 4/10 iter: 0/15 loss: 0.8213 Acc: 59.3750% F1: 0.410 Time: 0.95s (0.00s)
Fold 1 train - epoch: 4/10 iter: 1/15 loss: 0.8199 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 2/15 loss: 0.7487 Acc: 71.8750% F1: 0.503 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 3/15 loss: 0.7502 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 4/15 loss: 0.7507 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.05s)
Fold 1 train - epoch: 4/10 iter: 5/15 loss: 0.8256 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 6/15 loss: 0.6399 Acc: 68.7500% F1: 0.407 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 7/15 loss: 0.7776 Acc: 59.3750% F1: 0.366 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 8/15 loss: 0.7865 Acc: 59.3750% F1: 0.401 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 9/15 loss: 0.8157 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 10/15 loss: 0.7511 Acc: 62.5000% F1: 0.433 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 11/15 loss: 0.6630 Acc: 75.0000% F1: 0.498 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/10 iter: 12/15 loss: 0.8201 Acc: 65.6250% F1: 0.456 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/10 iter: 13/15 loss: 0.6392 Acc: 81.2500% F1: 0.566 Time: 0.94s (0.02s)
Fold 1 train - epoch: 4/10 iter: 14/15 loss: 0.0122 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 68.2222% F1: 0.4652 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/10 iter: 0/2 loss: 0.9931 Acc: 40.6250% F1: 0.289 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4791 Acc: 33.3333% F1: 0.200 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 38.0000% F1: 0.2601 *
*********************************************************
Performing epoch 5 of 10
Fold 1 train - epoch: 5/10 iter: 0/15 loss: 0.8652 Acc: 59.3750% F1: 0.415 Time: 0.94s (0.00s)
Fold 1 train - epoch: 5/10 iter: 1/15 loss: 0.6736 Acc: 68.7500% F1: 0.473 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 2/15 loss: 0.6572 Acc: 78.1250% F1: 0.699 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 3/15 loss: 0.6702 Acc: 71.8750% F1: 0.485 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 4/15 loss: 0.6352 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 5/15 loss: 0.6574 Acc: 68.7500% F1: 0.489 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 6/15 loss: 0.5596 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 7/15 loss: 0.6576 Acc: 71.8750% F1: 0.476 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 8/15 loss: 0.6454 Acc: 68.7500% F1: 0.495 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 9/15 loss: 0.7199 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.02s)
Fold 1 train - epoch: 5/10 iter: 10/15 loss: 0.7204 Acc: 68.7500% F1: 0.480 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 11/15 loss: 0.5529 Acc: 81.2500% F1: 0.562 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 12/15 loss: 0.7257 Acc: 71.8750% F1: 0.602 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 13/15 loss: 0.4810 Acc: 84.3750% F1: 0.734 Time: 0.93s (0.03s)
Fold 1 train - epoch: 5/10 iter: 14/15 loss: 0.0072 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 5 train Avg acc: 72.4444% F1: 0.5352 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 5/10 iter: 0/2 loss: 1.1137 Acc: 37.5000% F1: 0.206 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 5/10 iter: 1/2 loss: 1.5993 Acc: 33.3333% F1: 0.190 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 5 train-dev Avg acc: 36.0000% F1: 0.2549 *
*********************************************************
Performing epoch 6 of 10
Fold 1 train - epoch: 6/10 iter: 0/15 loss: 0.6919 Acc: 65.6250% F1: 0.575 Time: 0.95s (0.00s)
Fold 1 train - epoch: 6/10 iter: 1/15 loss: 0.5498 Acc: 78.1250% F1: 0.557 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 2/15 loss: 0.4651 Acc: 87.5000% F1: 0.766 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 3/15 loss: 0.4863 Acc: 84.3750% F1: 0.582 Time: 0.92s (0.03s)
Fold 1 train - epoch: 6/10 iter: 4/15 loss: 0.5356 Acc: 84.3750% F1: 0.607 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 5/15 loss: 0.4758 Acc: 81.2500% F1: 0.740 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 6/15 loss: 0.3346 Acc: 90.6250% F1: 0.829 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 7/15 loss: 0.4441 Acc: 78.1250% F1: 0.844 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 8/15 loss: 0.5182 Acc: 81.2500% F1: 0.709 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 9/15 loss: 0.5875 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 10/15 loss: 0.5849 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 11/15 loss: 0.4120 Acc: 87.5000% F1: 0.765 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 12/15 loss: 0.5464 Acc: 81.2500% F1: 0.677 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 13/15 loss: 0.3779 Acc: 90.6250% F1: 0.785 Time: 0.93s (0.03s)
Fold 1 train - epoch: 6/10 iter: 14/15 loss: 0.0022 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 6 train Avg acc: 81.5556% F1: 0.6760 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 6/10 iter: 0/2 loss: 1.3659 Acc: 43.7500% F1: 0.254 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 6/10 iter: 1/2 loss: 1.8090 Acc: 38.8889% F1: 0.212 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 6 train-dev Avg acc: 42.0000% F1: 0.2995 *
*********************************************************
Performing epoch 7 of 10
Fold 1 train - epoch: 7/10 iter: 0/15 loss: 0.5368 Acc: 78.1250% F1: 0.675 Time: 0.95s (0.00s)
Fold 1 train - epoch: 7/10 iter: 1/15 loss: 0.4556 Acc: 78.1250% F1: 0.557 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 2/15 loss: 0.3894 Acc: 87.5000% F1: 0.768 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 3/15 loss: 0.3297 Acc: 87.5000% F1: 0.603 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 4/15 loss: 0.3796 Acc: 81.2500% F1: 0.578 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 5/15 loss: 0.3668 Acc: 90.6250% F1: 0.867 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 6/15 loss: 0.2500 Acc: 93.7500% F1: 0.840 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 7/15 loss: 0.3450 Acc: 90.6250% F1: 0.614 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 8/15 loss: 0.3329 Acc: 84.3750% F1: 0.744 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 9/15 loss: 0.4580 Acc: 75.0000% F1: 0.631 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 10/15 loss: 0.3306 Acc: 90.6250% F1: 0.767 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 11/15 loss: 0.3793 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 12/15 loss: 0.4671 Acc: 84.3750% F1: 0.700 Time: 0.93s (0.02s)
Fold 1 train - epoch: 7/10 iter: 13/15 loss: 0.2144 Acc: 93.7500% F1: 0.907 Time: 0.93s (0.03s)
Fold 1 train - epoch: 7/10 iter: 14/15 loss: 0.0006 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 7 train Avg acc: 85.5556% F1: 0.7338 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 7/10 iter: 0/2 loss: 1.7686 Acc: 37.5000% F1: 0.232 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 7/10 iter: 1/2 loss: 1.8462 Acc: 38.8889% F1: 0.212 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 7 train-dev Avg acc: 38.0000% F1: 0.2813 *
*********************************************************
Performing epoch 8 of 10
Fold 1 train - epoch: 8/10 iter: 0/15 loss: 0.3480 Acc: 84.3750% F1: 0.792 Time: 0.94s (0.00s)
Fold 1 train - epoch: 8/10 iter: 1/15 loss: 0.3049 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 2/15 loss: 0.2408 Acc: 93.7500% F1: 0.954 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 3/15 loss: 0.3581 Acc: 90.6250% F1: 0.929 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 4/15 loss: 0.3152 Acc: 90.6250% F1: 0.771 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 5/15 loss: 0.2383 Acc: 87.5000% F1: 0.804 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 6/15 loss: 0.1836 Acc: 93.7500% F1: 0.938 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 7/15 loss: 0.2888 Acc: 87.5000% F1: 0.913 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 8/15 loss: 0.3181 Acc: 84.3750% F1: 0.781 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 9/15 loss: 0.2035 Acc: 96.8750% F1: 0.950 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 10/15 loss: 0.2777 Acc: 90.6250% F1: 0.869 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 11/15 loss: 0.1773 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 12/15 loss: 0.1931 Acc: 96.8750% F1: 0.945 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 13/15 loss: 0.1242 Acc: 93.7500% F1: 0.871 Time: 0.93s (0.03s)
Fold 1 train - epoch: 8/10 iter: 14/15 loss: 0.0003 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 1 Epoch 8 train Avg acc: 91.1111% F1: 0.8649 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 8/10 iter: 0/2 loss: 2.0816 Acc: 37.5000% F1: 0.246 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 8/10 iter: 1/2 loss: 2.0833 Acc: 38.8889% F1: 0.283 Time: 0.19s (0.00s)
*********************************************************
* Fold 1 Epoch 8 train-dev Avg acc: 38.0000% F1: 0.3261 *
*********************************************************
Performing epoch 9 of 10
Fold 1 train - epoch: 9/10 iter: 0/15 loss: 0.2667 Acc: 87.5000% F1: 0.814 Time: 0.94s (0.00s)
Fold 1 train - epoch: 9/10 iter: 1/15 loss: 0.1679 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 2/15 loss: 0.1772 Acc: 96.8750% F1: 0.977 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 3/15 loss: 0.1811 Acc: 93.7500% F1: 0.954 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 4/15 loss: 0.1211 Acc: 93.7500% F1: 0.865 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 5/15 loss: 0.1689 Acc: 93.7500% F1: 0.950 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 6/15 loss: 0.1215 Acc: 93.7500% F1: 0.818 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 7/15 loss: 0.1614 Acc: 90.6250% F1: 0.614 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 8/15 loss: 0.1901 Acc: 93.7500% F1: 0.913 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 9/15 loss: 0.3381 Acc: 87.5000% F1: 0.870 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 10/15 loss: 0.2922 Acc: 87.5000% F1: 0.870 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 11/15 loss: 0.1001 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 1 train - epoch: 9/10 iter: 12/15 loss: 0.2096 Acc: 93.7500% F1: 0.890 Time: 0.94s (0.03s)
Fold 1 train - epoch: 9/10 iter: 13/15 loss: 0.0621 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 1 train - epoch: 9/10 iter: 14/15 loss: 0.0001 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 9 train Avg acc: 93.3333% F1: 0.9033 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 9/10 iter: 0/2 loss: 2.3737 Acc: 37.5000% F1: 0.246 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 9/10 iter: 1/2 loss: 2.2357 Acc: 44.4444% F1: 0.344 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 9 train-dev Avg acc: 40.0000% F1: 0.3669 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/10 iter: 0/15 loss: 4.0994 Acc: 12.5000% F1: 0.076 Time: 0.95s (0.00s)
Fold 2 train - epoch: 0/10 iter: 1/15 loss: 2.6226 Acc: 25.0000% F1: 0.269 Time: 0.93s (0.04s)
Fold 2 train - epoch: 0/10 iter: 2/15 loss: 2.0633 Acc: 18.7500% F1: 0.189 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 3/15 loss: 1.6242 Acc: 31.2500% F1: 0.219 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 4/15 loss: 1.5334 Acc: 46.8750% F1: 0.285 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 5/15 loss: 1.1897 Acc: 50.0000% F1: 0.227 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 6/15 loss: 0.9835 Acc: 53.1250% F1: 0.271 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 7/15 loss: 0.9405 Acc: 50.0000% F1: 0.326 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 8/15 loss: 1.3866 Acc: 28.1250% F1: 0.210 Time: 0.94s (0.03s)
Fold 2 train - epoch: 0/10 iter: 9/15 loss: 1.3182 Acc: 37.5000% F1: 0.267 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 10/15 loss: 1.1819 Acc: 50.0000% F1: 0.357 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 11/15 loss: 0.9926 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 12/15 loss: 1.2520 Acc: 37.5000% F1: 0.267 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/10 iter: 13/15 loss: 0.9723 Acc: 53.1250% F1: 0.363 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/10 iter: 14/15 loss: 0.4623 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 38.8889% F1: 0.3218 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/10 iter: 0/2 loss: 0.4745 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 0/10 iter: 1/2 loss: 1.8467 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 10
Fold 2 train - epoch: 1/10 iter: 0/15 loss: 1.0793 Acc: 59.3750% F1: 0.344 Time: 0.95s (0.00s)
Fold 2 train - epoch: 1/10 iter: 1/15 loss: 0.9627 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 2/15 loss: 1.1175 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 3/15 loss: 0.9931 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 4/15 loss: 1.0687 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 5/15 loss: 1.0162 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 6/15 loss: 0.7460 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 7/15 loss: 0.8355 Acc: 56.2500% F1: 0.333 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 8/15 loss: 0.9547 Acc: 59.3750% F1: 0.368 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 9/15 loss: 0.9850 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 10/15 loss: 1.0241 Acc: 43.7500% F1: 0.305 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/10 iter: 11/15 loss: 0.8552 Acc: 59.3750% F1: 0.392 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/10 iter: 12/15 loss: 1.0562 Acc: 56.2500% F1: 0.395 Time: 0.94s (0.03s)
Fold 2 train - epoch: 1/10 iter: 13/15 loss: 0.9095 Acc: 53.1250% F1: 0.371 Time: 0.94s (0.03s)
Fold 2 train - epoch: 1/10 iter: 14/15 loss: 0.4672 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 56.0000% F1: 0.3349 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7001 Acc: 81.2500% F1: 0.571 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 1/10 iter: 1/2 loss: 1.2574 Acc: 22.2222% F1: 0.148 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 60.0000% F1: 0.3698 *
*********************************************************
Performing epoch 2 of 10
Fold 2 train - epoch: 2/10 iter: 0/15 loss: 0.9220 Acc: 53.1250% F1: 0.350 Time: 0.94s (0.00s)
Fold 2 train - epoch: 2/10 iter: 1/15 loss: 0.8475 Acc: 68.7500% F1: 0.473 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 2/15 loss: 0.9044 Acc: 53.1250% F1: 0.345 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 3/15 loss: 0.8507 Acc: 53.1250% F1: 0.271 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 4/15 loss: 0.9224 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 5/15 loss: 0.8512 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 6/15 loss: 0.6926 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 7/15 loss: 0.9053 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 8/15 loss: 1.0087 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.03s)
Fold 2 train - epoch: 2/10 iter: 9/15 loss: 0.9696 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 10/15 loss: 1.0338 Acc: 40.6250% F1: 0.193 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 11/15 loss: 0.7858 Acc: 65.6250% F1: 0.328 Time: 0.94s (0.02s)
Fold 2 train - epoch: 2/10 iter: 12/15 loss: 0.9100 Acc: 59.3750% F1: 0.312 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/10 iter: 13/15 loss: 0.8569 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/10 iter: 14/15 loss: 0.1220 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 56.6667% F1: 0.3193 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6378 Acc: 84.3750% F1: 0.599 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 2/10 iter: 1/2 loss: 1.3274 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 62.0000% F1: 0.3810 *
*********************************************************
Performing epoch 3 of 10
Fold 2 train - epoch: 3/10 iter: 0/15 loss: 0.8946 Acc: 50.0000% F1: 0.317 Time: 0.94s (0.00s)
Fold 2 train - epoch: 3/10 iter: 1/15 loss: 0.8644 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 2/15 loss: 0.8308 Acc: 68.7500% F1: 0.474 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 3/15 loss: 0.8217 Acc: 62.5000% F1: 0.421 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 4/15 loss: 0.8659 Acc: 68.7500% F1: 0.484 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 5/15 loss: 0.8418 Acc: 68.7500% F1: 0.476 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 6/15 loss: 0.6437 Acc: 81.2500% F1: 0.530 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 7/15 loss: 0.7539 Acc: 65.6250% F1: 0.426 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 8/15 loss: 0.9385 Acc: 59.3750% F1: 0.363 Time: 0.94s (0.03s)
Fold 2 train - epoch: 3/10 iter: 9/15 loss: 0.9572 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.04s)
Fold 2 train - epoch: 3/10 iter: 10/15 loss: 0.9719 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 11/15 loss: 0.7331 Acc: 75.0000% F1: 0.498 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 12/15 loss: 0.9406 Acc: 53.1250% F1: 0.316 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 13/15 loss: 0.7162 Acc: 78.1250% F1: 0.531 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/10 iter: 14/15 loss: 0.0281 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 64.4444% F1: 0.4276 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6719 Acc: 81.2500% F1: 0.304 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 3/10 iter: 1/2 loss: 1.3230 Acc: 27.7778% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 62.0000% F1: 0.3831 *
*********************************************************
Performing epoch 4 of 10
Fold 2 train - epoch: 4/10 iter: 0/15 loss: 0.8701 Acc: 53.1250% F1: 0.352 Time: 0.94s (0.00s)
Fold 2 train - epoch: 4/10 iter: 1/15 loss: 0.8175 Acc: 68.7500% F1: 0.475 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 2/15 loss: 0.7589 Acc: 71.8750% F1: 0.495 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 3/15 loss: 0.7558 Acc: 75.0000% F1: 0.509 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 4/15 loss: 0.8224 Acc: 71.8750% F1: 0.509 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 5/15 loss: 0.8021 Acc: 68.7500% F1: 0.471 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 6/15 loss: 0.5327 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 7/15 loss: 0.7066 Acc: 65.6250% F1: 0.438 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 8/15 loss: 0.8722 Acc: 62.5000% F1: 0.448 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 9/15 loss: 0.8078 Acc: 62.5000% F1: 0.419 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 10/15 loss: 0.9142 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.03s)
Fold 2 train - epoch: 4/10 iter: 11/15 loss: 0.6121 Acc: 78.1250% F1: 0.522 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/10 iter: 12/15 loss: 0.8035 Acc: 59.3750% F1: 0.404 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/10 iter: 13/15 loss: 0.6738 Acc: 75.0000% F1: 0.501 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/10 iter: 14/15 loss: 0.0102 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 68.2222% F1: 0.4624 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8193 Acc: 65.6250% F1: 0.275 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 4/10 iter: 1/2 loss: 1.2737 Acc: 38.8889% F1: 0.212 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 56.0000% F1: 0.3839 *
*********************************************************
Performing epoch 5 of 10
Fold 2 train - epoch: 5/10 iter: 0/15 loss: 0.9098 Acc: 53.1250% F1: 0.370 Time: 0.94s (0.00s)
Fold 2 train - epoch: 5/10 iter: 1/15 loss: 0.6544 Acc: 75.0000% F1: 0.525 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 2/15 loss: 0.6495 Acc: 81.2500% F1: 0.568 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 3/15 loss: 0.6863 Acc: 71.8750% F1: 0.492 Time: 0.92s (0.03s)
Fold 2 train - epoch: 5/10 iter: 4/15 loss: 0.7279 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 5/15 loss: 0.5936 Acc: 75.0000% F1: 0.547 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 6/15 loss: 0.5060 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 7/15 loss: 0.6688 Acc: 71.8750% F1: 0.479 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 8/15 loss: 0.7293 Acc: 68.7500% F1: 0.544 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 9/15 loss: 0.7235 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 10/15 loss: 0.6650 Acc: 71.8750% F1: 0.509 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 11/15 loss: 0.4763 Acc: 87.5000% F1: 0.606 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 12/15 loss: 0.7332 Acc: 78.1250% F1: 0.654 Time: 0.93s (0.03s)
Fold 2 train - epoch: 5/10 iter: 13/15 loss: 0.5653 Acc: 81.2500% F1: 0.547 Time: 0.94s (0.03s)
Fold 2 train - epoch: 5/10 iter: 14/15 loss: 0.0047 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.03s)
*****************************************************
* Fold 2 Epoch 5 train Avg acc: 74.2222% F1: 0.5396 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9270 Acc: 53.1250% F1: 0.241 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 5/10 iter: 1/2 loss: 1.3067 Acc: 38.8889% F1: 0.212 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3438 *
*********************************************************
Performing epoch 6 of 10
Fold 2 train - epoch: 6/10 iter: 0/15 loss: 0.8020 Acc: 62.5000% F1: 0.427 Time: 0.95s (0.00s)
Fold 2 train - epoch: 6/10 iter: 1/15 loss: 0.5854 Acc: 78.1250% F1: 0.541 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 2/15 loss: 0.5144 Acc: 84.3750% F1: 0.744 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 3/15 loss: 0.5572 Acc: 84.3750% F1: 0.578 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 4/15 loss: 0.5446 Acc: 84.3750% F1: 0.599 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 5/15 loss: 0.4707 Acc: 81.2500% F1: 0.694 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 6/15 loss: 0.3474 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 7/15 loss: 0.4938 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 8/15 loss: 0.5954 Acc: 71.8750% F1: 0.605 Time: 0.94s (0.03s)
Fold 2 train - epoch: 6/10 iter: 9/15 loss: 0.5714 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.04s)
Fold 2 train - epoch: 6/10 iter: 10/15 loss: 0.4312 Acc: 87.5000% F1: 0.871 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 11/15 loss: 0.4294 Acc: 81.2500% F1: 0.576 Time: 0.93s (0.03s)
Fold 2 train - epoch: 6/10 iter: 12/15 loss: 0.5852 Acc: 81.2500% F1: 0.677 Time: 0.94s (0.03s)
Fold 2 train - epoch: 6/10 iter: 13/15 loss: 0.5145 Acc: 81.2500% F1: 0.707 Time: 0.94s (0.03s)
Fold 2 train - epoch: 6/10 iter: 14/15 loss: 0.0015 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 6 train Avg acc: 80.6667% F1: 0.6631 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 6/10 iter: 0/2 loss: 1.0537 Acc: 46.8750% F1: 0.222 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 6/10 iter: 1/2 loss: 1.3967 Acc: 38.8889% F1: 0.203 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 6 train-dev Avg acc: 44.0000% F1: 0.3301 *
*********************************************************
Performing epoch 7 of 10
Fold 2 train - epoch: 7/10 iter: 0/15 loss: 0.5113 Acc: 81.2500% F1: 0.690 Time: 0.94s (0.00s)
Fold 2 train - epoch: 7/10 iter: 1/15 loss: 0.4140 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 2/15 loss: 0.3630 Acc: 87.5000% F1: 0.766 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 3/15 loss: 0.3747 Acc: 87.5000% F1: 0.612 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 4/15 loss: 0.4003 Acc: 90.6250% F1: 0.767 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 5/15 loss: 0.3364 Acc: 90.6250% F1: 0.865 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 6/15 loss: 0.2331 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 7/15 loss: 0.3533 Acc: 87.5000% F1: 0.592 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 8/15 loss: 0.5234 Acc: 78.1250% F1: 0.733 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 9/15 loss: 0.4136 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 10/15 loss: 0.3936 Acc: 90.6250% F1: 0.894 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 11/15 loss: 0.2282 Acc: 93.7500% F1: 0.817 Time: 0.93s (0.02s)
Fold 2 train - epoch: 7/10 iter: 12/15 loss: 0.3819 Acc: 87.5000% F1: 0.787 Time: 0.93s (0.03s)
Fold 2 train - epoch: 7/10 iter: 13/15 loss: 0.2842 Acc: 90.6250% F1: 0.761 Time: 0.94s (0.02s)
Fold 2 train - epoch: 7/10 iter: 14/15 loss: 0.0002 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 7 train Avg acc: 87.5556% F1: 0.7748 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 7/10 iter: 0/2 loss: 1.1757 Acc: 50.0000% F1: 0.232 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 7/10 iter: 1/2 loss: 1.6018 Acc: 38.8889% F1: 0.203 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 7 train-dev Avg acc: 46.0000% F1: 0.3481 *
*********************************************************
Performing epoch 8 of 10
Fold 2 train - epoch: 8/10 iter: 0/15 loss: 0.3695 Acc: 90.6250% F1: 0.836 Time: 0.94s (0.00s)
Fold 2 train - epoch: 8/10 iter: 1/15 loss: 0.3155 Acc: 84.3750% F1: 0.709 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 2/15 loss: 0.2119 Acc: 93.7500% F1: 0.868 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 3/15 loss: 0.2379 Acc: 90.6250% F1: 0.635 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 4/15 loss: 0.2797 Acc: 90.6250% F1: 0.827 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 5/15 loss: 0.2128 Acc: 90.6250% F1: 0.823 Time: 0.93s (0.02s)
Fold 2 train - epoch: 8/10 iter: 6/15 loss: 0.1288 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 7/15 loss: 0.1677 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 8/15 loss: 0.3246 Acc: 87.5000% F1: 0.832 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 9/15 loss: 0.2859 Acc: 87.5000% F1: 0.825 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 10/15 loss: 0.2634 Acc: 90.6250% F1: 0.894 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 11/15 loss: 0.1393 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 12/15 loss: 0.3056 Acc: 87.5000% F1: 0.836 Time: 0.93s (0.03s)
Fold 2 train - epoch: 8/10 iter: 13/15 loss: 0.2339 Acc: 93.7500% F1: 0.894 Time: 0.94s (0.03s)
Fold 2 train - epoch: 8/10 iter: 14/15 loss: 0.0002 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 8 train Avg acc: 91.7778% F1: 0.8533 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 8/10 iter: 0/2 loss: 1.3256 Acc: 53.1250% F1: 0.241 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 8/10 iter: 1/2 loss: 1.7514 Acc: 38.8889% F1: 0.203 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 8 train-dev Avg acc: 48.0000% F1: 0.3519 *
*********************************************************
Performing epoch 9 of 10
Fold 2 train - epoch: 9/10 iter: 0/15 loss: 0.2640 Acc: 90.6250% F1: 0.846 Time: 0.95s (0.00s)
Fold 2 train - epoch: 9/10 iter: 1/15 loss: 0.2488 Acc: 84.3750% F1: 0.714 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 2/15 loss: 0.1824 Acc: 90.6250% F1: 0.875 Time: 0.93s (0.04s)
Fold 2 train - epoch: 9/10 iter: 3/15 loss: 0.1378 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 4/15 loss: 0.1314 Acc: 93.7500% F1: 0.865 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 5/15 loss: 0.0966 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 6/15 loss: 0.1068 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 7/15 loss: 0.0997 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 8/15 loss: 0.2174 Acc: 93.7500% F1: 0.921 Time: 0.94s (0.03s)
Fold 2 train - epoch: 9/10 iter: 9/15 loss: 0.1988 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 10/15 loss: 0.2012 Acc: 90.6250% F1: 0.853 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 11/15 loss: 0.1985 Acc: 87.5000% F1: 0.815 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 12/15 loss: 0.1572 Acc: 93.7500% F1: 0.890 Time: 0.93s (0.03s)
Fold 2 train - epoch: 9/10 iter: 13/15 loss: 0.1103 Acc: 100.0000% F1: 1.000 Time: 0.94s (0.03s)
Fold 2 train - epoch: 9/10 iter: 14/15 loss: 0.0000 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 2 Epoch 9 train Avg acc: 94.0000% F1: 0.8989 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 9/10 iter: 0/2 loss: 1.9299 Acc: 40.6250% F1: 0.202 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 9/10 iter: 1/2 loss: 1.5307 Acc: 44.4444% F1: 0.314 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 9 train-dev Avg acc: 42.0000% F1: 0.3523 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/10 iter: 0/15 loss: 4.1957 Acc: 12.5000% F1: 0.078 Time: 0.96s (0.00s)
Fold 3 train - epoch: 0/10 iter: 1/15 loss: 2.8915 Acc: 15.6250% F1: 0.173 Time: 0.93s (0.03s)
Fold 3 train - epoch: 0/10 iter: 2/15 loss: 2.0039 Acc: 25.0000% F1: 0.248 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 3/15 loss: 1.5074 Acc: 34.3750% F1: 0.242 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 4/15 loss: 1.5500 Acc: 46.8750% F1: 0.290 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 5/15 loss: 0.9911 Acc: 56.2500% F1: 0.352 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 6/15 loss: 0.9504 Acc: 56.2500% F1: 0.284 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 7/15 loss: 0.9664 Acc: 56.2500% F1: 0.376 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 8/15 loss: 1.3362 Acc: 31.2500% F1: 0.234 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 9/15 loss: 1.3637 Acc: 37.5000% F1: 0.268 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 10/15 loss: 1.2631 Acc: 40.6250% F1: 0.290 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 11/15 loss: 0.9802 Acc: 46.8750% F1: 0.319 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 12/15 loss: 1.1350 Acc: 40.6250% F1: 0.286 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 13/15 loss: 0.9754 Acc: 53.1250% F1: 0.355 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/10 iter: 14/15 loss: 0.4595 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 39.7778% F1: 0.3261 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5747 Acc: 81.2500% F1: 0.448 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 0/10 iter: 1/2 loss: 1.9835 Acc: 5.5556% F1: 0.048 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2676 *
*********************************************************
Performing epoch 1 of 10
Fold 3 train - epoch: 1/10 iter: 0/15 loss: 1.0340 Acc: 53.1250% F1: 0.231 Time: 0.95s (0.00s)
Fold 3 train - epoch: 1/10 iter: 1/15 loss: 0.9310 Acc: 65.6250% F1: 0.404 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 2/15 loss: 1.0950 Acc: 53.1250% F1: 0.275 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 3/15 loss: 0.9445 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 4/15 loss: 1.0138 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 5/15 loss: 0.8630 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/10 iter: 6/15 loss: 0.7147 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 7/15 loss: 0.8514 Acc: 46.8750% F1: 0.244 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 8/15 loss: 1.0768 Acc: 53.1250% F1: 0.294 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 9/15 loss: 0.9950 Acc: 43.7500% F1: 0.265 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/10 iter: 10/15 loss: 0.9749 Acc: 43.7500% F1: 0.283 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 11/15 loss: 0.9363 Acc: 53.1250% F1: 0.344 Time: 0.94s (0.03s)
Fold 3 train - epoch: 1/10 iter: 12/15 loss: 1.0336 Acc: 46.8750% F1: 0.333 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 13/15 loss: 0.9569 Acc: 43.7500% F1: 0.306 Time: 0.93s (0.03s)
Fold 3 train - epoch: 1/10 iter: 14/15 loss: 0.6442 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 53.3333% F1: 0.3124 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/10 iter: 0/2 loss: 0.6920 Acc: 81.2500% F1: 0.571 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3056 Acc: 5.5556% F1: 0.042 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.2937 *
*********************************************************
Performing epoch 2 of 10
Fold 3 train - epoch: 2/10 iter: 0/15 loss: 0.9734 Acc: 62.5000% F1: 0.417 Time: 0.95s (0.00s)
Fold 3 train - epoch: 2/10 iter: 1/15 loss: 0.9529 Acc: 65.6250% F1: 0.445 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 2/15 loss: 0.9440 Acc: 53.1250% F1: 0.328 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 3/15 loss: 0.8141 Acc: 62.5000% F1: 0.371 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 4/15 loss: 0.9604 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/10 iter: 5/15 loss: 0.8346 Acc: 62.5000% F1: 0.320 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 6/15 loss: 0.7408 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 7/15 loss: 0.9379 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 8/15 loss: 1.0912 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 9/15 loss: 1.0166 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 10/15 loss: 1.0426 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 11/15 loss: 0.8039 Acc: 65.6250% F1: 0.370 Time: 0.94s (0.02s)
Fold 3 train - epoch: 2/10 iter: 12/15 loss: 0.8806 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 13/15 loss: 0.8382 Acc: 59.3750% F1: 0.336 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/10 iter: 14/15 loss: 0.2620 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 57.7778% F1: 0.3242 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6423 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/10 iter: 1/2 loss: 1.3941 Acc: 5.5556% F1: 0.042 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.2637 *
*********************************************************
Performing epoch 3 of 10
Fold 3 train - epoch: 3/10 iter: 0/15 loss: 0.9248 Acc: 68.7500% F1: 0.483 Time: 0.95s (0.00s)
Fold 3 train - epoch: 3/10 iter: 1/15 loss: 0.8404 Acc: 62.5000% F1: 0.417 Time: 0.92s (0.02s)
Fold 3 train - epoch: 3/10 iter: 2/15 loss: 0.7954 Acc: 59.3750% F1: 0.404 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 3/15 loss: 0.8258 Acc: 53.1250% F1: 0.338 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 4/15 loss: 0.9019 Acc: 62.5000% F1: 0.440 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 5/15 loss: 0.8906 Acc: 59.3750% F1: 0.403 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/10 iter: 6/15 loss: 0.7216 Acc: 75.0000% F1: 0.484 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 7/15 loss: 0.7943 Acc: 59.3750% F1: 0.379 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 8/15 loss: 0.9414 Acc: 59.3750% F1: 0.396 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 9/15 loss: 0.8705 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 10/15 loss: 0.8947 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 11/15 loss: 0.7492 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 12/15 loss: 0.8397 Acc: 59.3750% F1: 0.307 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 13/15 loss: 0.8058 Acc: 68.7500% F1: 0.418 Time: 0.93s (0.03s)
Fold 3 train - epoch: 3/10 iter: 14/15 loss: 0.1307 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 62.6667% F1: 0.4086 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6229 Acc: 81.2500% F1: 0.448 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/10 iter: 1/2 loss: 1.5074 Acc: 5.5556% F1: 0.042 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.2710 *
*********************************************************
Performing epoch 4 of 10
Fold 3 train - epoch: 4/10 iter: 0/15 loss: 0.9160 Acc: 59.3750% F1: 0.370 Time: 0.95s (0.00s)
Fold 3 train - epoch: 4/10 iter: 1/15 loss: 0.8360 Acc: 62.5000% F1: 0.381 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 2/15 loss: 0.7961 Acc: 71.8750% F1: 0.502 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 3/15 loss: 0.8518 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 4/15 loss: 0.7851 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 5/15 loss: 0.7498 Acc: 71.8750% F1: 0.500 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 6/15 loss: 0.6357 Acc: 78.1250% F1: 0.506 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 7/15 loss: 0.7238 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 8/15 loss: 0.9388 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/10 iter: 9/15 loss: 0.8396 Acc: 59.3750% F1: 0.413 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 10/15 loss: 0.8235 Acc: 59.3750% F1: 0.413 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 11/15 loss: 0.6980 Acc: 78.1250% F1: 0.531 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/10 iter: 12/15 loss: 0.7254 Acc: 68.7500% F1: 0.560 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/10 iter: 13/15 loss: 0.7930 Acc: 59.3750% F1: 0.335 Time: 0.94s (0.03s)
Fold 3 train - epoch: 4/10 iter: 14/15 loss: 0.0668 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 68.0000% F1: 0.4711 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7020 Acc: 75.0000% F1: 0.379 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4103 Acc: 11.1111% F1: 0.137 Time: 0.19s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 52.0000% F1: 0.3535 *
*********************************************************
Performing epoch 5 of 10
Fold 3 train - epoch: 5/10 iter: 0/15 loss: 0.8349 Acc: 62.5000% F1: 0.409 Time: 0.94s (0.00s)
Fold 3 train - epoch: 5/10 iter: 1/15 loss: 0.6613 Acc: 68.7500% F1: 0.473 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 2/15 loss: 0.6413 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 3/15 loss: 0.6618 Acc: 78.1250% F1: 0.531 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 4/15 loss: 0.7192 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 5/15 loss: 0.6363 Acc: 75.0000% F1: 0.638 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 6/15 loss: 0.5368 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 7/15 loss: 0.6446 Acc: 65.6250% F1: 0.443 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 8/15 loss: 0.8222 Acc: 62.5000% F1: 0.432 Time: 0.94s (0.02s)
Fold 3 train - epoch: 5/10 iter: 9/15 loss: 0.8695 Acc: 62.5000% F1: 0.444 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 10/15 loss: 0.7686 Acc: 65.6250% F1: 0.575 Time: 0.93s (0.03s)
Fold 3 train - epoch: 5/10 iter: 11/15 loss: 0.5469 Acc: 87.5000% F1: 0.606 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 12/15 loss: 0.6172 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 13/15 loss: 0.4967 Acc: 78.1250% F1: 0.537 Time: 0.93s (0.02s)
Fold 3 train - epoch: 5/10 iter: 14/15 loss: 0.0466 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 5 train Avg acc: 72.2222% F1: 0.5214 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8815 Acc: 59.3750% F1: 0.300 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 5/10 iter: 1/2 loss: 1.2742 Acc: 44.4444% F1: 0.295 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 5 train-dev Avg acc: 54.0000% F1: 0.4357 *
*********************************************************
Performing epoch 6 of 10
Fold 3 train - epoch: 6/10 iter: 0/15 loss: 0.7118 Acc: 71.8750% F1: 0.627 Time: 0.95s (0.00s)
Fold 3 train - epoch: 6/10 iter: 1/15 loss: 0.6501 Acc: 75.0000% F1: 0.641 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 2/15 loss: 0.5704 Acc: 81.2500% F1: 0.569 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 3/15 loss: 0.4930 Acc: 81.2500% F1: 0.566 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 4/15 loss: 0.6320 Acc: 78.1250% F1: 0.556 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 5/15 loss: 0.4674 Acc: 78.1250% F1: 0.669 Time: 0.93s (0.02s)
Fold 3 train - epoch: 6/10 iter: 6/15 loss: 0.3639 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 7/15 loss: 0.6576 Acc: 71.8750% F1: 0.483 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 8/15 loss: 0.6914 Acc: 71.8750% F1: 0.592 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 9/15 loss: 0.5728 Acc: 81.2500% F1: 0.583 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 10/15 loss: 0.5649 Acc: 84.3750% F1: 0.846 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 11/15 loss: 0.4881 Acc: 81.2500% F1: 0.559 Time: 0.93s (0.03s)
Fold 3 train - epoch: 6/10 iter: 12/15 loss: 0.5143 Acc: 87.5000% F1: 0.725 Time: 0.94s (0.03s)
Fold 3 train - epoch: 6/10 iter: 13/15 loss: 0.4699 Acc: 90.6250% F1: 0.783 Time: 0.94s (0.03s)
Fold 3 train - epoch: 6/10 iter: 14/15 loss: 0.0184 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 3 Epoch 6 train Avg acc: 80.4444% F1: 0.6635 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 6/10 iter: 0/2 loss: 1.0508 Acc: 56.2500% F1: 0.303 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 6/10 iter: 1/2 loss: 1.2528 Acc: 50.0000% F1: 0.306 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 6 train-dev Avg acc: 54.0000% F1: 0.4442 *
*********************************************************
Performing epoch 7 of 10
Fold 3 train - epoch: 7/10 iter: 0/15 loss: 0.5764 Acc: 78.1250% F1: 0.672 Time: 0.94s (0.00s)
Fold 3 train - epoch: 7/10 iter: 1/15 loss: 0.5127 Acc: 84.3750% F1: 0.709 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 2/15 loss: 0.4516 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 3/15 loss: 0.4418 Acc: 78.1250% F1: 0.533 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 4/15 loss: 0.5489 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 5/15 loss: 0.2943 Acc: 93.7500% F1: 0.907 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 6/15 loss: 0.3402 Acc: 87.5000% F1: 0.784 Time: 0.94s (0.03s)
Fold 3 train - epoch: 7/10 iter: 7/15 loss: 0.4079 Acc: 84.3750% F1: 0.570 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 8/15 loss: 0.5644 Acc: 71.8750% F1: 0.522 Time: 0.93s (0.03s)
Fold 3 train - epoch: 7/10 iter: 9/15 loss: 0.5229 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 10/15 loss: 0.3459 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 11/15 loss: 0.3297 Acc: 93.7500% F1: 0.817 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 12/15 loss: 0.4256 Acc: 84.3750% F1: 0.703 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 13/15 loss: 0.2819 Acc: 90.6250% F1: 0.781 Time: 0.93s (0.02s)
Fold 3 train - epoch: 7/10 iter: 14/15 loss: 0.0138 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 7 train Avg acc: 84.4444% F1: 0.7241 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 7/10 iter: 0/2 loss: 1.2256 Acc: 59.3750% F1: 0.328 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 7/10 iter: 1/2 loss: 1.5005 Acc: 33.3333% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 7 train-dev Avg acc: 50.0000% F1: 0.4044 *
*********************************************************
Performing epoch 8 of 10
Fold 3 train - epoch: 8/10 iter: 0/15 loss: 0.5029 Acc: 75.0000% F1: 0.694 Time: 0.94s (0.00s)
Fold 3 train - epoch: 8/10 iter: 1/15 loss: 0.3604 Acc: 87.5000% F1: 0.741 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 2/15 loss: 0.3105 Acc: 84.3750% F1: 0.795 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 3/15 loss: 0.2571 Acc: 84.3750% F1: 0.576 Time: 0.92s (0.02s)
Fold 3 train - epoch: 8/10 iter: 4/15 loss: 0.3683 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 5/15 loss: 0.2867 Acc: 90.6250% F1: 0.832 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 6/15 loss: 0.2129 Acc: 93.7500% F1: 0.840 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 7/15 loss: 0.2132 Acc: 93.7500% F1: 0.634 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 8/15 loss: 0.3688 Acc: 78.1250% F1: 0.675 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 9/15 loss: 0.4015 Acc: 84.3750% F1: 0.711 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 10/15 loss: 0.3340 Acc: 84.3750% F1: 0.817 Time: 0.94s (0.03s)
Fold 3 train - epoch: 8/10 iter: 11/15 loss: 0.2793 Acc: 90.6250% F1: 0.790 Time: 0.93s (0.04s)
Fold 3 train - epoch: 8/10 iter: 12/15 loss: 0.3115 Acc: 81.2500% F1: 0.680 Time: 0.93s (0.03s)
Fold 3 train - epoch: 8/10 iter: 13/15 loss: 0.2107 Acc: 93.7500% F1: 0.817 Time: 0.93s (0.02s)
Fold 3 train - epoch: 8/10 iter: 14/15 loss: 0.0055 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 3 Epoch 8 train Avg acc: 86.4444% F1: 0.7585 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 8/10 iter: 0/2 loss: 1.5322 Acc: 34.3750% F1: 0.224 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 8/10 iter: 1/2 loss: 1.3373 Acc: 55.5556% F1: 0.317 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 8 train-dev Avg acc: 42.0000% F1: 0.3586 *
*********************************************************
Performing epoch 9 of 10
Fold 3 train - epoch: 9/10 iter: 0/15 loss: 0.5247 Acc: 75.0000% F1: 0.530 Time: 0.95s (0.00s)
Fold 3 train - epoch: 9/10 iter: 1/15 loss: 0.3383 Acc: 90.6250% F1: 0.889 Time: 0.93s (0.04s)
Fold 3 train - epoch: 9/10 iter: 2/15 loss: 0.2086 Acc: 87.5000% F1: 0.908 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 3/15 loss: 0.2196 Acc: 90.6250% F1: 0.787 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 4/15 loss: 0.3066 Acc: 87.5000% F1: 0.620 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 5/15 loss: 0.0966 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 6/15 loss: 0.1455 Acc: 93.7500% F1: 0.840 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 7/15 loss: 0.3771 Acc: 81.2500% F1: 0.566 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 8/15 loss: 0.3071 Acc: 90.6250% F1: 0.900 Time: 0.93s (0.03s)
Fold 3 train - epoch: 9/10 iter: 9/15 loss: 0.3745 Acc: 78.1250% F1: 0.696 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 10/15 loss: 0.3473 Acc: 87.5000% F1: 0.825 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 11/15 loss: 0.2135 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 12/15 loss: 0.1894 Acc: 93.7500% F1: 0.864 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 13/15 loss: 0.2433 Acc: 90.6250% F1: 0.864 Time: 0.93s (0.02s)
Fold 3 train - epoch: 9/10 iter: 14/15 loss: 0.0009 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 9 train Avg acc: 88.8889% F1: 0.8268 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 9/10 iter: 0/2 loss: 1.6623 Acc: 37.5000% F1: 0.245 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 9/10 iter: 1/2 loss: 1.5810 Acc: 38.8889% F1: 0.246 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 9 train-dev Avg acc: 38.0000% F1: 0.3305 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/10 iter: 0/15 loss: 4.0786 Acc: 9.3750% F1: 0.061 Time: 0.96s (0.00s)
Fold 4 train - epoch: 0/10 iter: 1/15 loss: 2.8810 Acc: 21.8750% F1: 0.226 Time: 0.93s (0.03s)
Fold 4 train - epoch: 0/10 iter: 2/15 loss: 2.0129 Acc: 34.3750% F1: 0.342 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 3/15 loss: 1.5635 Acc: 40.6250% F1: 0.285 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 4/15 loss: 1.4401 Acc: 53.1250% F1: 0.342 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 5/15 loss: 0.9828 Acc: 59.3750% F1: 0.342 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 6/15 loss: 0.8784 Acc: 65.6250% F1: 0.361 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 7/15 loss: 0.9511 Acc: 53.1250% F1: 0.352 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 8/15 loss: 1.5190 Acc: 37.5000% F1: 0.246 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 9/15 loss: 1.2964 Acc: 34.3750% F1: 0.240 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 10/15 loss: 1.1316 Acc: 46.8750% F1: 0.334 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 11/15 loss: 1.0245 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/10 iter: 12/15 loss: 1.1560 Acc: 50.0000% F1: 0.355 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/10 iter: 13/15 loss: 1.0202 Acc: 50.0000% F1: 0.348 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/10 iter: 14/15 loss: 0.5044 Acc: 100.0000% F1: 1.000 Time: 0.10s (0.02s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 43.3333% F1: 0.3617 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5568 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 0/10 iter: 1/2 loss: 2.1079 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.3276 *
*********************************************************
Performing epoch 1 of 10
Fold 4 train - epoch: 1/10 iter: 0/15 loss: 0.9958 Acc: 53.1250% F1: 0.333 Time: 0.95s (0.00s)
Fold 4 train - epoch: 1/10 iter: 1/15 loss: 0.9530 Acc: 53.1250% F1: 0.280 Time: 0.92s (0.02s)
Fold 4 train - epoch: 1/10 iter: 2/15 loss: 1.0002 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 3/15 loss: 0.9803 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 4/15 loss: 1.0925 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 5/15 loss: 0.9175 Acc: 62.5000% F1: 0.320 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 6/15 loss: 0.6690 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/10 iter: 7/15 loss: 0.9020 Acc: 46.8750% F1: 0.244 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 8/15 loss: 1.1307 Acc: 56.2500% F1: 0.308 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 9/15 loss: 1.0591 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 10/15 loss: 0.9758 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 11/15 loss: 0.8087 Acc: 59.3750% F1: 0.360 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 12/15 loss: 0.9521 Acc: 56.2500% F1: 0.389 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 13/15 loss: 0.9671 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/10 iter: 14/15 loss: 0.6164 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 54.6667% F1: 0.3152 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7759 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 1/10 iter: 1/2 loss: 1.5097 Acc: 27.7778% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3398 *
*********************************************************
Performing epoch 2 of 10
Fold 4 train - epoch: 2/10 iter: 0/15 loss: 0.9891 Acc: 50.0000% F1: 0.343 Time: 0.94s (0.00s)
Fold 4 train - epoch: 2/10 iter: 1/15 loss: 0.8973 Acc: 53.1250% F1: 0.361 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 2/15 loss: 0.8845 Acc: 53.1250% F1: 0.365 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 3/15 loss: 0.9046 Acc: 53.1250% F1: 0.322 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 4/15 loss: 0.9309 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 5/15 loss: 0.8280 Acc: 71.8750% F1: 0.475 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 6/15 loss: 0.6719 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 7/15 loss: 0.7933 Acc: 53.1250% F1: 0.317 Time: 0.93s (0.03s)
Fold 4 train - epoch: 2/10 iter: 8/15 loss: 1.0164 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 9/15 loss: 0.9903 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 4 train - epoch: 2/10 iter: 10/15 loss: 1.0323 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 11/15 loss: 0.7541 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 12/15 loss: 0.9399 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 13/15 loss: 0.8977 Acc: 59.3750% F1: 0.336 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/10 iter: 14/15 loss: 0.1999 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 57.5556% F1: 0.3479 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6056 Acc: 84.3750% F1: 0.599 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 2/10 iter: 1/2 loss: 1.7826 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 58.0000% F1: 0.3316 *
*********************************************************
Performing epoch 3 of 10
Fold 4 train - epoch: 3/10 iter: 0/15 loss: 0.9029 Acc: 59.3750% F1: 0.342 Time: 0.95s (0.00s)
Fold 4 train - epoch: 3/10 iter: 1/15 loss: 0.8562 Acc: 68.7500% F1: 0.455 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 2/15 loss: 0.8373 Acc: 56.2500% F1: 0.375 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 3/15 loss: 0.8281 Acc: 62.5000% F1: 0.403 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 4/15 loss: 0.8832 Acc: 65.6250% F1: 0.462 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 5/15 loss: 0.7611 Acc: 75.0000% F1: 0.504 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 6/15 loss: 0.7467 Acc: 59.3750% F1: 0.296 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 7/15 loss: 0.7293 Acc: 68.7500% F1: 0.451 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 8/15 loss: 0.9932 Acc: 62.5000% F1: 0.428 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/10 iter: 9/15 loss: 0.9237 Acc: 53.1250% F1: 0.335 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 10/15 loss: 0.8911 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/10 iter: 11/15 loss: 0.7612 Acc: 68.7500% F1: 0.412 Time: 0.94s (0.03s)
Fold 4 train - epoch: 3/10 iter: 12/15 loss: 0.8738 Acc: 56.2500% F1: 0.326 Time: 0.94s (0.03s)
Fold 4 train - epoch: 3/10 iter: 13/15 loss: 0.7815 Acc: 65.6250% F1: 0.438 Time: 0.94s (0.03s)
Fold 4 train - epoch: 3/10 iter: 14/15 loss: 0.0928 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 62.6667% F1: 0.4017 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6083 Acc: 75.0000% F1: 0.358 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 3/10 iter: 1/2 loss: 1.8894 Acc: 22.2222% F1: 0.148 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 56.0000% F1: 0.3558 *
*********************************************************
Performing epoch 4 of 10
Fold 4 train - epoch: 4/10 iter: 0/15 loss: 0.8286 Acc: 65.6250% F1: 0.439 Time: 0.95s (0.00s)
Fold 4 train - epoch: 4/10 iter: 1/15 loss: 0.7810 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 2/15 loss: 0.7891 Acc: 65.6250% F1: 0.449 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 3/15 loss: 0.7167 Acc: 75.0000% F1: 0.495 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 4/15 loss: 0.7458 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 5/15 loss: 0.6988 Acc: 78.1250% F1: 0.548 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 6/15 loss: 0.6159 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 7/15 loss: 0.6838 Acc: 84.3750% F1: 0.571 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 8/15 loss: 0.9783 Acc: 59.3750% F1: 0.396 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 9/15 loss: 0.8850 Acc: 62.5000% F1: 0.433 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 10/15 loss: 0.7920 Acc: 59.3750% F1: 0.389 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 11/15 loss: 0.6852 Acc: 78.1250% F1: 0.523 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 12/15 loss: 0.7010 Acc: 71.8750% F1: 0.483 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/10 iter: 13/15 loss: 0.7100 Acc: 71.8750% F1: 0.486 Time: 0.93s (0.03s)
Fold 4 train - epoch: 4/10 iter: 14/15 loss: 0.0495 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 71.3333% F1: 0.4840 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7600 Acc: 68.7500% F1: 0.397 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 4/10 iter: 1/2 loss: 1.8420 Acc: 27.7778% F1: 0.175 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.3716 *
*********************************************************
Performing epoch 5 of 10
Fold 4 train - epoch: 5/10 iter: 0/15 loss: 0.7700 Acc: 62.5000% F1: 0.440 Time: 0.95s (0.00s)
Fold 4 train - epoch: 5/10 iter: 1/15 loss: 0.6773 Acc: 78.1250% F1: 0.546 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 2/15 loss: 0.6107 Acc: 75.0000% F1: 0.669 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 3/15 loss: 0.7220 Acc: 68.7500% F1: 0.479 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 4/15 loss: 0.7361 Acc: 75.0000% F1: 0.531 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 5/15 loss: 0.5878 Acc: 78.1250% F1: 0.662 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 6/15 loss: 0.4489 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 7/15 loss: 0.5676 Acc: 84.3750% F1: 0.571 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 8/15 loss: 0.7601 Acc: 59.3750% F1: 0.407 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 9/15 loss: 0.8018 Acc: 65.6250% F1: 0.462 Time: 0.94s (0.03s)
Fold 4 train - epoch: 5/10 iter: 10/15 loss: 0.6322 Acc: 75.0000% F1: 0.650 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 11/15 loss: 0.5124 Acc: 75.0000% F1: 0.490 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 12/15 loss: 0.6402 Acc: 78.1250% F1: 0.662 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 13/15 loss: 0.5561 Acc: 78.1250% F1: 0.544 Time: 0.93s (0.02s)
Fold 4 train - epoch: 5/10 iter: 14/15 loss: 0.0481 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 5 train Avg acc: 73.5556% F1: 0.5540 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8811 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 5/10 iter: 1/2 loss: 1.9566 Acc: 27.7778% F1: 0.159 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3461 *
*********************************************************
Performing epoch 6 of 10
Fold 4 train - epoch: 6/10 iter: 0/15 loss: 0.6876 Acc: 75.0000% F1: 0.533 Time: 0.95s (0.00s)
Fold 4 train - epoch: 6/10 iter: 1/15 loss: 0.5863 Acc: 75.0000% F1: 0.520 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 2/15 loss: 0.4861 Acc: 84.3750% F1: 0.591 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 3/15 loss: 0.4927 Acc: 75.0000% F1: 0.525 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 4/15 loss: 0.6668 Acc: 71.8750% F1: 0.513 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 5/15 loss: 0.4301 Acc: 87.5000% F1: 0.806 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 6/15 loss: 0.4373 Acc: 75.0000% F1: 0.447 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 7/15 loss: 0.5239 Acc: 78.1250% F1: 0.527 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 8/15 loss: 0.6192 Acc: 71.8750% F1: 0.582 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 9/15 loss: 0.5813 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 10/15 loss: 0.4310 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.03s)
Fold 4 train - epoch: 6/10 iter: 11/15 loss: 0.4945 Acc: 84.3750% F1: 0.733 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 12/15 loss: 0.4362 Acc: 87.5000% F1: 0.814 Time: 0.93s (0.02s)
Fold 4 train - epoch: 6/10 iter: 13/15 loss: 0.4293 Acc: 87.5000% F1: 0.763 Time: 0.94s (0.03s)
Fold 4 train - epoch: 6/10 iter: 14/15 loss: 0.0200 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 6 train Avg acc: 79.7778% F1: 0.6430 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9738 Acc: 53.1250% F1: 0.345 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 6/10 iter: 1/2 loss: 2.2481 Acc: 22.2222% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 6 train-dev Avg acc: 42.0000% F1: 0.3067 *
*********************************************************
Performing epoch 7 of 10
Fold 4 train - epoch: 7/10 iter: 0/15 loss: 0.5785 Acc: 75.0000% F1: 0.531 Time: 0.94s (0.00s)
Fold 4 train - epoch: 7/10 iter: 1/15 loss: 0.4030 Acc: 87.5000% F1: 0.730 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 2/15 loss: 0.3816 Acc: 90.6250% F1: 0.789 Time: 0.93s (0.05s)
Fold 4 train - epoch: 7/10 iter: 3/15 loss: 0.3557 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 4/15 loss: 0.5220 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 5/15 loss: 0.3265 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 6/15 loss: 0.2548 Acc: 93.7500% F1: 0.818 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 7/15 loss: 0.3438 Acc: 90.6250% F1: 0.624 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 8/15 loss: 0.5004 Acc: 71.8750% F1: 0.603 Time: 0.94s (0.03s)
Fold 4 train - epoch: 7/10 iter: 9/15 loss: 0.4501 Acc: 81.2500% F1: 0.701 Time: 0.93s (0.03s)
Fold 4 train - epoch: 7/10 iter: 10/15 loss: 0.3645 Acc: 93.7500% F1: 0.868 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 11/15 loss: 0.2257 Acc: 93.7500% F1: 0.899 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 12/15 loss: 0.3766 Acc: 84.3750% F1: 0.593 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 13/15 loss: 0.2922 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.02s)
Fold 4 train - epoch: 7/10 iter: 14/15 loss: 0.0108 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 4 Epoch 7 train Avg acc: 87.3333% F1: 0.7541 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 7/10 iter: 0/2 loss: 1.3562 Acc: 43.7500% F1: 0.302 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 7/10 iter: 1/2 loss: 2.4820 Acc: 22.2222% F1: 0.127 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 7 train-dev Avg acc: 36.0000% F1: 0.2719 *
*********************************************************
Performing epoch 8 of 10
Fold 4 train - epoch: 8/10 iter: 0/15 loss: 0.4865 Acc: 87.5000% F1: 0.814 Time: 0.94s (0.00s)
Fold 4 train - epoch: 8/10 iter: 1/15 loss: 0.2733 Acc: 90.6250% F1: 0.757 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 2/15 loss: 0.2194 Acc: 90.6250% F1: 0.791 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 3/15 loss: 0.3584 Acc: 84.3750% F1: 0.721 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 4/15 loss: 0.3468 Acc: 87.5000% F1: 0.747 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 5/15 loss: 0.1994 Acc: 90.6250% F1: 0.838 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 6/15 loss: 0.2096 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 7/15 loss: 0.2380 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 8/15 loss: 0.2948 Acc: 87.5000% F1: 0.832 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 9/15 loss: 0.1910 Acc: 90.6250% F1: 0.770 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 10/15 loss: 0.3749 Acc: 84.3750% F1: 0.798 Time: 0.93s (0.03s)
Fold 4 train - epoch: 8/10 iter: 11/15 loss: 0.1714 Acc: 96.8750% F1: 0.925 Time: 0.93s (0.02s)
Fold 4 train - epoch: 8/10 iter: 12/15 loss: 0.2856 Acc: 84.3750% F1: 0.757 Time: 0.94s (0.02s)
Fold 4 train - epoch: 8/10 iter: 13/15 loss: 0.2771 Acc: 90.6250% F1: 0.842 Time: 0.94s (0.03s)
Fold 4 train - epoch: 8/10 iter: 14/15 loss: 0.0046 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 8 train Avg acc: 89.3333% F1: 0.8073 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 8/10 iter: 0/2 loss: 1.7860 Acc: 34.3750% F1: 0.255 Time: 0.35s (0.00s)
Fold 4 train-dev - epoch: 8/10 iter: 1/2 loss: 2.6520 Acc: 22.2222% F1: 0.127 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 8 train-dev Avg acc: 30.0000% F1: 0.2326 *
*********************************************************
Performing epoch 9 of 10
Fold 4 train - epoch: 9/10 iter: 0/15 loss: 0.3541 Acc: 87.5000% F1: 0.817 Time: 0.94s (0.00s)
Fold 4 train - epoch: 9/10 iter: 1/15 loss: 0.3287 Acc: 84.3750% F1: 0.714 Time: 0.93s (0.04s)
Fold 4 train - epoch: 9/10 iter: 2/15 loss: 0.2313 Acc: 93.7500% F1: 0.954 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 3/15 loss: 0.2185 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 4/15 loss: 0.2259 Acc: 90.6250% F1: 0.841 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 5/15 loss: 0.0818 Acc: 100.0000% F1: 1.000 Time: 0.95s (0.09s)
Fold 4 train - epoch: 9/10 iter: 6/15 loss: 0.1759 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 7/15 loss: 0.1723 Acc: 93.7500% F1: 0.856 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 8/15 loss: 0.2578 Acc: 87.5000% F1: 0.840 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 9/15 loss: 0.3142 Acc: 87.5000% F1: 0.822 Time: 0.93s (0.03s)
Fold 4 train - epoch: 9/10 iter: 10/15 loss: 0.3988 Acc: 87.5000% F1: 0.870 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 11/15 loss: 0.3443 Acc: 84.3750% F1: 0.801 Time: 0.94s (0.02s)
Fold 4 train - epoch: 9/10 iter: 12/15 loss: 0.3356 Acc: 90.6250% F1: 0.890 Time: 0.94s (0.02s)
Fold 4 train - epoch: 9/10 iter: 13/15 loss: 0.2615 Acc: 90.6250% F1: 0.874 Time: 0.93s (0.02s)
Fold 4 train - epoch: 9/10 iter: 14/15 loss: 0.0027 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 9 train Avg acc: 90.0000% F1: 0.8630 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 9/10 iter: 0/2 loss: 1.5833 Acc: 40.6250% F1: 0.289 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 9/10 iter: 1/2 loss: 3.1383 Acc: 16.6667% F1: 0.100 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 9 train-dev Avg acc: 32.0000% F1: 0.2472 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/10 iter: 0/15 loss: 4.0546 Acc: 9.3750% F1: 0.061 Time: 0.97s (0.00s)
Fold 5 train - epoch: 0/10 iter: 1/15 loss: 2.9240 Acc: 25.0000% F1: 0.274 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 2/15 loss: 1.9083 Acc: 34.3750% F1: 0.332 Time: 0.93s (0.04s)
Fold 5 train - epoch: 0/10 iter: 3/15 loss: 1.3311 Acc: 37.5000% F1: 0.264 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 4/15 loss: 1.3909 Acc: 46.8750% F1: 0.290 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 5/15 loss: 1.0054 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 6/15 loss: 0.9388 Acc: 65.6250% F1: 0.328 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 7/15 loss: 1.1140 Acc: 46.8750% F1: 0.300 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/10 iter: 8/15 loss: 1.5189 Acc: 28.1250% F1: 0.199 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 9/15 loss: 1.3081 Acc: 37.5000% F1: 0.265 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 10/15 loss: 1.0856 Acc: 43.7500% F1: 0.312 Time: 0.94s (0.03s)
Fold 5 train - epoch: 0/10 iter: 11/15 loss: 1.0663 Acc: 46.8750% F1: 0.319 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 12/15 loss: 1.0581 Acc: 50.0000% F1: 0.356 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 13/15 loss: 1.0398 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/10 iter: 14/15 loss: 0.5001 Acc: 50.0000% F1: 0.333 Time: 0.10s (0.02s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 41.1111% F1: 0.3455 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5525 Acc: 75.0000% F1: 0.358 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 0/10 iter: 1/2 loss: 1.9243 Acc: 16.6667% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.3333 *
*********************************************************
Performing epoch 1 of 10
Fold 5 train - epoch: 1/10 iter: 0/15 loss: 1.0625 Acc: 46.8750% F1: 0.252 Time: 0.95s (0.00s)
Fold 5 train - epoch: 1/10 iter: 1/15 loss: 0.9891 Acc: 62.5000% F1: 0.383 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 2/15 loss: 0.9997 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 3/15 loss: 0.9345 Acc: 59.3750% F1: 0.296 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 4/15 loss: 1.0649 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 5/15 loss: 0.8809 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 6/15 loss: 0.7242 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 7/15 loss: 0.9386 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 8/15 loss: 1.1658 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 9/15 loss: 1.0361 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 10/15 loss: 0.9882 Acc: 43.7500% F1: 0.246 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 11/15 loss: 0.8176 Acc: 59.3750% F1: 0.381 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/10 iter: 12/15 loss: 0.9846 Acc: 50.0000% F1: 0.298 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/10 iter: 13/15 loss: 0.9259 Acc: 56.2500% F1: 0.389 Time: 0.94s (0.03s)
Fold 5 train - epoch: 1/10 iter: 14/15 loss: 0.5607 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 54.4444% F1: 0.3018 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7299 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3903 Acc: 22.2222% F1: 0.148 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 56.0000% F1: 0.3514 *
*********************************************************
Performing epoch 2 of 10
Fold 5 train - epoch: 2/10 iter: 0/15 loss: 0.9353 Acc: 53.1250% F1: 0.370 Time: 0.95s (0.00s)
Fold 5 train - epoch: 2/10 iter: 1/15 loss: 0.9002 Acc: 65.6250% F1: 0.447 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 2/15 loss: 0.8865 Acc: 59.3750% F1: 0.404 Time: 0.93s (0.04s)
Fold 5 train - epoch: 2/10 iter: 3/15 loss: 0.8452 Acc: 62.5000% F1: 0.391 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 4/15 loss: 0.9116 Acc: 53.1250% F1: 0.352 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 5/15 loss: 0.8293 Acc: 68.7500% F1: 0.466 Time: 0.94s (0.02s)
Fold 5 train - epoch: 2/10 iter: 6/15 loss: 0.7060 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.10s)
Fold 5 train - epoch: 2/10 iter: 7/15 loss: 0.8618 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 8/15 loss: 1.0064 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 9/15 loss: 0.9937 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 10/15 loss: 1.0403 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 11/15 loss: 0.7551 Acc: 62.5000% F1: 0.314 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 12/15 loss: 0.9323 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.03s)
Fold 5 train - epoch: 2/10 iter: 13/15 loss: 0.8799 Acc: 59.3750% F1: 0.335 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/10 iter: 14/15 loss: 0.2058 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 57.7778% F1: 0.3458 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6310 Acc: 78.1250% F1: 0.439 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5519 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.2918 *
*********************************************************
Performing epoch 3 of 10
Fold 5 train - epoch: 3/10 iter: 0/15 loss: 0.8799 Acc: 56.2500% F1: 0.383 Time: 0.94s (0.00s)
Fold 5 train - epoch: 3/10 iter: 1/15 loss: 0.8501 Acc: 62.5000% F1: 0.381 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 2/15 loss: 0.8609 Acc: 68.7500% F1: 0.469 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 3/15 loss: 0.8022 Acc: 68.7500% F1: 0.462 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 4/15 loss: 0.8583 Acc: 68.7500% F1: 0.488 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 5/15 loss: 0.8224 Acc: 68.7500% F1: 0.460 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 6/15 loss: 0.6939 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 7/15 loss: 0.8036 Acc: 68.7500% F1: 0.459 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 8/15 loss: 1.0503 Acc: 53.1250% F1: 0.328 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 9/15 loss: 0.9369 Acc: 53.1250% F1: 0.352 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 10/15 loss: 0.9263 Acc: 46.8750% F1: 0.300 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 11/15 loss: 0.7478 Acc: 68.7500% F1: 0.383 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/10 iter: 12/15 loss: 0.8139 Acc: 62.5000% F1: 0.402 Time: 0.94s (0.03s)
Fold 5 train - epoch: 3/10 iter: 13/15 loss: 0.8185 Acc: 75.0000% F1: 0.496 Time: 0.93s (0.03s)
Fold 5 train - epoch: 3/10 iter: 14/15 loss: 0.0992 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 64.2222% F1: 0.4234 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6326 Acc: 84.3750% F1: 0.677 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 3/10 iter: 1/2 loss: 1.5678 Acc: 27.7778% F1: 0.185 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 64.0000% F1: 0.4216 *
*********************************************************
Performing epoch 4 of 10
Fold 5 train - epoch: 4/10 iter: 0/15 loss: 0.9298 Acc: 59.3750% F1: 0.400 Time: 0.94s (0.00s)
Fold 5 train - epoch: 4/10 iter: 1/15 loss: 0.7992 Acc: 68.7500% F1: 0.464 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 2/15 loss: 0.6874 Acc: 78.1250% F1: 0.548 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 3/15 loss: 0.7828 Acc: 59.3750% F1: 0.406 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 4/15 loss: 0.8469 Acc: 59.3750% F1: 0.403 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 5/15 loss: 0.7003 Acc: 78.1250% F1: 0.545 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 6/15 loss: 0.6055 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 7/15 loss: 0.7309 Acc: 81.2500% F1: 0.549 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 8/15 loss: 1.0017 Acc: 56.2500% F1: 0.348 Time: 0.93s (0.03s)
Fold 5 train - epoch: 4/10 iter: 9/15 loss: 0.9382 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 10/15 loss: 0.8480 Acc: 59.3750% F1: 0.398 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/10 iter: 11/15 loss: 0.6440 Acc: 78.1250% F1: 0.512 Time: 0.94s (0.02s)
Fold 5 train - epoch: 4/10 iter: 12/15 loss: 0.7364 Acc: 71.8750% F1: 0.481 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/10 iter: 13/15 loss: 0.6671 Acc: 75.0000% F1: 0.506 Time: 0.94s (0.03s)
Fold 5 train - epoch: 4/10 iter: 14/15 loss: 0.0715 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 68.0000% F1: 0.4588 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7475 Acc: 75.0000% F1: 0.434 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4932 Acc: 27.7778% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 58.0000% F1: 0.3949 *
*********************************************************
Performing epoch 5 of 10
Fold 5 train - epoch: 5/10 iter: 0/15 loss: 0.8120 Acc: 62.5000% F1: 0.426 Time: 0.94s (0.00s)
Fold 5 train - epoch: 5/10 iter: 1/15 loss: 0.7171 Acc: 78.1250% F1: 0.541 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 2/15 loss: 0.6488 Acc: 78.1250% F1: 0.546 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 3/15 loss: 0.7009 Acc: 65.6250% F1: 0.446 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 4/15 loss: 0.7277 Acc: 68.7500% F1: 0.488 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 5/15 loss: 0.6006 Acc: 78.1250% F1: 0.538 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 6/15 loss: 0.5214 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 7/15 loss: 0.6810 Acc: 71.8750% F1: 0.481 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 8/15 loss: 0.7978 Acc: 59.3750% F1: 0.363 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 9/15 loss: 0.6775 Acc: 75.0000% F1: 0.536 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 10/15 loss: 0.7526 Acc: 71.8750% F1: 0.513 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 11/15 loss: 0.5658 Acc: 81.2500% F1: 0.705 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 12/15 loss: 0.6163 Acc: 78.1250% F1: 0.538 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 13/15 loss: 0.5274 Acc: 84.3750% F1: 0.588 Time: 0.93s (0.03s)
Fold 5 train - epoch: 5/10 iter: 14/15 loss: 0.0409 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 5 Epoch 5 train Avg acc: 73.7778% F1: 0.5210 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8444 Acc: 56.2500% F1: 0.315 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 5/10 iter: 1/2 loss: 1.6011 Acc: 27.7778% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 5 train-dev Avg acc: 46.0000% F1: 0.3201 *
*********************************************************
Performing epoch 6 of 10
Fold 5 train - epoch: 6/10 iter: 0/15 loss: 0.7297 Acc: 75.0000% F1: 0.651 Time: 0.94s (0.00s)
Fold 5 train - epoch: 6/10 iter: 1/15 loss: 0.6239 Acc: 84.3750% F1: 0.589 Time: 0.93s (0.05s)
Fold 5 train - epoch: 6/10 iter: 2/15 loss: 0.4955 Acc: 84.3750% F1: 0.746 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 3/15 loss: 0.5776 Acc: 68.7500% F1: 0.481 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 4/15 loss: 0.5587 Acc: 78.1250% F1: 0.556 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 5/15 loss: 0.4165 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 6/15 loss: 0.3450 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 7/15 loss: 0.5834 Acc: 75.0000% F1: 0.503 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 8/15 loss: 0.7733 Acc: 65.6250% F1: 0.468 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 9/15 loss: 0.6435 Acc: 71.8750% F1: 0.513 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 10/15 loss: 0.5156 Acc: 90.6250% F1: 0.844 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 11/15 loss: 0.4988 Acc: 78.1250% F1: 0.675 Time: 0.93s (0.03s)
Fold 5 train - epoch: 6/10 iter: 12/15 loss: 0.4634 Acc: 84.3750% F1: 0.712 Time: 0.94s (0.02s)
Fold 5 train - epoch: 6/10 iter: 13/15 loss: 0.3858 Acc: 90.6250% F1: 0.781 Time: 0.93s (0.02s)
Fold 5 train - epoch: 6/10 iter: 14/15 loss: 0.0148 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 6 train Avg acc: 80.4444% F1: 0.6630 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9146 Acc: 56.2500% F1: 0.334 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 6/10 iter: 1/2 loss: 1.8212 Acc: 27.7778% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 6 train-dev Avg acc: 46.0000% F1: 0.3354 *
*********************************************************
Performing epoch 7 of 10
Fold 5 train - epoch: 7/10 iter: 0/15 loss: 0.4777 Acc: 81.2500% F1: 0.694 Time: 0.94s (0.00s)
Fold 5 train - epoch: 7/10 iter: 1/15 loss: 0.4385 Acc: 84.3750% F1: 0.593 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 2/15 loss: 0.2986 Acc: 93.7500% F1: 0.900 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 3/15 loss: 0.4551 Acc: 81.2500% F1: 0.578 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 4/15 loss: 0.3939 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 5/15 loss: 0.3246 Acc: 90.6250% F1: 0.835 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 6/15 loss: 0.2920 Acc: 90.6250% F1: 0.820 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 7/15 loss: 0.4266 Acc: 90.6250% F1: 0.614 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 8/15 loss: 0.5349 Acc: 78.1250% F1: 0.698 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 9/15 loss: 0.3524 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 10/15 loss: 0.4697 Acc: 81.2500% F1: 0.749 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 11/15 loss: 0.2969 Acc: 90.6250% F1: 0.874 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 12/15 loss: 0.3247 Acc: 93.7500% F1: 0.910 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 13/15 loss: 0.3302 Acc: 87.5000% F1: 0.611 Time: 0.93s (0.03s)
Fold 5 train - epoch: 7/10 iter: 14/15 loss: 0.0033 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 7 train Avg acc: 87.3333% F1: 0.7712 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 7/10 iter: 0/2 loss: 1.1894 Acc: 46.8750% F1: 0.297 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 7/10 iter: 1/2 loss: 1.9289 Acc: 27.7778% F1: 0.175 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 7 train-dev Avg acc: 40.0000% F1: 0.3081 *
*********************************************************
Performing epoch 8 of 10
Fold 5 train - epoch: 8/10 iter: 0/15 loss: 0.3590 Acc: 87.5000% F1: 0.735 Time: 0.95s (0.00s)
Fold 5 train - epoch: 8/10 iter: 1/15 loss: 0.3862 Acc: 84.3750% F1: 0.714 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 2/15 loss: 0.2018 Acc: 93.7500% F1: 0.954 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 3/15 loss: 0.3125 Acc: 84.3750% F1: 0.598 Time: 0.93s (0.04s)
Fold 5 train - epoch: 8/10 iter: 4/15 loss: 0.3456 Acc: 87.5000% F1: 0.799 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 5/15 loss: 0.2157 Acc: 93.7500% F1: 0.948 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 6/15 loss: 0.2620 Acc: 87.5000% F1: 0.784 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 7/15 loss: 0.2533 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 8/15 loss: 0.3194 Acc: 90.6250% F1: 0.876 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 9/15 loss: 0.4584 Acc: 78.1250% F1: 0.560 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 10/15 loss: 0.2916 Acc: 90.6250% F1: 0.844 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 11/15 loss: 0.2257 Acc: 90.6250% F1: 0.790 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 12/15 loss: 0.2069 Acc: 96.8750% F1: 0.943 Time: 0.94s (0.03s)
Fold 5 train - epoch: 8/10 iter: 13/15 loss: 0.2344 Acc: 93.7500% F1: 0.806 Time: 0.93s (0.03s)
Fold 5 train - epoch: 8/10 iter: 14/15 loss: 0.0063 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 5 Epoch 8 train Avg acc: 89.1111% F1: 0.8176 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 8/10 iter: 0/2 loss: 1.5035 Acc: 43.7500% F1: 0.286 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 8/10 iter: 1/2 loss: 2.0136 Acc: 27.7778% F1: 0.215 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 8 train-dev Avg acc: 38.0000% F1: 0.3220 *
*********************************************************
Performing epoch 9 of 10
Fold 5 train - epoch: 9/10 iter: 0/15 loss: 0.2529 Acc: 93.7500% F1: 0.948 Time: 0.94s (0.00s)
Fold 5 train - epoch: 9/10 iter: 1/15 loss: 0.4288 Acc: 84.3750% F1: 0.714 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 2/15 loss: 0.2205 Acc: 90.6250% F1: 0.894 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 3/15 loss: 0.4869 Acc: 81.2500% F1: 0.721 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 4/15 loss: 0.3265 Acc: 81.2500% F1: 0.577 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 5/15 loss: 0.2498 Acc: 93.7500% F1: 0.917 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 6/15 loss: 0.1742 Acc: 96.8750% F1: 0.973 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 7/15 loss: 0.2336 Acc: 90.6250% F1: 0.624 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 8/15 loss: 0.1519 Acc: 96.8750% F1: 0.955 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 9/15 loss: 0.3203 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 10/15 loss: 0.3837 Acc: 81.2500% F1: 0.749 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 11/15 loss: 0.4078 Acc: 81.2500% F1: 0.762 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 12/15 loss: 0.4234 Acc: 81.2500% F1: 0.794 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 13/15 loss: 0.5162 Acc: 81.2500% F1: 0.712 Time: 0.93s (0.03s)
Fold 5 train - epoch: 9/10 iter: 14/15 loss: 0.0012 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 9 train Avg acc: 87.5556% F1: 0.8319 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 9/10 iter: 0/2 loss: 1.2233 Acc: 53.1250% F1: 0.231 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 9/10 iter: 1/2 loss: 3.1419 Acc: 11.1111% F1: 0.122 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 9 train-dev Avg acc: 38.0000% F1: 0.2548 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/10 iter: 0/15 loss: 4.0672 Acc: 9.3750% F1: 0.062 Time: 0.96s (0.00s)
Fold 6 train - epoch: 0/10 iter: 1/15 loss: 2.9049 Acc: 25.0000% F1: 0.255 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/10 iter: 2/15 loss: 1.8471 Acc: 43.7500% F1: 0.398 Time: 0.93s (0.06s)
Fold 6 train - epoch: 0/10 iter: 3/15 loss: 1.4642 Acc: 31.2500% F1: 0.300 Time: 0.93s (0.04s)
Fold 6 train - epoch: 0/10 iter: 4/15 loss: 1.5041 Acc: 40.6250% F1: 0.281 Time: 0.93s (0.04s)
Fold 6 train - epoch: 0/10 iter: 5/15 loss: 1.3089 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 6 train - epoch: 0/10 iter: 6/15 loss: 0.9798 Acc: 68.7500% F1: 0.336 Time: 0.93s (0.04s)
Fold 6 train - epoch: 0/10 iter: 7/15 loss: 1.1401 Acc: 53.1250% F1: 0.317 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 8/15 loss: 1.4027 Acc: 43.7500% F1: 0.298 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 9/15 loss: 1.2478 Acc: 40.6250% F1: 0.289 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 10/15 loss: 1.1783 Acc: 40.6250% F1: 0.278 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 11/15 loss: 0.9981 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 12/15 loss: 1.1755 Acc: 46.8750% F1: 0.331 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 13/15 loss: 1.0578 Acc: 46.8750% F1: 0.328 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/10 iter: 14/15 loss: 0.5877 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 42.4444% F1: 0.3568 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6952 Acc: 59.3750% F1: 0.434 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 0/10 iter: 1/2 loss: 1.6348 Acc: 33.3333% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 50.0000% F1: 0.3381 *
*********************************************************
Performing epoch 1 of 10
Fold 6 train - epoch: 1/10 iter: 0/15 loss: 1.0008 Acc: 53.1250% F1: 0.362 Time: 0.94s (0.00s)
Fold 6 train - epoch: 1/10 iter: 1/15 loss: 0.9706 Acc: 56.2500% F1: 0.326 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 2/15 loss: 0.9297 Acc: 50.0000% F1: 0.291 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 3/15 loss: 0.9065 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.05s)
Fold 6 train - epoch: 1/10 iter: 4/15 loss: 1.0308 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 5/15 loss: 0.9720 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 6/15 loss: 0.7236 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 7/15 loss: 0.9930 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/10 iter: 8/15 loss: 1.0826 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.04s)
Fold 6 train - epoch: 1/10 iter: 9/15 loss: 1.0252 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.04s)
Fold 6 train - epoch: 1/10 iter: 10/15 loss: 0.9990 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.04s)
Fold 6 train - epoch: 1/10 iter: 11/15 loss: 0.7874 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.04s)
Fold 6 train - epoch: 1/10 iter: 12/15 loss: 0.9558 Acc: 53.1250% F1: 0.311 Time: 0.94s (0.03s)
Fold 6 train - epoch: 1/10 iter: 13/15 loss: 0.9390 Acc: 50.0000% F1: 0.326 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/10 iter: 14/15 loss: 0.3961 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 54.8889% F1: 0.2967 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7860 Acc: 53.1250% F1: 0.399 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3145 Acc: 38.8889% F1: 0.246 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.3321 *
*********************************************************
Performing epoch 2 of 10
Fold 6 train - epoch: 2/10 iter: 0/15 loss: 0.9025 Acc: 62.5000% F1: 0.427 Time: 0.95s (0.00s)
Fold 6 train - epoch: 2/10 iter: 1/15 loss: 0.9175 Acc: 50.0000% F1: 0.356 Time: 0.94s (0.03s)
Fold 6 train - epoch: 2/10 iter: 2/15 loss: 0.8867 Acc: 53.1250% F1: 0.371 Time: 0.93s (0.04s)
Fold 6 train - epoch: 2/10 iter: 3/15 loss: 0.9143 Acc: 46.8750% F1: 0.304 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 4/15 loss: 0.8627 Acc: 59.3750% F1: 0.419 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 5/15 loss: 0.9210 Acc: 62.5000% F1: 0.385 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 6/15 loss: 0.7660 Acc: 65.6250% F1: 0.359 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 7/15 loss: 0.8769 Acc: 62.5000% F1: 0.407 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 8/15 loss: 1.0709 Acc: 53.1250% F1: 0.241 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 9/15 loss: 0.9389 Acc: 46.8750% F1: 0.252 Time: 0.94s (0.03s)
Fold 6 train - epoch: 2/10 iter: 10/15 loss: 0.9973 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 11/15 loss: 0.7219 Acc: 68.7500% F1: 0.388 Time: 0.93s (0.04s)
Fold 6 train - epoch: 2/10 iter: 12/15 loss: 0.8514 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/10 iter: 13/15 loss: 0.8902 Acc: 59.3750% F1: 0.336 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/10 iter: 14/15 loss: 0.1644 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 56.8889% F1: 0.3609 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6375 Acc: 81.2500% F1: 0.571 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 2/10 iter: 1/2 loss: 1.5966 Acc: 22.2222% F1: 0.167 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 60.0000% F1: 0.3770 *
*********************************************************
Performing epoch 3 of 10
Fold 6 train - epoch: 3/10 iter: 0/15 loss: 0.8618 Acc: 62.5000% F1: 0.389 Time: 0.94s (0.00s)
Fold 6 train - epoch: 3/10 iter: 1/15 loss: 0.8666 Acc: 68.7500% F1: 0.444 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 2/15 loss: 0.8102 Acc: 65.6250% F1: 0.451 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 3/15 loss: 0.8658 Acc: 56.2500% F1: 0.338 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 4/15 loss: 0.8718 Acc: 68.7500% F1: 0.483 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 5/15 loss: 0.7896 Acc: 62.5000% F1: 0.405 Time: 0.93s (0.03s)
Fold 6 train - epoch: 3/10 iter: 6/15 loss: 0.7113 Acc: 81.2500% F1: 0.530 Time: 0.93s (0.04s)
Fold 6 train - epoch: 3/10 iter: 7/15 loss: 0.8384 Acc: 68.7500% F1: 0.465 Time: 0.94s (0.04s)
Fold 6 train - epoch: 3/10 iter: 8/15 loss: 1.0266 Acc: 56.2500% F1: 0.371 Time: 0.94s (0.04s)
Fold 6 train - epoch: 3/10 iter: 9/15 loss: 0.9177 Acc: 59.3750% F1: 0.413 Time: 0.94s (0.04s)
Fold 6 train - epoch: 3/10 iter: 10/15 loss: 0.9249 Acc: 46.8750% F1: 0.300 Time: 0.94s (0.04s)
Fold 6 train - epoch: 3/10 iter: 11/15 loss: 0.7251 Acc: 71.8750% F1: 0.460 Time: 0.94s (0.04s)
Fold 6 train - epoch: 3/10 iter: 12/15 loss: 0.8581 Acc: 65.6250% F1: 0.436 Time: 0.94s (0.04s)
Fold 6 train - epoch: 3/10 iter: 13/15 loss: 0.8201 Acc: 75.0000% F1: 0.508 Time: 0.94s (0.04s)
Fold 6 train - epoch: 3/10 iter: 14/15 loss: 0.1394 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.04s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 65.1111% F1: 0.4327 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/10 iter: 0/2 loss: 0.7444 Acc: 59.3750% F1: 0.434 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 3/10 iter: 1/2 loss: 1.4714 Acc: 33.3333% F1: 0.222 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3381 *
*********************************************************
Performing epoch 4 of 10
Fold 6 train - epoch: 4/10 iter: 0/15 loss: 0.8909 Acc: 59.3750% F1: 0.400 Time: 0.95s (0.00s)
Fold 6 train - epoch: 4/10 iter: 1/15 loss: 0.8088 Acc: 71.8750% F1: 0.483 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 2/15 loss: 0.7161 Acc: 75.0000% F1: 0.526 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 3/15 loss: 0.7752 Acc: 65.6250% F1: 0.424 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 4/15 loss: 0.8193 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 5/15 loss: 0.7676 Acc: 68.7500% F1: 0.444 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 6/15 loss: 0.6565 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 7/15 loss: 0.7768 Acc: 68.7500% F1: 0.462 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 8/15 loss: 0.9875 Acc: 56.2500% F1: 0.387 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 9/15 loss: 0.8097 Acc: 68.7500% F1: 0.491 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 10/15 loss: 0.7671 Acc: 65.6250% F1: 0.450 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 11/15 loss: 0.6130 Acc: 78.1250% F1: 0.522 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 12/15 loss: 0.7088 Acc: 68.7500% F1: 0.452 Time: 0.94s (0.03s)
Fold 6 train - epoch: 4/10 iter: 13/15 loss: 0.7661 Acc: 75.0000% F1: 0.513 Time: 0.93s (0.03s)
Fold 6 train - epoch: 4/10 iter: 14/15 loss: 0.0567 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 6 Epoch 4 train Avg acc: 68.6667% F1: 0.4647 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8712 Acc: 53.1250% F1: 0.399 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4283 Acc: 33.3333% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3141 *
*********************************************************
Performing epoch 5 of 10
Fold 6 train - epoch: 5/10 iter: 0/15 loss: 0.7917 Acc: 62.5000% F1: 0.427 Time: 0.95s (0.00s)
Fold 6 train - epoch: 5/10 iter: 1/15 loss: 0.6499 Acc: 81.2500% F1: 0.562 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 2/15 loss: 0.6499 Acc: 78.1250% F1: 0.546 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 3/15 loss: 0.6916 Acc: 68.7500% F1: 0.459 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 4/15 loss: 0.7342 Acc: 68.7500% F1: 0.486 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 5/15 loss: 0.5958 Acc: 78.1250% F1: 0.669 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 6/15 loss: 0.5069 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 7/15 loss: 0.6499 Acc: 71.8750% F1: 0.485 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 8/15 loss: 0.7503 Acc: 71.8750% F1: 0.582 Time: 0.94s (0.03s)
Fold 6 train - epoch: 5/10 iter: 9/15 loss: 0.7199 Acc: 68.7500% F1: 0.598 Time: 0.94s (0.03s)
Fold 6 train - epoch: 5/10 iter: 10/15 loss: 0.7409 Acc: 62.5000% F1: 0.439 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 11/15 loss: 0.5736 Acc: 75.0000% F1: 0.648 Time: 0.93s (0.03s)
Fold 6 train - epoch: 5/10 iter: 12/15 loss: 0.6123 Acc: 81.2500% F1: 0.565 Time: 0.94s (0.03s)
Fold 6 train - epoch: 5/10 iter: 13/15 loss: 0.6035 Acc: 81.2500% F1: 0.559 Time: 0.94s (0.03s)
Fold 6 train - epoch: 5/10 iter: 14/15 loss: 0.0304 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 6 Epoch 5 train Avg acc: 73.7778% F1: 0.5524 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 5/10 iter: 0/2 loss: 1.1500 Acc: 40.6250% F1: 0.219 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 5/10 iter: 1/2 loss: 1.3834 Acc: 38.8889% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 5 train-dev Avg acc: 40.0000% F1: 0.2839 *
*********************************************************
Performing epoch 6 of 10
Fold 6 train - epoch: 6/10 iter: 0/15 loss: 0.6483 Acc: 71.8750% F1: 0.510 Time: 0.94s (0.00s)
Fold 6 train - epoch: 6/10 iter: 1/15 loss: 0.6048 Acc: 81.2500% F1: 0.565 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 2/15 loss: 0.4940 Acc: 75.0000% F1: 0.524 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 3/15 loss: 0.5608 Acc: 78.1250% F1: 0.536 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 4/15 loss: 0.7177 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 5/15 loss: 0.4827 Acc: 84.3750% F1: 0.783 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 6/15 loss: 0.4552 Acc: 81.2500% F1: 0.530 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 7/15 loss: 0.5508 Acc: 75.0000% F1: 0.503 Time: 0.94s (0.03s)
Fold 6 train - epoch: 6/10 iter: 8/15 loss: 0.6734 Acc: 71.8750% F1: 0.533 Time: 0.93s (0.04s)
Fold 6 train - epoch: 6/10 iter: 9/15 loss: 0.6134 Acc: 71.8750% F1: 0.510 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 10/15 loss: 0.5024 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 11/15 loss: 0.3405 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 12/15 loss: 0.4699 Acc: 84.3750% F1: 0.703 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 13/15 loss: 0.4139 Acc: 87.5000% F1: 0.755 Time: 0.93s (0.03s)
Fold 6 train - epoch: 6/10 iter: 14/15 loss: 0.0204 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 6 train Avg acc: 79.7778% F1: 0.6324 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 6/10 iter: 0/2 loss: 1.4340 Acc: 37.5000% F1: 0.208 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 6/10 iter: 1/2 loss: 1.4127 Acc: 44.4444% F1: 0.242 Time: 0.19s (0.00s)
*********************************************************
* Fold 6 Epoch 6 train-dev Avg acc: 40.0000% F1: 0.2895 *
*********************************************************
Performing epoch 7 of 10
Fold 6 train - epoch: 7/10 iter: 0/15 loss: 0.5813 Acc: 75.0000% F1: 0.648 Time: 0.95s (0.00s)
Fold 6 train - epoch: 7/10 iter: 1/15 loss: 0.4399 Acc: 87.5000% F1: 0.735 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 2/15 loss: 0.3776 Acc: 90.6250% F1: 0.877 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 3/15 loss: 0.5635 Acc: 78.1250% F1: 0.545 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 4/15 loss: 0.5024 Acc: 75.0000% F1: 0.530 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 5/15 loss: 0.4005 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 6/15 loss: 0.3519 Acc: 87.5000% F1: 0.579 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 7/15 loss: 0.4809 Acc: 84.3750% F1: 0.580 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 8/15 loss: 0.4760 Acc: 81.2500% F1: 0.721 Time: 0.94s (0.03s)
Fold 6 train - epoch: 7/10 iter: 9/15 loss: 0.4488 Acc: 81.2500% F1: 0.701 Time: 0.94s (0.03s)
Fold 6 train - epoch: 7/10 iter: 10/15 loss: 0.4221 Acc: 87.5000% F1: 0.747 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 11/15 loss: 0.2550 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 12/15 loss: 0.3792 Acc: 87.5000% F1: 0.791 Time: 0.93s (0.03s)
Fold 6 train - epoch: 7/10 iter: 13/15 loss: 0.4209 Acc: 87.5000% F1: 0.763 Time: 0.94s (0.03s)
Fold 6 train - epoch: 7/10 iter: 14/15 loss: 0.0046 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 7 train Avg acc: 84.8889% F1: 0.7324 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 7/10 iter: 0/2 loss: 1.8372 Acc: 34.3750% F1: 0.194 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 7/10 iter: 1/2 loss: 1.4047 Acc: 44.4444% F1: 0.242 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 7 train-dev Avg acc: 38.0000% F1: 0.2756 *
*********************************************************
Performing epoch 8 of 10
Fold 6 train - epoch: 8/10 iter: 0/15 loss: 0.4621 Acc: 84.3750% F1: 0.714 Time: 0.95s (0.00s)
Fold 6 train - epoch: 8/10 iter: 1/15 loss: 0.3547 Acc: 87.5000% F1: 0.730 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 2/15 loss: 0.3012 Acc: 87.5000% F1: 0.820 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 3/15 loss: 0.4757 Acc: 75.0000% F1: 0.524 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 4/15 loss: 0.4248 Acc: 81.2500% F1: 0.775 Time: 0.94s (0.03s)
Fold 6 train - epoch: 8/10 iter: 5/15 loss: 0.2684 Acc: 87.5000% F1: 0.865 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 6/15 loss: 0.2181 Acc: 93.7500% F1: 0.652 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 7/15 loss: 0.1956 Acc: 96.8750% F1: 0.978 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 8/15 loss: 0.4924 Acc: 78.1250% F1: 0.675 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 9/15 loss: 0.3159 Acc: 84.3750% F1: 0.725 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 10/15 loss: 0.5652 Acc: 68.7500% F1: 0.594 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 11/15 loss: 0.3663 Acc: 87.5000% F1: 0.836 Time: 0.94s (0.03s)
Fold 6 train - epoch: 8/10 iter: 12/15 loss: 0.2833 Acc: 90.6250% F1: 0.846 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 13/15 loss: 0.4558 Acc: 78.1250% F1: 0.669 Time: 0.93s (0.03s)
Fold 6 train - epoch: 8/10 iter: 14/15 loss: 0.0016 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 8 train Avg acc: 84.4444% F1: 0.7527 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 8/10 iter: 0/2 loss: 1.8486 Acc: 34.3750% F1: 0.175 Time: 0.33s (0.00s)
Fold 6 train-dev - epoch: 8/10 iter: 1/2 loss: 1.9045 Acc: 38.8889% F1: 0.295 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 8 train-dev Avg acc: 36.0000% F1: 0.3057 *
*********************************************************
Performing epoch 9 of 10
Fold 6 train - epoch: 9/10 iter: 0/15 loss: 0.3187 Acc: 90.6250% F1: 0.892 Time: 0.95s (0.00s)
Fold 6 train - epoch: 9/10 iter: 1/15 loss: 0.2021 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 2/15 loss: 0.3414 Acc: 87.5000% F1: 0.854 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 3/15 loss: 0.4637 Acc: 78.1250% F1: 0.699 Time: 0.92s (0.03s)
Fold 6 train - epoch: 9/10 iter: 4/15 loss: 0.4671 Acc: 75.0000% F1: 0.690 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 5/15 loss: 0.5819 Acc: 78.1250% F1: 0.786 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 6/15 loss: 0.5666 Acc: 75.0000% F1: 0.516 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 7/15 loss: 0.5238 Acc: 75.0000% F1: 0.499 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 8/15 loss: 0.3723 Acc: 84.3750% F1: 0.777 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 9/15 loss: 0.2199 Acc: 93.7500% F1: 0.952 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 10/15 loss: 0.1900 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 11/15 loss: 0.3397 Acc: 84.3750% F1: 0.801 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 12/15 loss: 0.6013 Acc: 75.0000% F1: 0.685 Time: 0.93s (0.03s)
Fold 6 train - epoch: 9/10 iter: 13/15 loss: 0.5320 Acc: 78.1250% F1: 0.681 Time: 0.94s (0.03s)
Fold 6 train - epoch: 9/10 iter: 14/15 loss: 0.0008 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 6 Epoch 9 train Avg acc: 83.1111% F1: 0.7907 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 9/10 iter: 0/2 loss: 1.0039 Acc: 75.0000% F1: 0.291 Time: 0.35s (0.00s)
Fold 6 train-dev - epoch: 9/10 iter: 1/2 loss: 3.9811 Acc: 22.2222% F1: 0.167 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 9 train-dev Avg acc: 56.0000% F1: 0.3531 *
*********************************************************
Creating 1 distributed models for fold 7...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train - epoch: 0/10 iter: 0/15 loss: 4.0175 Acc: 9.3750% F1: 0.059 Time: 0.97s (0.00s)
Fold 7 train - epoch: 0/10 iter: 1/15 loss: 3.0767 Acc: 28.1250% F1: 0.289 Time: 0.92s (0.03s)
Fold 7 train - epoch: 0/10 iter: 2/15 loss: 1.7906 Acc: 40.6250% F1: 0.394 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 3/15 loss: 1.2658 Acc: 40.6250% F1: 0.275 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 4/15 loss: 1.6077 Acc: 46.8750% F1: 0.313 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 5/15 loss: 1.3485 Acc: 50.0000% F1: 0.232 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 6/15 loss: 1.0562 Acc: 68.7500% F1: 0.336 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 7/15 loss: 1.1291 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 8/15 loss: 1.5422 Acc: 28.1250% F1: 0.183 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 9/15 loss: 1.3474 Acc: 28.1250% F1: 0.196 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 10/15 loss: 1.2448 Acc: 40.6250% F1: 0.249 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 11/15 loss: 0.9902 Acc: 40.6250% F1: 0.284 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 12/15 loss: 1.1016 Acc: 40.6250% F1: 0.278 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 13/15 loss: 0.9787 Acc: 53.1250% F1: 0.355 Time: 0.93s (0.02s)
Fold 7 train - epoch: 0/10 iter: 14/15 loss: 0.6007 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 7 Epoch 0 train Avg acc: 40.8889% F1: 0.3415 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 7 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5570 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 0/10 iter: 1/2 loss: 1.8613 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.3119 *
*********************************************************
Performing epoch 1 of 10
Fold 7 train - epoch: 1/10 iter: 0/15 loss: 1.0191 Acc: 50.0000% F1: 0.222 Time: 0.94s (0.00s)
Fold 7 train - epoch: 1/10 iter: 1/15 loss: 0.9580 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 2/15 loss: 1.0435 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 3/15 loss: 0.9543 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 4/15 loss: 1.0120 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 5/15 loss: 0.9625 Acc: 59.3750% F1: 0.248 Time: 0.94s (0.03s)
Fold 7 train - epoch: 1/10 iter: 6/15 loss: 0.7482 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 7 train - epoch: 1/10 iter: 7/15 loss: 0.9638 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 8/15 loss: 1.0598 Acc: 56.2500% F1: 0.308 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 9/15 loss: 1.1276 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 10/15 loss: 1.0358 Acc: 46.8750% F1: 0.280 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 11/15 loss: 0.7835 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 12/15 loss: 0.9658 Acc: 56.2500% F1: 0.399 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 13/15 loss: 0.9495 Acc: 50.0000% F1: 0.311 Time: 0.93s (0.02s)
Fold 7 train - epoch: 1/10 iter: 14/15 loss: 0.6320 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 1 train Avg acc: 55.1111% F1: 0.3002 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7470 Acc: 56.2500% F1: 0.360 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3740 Acc: 33.3333% F1: 0.211 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.3179 *
*********************************************************
Performing epoch 2 of 10
Fold 7 train - epoch: 2/10 iter: 0/15 loss: 0.9727 Acc: 53.1250% F1: 0.370 Time: 0.94s (0.00s)
Fold 7 train - epoch: 2/10 iter: 1/15 loss: 0.8990 Acc: 65.6250% F1: 0.445 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 2/15 loss: 0.8493 Acc: 71.8750% F1: 0.503 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 3/15 loss: 0.9171 Acc: 46.8750% F1: 0.247 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 4/15 loss: 0.9219 Acc: 53.1250% F1: 0.278 Time: 0.93s (0.03s)
Fold 7 train - epoch: 2/10 iter: 5/15 loss: 0.8516 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 6/15 loss: 0.7613 Acc: 65.6250% F1: 0.264 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 7/15 loss: 0.8677 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 8/15 loss: 1.0092 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 9/15 loss: 0.9893 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 10/15 loss: 1.0516 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 11/15 loss: 0.7421 Acc: 65.6250% F1: 0.264 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 12/15 loss: 0.9030 Acc: 59.3750% F1: 0.306 Time: 0.94s (0.02s)
Fold 7 train - epoch: 2/10 iter: 13/15 loss: 0.8922 Acc: 59.3750% F1: 0.301 Time: 0.93s (0.02s)
Fold 7 train - epoch: 2/10 iter: 14/15 loss: 0.1767 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 7 Epoch 2 train Avg acc: 57.7778% F1: 0.3385 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6381 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 2/10 iter: 1/2 loss: 1.6111 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.2509 *
*********************************************************
Performing epoch 3 of 10
Fold 7 train - epoch: 3/10 iter: 0/15 loss: 0.9143 Acc: 56.2500% F1: 0.326 Time: 0.94s (0.00s)
Fold 7 train - epoch: 3/10 iter: 1/15 loss: 0.8841 Acc: 71.8750% F1: 0.471 Time: 0.92s (0.02s)
Fold 7 train - epoch: 3/10 iter: 2/15 loss: 0.8477 Acc: 53.1250% F1: 0.345 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 3/15 loss: 0.8505 Acc: 53.1250% F1: 0.322 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 4/15 loss: 0.8830 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 5/15 loss: 0.8132 Acc: 78.1250% F1: 0.554 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 6/15 loss: 0.7512 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 7/15 loss: 0.8128 Acc: 68.7500% F1: 0.459 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 8/15 loss: 1.0127 Acc: 62.5000% F1: 0.439 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 9/15 loss: 0.9485 Acc: 53.1250% F1: 0.335 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 10/15 loss: 0.9290 Acc: 59.3750% F1: 0.400 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 11/15 loss: 0.6721 Acc: 78.1250% F1: 0.508 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 12/15 loss: 0.8160 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 13/15 loss: 0.8318 Acc: 65.6250% F1: 0.398 Time: 0.93s (0.02s)
Fold 7 train - epoch: 3/10 iter: 14/15 loss: 0.1393 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 3 train Avg acc: 63.7778% F1: 0.4091 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6590 Acc: 75.0000% F1: 0.429 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 3/10 iter: 1/2 loss: 1.6734 Acc: 11.1111% F1: 0.089 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 3 train-dev Avg acc: 52.0000% F1: 0.2833 *
*********************************************************
Performing epoch 4 of 10
Fold 7 train - epoch: 4/10 iter: 0/15 loss: 0.8678 Acc: 59.3750% F1: 0.386 Time: 0.95s (0.00s)
Fold 7 train - epoch: 4/10 iter: 1/15 loss: 0.8296 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 2/15 loss: 0.7438 Acc: 68.7500% F1: 0.469 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 3/15 loss: 0.7267 Acc: 68.7500% F1: 0.443 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 4/15 loss: 0.8260 Acc: 65.6250% F1: 0.458 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 5/15 loss: 0.7020 Acc: 75.0000% F1: 0.504 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 6/15 loss: 0.6617 Acc: 75.0000% F1: 0.440 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 7/15 loss: 0.7840 Acc: 71.8750% F1: 0.479 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 8/15 loss: 0.8610 Acc: 65.6250% F1: 0.454 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 9/15 loss: 0.8658 Acc: 62.5000% F1: 0.433 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 10/15 loss: 0.8703 Acc: 59.3750% F1: 0.413 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 11/15 loss: 0.6115 Acc: 71.8750% F1: 0.453 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 12/15 loss: 0.7679 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 13/15 loss: 0.7148 Acc: 75.0000% F1: 0.506 Time: 0.93s (0.02s)
Fold 7 train - epoch: 4/10 iter: 14/15 loss: 0.1006 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 4 train Avg acc: 68.8889% F1: 0.4618 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 4/10 iter: 0/2 loss: 0.8113 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 4/10 iter: 1/2 loss: 1.6208 Acc: 27.7778% F1: 0.185 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 4 train-dev Avg acc: 48.0000% F1: 0.3086 *
*********************************************************
Performing epoch 5 of 10
Fold 7 train - epoch: 5/10 iter: 0/15 loss: 0.7839 Acc: 62.5000% F1: 0.427 Time: 0.94s (0.00s)
Fold 7 train - epoch: 5/10 iter: 1/15 loss: 0.7177 Acc: 71.8750% F1: 0.493 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 2/15 loss: 0.5885 Acc: 75.0000% F1: 0.526 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 3/15 loss: 0.6128 Acc: 65.6250% F1: 0.433 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 4/15 loss: 0.7521 Acc: 68.7500% F1: 0.471 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 5/15 loss: 0.6238 Acc: 71.8750% F1: 0.489 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 6/15 loss: 0.5898 Acc: 84.3750% F1: 0.556 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 7/15 loss: 0.5639 Acc: 84.3750% F1: 0.571 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 8/15 loss: 0.7801 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.03s)
Fold 7 train - epoch: 5/10 iter: 9/15 loss: 0.7200 Acc: 71.8750% F1: 0.506 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 10/15 loss: 0.7692 Acc: 59.3750% F1: 0.421 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 11/15 loss: 0.5549 Acc: 84.3750% F1: 0.728 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 12/15 loss: 0.6723 Acc: 68.7500% F1: 0.489 Time: 0.94s (0.02s)
Fold 7 train - epoch: 5/10 iter: 13/15 loss: 0.5825 Acc: 75.0000% F1: 0.506 Time: 0.93s (0.02s)
Fold 7 train - epoch: 5/10 iter: 14/15 loss: 0.0351 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 5 train Avg acc: 72.0000% F1: 0.5072 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 5/10 iter: 0/2 loss: 0.9414 Acc: 50.0000% F1: 0.333 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 5/10 iter: 1/2 loss: 1.7997 Acc: 27.7778% F1: 0.175 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 5 train-dev Avg acc: 42.0000% F1: 0.2760 *
*********************************************************
Performing epoch 6 of 10
Fold 7 train - epoch: 6/10 iter: 0/15 loss: 0.5856 Acc: 78.1250% F1: 0.550 Time: 0.93s (0.00s)
Fold 7 train - epoch: 6/10 iter: 1/15 loss: 0.6258 Acc: 78.1250% F1: 0.541 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 2/15 loss: 0.4032 Acc: 84.3750% F1: 0.592 Time: 0.92s (0.02s)
Fold 7 train - epoch: 6/10 iter: 3/15 loss: 0.5855 Acc: 71.8750% F1: 0.492 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 4/15 loss: 0.7158 Acc: 65.6250% F1: 0.455 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 5/15 loss: 0.5097 Acc: 84.3750% F1: 0.720 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 6/15 loss: 0.4073 Acc: 81.2500% F1: 0.520 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 7/15 loss: 0.4018 Acc: 87.5000% F1: 0.593 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 8/15 loss: 0.6288 Acc: 75.0000% F1: 0.617 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 9/15 loss: 0.5679 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 10/15 loss: 0.5546 Acc: 81.2500% F1: 0.773 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 11/15 loss: 0.4731 Acc: 81.2500% F1: 0.705 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 12/15 loss: 0.5641 Acc: 84.3750% F1: 0.788 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 13/15 loss: 0.4630 Acc: 84.3750% F1: 0.580 Time: 0.93s (0.02s)
Fold 7 train - epoch: 6/10 iter: 14/15 loss: 0.0194 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 7 Epoch 6 train Avg acc: 80.0000% F1: 0.6350 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 6/10 iter: 0/2 loss: 1.1293 Acc: 46.8750% F1: 0.213 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 6/10 iter: 1/2 loss: 2.0921 Acc: 27.7778% F1: 0.175 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 6 train-dev Avg acc: 40.0000% F1: 0.2766 *
*********************************************************
Performing epoch 7 of 10
Fold 7 train - epoch: 7/10 iter: 0/15 loss: 0.4369 Acc: 81.2500% F1: 0.582 Time: 0.95s (0.00s)
Fold 7 train - epoch: 7/10 iter: 1/15 loss: 0.4951 Acc: 78.1250% F1: 0.541 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 2/15 loss: 0.2726 Acc: 90.6250% F1: 0.894 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 3/15 loss: 0.3887 Acc: 87.5000% F1: 0.763 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 4/15 loss: 0.3996 Acc: 84.3750% F1: 0.598 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 5/15 loss: 0.4015 Acc: 84.3750% F1: 0.718 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 6/15 loss: 0.3671 Acc: 87.5000% F1: 0.579 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 7/15 loss: 0.3697 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 8/15 loss: 0.4409 Acc: 78.1250% F1: 0.635 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 9/15 loss: 0.4219 Acc: 81.2500% F1: 0.579 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 10/15 loss: 0.3290 Acc: 90.6250% F1: 0.842 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 11/15 loss: 0.3709 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.03s)
Fold 7 train - epoch: 7/10 iter: 12/15 loss: 0.3425 Acc: 87.5000% F1: 0.794 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 13/15 loss: 0.2829 Acc: 90.6250% F1: 0.792 Time: 0.93s (0.02s)
Fold 7 train - epoch: 7/10 iter: 14/15 loss: 0.0071 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 7 train Avg acc: 86.2222% F1: 0.7352 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 7/10 iter: 0/2 loss: 1.4810 Acc: 43.7500% F1: 0.212 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 7/10 iter: 1/2 loss: 2.1480 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 7 train-dev Avg acc: 40.0000% F1: 0.2937 *
*********************************************************
Performing epoch 8 of 10
Fold 7 train - epoch: 8/10 iter: 0/15 loss: 0.3915 Acc: 81.2500% F1: 0.744 Time: 0.94s (0.00s)
Fold 7 train - epoch: 8/10 iter: 1/15 loss: 0.3940 Acc: 78.1250% F1: 0.747 Time: 0.92s (0.02s)
Fold 7 train - epoch: 8/10 iter: 2/15 loss: 0.1530 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 3/15 loss: 0.2388 Acc: 93.7500% F1: 0.900 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 4/15 loss: 0.3145 Acc: 87.5000% F1: 0.740 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 5/15 loss: 0.2825 Acc: 93.7500% F1: 0.916 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 6/15 loss: 0.2779 Acc: 93.7500% F1: 0.652 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 7/15 loss: 0.2482 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 8/15 loss: 0.2757 Acc: 90.6250% F1: 0.863 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 9/15 loss: 0.2029 Acc: 93.7500% F1: 0.866 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 10/15 loss: 0.2305 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 11/15 loss: 0.2028 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 12/15 loss: 0.2368 Acc: 93.7500% F1: 0.891 Time: 0.94s (0.02s)
Fold 7 train - epoch: 8/10 iter: 13/15 loss: 0.3096 Acc: 87.5000% F1: 0.760 Time: 0.93s (0.02s)
Fold 7 train - epoch: 8/10 iter: 14/15 loss: 0.0045 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 8 train Avg acc: 90.8889% F1: 0.8530 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 8/10 iter: 0/2 loss: 1.6541 Acc: 43.7500% F1: 0.212 Time: 0.33s (0.00s)
Fold 7 train-dev - epoch: 8/10 iter: 1/2 loss: 2.7488 Acc: 33.3333% F1: 0.200 Time: 0.19s (0.00s)
*********************************************************
* Fold 7 Epoch 8 train-dev Avg acc: 40.0000% F1: 0.3011 *
*********************************************************
Performing epoch 9 of 10
Fold 7 train - epoch: 9/10 iter: 0/15 loss: 0.1919 Acc: 93.7500% F1: 0.871 Time: 0.95s (0.00s)
Fold 7 train - epoch: 9/10 iter: 1/15 loss: 0.1843 Acc: 96.8750% F1: 0.975 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 2/15 loss: 0.2599 Acc: 87.5000% F1: 0.908 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 3/15 loss: 0.1865 Acc: 90.6250% F1: 0.787 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 4/15 loss: 0.2304 Acc: 93.7500% F1: 0.863 Time: 0.92s (0.02s)
Fold 7 train - epoch: 9/10 iter: 5/15 loss: 0.2363 Acc: 90.6250% F1: 0.766 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 6/15 loss: 0.2282 Acc: 90.6250% F1: 0.623 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 7/15 loss: 0.1478 Acc: 93.7500% F1: 0.645 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 8/15 loss: 0.2795 Acc: 84.3750% F1: 0.819 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 9/15 loss: 0.2412 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 10/15 loss: 0.2836 Acc: 87.5000% F1: 0.843 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 11/15 loss: 0.0780 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 12/15 loss: 0.1143 Acc: 93.7500% F1: 0.859 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 13/15 loss: 0.2207 Acc: 93.7500% F1: 0.896 Time: 0.93s (0.02s)
Fold 7 train - epoch: 9/10 iter: 14/15 loss: 0.0016 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 7 Epoch 9 train Avg acc: 92.2222% F1: 0.8699 *
*****************************************************
Saving model...
Fold 7 train-dev - epoch: 9/10 iter: 0/2 loss: 2.2517 Acc: 34.3750% F1: 0.215 Time: 0.34s (0.00s)
Fold 7 train-dev - epoch: 9/10 iter: 1/2 loss: 2.1883 Acc: 38.8889% F1: 0.283 Time: 0.18s (0.00s)
*********************************************************
* Fold 7 Epoch 9 train-dev Avg acc: 36.0000% F1: 0.3122 *
*********************************************************
Creating 1 distributed models for fold 8...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train - epoch: 0/10 iter: 0/15 loss: 3.9475 Acc: 12.5000% F1: 0.076 Time: 0.96s (0.00s)
Fold 8 train - epoch: 0/10 iter: 1/15 loss: 2.9211 Acc: 21.8750% F1: 0.221 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 2/15 loss: 1.7443 Acc: 40.6250% F1: 0.389 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 3/15 loss: 1.3202 Acc: 40.6250% F1: 0.369 Time: 0.93s (0.03s)
Fold 8 train - epoch: 0/10 iter: 4/15 loss: 1.4877 Acc: 46.8750% F1: 0.290 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 5/15 loss: 1.3036 Acc: 53.1250% F1: 0.280 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 6/15 loss: 0.9296 Acc: 59.3750% F1: 0.330 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 7/15 loss: 1.0073 Acc: 56.2500% F1: 0.377 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 8/15 loss: 1.3469 Acc: 34.3750% F1: 0.243 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 9/15 loss: 1.2599 Acc: 31.2500% F1: 0.215 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 10/15 loss: 1.2750 Acc: 37.5000% F1: 0.249 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 11/15 loss: 0.9601 Acc: 50.0000% F1: 0.344 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 12/15 loss: 1.0176 Acc: 62.5000% F1: 0.446 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 13/15 loss: 1.0388 Acc: 50.0000% F1: 0.312 Time: 0.93s (0.02s)
Fold 8 train - epoch: 0/10 iter: 14/15 loss: 0.6496 Acc: 50.0000% F1: 0.333 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 0 train Avg acc: 42.6667% F1: 0.3615 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 8 train-dev - epoch: 0/10 iter: 0/2 loss: 0.5952 Acc: 81.2500% F1: 0.448 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 0/10 iter: 1/2 loss: 1.8034 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 0 train-dev Avg acc: 58.0000% F1: 0.3316 *
*********************************************************
Performing epoch 1 of 10
Fold 8 train - epoch: 1/10 iter: 0/15 loss: 1.0167 Acc: 50.0000% F1: 0.222 Time: 0.95s (0.00s)
Fold 8 train - epoch: 1/10 iter: 1/15 loss: 0.9567 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 2/15 loss: 0.9558 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 3/15 loss: 0.9331 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 4/15 loss: 1.0843 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 5/15 loss: 0.8864 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 6/15 loss: 0.7600 Acc: 68.7500% F1: 0.336 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 7/15 loss: 0.9151 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 8/15 loss: 1.1455 Acc: 53.1250% F1: 0.241 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 9/15 loss: 1.0186 Acc: 37.5000% F1: 0.182 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 10/15 loss: 0.9860 Acc: 37.5000% F1: 0.214 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 11/15 loss: 0.7981 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 12/15 loss: 0.8936 Acc: 62.5000% F1: 0.415 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 13/15 loss: 0.9551 Acc: 56.2500% F1: 0.361 Time: 0.93s (0.02s)
Fold 8 train - epoch: 1/10 iter: 14/15 loss: 0.6402 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 1 train Avg acc: 54.2222% F1: 0.2840 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7328 Acc: 65.6250% F1: 0.521 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 1/10 iter: 1/2 loss: 1.3544 Acc: 27.7778% F1: 0.175 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3484 *
*********************************************************
Performing epoch 2 of 10
Fold 8 train - epoch: 2/10 iter: 0/15 loss: 0.9468 Acc: 56.2500% F1: 0.390 Time: 0.95s (0.00s)
Fold 8 train - epoch: 2/10 iter: 1/15 loss: 0.9276 Acc: 62.5000% F1: 0.417 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 2/15 loss: 0.8875 Acc: 65.6250% F1: 0.449 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 3/15 loss: 0.8547 Acc: 65.6250% F1: 0.361 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 4/15 loss: 0.9296 Acc: 56.2500% F1: 0.327 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 5/15 loss: 0.8692 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 6/15 loss: 0.7695 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.03s)
Fold 8 train - epoch: 2/10 iter: 7/15 loss: 0.9052 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 8/15 loss: 1.0757 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 9/15 loss: 1.0417 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 10/15 loss: 1.0092 Acc: 37.5000% F1: 0.182 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 11/15 loss: 0.7905 Acc: 62.5000% F1: 0.256 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 12/15 loss: 0.9094 Acc: 53.1250% F1: 0.231 Time: 0.94s (0.03s)
Fold 8 train - epoch: 2/10 iter: 13/15 loss: 0.8705 Acc: 62.5000% F1: 0.351 Time: 0.93s (0.02s)
Fold 8 train - epoch: 2/10 iter: 14/15 loss: 0.2360 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 2 train Avg acc: 57.3333% F1: 0.3242 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 2/10 iter: 0/2 loss: 0.6612 Acc: 81.2500% F1: 0.571 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 2/10 iter: 1/2 loss: 1.4347 Acc: 27.7778% F1: 0.185 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 2 train-dev Avg acc: 62.0000% F1: 0.3989 *
*********************************************************
Performing epoch 3 of 10
Fold 8 train - epoch: 3/10 iter: 0/15 loss: 0.9201 Acc: 62.5000% F1: 0.419 Time: 0.95s (0.00s)
Fold 8 train - epoch: 3/10 iter: 1/15 loss: 0.8934 Acc: 56.2500% F1: 0.389 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 2/15 loss: 0.8391 Acc: 53.1250% F1: 0.370 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 3/15 loss: 0.8272 Acc: 53.1250% F1: 0.338 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 4/15 loss: 0.8983 Acc: 59.3750% F1: 0.410 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 5/15 loss: 0.7693 Acc: 75.0000% F1: 0.529 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 6/15 loss: 0.7347 Acc: 71.8750% F1: 0.396 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 7/15 loss: 0.7933 Acc: 62.5000% F1: 0.397 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 8/15 loss: 1.0003 Acc: 59.3750% F1: 0.368 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 9/15 loss: 0.9430 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 10/15 loss: 0.9058 Acc: 53.1250% F1: 0.350 Time: 0.93s (0.02s)
Fold 8 train - epoch: 3/10 iter: 11/15 loss: 0.7430 Acc: 71.8750% F1: 0.439 Time: 0.94s (0.02s)
Fold 8 train - epoch: 3/10 iter: 12/15 loss: 0.8338 Acc: 68.7500% F1: 0.436 Time: 0.94s (0.02s)
Fold 8 train - epoch: 3/10 iter: 13/15 loss: 0.8202 Acc: 56.2500% F1: 0.322 Time: 0.93s (0.03s)
Fold 8 train - epoch: 3/10 iter: 14/15 loss: 0.1163 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 3 train Avg acc: 61.3333% F1: 0.3968 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6426 Acc: 78.1250% F1: 0.547 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 3/10 iter: 1/2 loss: 1.4408 Acc: 27.7778% F1: 0.185 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 3 train-dev Avg acc: 60.0000% F1: 0.3870 *
*********************************************************
Performing epoch 4 of 10
Fold 8 train - epoch: 4/10 iter: 0/15 loss: 0.8160 Acc: 78.1250% F1: 0.557 Time: 0.95s (0.00s)
Fold 8 train - epoch: 4/10 iter: 1/15 loss: 0.8611 Acc: 68.7500% F1: 0.467 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 2/15 loss: 0.7915 Acc: 65.6250% F1: 0.459 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 3/15 loss: 0.7388 Acc: 81.2500% F1: 0.540 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 4/15 loss: 0.8660 Acc: 56.2500% F1: 0.383 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 5/15 loss: 0.7246 Acc: 75.0000% F1: 0.514 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 6/15 loss: 0.5938 Acc: 81.2500% F1: 0.516 Time: 0.93s (0.03s)
Fold 8 train - epoch: 4/10 iter: 7/15 loss: 0.7822 Acc: 56.2500% F1: 0.349 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 8/15 loss: 0.9641 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 9/15 loss: 0.8737 Acc: 59.3750% F1: 0.389 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 10/15 loss: 0.8732 Acc: 53.1250% F1: 0.364 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 11/15 loss: 0.6497 Acc: 71.8750% F1: 0.486 Time: 0.94s (0.02s)
Fold 8 train - epoch: 4/10 iter: 12/15 loss: 0.8170 Acc: 75.0000% F1: 0.520 Time: 0.94s (0.04s)
Fold 8 train - epoch: 4/10 iter: 13/15 loss: 0.7853 Acc: 71.8750% F1: 0.474 Time: 0.93s (0.02s)
Fold 8 train - epoch: 4/10 iter: 14/15 loss: 0.0897 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 4 train Avg acc: 68.4444% F1: 0.4614 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 4/10 iter: 0/2 loss: 0.6838 Acc: 81.2500% F1: 0.692 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 4/10 iter: 1/2 loss: 1.3397 Acc: 38.8889% F1: 0.222 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 4 train-dev Avg acc: 66.0000% F1: 0.4534 *
*********************************************************
Performing epoch 5 of 10
Fold 8 train - epoch: 5/10 iter: 0/15 loss: 0.7224 Acc: 71.8750% F1: 0.506 Time: 0.94s (0.00s)
Fold 8 train - epoch: 5/10 iter: 1/15 loss: 0.7878 Acc: 71.8750% F1: 0.487 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 2/15 loss: 0.6796 Acc: 71.8750% F1: 0.504 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 3/15 loss: 0.6953 Acc: 68.7500% F1: 0.454 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 4/15 loss: 0.7319 Acc: 68.7500% F1: 0.476 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 5/15 loss: 0.6573 Acc: 75.0000% F1: 0.504 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 6/15 loss: 0.5616 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 7/15 loss: 0.6415 Acc: 81.2500% F1: 0.550 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 8/15 loss: 0.7830 Acc: 68.7500% F1: 0.492 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 9/15 loss: 0.8170 Acc: 65.6250% F1: 0.458 Time: 0.93s (0.03s)
Fold 8 train - epoch: 5/10 iter: 10/15 loss: 0.7221 Acc: 75.0000% F1: 0.533 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 11/15 loss: 0.5366 Acc: 87.5000% F1: 0.617 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 12/15 loss: 0.6401 Acc: 81.2500% F1: 0.570 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 13/15 loss: 0.6383 Acc: 78.1250% F1: 0.530 Time: 0.93s (0.02s)
Fold 8 train - epoch: 5/10 iter: 14/15 loss: 0.0233 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 8 Epoch 5 train Avg acc: 74.6667% F1: 0.5159 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 5/10 iter: 0/2 loss: 0.7263 Acc: 65.6250% F1: 0.380 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 5/10 iter: 1/2 loss: 1.2894 Acc: 44.4444% F1: 0.329 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 5 train-dev Avg acc: 58.0000% F1: 0.4828 *
*********************************************************
Performing epoch 6 of 10
Fold 8 train - epoch: 6/10 iter: 0/15 loss: 0.6469 Acc: 81.2500% F1: 0.576 Time: 0.94s (0.00s)
Fold 8 train - epoch: 6/10 iter: 1/15 loss: 0.5536 Acc: 78.1250% F1: 0.545 Time: 0.93s (0.03s)
Fold 8 train - epoch: 6/10 iter: 2/15 loss: 0.5380 Acc: 81.2500% F1: 0.571 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 3/15 loss: 0.5733 Acc: 71.8750% F1: 0.474 Time: 0.93s (0.03s)
Fold 8 train - epoch: 6/10 iter: 4/15 loss: 0.6368 Acc: 71.8750% F1: 0.500 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 5/15 loss: 0.5157 Acc: 84.3750% F1: 0.593 Time: 0.93s (0.03s)
Fold 8 train - epoch: 6/10 iter: 6/15 loss: 0.4456 Acc: 81.2500% F1: 0.504 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 7/15 loss: 0.4653 Acc: 90.6250% F1: 0.935 Time: 0.94s (0.02s)
Fold 8 train - epoch: 6/10 iter: 8/15 loss: 0.6679 Acc: 71.8750% F1: 0.592 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 9/15 loss: 0.6050 Acc: 78.1250% F1: 0.557 Time: 0.93s (0.04s)
Fold 8 train - epoch: 6/10 iter: 10/15 loss: 0.5610 Acc: 84.3750% F1: 0.796 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 11/15 loss: 0.4147 Acc: 87.5000% F1: 0.761 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 12/15 loss: 0.5446 Acc: 81.2500% F1: 0.580 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 13/15 loss: 0.5058 Acc: 84.3750% F1: 0.734 Time: 0.93s (0.02s)
Fold 8 train - epoch: 6/10 iter: 14/15 loss: 0.0375 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 8 Epoch 6 train Avg acc: 80.6667% F1: 0.6294 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 6/10 iter: 0/2 loss: 0.8117 Acc: 71.8750% F1: 0.436 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 6/10 iter: 1/2 loss: 1.3275 Acc: 50.0000% F1: 0.370 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 6 train-dev Avg acc: 64.0000% F1: 0.5705 *
*********************************************************
Performing epoch 7 of 10
Fold 8 train - epoch: 7/10 iter: 0/15 loss: 0.5049 Acc: 81.2500% F1: 0.671 Time: 0.94s (0.00s)
Fold 8 train - epoch: 7/10 iter: 1/15 loss: 0.4679 Acc: 78.1250% F1: 0.545 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 2/15 loss: 0.4538 Acc: 78.1250% F1: 0.549 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 3/15 loss: 0.4057 Acc: 78.1250% F1: 0.553 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 4/15 loss: 0.3863 Acc: 84.3750% F1: 0.602 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 5/15 loss: 0.4309 Acc: 78.1250% F1: 0.548 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 6/15 loss: 0.2984 Acc: 90.6250% F1: 0.623 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 7/15 loss: 0.3173 Acc: 93.7500% F1: 0.957 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 8/15 loss: 0.4085 Acc: 87.5000% F1: 0.805 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 9/15 loss: 0.4293 Acc: 87.5000% F1: 0.744 Time: 0.93s (0.03s)
Fold 8 train - epoch: 7/10 iter: 10/15 loss: 0.4077 Acc: 87.5000% F1: 0.799 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 11/15 loss: 0.2704 Acc: 87.5000% F1: 0.761 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 12/15 loss: 0.4255 Acc: 87.5000% F1: 0.730 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 13/15 loss: 0.3036 Acc: 93.7500% F1: 0.810 Time: 0.93s (0.02s)
Fold 8 train - epoch: 7/10 iter: 14/15 loss: 0.0072 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 7 train Avg acc: 85.3333% F1: 0.7064 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 7/10 iter: 0/2 loss: 0.8910 Acc: 78.1250% F1: 0.476 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 7/10 iter: 1/2 loss: 1.4546 Acc: 50.0000% F1: 0.370 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 7 train-dev Avg acc: 68.0000% F1: 0.5974 *
*********************************************************
Performing epoch 8 of 10
Fold 8 train - epoch: 8/10 iter: 0/15 loss: 0.3491 Acc: 87.5000% F1: 0.729 Time: 0.94s (0.00s)
Fold 8 train - epoch: 8/10 iter: 1/15 loss: 0.2974 Acc: 90.6250% F1: 0.761 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 2/15 loss: 0.2367 Acc: 93.7500% F1: 0.900 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 3/15 loss: 0.2249 Acc: 93.7500% F1: 0.857 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 4/15 loss: 0.3312 Acc: 87.5000% F1: 0.742 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 5/15 loss: 0.2881 Acc: 90.6250% F1: 0.845 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 6/15 loss: 0.1793 Acc: 87.5000% F1: 0.591 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 7/15 loss: 0.3826 Acc: 84.3750% F1: 0.741 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 8/15 loss: 0.2780 Acc: 90.6250% F1: 0.884 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 9/15 loss: 0.2836 Acc: 87.5000% F1: 0.819 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 10/15 loss: 0.2343 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 11/15 loss: 0.1670 Acc: 93.7500% F1: 0.898 Time: 0.93s (0.03s)
Fold 8 train - epoch: 8/10 iter: 12/15 loss: 0.4695 Acc: 81.2500% F1: 0.570 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 13/15 loss: 0.2198 Acc: 93.7500% F1: 0.806 Time: 0.93s (0.02s)
Fold 8 train - epoch: 8/10 iter: 14/15 loss: 0.0013 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 8 train Avg acc: 89.7778% F1: 0.8098 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 8/10 iter: 0/2 loss: 0.9407 Acc: 71.8750% F1: 0.436 Time: 0.33s (0.00s)
Fold 8 train-dev - epoch: 8/10 iter: 1/2 loss: 1.9221 Acc: 38.8889% F1: 0.283 Time: 0.19s (0.00s)
*********************************************************
* Fold 8 Epoch 8 train-dev Avg acc: 60.0000% F1: 0.4896 *
*********************************************************
Performing epoch 9 of 10
Fold 8 train - epoch: 9/10 iter: 0/15 loss: 0.2178 Acc: 87.5000% F1: 0.614 Time: 0.94s (0.00s)
Fold 8 train - epoch: 9/10 iter: 1/15 loss: 0.2398 Acc: 84.3750% F1: 0.593 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 2/15 loss: 0.0908 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 3/15 loss: 0.1234 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 4/15 loss: 0.1398 Acc: 96.8750% F1: 0.939 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 5/15 loss: 0.2112 Acc: 93.7500% F1: 0.872 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 6/15 loss: 0.1681 Acc: 93.7500% F1: 0.852 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 7/15 loss: 0.1853 Acc: 96.8750% F1: 0.878 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 8/15 loss: 0.2051 Acc: 96.8750% F1: 0.956 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 9/15 loss: 0.1667 Acc: 96.8750% F1: 0.942 Time: 0.93s (0.03s)
Fold 8 train - epoch: 9/10 iter: 10/15 loss: 0.2534 Acc: 90.6250% F1: 0.868 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 11/15 loss: 0.0794 Acc: 100.0000% F1: 1.000 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 12/15 loss: 0.2491 Acc: 90.6250% F1: 0.836 Time: 0.94s (0.02s)
Fold 8 train - epoch: 9/10 iter: 13/15 loss: 0.1646 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.02s)
Fold 8 train - epoch: 9/10 iter: 14/15 loss: 0.0034 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 8 Epoch 9 train Avg acc: 94.6667% F1: 0.8996 *
*****************************************************
Saving model...
Fold 8 train-dev - epoch: 9/10 iter: 0/2 loss: 2.0554 Acc: 40.6250% F1: 0.283 Time: 0.34s (0.00s)
Fold 8 train-dev - epoch: 9/10 iter: 1/2 loss: 1.2690 Acc: 66.6667% F1: 0.461 Time: 0.18s (0.00s)
*********************************************************
* Fold 8 Epoch 9 train-dev Avg acc: 50.0000% F1: 0.4759 *
*********************************************************
Creating 1 distributed models for fold 9...
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at textattack/bert-base-uncased-snli were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[-7.7383e-02, -4.0688e-02, -1.7081e-01, -1.3104e-02,  1.1034e-01,
         -1.7132e-02, -4.4995e-01,  3.2306e-02,  1.4959e-01, -3.7504e-01,
         -6.7424e-02,  1.7100e-01,  3.2128e-01, -1.9967e-01,  2.3202e-01,
          5.8673e-02,  1.1128e-01,  2.2970e-02, -2.6552e-02,  4.3537e-01,
          1.6131e-01,  4.8784e-01, -2.1551e-02,  4.3200e-02, -2.6227e-02,
          6.9030e-02, -1.2308e-01,  4.2657e-01,  3.8805e-01,  1.8337e-01,
         -3.3134e-02,  1.0518e-02, -4.7912e-01,  7.3158e-03, -3.5362e-01,
         -4.6070e-01,  7.9685e-02,  2.1714e-01,  4.5720e-02, -7.5719e-02,
         -2.5599e-01,  5.5910e-03,  4.7530e-01, -9.6272e-02,  2.0493e-02,
          7.2438e-02, -2.3411e-01, -5.5306e-02, -1.6075e-01, -8.1183e-02,
         -4.2391e-02,  9.0706e-02,  7.2633e-02,  1.0814e-01,  2.8750e-02,
         -3.0013e-01, -8.1501e-02, -6.4192e-02, -5.4835e-02, -7.6253e-02,
         -2.0043e-02, -8.0121e-03, -8.2178e-02,  1.5232e-01, -6.1368e-02,
         -3.1753e-01,  3.6873e-02, -8.5572e-02,  5.8899e-02, -9.1346e-02,
         -3.2458e-01,  1.8308e-03, -2.2063e-02, -2.2457e-01, -8.9407e-02,
          2.6433e-02, -8.0636e-02,  4.9213e-01,  5.9206e-02, -4.7925e-01,
          1.2012e-01, -1.5226e-01, -6.2892e-02,  2.6798e-01,  3.3086e-02,
         -4.2968e-01,  1.1475e-01,  4.3560e-02, -4.9917e-01, -1.3810e-03,
          5.2060e-02,  7.8432e-04,  2.7484e-01, -4.7789e-02,  1.5952e-01,
         -2.2742e-01,  9.8457e-02, -1.7220e-01, -5.4915e-02, -3.9383e-02,
          5.2835e-02,  2.9362e-03,  1.1751e-01,  6.0656e-02,  3.9301e-02,
         -7.4871e-02,  2.4123e-01,  2.9461e-01, -2.6133e-01, -2.6161e-02,
          2.4848e-02, -1.1793e-02,  8.0423e-02, -2.4583e-01,  7.1826e-02,
         -1.5824e-03, -4.9058e-01,  2.1372e-03, -4.6826e-01, -3.1127e-02,
          1.1264e-02, -7.5144e-02, -1.9557e-02, -2.7351e-01,  2.9722e-02,
         -3.8069e-02,  1.8402e-01, -5.3454e-01, -6.7468e-03, -1.2695e-01,
          8.4026e-02, -4.7797e-02, -4.4048e-01, -4.8113e-01, -3.0359e-02,
          2.8339e-01, -2.4723e-02,  4.5644e-01, -1.3425e-02,  3.7951e-01,
          2.4294e-01, -1.6238e-01, -1.1790e-01, -3.6208e-02,  9.6996e-02,
         -3.9698e-01,  2.3457e-02, -6.9716e-02, -1.4906e-01, -1.3096e-01,
         -2.2889e-01, -2.0278e-02,  9.2187e-02, -2.0651e-01, -4.7865e-02,
          4.4234e-01,  7.8321e-02,  2.3677e-01,  2.9183e-01,  3.6472e-02,
         -5.5025e-02,  1.9279e-01,  2.2404e-01, -1.4092e-02,  1.9491e-01,
          3.5687e-02, -2.9023e-01, -1.1506e-01, -3.8342e-02,  1.2236e-01,
         -5.2218e-02,  3.4768e-03,  5.7581e-02, -4.4084e-01, -9.2940e-02,
          2.1075e-02,  4.4594e-01, -2.0420e-01, -4.1458e-02, -2.3127e-01,
         -5.2075e-02, -1.5328e-01, -2.4263e-01,  4.3999e-01,  1.2031e-02,
         -4.6688e-02, -4.6823e-01,  5.9518e-02,  1.8354e-02, -4.3341e-02,
         -3.4900e-01,  9.2408e-02, -5.2343e-02,  6.5838e-03, -4.0737e-02,
          5.1015e-03, -2.4648e-01,  1.3569e-01,  2.6248e-02,  1.9103e-02,
         -7.7506e-02,  3.3864e-01, -2.9237e-01,  3.1834e-02, -7.7749e-02,
          1.5550e-01, -1.0469e-01,  9.1525e-02, -6.2125e-02,  3.9470e-02,
         -5.2484e-02,  4.0757e-01, -4.4413e-01, -1.8884e-03,  1.7378e-01,
         -4.7216e-01, -1.4415e-02,  2.4949e-01,  1.0595e-01, -1.7765e-01,
          4.9913e-03, -2.7921e-01, -3.7402e-01, -3.0710e-02,  4.6861e-01,
         -2.7032e-02, -8.6009e-02, -7.9608e-02, -3.2431e-02,  2.5606e-02,
          4.2297e-01,  2.6683e-01,  4.8513e-02, -3.4772e-01,  4.3845e-01,
         -1.7086e-01, -3.9735e-01, -2.3502e-01,  6.4323e-02, -4.2045e-01,
          1.6344e-02,  2.0853e-01,  1.1652e-01, -6.1308e-02, -3.1786e-01,
         -1.9119e-02,  4.3445e-01,  2.4261e-01, -5.9754e-02, -2.8433e-01,
         -1.7867e-01, -5.0677e-02, -2.7752e-02,  3.2111e-01, -5.3971e-02,
         -4.5692e-01, -4.5844e-01, -3.5051e-01, -4.5231e-01,  9.2142e-02,
         -3.5668e-01,  1.8686e-01,  1.2913e-02,  1.5901e-01,  3.8510e-02,
          1.0429e-01, -4.0587e-01, -3.7406e-01, -6.2230e-02, -5.6517e-02,
         -2.5597e-01,  1.2390e-01, -4.5107e-02, -2.9008e-01,  1.5801e-01,
          4.7917e-02,  3.2929e-01,  8.5441e-03, -2.3124e-02,  3.5059e-02,
         -3.5692e-02,  1.9386e-01, -7.4441e-02, -1.9442e-01,  4.8975e-01,
          4.3752e-01,  7.8040e-02, -3.4206e-01, -5.0285e-01, -2.1571e-01,
          4.6930e-01, -3.7690e-01, -4.9470e-01, -2.3950e-01, -9.1227e-03,
          4.1448e-02, -5.1821e-01,  1.9948e-02,  1.7913e-02, -3.1851e-01,
         -1.9386e-01,  3.7742e-01, -3.7731e-01, -5.1041e-01,  1.1445e-01,
          5.7981e-02, -1.0586e-01,  5.7754e-02, -7.3233e-02,  3.9589e-01,
         -5.9681e-02,  5.3035e-02, -1.3185e-02, -1.1282e-02, -2.9936e-01,
          8.2279e-02,  9.6473e-02, -4.2045e-01,  4.8352e-01, -1.5834e-01,
         -3.3670e-02, -2.0552e-01,  2.4111e-01, -2.3435e-02,  1.2151e-01,
         -4.7768e-01,  3.4347e-02, -2.1431e-01,  3.2031e-02,  7.1767e-02,
         -1.1755e-02,  2.0022e-01, -5.1890e-02,  8.6908e-02, -2.4405e-01,
          8.0753e-02, -2.9132e-01,  1.7747e-01, -1.7312e-01, -5.9781e-02,
          1.9357e-01, -4.1030e-01,  3.0507e-01,  4.8975e-02, -2.2409e-01,
          4.9830e-01,  4.3369e-01, -1.1216e-01,  1.2388e-01,  2.6519e-02,
         -3.2693e-01,  3.2967e-01,  1.2012e-01, -3.2448e-01,  1.2499e-02,
          3.2909e-03, -3.0539e-02, -3.3706e-02,  4.2670e-01,  3.3130e-02,
         -1.6591e-02, -8.7533e-02,  4.8113e-01, -4.1692e-01,  4.7332e-01,
          1.1632e-01, -4.7767e-01,  3.4707e-01,  3.9956e-01, -6.2726e-03,
         -4.8789e-02, -9.0623e-02,  1.3255e-01,  5.2980e-02,  4.7144e-01,
          6.2263e-03, -5.9104e-02, -3.7673e-02,  1.8287e-01,  3.5025e-01,
          1.3524e-02,  7.5477e-03, -1.9018e-01,  1.4017e-01,  1.3812e-01,
         -5.3427e-03,  6.1317e-02, -4.8141e-02,  7.0736e-02, -3.4637e-01,
         -1.7600e-01,  1.4899e-01,  4.8151e-01,  2.7381e-02,  4.6344e-01,
          1.8392e-01,  1.6401e-02, -5.0981e-02, -1.4532e-02,  1.5804e-01,
          9.7657e-02, -1.1398e-01,  8.1096e-02,  3.8284e-01, -5.0999e-01,
         -4.2650e-01, -4.2067e-02,  7.3099e-03,  4.1592e-01, -4.9142e-02,
          6.5282e-02, -4.0436e-02,  1.0793e-01, -4.4649e-02, -2.8372e-01,
         -1.6598e-01,  4.6612e-01,  2.0965e-02, -5.6584e-02, -4.1248e-01,
          2.0115e-01,  4.0813e-03, -4.7681e-02, -1.0682e-01, -4.3563e-01,
          3.6304e-02, -4.6356e-01,  4.4351e-01,  9.0727e-02,  6.1183e-02,
         -7.5164e-02,  2.4678e-01,  4.8213e-01, -4.8421e-01,  2.2666e-02,
          4.6645e-01, -1.5815e-01, -4.9774e-01,  2.7453e-01,  9.9151e-03,
          3.6555e-02,  1.1921e-01,  1.5389e-02,  2.1130e-02, -4.4308e-01,
         -2.2707e-01,  1.3701e-01,  4.3492e-01, -4.6021e-01, -7.6070e-02,
         -1.5912e-01,  8.8943e-02, -4.8885e-01,  1.6643e-01, -1.3388e-01,
         -4.9871e-02,  1.3900e-02, -2.0153e-01,  3.7468e-01, -5.9968e-03,
          3.8768e-04, -4.9245e-02,  1.1510e-01, -5.1104e-02,  3.8472e-01,
         -4.8970e-01, -7.8404e-02, -4.1465e-02,  1.5391e-01,  1.1730e-01,
         -1.1151e-02, -1.6685e-01, -4.5027e-02,  4.8225e-01, -1.6224e-01,
          1.1377e-01,  1.1002e-01, -6.6905e-02,  2.0785e-02,  1.1262e-02,
          4.0801e-01, -3.8793e-02,  1.4154e-01,  2.4705e-01,  4.3559e-01,
          1.4418e-02,  1.3418e-02,  8.6634e-02, -6.9394e-02,  9.0478e-02,
          1.5054e-01,  1.2191e-02,  1.7336e-02, -4.7857e-02, -4.1988e-01,
         -4.9221e-02, -1.0997e-01,  8.7315e-03, -2.4996e-03, -1.4706e-01,
          2.9248e-01,  4.8266e-01,  1.7263e-02,  2.9136e-01, -4.4800e-01,
         -2.0183e-02,  4.1882e-02,  4.9027e-01,  4.2771e-01,  3.1354e-02,
         -6.2722e-03,  5.8889e-02, -5.1623e-02, -3.2006e-01, -2.7625e-03,
         -9.1114e-02, -3.0077e-02, -1.2492e-01,  4.1459e-01,  5.9747e-03,
         -5.0131e-01, -2.2950e-01,  1.3557e-02, -3.7353e-01,  4.8965e-01,
         -1.3554e-01, -1.2883e-01, -2.8964e-02, -1.0370e-01, -3.0552e-01,
          8.4323e-02, -3.5125e-01, -1.9096e-02,  3.7838e-03,  4.7501e-01,
         -6.2218e-02, -4.1479e-02, -2.6593e-01,  1.9936e-02, -8.3595e-02,
          1.7326e-01, -3.5262e-01,  2.5827e-01, -2.6408e-01,  4.8381e-02,
          3.5562e-01,  1.6598e-03, -1.2032e-01, -5.7122e-02,  9.1160e-02,
          3.0897e-03, -3.2683e-01,  5.9020e-02, -3.4228e-01, -1.1268e-02,
         -2.0633e-02,  6.5649e-02,  1.1681e-02, -1.9324e-01, -8.3083e-02,
          6.2886e-02, -4.4318e-02, -5.5888e-02, -7.0098e-02, -4.2235e-02,
          2.9214e-04, -1.6173e-01,  2.6624e-03, -8.3204e-03,  7.4549e-02,
         -2.1624e-01, -6.7542e-02, -5.1503e-04, -5.0754e-01, -1.4292e-01,
         -5.1079e-01,  2.4003e-01, -3.0540e-01,  2.0693e-02,  3.4991e-01,
          2.0177e-01,  2.7267e-01, -1.2714e-02,  2.2985e-01,  2.5193e-01,
          1.4974e-01, -2.0885e-02,  4.1563e-02, -1.1045e-01,  3.5432e-03,
         -1.1650e-02, -1.4022e-02, -1.9467e-01, -8.3537e-02,  7.9639e-02,
          4.7749e-01, -2.0740e-02, -4.8659e-02,  4.5943e-01,  1.4641e-03,
         -8.9822e-03,  4.9773e-01,  3.5071e-01, -4.5593e-01, -2.1385e-02,
         -1.8742e-01, -1.2728e-01, -1.7560e-03, -2.5974e-02, -2.6189e-02,
         -1.2826e-01, -1.3351e-01, -1.8715e-01, -1.2843e-01,  1.6161e-01,
          3.8766e-02, -5.0523e-02, -1.4398e-02,  9.5633e-02,  4.3250e-01,
          3.2928e-01, -1.5434e-01, -3.8972e-01, -2.2715e-01,  2.0893e-01,
          9.6741e-03, -1.0017e-01,  4.2213e-02,  4.8666e-01, -7.6266e-03,
         -2.9496e-01,  6.5341e-02,  1.2951e-02,  6.6755e-02, -6.6803e-02,
          1.7207e-03, -4.0673e-02,  3.4523e-01,  1.6227e-02,  4.0972e-01,
          2.7315e-01, -1.6350e-01, -4.7076e-04,  2.9043e-01, -5.1843e-02,
         -4.4597e-01, -3.6455e-01, -2.9959e-01,  1.6935e-01,  8.6207e-03,
          5.1284e-02, -2.4220e-02, -1.7418e-02, -1.8927e-02,  2.2287e-02,
         -5.0963e-01,  1.1180e-01, -1.3396e-02, -4.8774e-02,  4.4882e-01,
          9.4202e-02,  2.2544e-01,  3.5667e-03, -4.8341e-01,  2.5499e-01,
         -1.4816e-02, -2.0834e-02,  1.2354e-01, -2.2767e-02, -2.8940e-01,
         -7.3240e-02, -5.8016e-02, -2.5188e-01, -3.0964e-01, -4.0966e-01,
         -4.5983e-01,  4.2942e-02,  1.4158e-01,  3.8662e-01,  3.6439e-01,
         -1.7969e-01,  6.8974e-02,  2.5693e-01, -3.2175e-01, -4.7702e-01,
          3.8868e-02, -2.9651e-01, -2.2764e-02,  1.6083e-02,  2.6270e-01,
         -1.0213e-01,  3.2599e-01,  1.0496e-02, -3.7940e-01, -1.0186e-01,
         -3.9522e-03,  4.0503e-01, -3.7733e-01, -5.5236e-03,  2.5423e-01,
          5.3951e-02, -7.0886e-02,  2.2178e-02,  4.4483e-01,  3.8524e-01,
         -3.3132e-02,  1.2768e-01, -5.3403e-02,  6.0125e-03, -1.2225e-01,
          1.1161e-01, -3.0766e-02, -6.2926e-02, -1.5330e-01, -1.1143e-01,
          3.2348e-01,  3.0780e-02, -7.7353e-02,  8.1554e-02,  7.2723e-02,
          1.8812e-01, -1.5985e-01, -4.6046e-01,  3.1271e-02,  2.6476e-01,
          3.5740e-01, -1.7017e-01,  4.2578e-01,  5.0446e-02,  1.3172e-02,
         -6.0822e-02, -6.7830e-03, -1.6121e-01, -2.9410e-01, -7.0635e-03,
         -9.6423e-02, -9.9729e-04, -1.2634e-02,  4.9109e-01,  1.7178e-01,
          2.5327e-01, -1.9130e-01,  1.7019e-02, -2.5902e-02, -7.0339e-02,
         -5.0762e-01, -2.7079e-02,  7.0226e-02,  1.4221e-02, -5.3828e-03,
         -2.5523e-02, -1.3663e-02,  3.1155e-01, -8.1648e-02,  2.5756e-01,
          1.0611e-01,  5.7619e-02, -1.6186e-02,  1.2366e-01,  7.7126e-02,
         -6.2280e-02, -8.0140e-02, -3.0953e-01,  2.4767e-01, -1.2793e-02,
         -2.8231e-01, -1.6712e-02, -6.4691e-02]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: textattack/bert-base-uncased-snli
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 10
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train - epoch: 0/10 iter: 0/15 loss: 3.9453 Acc: 12.5000% F1: 0.076 Time: 0.96s (0.00s)
Fold 9 train - epoch: 0/10 iter: 1/15 loss: 2.8345 Acc: 18.7500% F1: 0.169 Time: 0.93s (0.03s)
Fold 9 train - epoch: 0/10 iter: 2/15 loss: 1.6331 Acc: 43.7500% F1: 0.425 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 3/15 loss: 1.3476 Acc: 46.8750% F1: 0.324 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 4/15 loss: 1.6599 Acc: 34.3750% F1: 0.175 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 5/15 loss: 1.4529 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 6/15 loss: 1.0120 Acc: 62.5000% F1: 0.305 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 7/15 loss: 1.0632 Acc: 56.2500% F1: 0.377 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 8/15 loss: 1.5582 Acc: 34.3750% F1: 0.230 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 9/15 loss: 1.4685 Acc: 34.3750% F1: 0.246 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 10/15 loss: 1.3160 Acc: 40.6250% F1: 0.283 Time: 0.94s (0.03s)
Fold 9 train - epoch: 0/10 iter: 11/15 loss: 0.9759 Acc: 43.7500% F1: 0.301 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 12/15 loss: 1.0708 Acc: 53.1250% F1: 0.377 Time: 0.94s (0.02s)
Fold 9 train - epoch: 0/10 iter: 13/15 loss: 1.0930 Acc: 56.2500% F1: 0.373 Time: 0.93s (0.02s)
Fold 9 train - epoch: 0/10 iter: 14/15 loss: 0.6415 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 0 train Avg acc: 42.6667% F1: 0.3583 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 9 train-dev - epoch: 0/10 iter: 0/2 loss: 0.6456 Acc: 68.7500% F1: 0.407 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 0/10 iter: 1/2 loss: 1.3573 Acc: 33.3333% F1: 0.229 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2898 *
*********************************************************
Performing epoch 1 of 10
Fold 9 train - epoch: 1/10 iter: 0/15 loss: 1.0551 Acc: 53.1250% F1: 0.317 Time: 0.94s (0.00s)
Fold 9 train - epoch: 1/10 iter: 1/15 loss: 1.0349 Acc: 59.3750% F1: 0.344 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 2/15 loss: 1.0789 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 3/15 loss: 0.9456 Acc: 62.5000% F1: 0.256 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 4/15 loss: 1.1738 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 5/15 loss: 0.9345 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 6/15 loss: 0.7606 Acc: 68.7500% F1: 0.272 Time: 0.92s (0.02s)
Fold 9 train - epoch: 1/10 iter: 7/15 loss: 0.9749 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 8/15 loss: 1.1359 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.03s)
Fold 9 train - epoch: 1/10 iter: 9/15 loss: 1.1096 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 10/15 loss: 1.0396 Acc: 40.6250% F1: 0.197 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 11/15 loss: 0.7296 Acc: 71.8750% F1: 0.447 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 12/15 loss: 0.9444 Acc: 46.8750% F1: 0.282 Time: 0.93s (0.02s)
Fold 9 train - epoch: 1/10 iter: 13/15 loss: 0.9245 Acc: 59.3750% F1: 0.377 Time: 0.94s (0.02s)
Fold 9 train - epoch: 1/10 iter: 14/15 loss: 0.6236 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 1 train Avg acc: 54.4444% F1: 0.2832 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 1/10 iter: 0/2 loss: 0.7922 Acc: 56.2500% F1: 0.491 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 1/10 iter: 1/2 loss: 1.0746 Acc: 44.4444% F1: 0.315 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3339 *
*********************************************************
Performing epoch 2 of 10
Fold 9 train - epoch: 2/10 iter: 0/15 loss: 0.9936 Acc: 53.1250% F1: 0.384 Time: 0.95s (0.00s)
Fold 9 train - epoch: 2/10 iter: 1/15 loss: 0.9558 Acc: 56.2500% F1: 0.395 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 2/15 loss: 0.8792 Acc: 62.5000% F1: 0.444 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 3/15 loss: 0.8764 Acc: 56.2500% F1: 0.338 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 4/15 loss: 0.9470 Acc: 50.0000% F1: 0.322 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 5/15 loss: 0.8574 Acc: 71.8750% F1: 0.481 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 6/15 loss: 0.7493 Acc: 65.6250% F1: 0.317 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 7/15 loss: 0.9307 Acc: 50.0000% F1: 0.286 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 8/15 loss: 1.0376 Acc: 53.1250% F1: 0.293 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 9/15 loss: 1.0808 Acc: 46.8750% F1: 0.217 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 10/15 loss: 1.0359 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 11/15 loss: 0.7036 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 12/15 loss: 0.8881 Acc: 62.5000% F1: 0.361 Time: 0.93s (0.02s)
Fold 9 train - epoch: 2/10 iter: 13/15 loss: 0.9671 Acc: 56.2500% F1: 0.289 Time: 0.93s (0.03s)
Fold 9 train - epoch: 2/10 iter: 14/15 loss: 0.2322 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 2 train Avg acc: 57.1111% F1: 0.3534 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 2/10 iter: 0/2 loss: 0.7008 Acc: 65.6250% F1: 0.469 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 2/10 iter: 1/2 loss: 1.2196 Acc: 38.8889% F1: 0.274 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 2 train-dev Avg acc: 56.0000% F1: 0.3238 *
*********************************************************
Performing epoch 3 of 10
Fold 9 train - epoch: 3/10 iter: 0/15 loss: 0.9136 Acc: 68.7500% F1: 0.482 Time: 0.94s (0.00s)
Fold 9 train - epoch: 3/10 iter: 1/15 loss: 0.9012 Acc: 56.2500% F1: 0.368 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 2/15 loss: 0.8588 Acc: 56.2500% F1: 0.370 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 3/15 loss: 0.8024 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 4/15 loss: 0.8871 Acc: 59.3750% F1: 0.406 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 5/15 loss: 0.8316 Acc: 71.8750% F1: 0.492 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 6/15 loss: 0.6594 Acc: 75.0000% F1: 0.450 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 7/15 loss: 0.8493 Acc: 62.5000% F1: 0.421 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 8/15 loss: 0.9749 Acc: 59.3750% F1: 0.363 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 9/15 loss: 0.9772 Acc: 50.0000% F1: 0.340 Time: 0.93s (0.03s)
Fold 9 train - epoch: 3/10 iter: 10/15 loss: 0.9413 Acc: 56.2500% F1: 0.392 Time: 0.94s (0.02s)
Fold 9 train - epoch: 3/10 iter: 11/15 loss: 0.6246 Acc: 78.1250% F1: 0.506 Time: 0.94s (0.02s)
Fold 9 train - epoch: 3/10 iter: 12/15 loss: 0.8295 Acc: 62.5000% F1: 0.405 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 13/15 loss: 0.8284 Acc: 65.6250% F1: 0.394 Time: 0.93s (0.02s)
Fold 9 train - epoch: 3/10 iter: 14/15 loss: 0.1543 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 9 Epoch 3 train Avg acc: 63.1111% F1: 0.4158 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 3/10 iter: 0/2 loss: 0.6989 Acc: 68.7500% F1: 0.543 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 3/10 iter: 1/2 loss: 1.3025 Acc: 44.4444% F1: 0.315 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 3 train-dev Avg acc: 60.0000% F1: 0.3686 *
*********************************************************
Performing epoch 4 of 10
Fold 9 train - epoch: 4/10 iter: 0/15 loss: 0.8682 Acc: 68.7500% F1: 0.489 Time: 0.95s (0.00s)
Fold 9 train - epoch: 4/10 iter: 1/15 loss: 0.8345 Acc: 65.6250% F1: 0.430 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 2/15 loss: 0.7628 Acc: 65.6250% F1: 0.466 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 3/15 loss: 0.7345 Acc: 75.0000% F1: 0.484 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 4/15 loss: 0.8756 Acc: 59.3750% F1: 0.409 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 5/15 loss: 0.7350 Acc: 75.0000% F1: 0.513 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 6/15 loss: 0.6509 Acc: 78.1250% F1: 0.476 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 7/15 loss: 0.7709 Acc: 68.7500% F1: 0.472 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 8/15 loss: 0.9059 Acc: 62.5000% F1: 0.432 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 9/15 loss: 0.9410 Acc: 59.3750% F1: 0.395 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 10/15 loss: 0.7917 Acc: 65.6250% F1: 0.473 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 11/15 loss: 0.6540 Acc: 62.5000% F1: 0.391 Time: 0.93s (0.03s)
Fold 9 train - epoch: 4/10 iter: 12/15 loss: 0.7978 Acc: 65.6250% F1: 0.433 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 13/15 loss: 0.8122 Acc: 65.6250% F1: 0.414 Time: 0.93s (0.02s)
Fold 9 train - epoch: 4/10 iter: 14/15 loss: 0.0598 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 4 train Avg acc: 67.1111% F1: 0.4520 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 4/10 iter: 0/2 loss: 0.7705 Acc: 53.1250% F1: 0.439 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 4/10 iter: 1/2 loss: 1.4089 Acc: 44.4444% F1: 0.315 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 4 train-dev Avg acc: 50.0000% F1: 0.3153 *
*********************************************************
Performing epoch 5 of 10
Fold 9 train - epoch: 5/10 iter: 0/15 loss: 0.7020 Acc: 78.1250% F1: 0.561 Time: 0.94s (0.00s)
Fold 9 train - epoch: 5/10 iter: 1/15 loss: 0.7462 Acc: 68.7500% F1: 0.467 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 2/15 loss: 0.6290 Acc: 78.1250% F1: 0.557 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 3/15 loss: 0.6930 Acc: 68.7500% F1: 0.457 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 4/15 loss: 0.8682 Acc: 65.6250% F1: 0.452 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 5/15 loss: 0.5882 Acc: 78.1250% F1: 0.538 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 6/15 loss: 0.5136 Acc: 78.1250% F1: 0.486 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 7/15 loss: 0.7226 Acc: 71.8750% F1: 0.492 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 8/15 loss: 0.8535 Acc: 62.5000% F1: 0.411 Time: 0.93s (0.02s)
Fold 9 train - epoch: 5/10 iter: 9/15 loss: 0.7890 Acc: 59.3750% F1: 0.409 Time: 0.94s (0.04s)
Fold 9 train - epoch: 5/10 iter: 10/15 loss: 0.6612 Acc: 75.0000% F1: 0.704 Time: 0.94s (0.04s)
Fold 9 train - epoch: 5/10 iter: 11/15 loss: 0.4856 Acc: 93.7500% F1: 0.844 Time: 0.93s (0.04s)
Fold 9 train - epoch: 5/10 iter: 12/15 loss: 0.5961 Acc: 81.2500% F1: 0.570 Time: 0.94s (0.03s)
Fold 9 train - epoch: 5/10 iter: 13/15 loss: 0.7062 Acc: 75.0000% F1: 0.513 Time: 0.93s (0.03s)
Fold 9 train - epoch: 5/10 iter: 14/15 loss: 0.0294 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 9 Epoch 5 train Avg acc: 74.0000% F1: 0.5425 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 5/10 iter: 0/2 loss: 0.8579 Acc: 53.1250% F1: 0.468 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 5/10 iter: 1/2 loss: 1.5590 Acc: 38.8889% F1: 0.281 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 5 train-dev Avg acc: 48.0000% F1: 0.3078 *
*********************************************************
Performing epoch 6 of 10
Fold 9 train - epoch: 6/10 iter: 0/15 loss: 0.6928 Acc: 71.8750% F1: 0.610 Time: 0.94s (0.00s)
Fold 9 train - epoch: 6/10 iter: 1/15 loss: 0.5967 Acc: 71.8750% F1: 0.517 Time: 0.94s (0.02s)
Fold 9 train - epoch: 6/10 iter: 2/15 loss: 0.4428 Acc: 87.5000% F1: 0.821 Time: 0.93s (0.04s)
Fold 9 train - epoch: 6/10 iter: 3/15 loss: 0.5771 Acc: 71.8750% F1: 0.500 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 4/15 loss: 0.7185 Acc: 62.5000% F1: 0.429 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 5/15 loss: 0.5087 Acc: 81.2500% F1: 0.580 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 6/15 loss: 0.3236 Acc: 90.6250% F1: 0.604 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 7/15 loss: 0.4297 Acc: 87.5000% F1: 0.603 Time: 0.94s (0.02s)
Fold 9 train - epoch: 6/10 iter: 8/15 loss: 0.6713 Acc: 65.6250% F1: 0.463 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 9/15 loss: 0.6360 Acc: 75.0000% F1: 0.700 Time: 0.94s (0.02s)
Fold 9 train - epoch: 6/10 iter: 10/15 loss: 0.5064 Acc: 84.3750% F1: 0.823 Time: 0.94s (0.03s)
Fold 9 train - epoch: 6/10 iter: 11/15 loss: 0.4609 Acc: 87.5000% F1: 0.805 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 12/15 loss: 0.4792 Acc: 84.3750% F1: 0.590 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 13/15 loss: 0.5298 Acc: 84.3750% F1: 0.738 Time: 0.93s (0.02s)
Fold 9 train - epoch: 6/10 iter: 14/15 loss: 0.0216 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 9 Epoch 6 train Avg acc: 79.1111% F1: 0.6488 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 6/10 iter: 0/2 loss: 0.9851 Acc: 43.7500% F1: 0.274 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 6/10 iter: 1/2 loss: 1.7871 Acc: 38.8889% F1: 0.282 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 6 train-dev Avg acc: 42.0000% F1: 0.2863 *
*********************************************************
Performing epoch 7 of 10
Fold 9 train - epoch: 7/10 iter: 0/15 loss: 0.5784 Acc: 78.1250% F1: 0.656 Time: 0.94s (0.00s)
Fold 9 train - epoch: 7/10 iter: 1/15 loss: 0.5213 Acc: 81.2500% F1: 0.574 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 2/15 loss: 0.2724 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 3/15 loss: 0.5242 Acc: 87.5000% F1: 0.768 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 4/15 loss: 0.4826 Acc: 81.2500% F1: 0.586 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 5/15 loss: 0.6092 Acc: 65.6250% F1: 0.456 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 6/15 loss: 0.2520 Acc: 90.6250% F1: 0.593 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 7/15 loss: 0.4159 Acc: 84.3750% F1: 0.744 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 8/15 loss: 0.4320 Acc: 81.2500% F1: 0.670 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 9/15 loss: 0.3891 Acc: 87.5000% F1: 0.796 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 10/15 loss: 0.4214 Acc: 78.1250% F1: 0.728 Time: 0.93s (0.03s)
Fold 9 train - epoch: 7/10 iter: 11/15 loss: 0.4082 Acc: 90.6250% F1: 0.829 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 12/15 loss: 0.3703 Acc: 87.5000% F1: 0.729 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 13/15 loss: 0.2694 Acc: 93.7500% F1: 0.810 Time: 0.93s (0.02s)
Fold 9 train - epoch: 7/10 iter: 14/15 loss: 0.0073 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 9 Epoch 7 train Avg acc: 84.4444% F1: 0.7145 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 7/10 iter: 0/2 loss: 1.0276 Acc: 53.1250% F1: 0.322 Time: 0.33s (0.00s)
Fold 9 train-dev - epoch: 7/10 iter: 1/2 loss: 2.1725 Acc: 33.3333% F1: 0.243 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 7 train-dev Avg acc: 46.0000% F1: 0.3032 *
*********************************************************
Performing epoch 8 of 10
Fold 9 train - epoch: 8/10 iter: 0/15 loss: 0.3291 Acc: 90.6250% F1: 0.865 Time: 0.94s (0.00s)
Fold 9 train - epoch: 8/10 iter: 1/15 loss: 0.3646 Acc: 84.3750% F1: 0.601 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 2/15 loss: 0.2144 Acc: 90.6250% F1: 0.894 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 3/15 loss: 0.3005 Acc: 87.5000% F1: 0.594 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 4/15 loss: 0.3400 Acc: 84.3750% F1: 0.703 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 5/15 loss: 0.2639 Acc: 93.7500% F1: 0.872 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 6/15 loss: 0.1546 Acc: 96.8750% F1: 0.649 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 7/15 loss: 0.3602 Acc: 87.5000% F1: 0.767 Time: 0.93s (0.03s)
Fold 9 train - epoch: 8/10 iter: 8/15 loss: 0.3562 Acc: 87.5000% F1: 0.840 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 9/15 loss: 0.3844 Acc: 90.6250% F1: 0.827 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 10/15 loss: 0.2648 Acc: 87.5000% F1: 0.803 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 11/15 loss: 0.2341 Acc: 90.6250% F1: 0.872 Time: 0.94s (0.02s)
Fold 9 train - epoch: 8/10 iter: 12/15 loss: 0.2343 Acc: 90.6250% F1: 0.757 Time: 0.93s (0.02s)
Fold 9 train - epoch: 8/10 iter: 13/15 loss: 0.1969 Acc: 90.6250% F1: 0.792 Time: 0.94s (0.02s)
Fold 9 train - epoch: 8/10 iter: 14/15 loss: 0.0065 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 9 Epoch 8 train Avg acc: 89.5556% F1: 0.8060 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 8/10 iter: 0/2 loss: 1.5637 Acc: 25.0000% F1: 0.193 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 8/10 iter: 1/2 loss: 2.1507 Acc: 33.3333% F1: 0.259 Time: 0.18s (0.00s)
*********************************************************
* Fold 9 Epoch 8 train-dev Avg acc: 28.0000% F1: 0.2171 *
*********************************************************
Performing epoch 9 of 10
Fold 9 train - epoch: 9/10 iter: 0/15 loss: 0.3211 Acc: 84.3750% F1: 0.801 Time: 0.94s (0.00s)
Fold 9 train - epoch: 9/10 iter: 1/15 loss: 0.3533 Acc: 84.3750% F1: 0.823 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 2/15 loss: 0.1503 Acc: 96.8750% F1: 0.976 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 3/15 loss: 0.3405 Acc: 84.3750% F1: 0.830 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 4/15 loss: 0.1706 Acc: 93.7500% F1: 0.918 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 5/15 loss: 0.2041 Acc: 93.7500% F1: 0.880 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 6/15 loss: 0.1295 Acc: 93.7500% F1: 0.633 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 7/15 loss: 0.2565 Acc: 90.6250% F1: 0.843 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 8/15 loss: 0.1961 Acc: 90.6250% F1: 0.900 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 9/15 loss: 0.4100 Acc: 87.5000% F1: 0.876 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 10/15 loss: 0.6343 Acc: 84.3750% F1: 0.844 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 11/15 loss: 0.1330 Acc: 93.7500% F1: 0.938 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 12/15 loss: 0.2043 Acc: 90.6250% F1: 0.846 Time: 0.94s (0.02s)
Fold 9 train - epoch: 9/10 iter: 13/15 loss: 0.2415 Acc: 93.7500% F1: 0.810 Time: 0.93s (0.02s)
Fold 9 train - epoch: 9/10 iter: 14/15 loss: 0.0019 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 9 Epoch 9 train Avg acc: 90.2222% F1: 0.8742 *
*****************************************************
Saving model...
Fold 9 train-dev - epoch: 9/10 iter: 0/2 loss: 2.1791 Acc: 34.3750% F1: 0.246 Time: 0.34s (0.00s)
Fold 9 train-dev - epoch: 9/10 iter: 1/2 loss: 1.8516 Acc: 50.0000% F1: 0.343 Time: 0.19s (0.00s)
*********************************************************
* Fold 9 Epoch 9 train-dev Avg acc: 40.0000% F1: 0.2821 *
*********************************************************
Evaluating stats...
Evaluating predictions...
	Epoch 0 Accuracy: 54.2000% F1: 0.3018
	Epoch 1 Accuracy: 52.2000% F1: 0.3326
	Epoch 2 Accuracy: 56.4000% F1: 0.3253
	Epoch 3 Accuracy: 56.0000% F1: 0.3429
	Epoch 4 Accuracy: 52.0000% F1: 0.3497
	Epoch 5 Accuracy: 46.6000% F1: 0.3364
	Epoch 6 Accuracy: 45.8000% F1: 0.3448
	Epoch 7 Accuracy: 44.2000% F1: 0.3371
	Epoch 8 Accuracy: 39.0000% F1: 0.3175
	Epoch 9 Accuracy: 40.8000% F1: 0.3269
all done :)
************************************
** MODEL TIME ID: 20220225-162906 **
************************************
Found 10 fold dirs: /home/CDQA-project/data/pqal_fold0, /home/CDQA-project/data/pqal_fold1, /home/CDQA-project/data/pqal_fold2, /home/CDQA-project/data/pqal_fold3, /home/CDQA-project/data/pqal_fold4, /home/CDQA-project/data/pqal_fold5, /home/CDQA-project/data/pqal_fold6, /home/CDQA-project/data/pqal_fold7, /home/CDQA-project/data/pqal_fold8, /home/CDQA-project/data/pqal_fold9
Creating 1 distributed models for fold 0...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[ 1.3939e-03, -6.8716e-02, -2.5557e-01,  2.8171e-02, -8.7808e-03,
         -2.5491e-01,  2.4635e-01, -1.7451e-01, -2.3248e-02,  2.3335e-01,
          2.6176e-01,  1.3909e-01, -1.2589e-01,  1.6319e-01,  2.4349e-02,
         -1.3283e-01,  4.8036e-02,  4.0980e-01, -2.1196e-01, -2.3831e-01,
         -9.6281e-03, -5.5775e-02,  1.1464e-02, -2.9264e-01,  9.9885e-02,
         -2.0102e-01, -1.9214e-01, -3.0318e-01, -4.2442e-02, -5.4449e-02,
         -1.2967e-04, -7.1719e-02,  2.0007e-01,  9.2549e-03,  5.3880e-02,
         -6.0826e-02,  8.9025e-02,  2.9168e-01,  2.3036e-01, -1.1622e-01,
         -6.1775e-02,  1.8710e-01, -1.6608e-01, -2.2137e-01,  1.6387e-01,
         -1.2285e-01,  3.0026e-01,  2.8564e-01, -3.5229e-01, -1.0878e-01,
          4.0280e-01,  1.5114e-01,  1.1360e-02, -2.0095e-02, -1.3847e-01,
          7.2091e-04, -1.1731e-01, -2.7096e-01,  2.7509e-02,  1.0600e-01,
          4.5664e-02,  1.3611e-01, -1.6287e-01, -2.6011e-01, -8.1596e-02,
          1.9391e-01, -6.8843e-02,  6.6795e-02,  3.0044e-01,  1.4518e-04,
          1.3836e-01, -2.1662e-01, -1.4751e-01, -4.5692e-03,  7.7596e-03,
          1.5091e-01, -1.9701e-01,  3.9364e-02,  2.4326e-02, -1.4290e-01,
         -2.3443e-01, -2.8338e-01,  2.4571e-01, -3.4772e-01, -2.1874e-01,
          4.0258e-02,  1.0768e-01,  2.4987e-01,  2.3627e-01, -1.1554e-01,
          2.1028e-01,  1.6711e-01, -1.5543e-01, -1.8313e-02, -3.3042e-01,
         -2.3139e-01,  1.1625e-01, -3.2922e-01, -1.7406e-01, -2.3016e-01,
         -1.3942e-01, -4.9668e-02, -2.7628e-01,  1.0165e-02,  8.2532e-02,
          6.4771e-02,  1.9546e-01, -3.2410e-01,  1.8964e-01, -1.6898e-01,
         -2.2098e-01, -1.2092e-01, -1.3189e-02,  2.6864e-02,  2.7784e-01,
         -6.3231e-02,  6.4781e-02,  3.4362e-01, -2.2732e-02, -3.5275e-01,
          9.8142e-03, -2.4852e-01, -2.7906e-02, -5.1745e-02, -9.5929e-02,
          1.7100e-02,  4.3980e-02, -4.3783e-02,  3.7871e-01,  1.2682e-01,
         -3.2154e-01, -2.1401e-01,  6.4307e-02,  7.8588e-02,  2.3951e-01,
          1.3997e-02, -7.8034e-02, -2.2260e-03,  1.9591e-02, -2.4945e-01,
          8.2607e-02, -3.2764e-02,  1.7143e-02,  2.0525e-01, -5.7016e-02,
          1.8229e-01, -2.7006e-02,  1.1357e-01,  9.7994e-02,  5.1746e-02,
          7.9860e-02,  3.8248e-02, -2.3227e-01, -1.3862e-01,  6.6153e-02,
          2.6249e-01, -5.7370e-02, -1.3890e-01, -4.0898e-01, -2.8864e-01,
         -2.3905e-02,  2.0195e-01,  1.1847e-01, -2.4378e-01, -1.7138e-01,
         -1.7932e-01,  2.7026e-01, -1.0431e-01,  8.6107e-02, -1.0131e-01,
         -8.2868e-02,  9.2653e-02, -3.3071e-02, -3.0312e-01, -2.7937e-01,
         -4.0222e-01, -3.8210e-01,  1.1642e-01, -3.3853e-02,  7.7303e-02,
          1.2355e-01, -2.2809e-01,  4.3379e-02,  1.0617e-02, -1.7532e-01,
         -1.5698e-01,  1.7562e-01, -6.4621e-02, -2.2386e-01, -5.4332e-02,
         -1.4223e-01,  1.1155e-01,  2.3546e-01,  3.8853e-02,  1.1227e-01,
         -2.9981e-02, -3.5384e-02, -1.3603e-01,  3.1001e-01, -1.4615e-01,
          2.5058e-02, -9.1489e-02, -2.1435e-01,  3.0697e-02, -2.5929e-02,
          3.2241e-01,  3.7920e-01,  2.0833e-01,  2.8212e-01, -1.0324e-01,
         -1.6585e-01, -1.8386e-01,  2.0189e-02,  2.7767e-01, -1.3822e-01,
         -1.3711e-01,  9.5865e-03,  1.1056e-02, -1.0575e-01, -5.5155e-02,
          1.1809e-01, -1.2300e-01,  2.4179e-01, -2.8298e-02,  7.4502e-02,
         -3.0307e-01,  2.4275e-02,  7.3545e-02, -6.9410e-02,  1.2794e-01,
          7.5693e-02,  1.5065e-01, -9.7550e-02,  3.4761e-01,  2.7095e-01,
         -4.5170e-02, -1.2539e-01,  3.8548e-03,  2.8808e-01, -9.7298e-02,
          3.0336e-02, -3.0973e-01,  7.9866e-02,  2.3738e-01,  1.2550e-01,
          7.3314e-03, -1.5217e-01,  2.0428e-01, -3.5740e-02,  1.4864e-01,
         -3.2093e-01, -5.4270e-02,  1.2042e-02, -1.4123e-01, -1.0701e-01,
         -3.3997e-01, -3.5408e-02, -1.8080e-01,  1.0898e-01,  1.1176e-01,
          1.4616e-01,  5.3214e-02, -8.9582e-02, -2.4846e-01, -2.1512e-01,
         -1.8585e-01, -7.4543e-02,  1.8495e-01,  1.3112e-01, -2.9189e-01,
         -2.3716e-01,  1.4231e-01,  1.8316e-01,  1.1146e-01,  1.0917e-01,
         -7.8276e-02, -3.5784e-01, -2.0579e-01, -1.9995e-01, -2.0056e-02,
          1.9377e-01,  2.1209e-01,  1.1557e-01,  1.5575e-01,  3.7109e-02,
         -1.9799e-01,  1.2692e-01, -1.2687e-01,  3.2986e-01, -4.0038e-01,
         -5.4040e-02,  1.9603e-01, -1.0432e-01, -4.4281e-02, -9.4947e-02,
          1.9225e-01,  2.4538e-01,  1.6693e-01,  1.7623e-02,  2.7655e-01,
         -2.7252e-01, -3.5374e-02,  2.1649e-01,  1.1542e-01, -3.2575e-01,
         -9.5898e-02, -1.7769e-01, -1.7478e-01,  1.2413e-02, -9.4918e-02,
         -1.7591e-01, -7.7178e-02,  8.4715e-02,  1.1410e-01,  2.3401e-01,
          4.6640e-02,  1.6343e-02,  8.4249e-02, -9.6963e-02,  4.9761e-03,
         -1.2689e-01, -8.9776e-02,  1.4109e-01, -2.3152e-01,  3.0876e-01,
         -2.1192e-01,  2.8687e-01, -1.6530e-01,  1.0156e-01,  1.3087e-01,
         -1.3041e-01,  2.9489e-01, -1.7078e-01,  2.2375e-01,  1.1248e-01,
          7.6252e-02, -2.7074e-02,  7.4888e-02, -2.2466e-01,  5.8946e-02,
         -1.5730e-01,  1.9001e-01,  1.6656e-01,  2.3558e-01,  8.8893e-02,
          4.6385e-02, -4.6853e-03, -3.9195e-02, -6.8834e-02,  6.7098e-02,
         -2.5421e-01, -1.3098e-01,  3.3280e-01, -1.1145e-01,  3.0578e-01,
          1.4134e-01, -7.8914e-02,  1.6024e-01, -2.5977e-01,  4.2665e-02,
          1.7734e-01, -1.5616e-01, -3.0725e-01, -1.6099e-01, -9.4044e-02,
          5.9506e-02,  2.2976e-01, -1.7763e-01, -2.6950e-01,  3.8000e-01,
          1.7562e-01,  3.7771e-02,  1.0278e-01,  3.2222e-01, -2.1560e-02,
         -1.5225e-01, -2.3036e-01, -1.3021e-01, -3.0180e-01,  2.4546e-01,
          6.7754e-02, -3.0570e-01, -2.9963e-01,  1.5654e-02, -1.8943e-01,
         -2.3144e-01,  2.8879e-01,  7.4722e-02, -1.5568e-01, -2.0820e-01,
          4.9041e-02,  3.9942e-01,  4.3492e-02,  8.4227e-02,  1.3005e-01,
         -9.3410e-02,  2.1089e-01,  8.5407e-02, -2.3646e-01,  1.1235e-01,
          2.1036e-01, -1.5971e-01,  7.9207e-02,  4.4568e-03, -2.2224e-01,
          3.1588e-02, -2.5247e-01, -1.8240e-01, -8.3811e-02, -3.5396e-01,
         -9.2816e-02,  3.5115e-02,  2.7964e-01, -9.2329e-02, -3.5224e-01,
         -2.4336e-01,  1.5316e-01, -1.8951e-01, -1.5665e-01, -2.2028e-01,
         -2.0556e-01, -1.5417e-01,  4.3107e-02, -1.4996e-01, -1.7838e-01,
         -1.0692e-01, -1.7765e-01, -8.7658e-03,  1.6875e-01, -2.1353e-01,
          1.7508e-01, -8.9377e-02, -1.9662e-01,  2.5831e-02, -5.6539e-02,
          5.4678e-02,  2.4562e-01,  9.2557e-02, -5.1224e-02, -2.1296e-02,
         -2.0423e-01,  4.4212e-03, -1.5627e-01,  1.5368e-01, -9.0215e-02,
          4.5537e-02,  2.7957e-01, -1.7029e-01,  2.9199e-01, -9.2008e-02,
         -1.1972e-01, -1.1973e-01, -3.9062e-01, -6.3714e-03, -1.8681e-02,
          4.3476e-02,  1.0941e-01,  1.5459e-02, -2.8024e-02, -2.9083e-01,
         -2.1282e-01,  1.7896e-01, -1.1782e-01, -2.9580e-01,  9.5791e-02,
         -1.6213e-01, -1.0290e-03,  9.6139e-02, -6.0271e-02,  7.4006e-02,
          1.4452e-01, -6.5647e-02,  2.0084e-01, -1.4069e-02, -1.2897e-01,
          2.6349e-01, -2.3983e-01,  1.7705e-02,  1.7037e-01,  3.0223e-02,
         -2.9845e-01,  1.0508e-01, -1.6406e-01, -3.5936e-02, -8.8760e-02,
         -2.6679e-01,  3.3564e-01,  3.7482e-04,  5.0903e-02, -7.3588e-02,
         -1.6273e-01, -5.2870e-02, -2.3405e-01, -7.3421e-02,  6.7705e-03,
         -1.6738e-01, -3.5341e-02,  1.6683e-01, -1.5272e-01,  3.9206e-01,
          2.2943e-01, -1.5948e-01, -2.8566e-02, -1.0889e-02, -2.4224e-01,
          6.9751e-02,  9.7223e-03, -2.8483e-01, -6.3500e-02,  1.8432e-01,
          4.8258e-02,  3.6758e-01, -1.7792e-01,  1.7039e-02,  3.1149e-01,
          9.3328e-02, -2.3158e-01, -3.5395e-01, -1.0560e-01,  2.2844e-01,
          8.0793e-02,  1.4810e-01, -3.0206e-01, -3.4022e-01,  1.7620e-01,
          4.3809e-02,  9.0087e-02,  1.5914e-01, -6.8204e-02,  2.5665e-01,
         -1.8012e-01,  3.7484e-02,  4.8088e-02,  2.3510e-02, -1.1832e-01,
          1.0292e-01,  1.4461e-01,  9.2944e-02, -1.4286e-03,  1.8287e-01,
          1.7623e-01,  6.0158e-02,  1.7079e-01,  1.9798e-01, -1.8351e-01,
         -4.1477e-01,  2.4582e-01,  1.0249e-01, -3.3297e-02, -2.0311e-01,
         -2.5807e-01, -2.9441e-02,  2.5160e-01, -2.8196e-01, -2.2125e-01,
         -1.3952e-01, -5.6166e-02,  2.8915e-02, -1.9787e-01, -1.6558e-01,
         -1.7816e-01, -3.1428e-01, -1.7320e-01,  1.2468e-02, -3.4252e-01,
         -2.5784e-01, -7.1416e-02, -9.2996e-02,  1.6461e-01, -3.2702e-01,
          1.4028e-01,  2.0673e-01, -5.7455e-02, -2.6848e-01,  1.1147e-01,
         -4.8179e-02,  9.7699e-02, -2.2503e-01, -1.3832e-01,  1.9701e-02,
         -8.6869e-02, -4.8751e-02,  6.2277e-02, -3.3196e-01,  2.0513e-01,
          4.2906e-02, -3.1485e-01, -1.9783e-01, -1.8252e-01,  8.9203e-03,
          2.3651e-01, -1.7068e-01,  9.3493e-02,  1.2389e-01, -2.1146e-01,
          8.2649e-03,  1.5572e-01, -2.2668e-01,  6.7394e-02,  1.9482e-02,
          7.9862e-02, -7.1562e-02,  8.8041e-02,  2.9078e-01, -2.9057e-01,
          1.9924e-01,  7.9529e-02,  2.3030e-01, -1.8758e-01, -2.1552e-01,
          1.6985e-01, -1.6580e-01, -2.1389e-01, -2.4635e-01, -1.9783e-01,
          7.4881e-02, -9.2235e-02,  5.3853e-02, -2.7720e-01, -4.0823e-02,
          2.2342e-01,  1.6657e-01, -1.2662e-01, -1.9808e-02,  5.6801e-02,
          2.1904e-02,  6.3948e-02, -2.0601e-01, -2.2994e-01, -1.4389e-01,
          5.3174e-02,  6.4131e-02,  4.4927e-02, -3.7137e-01, -1.5741e-01,
         -2.5703e-01,  1.2718e-01,  2.0193e-02,  3.8737e-02, -7.7474e-02,
          1.9992e-02, -1.8949e-01, -1.0489e-01, -6.3354e-02, -2.5072e-01,
         -2.2850e-01, -1.9215e-01, -2.7560e-01, -5.5963e-02, -1.2828e-01,
          1.2989e-01,  8.0240e-02,  1.1768e-01,  3.6903e-01,  3.2748e-01,
          2.8244e-01,  7.6465e-02, -1.3270e-01,  1.6062e-01,  1.6020e-02,
          1.8847e-01, -1.0488e-01, -2.7101e-01,  2.8572e-01,  2.2558e-01,
         -1.0301e-01,  1.4395e-01,  1.6343e-01, -2.3641e-01, -1.2907e-01,
         -2.7267e-02,  2.6302e-01, -1.8550e-01,  2.4225e-01,  1.8058e-01,
         -2.0332e-01, -1.1652e-01,  2.2879e-01, -6.5583e-02,  3.0162e-02,
          5.5261e-02,  3.3168e-02,  1.4287e-03,  2.4233e-01, -2.5623e-01,
          1.4555e-01, -9.7221e-02, -1.4414e-01,  2.5351e-01, -1.6486e-01,
          8.8751e-03, -1.3138e-01, -5.0337e-02, -2.3413e-01, -1.2613e-01,
          1.0165e-01,  1.7950e-01, -4.9932e-02,  1.9990e-01, -2.5763e-01,
         -3.1997e-01, -1.8444e-01, -2.4811e-01, -6.7918e-02,  2.8393e-01,
          3.9342e-02, -3.8641e-01, -2.4957e-01, -4.2098e-02,  1.2375e-01,
         -2.3292e-01, -1.2942e-01, -9.9817e-02,  2.1444e-02, -2.8142e-02,
         -6.4317e-02, -2.3902e-01, -3.0746e-01, -8.9892e-02, -1.2646e-01,
         -3.2790e-01,  2.3242e-01,  9.5534e-02, -1.0982e-01,  2.6479e-01,
          1.5996e-01, -3.1966e-01, -1.9431e-01,  7.6029e-02, -2.0056e-01,
         -4.4122e-02, -1.9894e-01, -3.2973e-01,  1.0175e-01, -3.6016e-02,
         -1.1808e-01, -1.6042e-01, -2.7750e-01,  4.7654e-02, -6.1258e-02,
         -4.5565e-01, -1.4322e-01, -1.6719e-01,  1.1150e-01, -6.1786e-02,
         -1.3581e-01, -1.2923e-01, -8.6849e-02, -2.0869e-01, -9.5694e-02,
         -3.2178e-01,  4.5601e-02,  9.0810e-02, -3.4218e-01, -1.2418e-01,
         -2.1381e-01, -1.2038e-01, -4.3754e-02,  6.3526e-02,  9.4710e-02,
          1.8775e-02, -2.5421e-01,  9.3967e-03,  2.0880e-01,  1.6493e-01,
         -2.8366e-01, -1.0564e-01,  2.7419e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train - epoch: 0/5 iter: 0/15 loss: 1.1750 Acc: 31.2500% F1: 0.217 Time: 0.96s (0.00s)
Fold 0 train - epoch: 0/5 iter: 1/15 loss: 0.9561 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.04s)
Fold 0 train - epoch: 0/5 iter: 2/15 loss: 1.1196 Acc: 50.0000% F1: 0.227 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 3/15 loss: 0.8723 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.31s)
Fold 0 train - epoch: 0/5 iter: 4/15 loss: 1.0397 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 5/15 loss: 0.8910 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 6/15 loss: 0.8558 Acc: 56.2500% F1: 0.338 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 7/15 loss: 0.9124 Acc: 43.7500% F1: 0.282 Time: 0.93s (0.04s)
Fold 0 train - epoch: 0/5 iter: 8/15 loss: 1.0940 Acc: 50.0000% F1: 0.329 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 9/15 loss: 1.0147 Acc: 53.1250% F1: 0.376 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 10/15 loss: 1.0011 Acc: 56.2500% F1: 0.381 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 11/15 loss: 0.9042 Acc: 59.3750% F1: 0.301 Time: 0.93s (0.03s)
Fold 0 train - epoch: 0/5 iter: 12/15 loss: 0.9872 Acc: 56.2500% F1: 0.332 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 13/15 loss: 0.9564 Acc: 56.2500% F1: 0.245 Time: 0.94s (0.03s)
Fold 0 train - epoch: 0/5 iter: 14/15 loss: 0.5186 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.03s)
*****************************************************
* Fold 0 Epoch 0 train Avg acc: 53.3333% F1: 0.3215 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 0 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5934 Acc: 84.3750% F1: 0.458 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 0/5 iter: 1/2 loss: 1.7507 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 0 train - epoch: 1/5 iter: 0/15 loss: 0.9230 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 0 train - epoch: 1/5 iter: 1/15 loss: 0.9101 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 2/15 loss: 0.9990 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 3/15 loss: 0.8319 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.30s)
Fold 0 train - epoch: 1/5 iter: 4/15 loss: 1.0248 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 5/15 loss: 0.9012 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/5 iter: 6/15 loss: 0.7655 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 7/15 loss: 0.9021 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 8/15 loss: 0.9560 Acc: 56.2500% F1: 0.250 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/5 iter: 9/15 loss: 0.9450 Acc: 62.5000% F1: 0.430 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 10/15 loss: 0.9598 Acc: 53.1250% F1: 0.362 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 11/15 loss: 0.9124 Acc: 56.2500% F1: 0.343 Time: 0.93s (0.02s)
Fold 0 train - epoch: 1/5 iter: 12/15 loss: 1.0698 Acc: 43.7500% F1: 0.301 Time: 0.94s (0.03s)
Fold 0 train - epoch: 1/5 iter: 13/15 loss: 0.9050 Acc: 43.7500% F1: 0.263 Time: 0.93s (0.03s)
Fold 0 train - epoch: 1/5 iter: 14/15 loss: 0.5728 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 1 train Avg acc: 54.4444% F1: 0.2972 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7510 Acc: 65.6250% F1: 0.396 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 1/5 iter: 1/2 loss: 1.4994 Acc: 5.5556% F1: 0.044 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 1 train-dev Avg acc: 44.0000% F1: 0.2315 *
*********************************************************
Performing epoch 2 of 5
Fold 0 train - epoch: 2/5 iter: 0/15 loss: 0.8637 Acc: 56.2500% F1: 0.326 Time: 0.95s (0.00s)
Fold 0 train - epoch: 2/5 iter: 1/15 loss: 0.8293 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 2/15 loss: 0.9052 Acc: 53.1250% F1: 0.275 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 3/15 loss: 0.8106 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.36s)
Fold 0 train - epoch: 2/5 iter: 4/15 loss: 0.9560 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 5/15 loss: 0.8896 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 6/15 loss: 0.7309 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 7/15 loss: 0.9525 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 8/15 loss: 0.9420 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 9/15 loss: 0.9703 Acc: 50.0000% F1: 0.265 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/5 iter: 10/15 loss: 0.9352 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 11/15 loss: 0.8432 Acc: 65.6250% F1: 0.327 Time: 0.94s (0.03s)
Fold 0 train - epoch: 2/5 iter: 12/15 loss: 0.9513 Acc: 56.2500% F1: 0.361 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 13/15 loss: 0.8196 Acc: 68.7500% F1: 0.409 Time: 0.93s (0.03s)
Fold 0 train - epoch: 2/5 iter: 14/15 loss: 0.3804 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 2 train Avg acc: 56.6667% F1: 0.2855 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8439 Acc: 65.6250% F1: 0.380 Time: 0.34s (0.00s)
Fold 0 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4062 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.3010 *
*********************************************************
Performing epoch 3 of 5
Fold 0 train - epoch: 3/5 iter: 0/15 loss: 0.8209 Acc: 56.2500% F1: 0.449 Time: 0.94s (0.00s)
Fold 0 train - epoch: 3/5 iter: 1/15 loss: 0.7746 Acc: 75.0000% F1: 0.640 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 2/15 loss: 0.8431 Acc: 50.0000% F1: 0.269 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 3/15 loss: 0.7051 Acc: 68.7500% F1: 0.428 Time: 0.93s (0.22s)
Fold 0 train - epoch: 3/5 iter: 4/15 loss: 0.7925 Acc: 62.5000% F1: 0.509 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 5/15 loss: 0.8460 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 6/15 loss: 0.6633 Acc: 78.1250% F1: 0.469 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 7/15 loss: 0.8220 Acc: 53.1250% F1: 0.297 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 8/15 loss: 0.8658 Acc: 59.3750% F1: 0.316 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 9/15 loss: 0.9135 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 10/15 loss: 0.8413 Acc: 71.8750% F1: 0.620 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 11/15 loss: 0.7376 Acc: 62.5000% F1: 0.532 Time: 0.94s (0.03s)
Fold 0 train - epoch: 3/5 iter: 12/15 loss: 0.9337 Acc: 68.7500% F1: 0.585 Time: 0.94s (0.03s)
Fold 0 train - epoch: 3/5 iter: 13/15 loss: 0.6722 Acc: 78.1250% F1: 0.749 Time: 0.93s (0.03s)
Fold 0 train - epoch: 3/5 iter: 14/15 loss: 0.1156 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 3 train Avg acc: 64.6667% F1: 0.4997 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8309 Acc: 62.5000% F1: 0.371 Time: 0.33s (0.00s)
Fold 0 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6231 Acc: 11.1111% F1: 0.083 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 3 train-dev Avg acc: 44.0000% F1: 0.2963 *
*********************************************************
Performing epoch 4 of 5
Fold 0 train - epoch: 4/5 iter: 0/15 loss: 0.7137 Acc: 75.0000% F1: 0.712 Time: 0.95s (0.00s)
Fold 0 train - epoch: 4/5 iter: 1/15 loss: 0.6400 Acc: 78.1250% F1: 0.716 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 2/15 loss: 0.7324 Acc: 62.5000% F1: 0.584 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 3/15 loss: 0.5699 Acc: 71.8750% F1: 0.585 Time: 0.93s (0.28s)
Fold 0 train - epoch: 4/5 iter: 4/15 loss: 0.6941 Acc: 65.6250% F1: 0.620 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 5/15 loss: 0.6938 Acc: 75.0000% F1: 0.628 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 6/15 loss: 0.5660 Acc: 81.2500% F1: 0.719 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 7/15 loss: 0.6812 Acc: 62.5000% F1: 0.406 Time: 0.93s (0.06s)
Fold 0 train - epoch: 4/5 iter: 8/15 loss: 0.7106 Acc: 71.8750% F1: 0.633 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 9/15 loss: 0.7197 Acc: 71.8750% F1: 0.631 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 10/15 loss: 0.7153 Acc: 68.7500% F1: 0.591 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 11/15 loss: 0.5432 Acc: 81.2500% F1: 0.791 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 12/15 loss: 0.6718 Acc: 78.1250% F1: 0.722 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 13/15 loss: 0.6811 Acc: 71.8750% F1: 0.663 Time: 0.93s (0.03s)
Fold 0 train - epoch: 4/5 iter: 14/15 loss: 0.0403 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.03s)
*****************************************************
* Fold 0 Epoch 4 train Avg acc: 72.6667% F1: 0.6672 *
*****************************************************
Saving model...
Fold 0 train-dev - epoch: 4/5 iter: 0/2 loss: 0.7579 Acc: 56.2500% F1: 0.240 Time: 0.35s (0.00s)
Fold 0 train-dev - epoch: 4/5 iter: 1/2 loss: 2.1809 Acc: 5.5556% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 0 Epoch 4 train-dev Avg acc: 38.0000% F1: 0.2532 *
*********************************************************
Creating 1 distributed models for fold 1...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[ 1.3939e-03, -6.8716e-02, -2.5557e-01,  2.8171e-02, -8.7808e-03,
         -2.5491e-01,  2.4635e-01, -1.7451e-01, -2.3248e-02,  2.3335e-01,
          2.6176e-01,  1.3909e-01, -1.2589e-01,  1.6319e-01,  2.4349e-02,
         -1.3283e-01,  4.8036e-02,  4.0980e-01, -2.1196e-01, -2.3831e-01,
         -9.6281e-03, -5.5775e-02,  1.1464e-02, -2.9264e-01,  9.9885e-02,
         -2.0102e-01, -1.9214e-01, -3.0318e-01, -4.2442e-02, -5.4449e-02,
         -1.2967e-04, -7.1719e-02,  2.0007e-01,  9.2549e-03,  5.3880e-02,
         -6.0826e-02,  8.9025e-02,  2.9168e-01,  2.3036e-01, -1.1622e-01,
         -6.1775e-02,  1.8710e-01, -1.6608e-01, -2.2137e-01,  1.6387e-01,
         -1.2285e-01,  3.0026e-01,  2.8564e-01, -3.5229e-01, -1.0878e-01,
          4.0280e-01,  1.5114e-01,  1.1360e-02, -2.0095e-02, -1.3847e-01,
          7.2091e-04, -1.1731e-01, -2.7096e-01,  2.7509e-02,  1.0600e-01,
          4.5664e-02,  1.3611e-01, -1.6287e-01, -2.6011e-01, -8.1596e-02,
          1.9391e-01, -6.8843e-02,  6.6795e-02,  3.0044e-01,  1.4518e-04,
          1.3836e-01, -2.1662e-01, -1.4751e-01, -4.5692e-03,  7.7596e-03,
          1.5091e-01, -1.9701e-01,  3.9364e-02,  2.4326e-02, -1.4290e-01,
         -2.3443e-01, -2.8338e-01,  2.4571e-01, -3.4772e-01, -2.1874e-01,
          4.0258e-02,  1.0768e-01,  2.4987e-01,  2.3627e-01, -1.1554e-01,
          2.1028e-01,  1.6711e-01, -1.5543e-01, -1.8313e-02, -3.3042e-01,
         -2.3139e-01,  1.1625e-01, -3.2922e-01, -1.7406e-01, -2.3016e-01,
         -1.3942e-01, -4.9668e-02, -2.7628e-01,  1.0165e-02,  8.2532e-02,
          6.4771e-02,  1.9546e-01, -3.2410e-01,  1.8964e-01, -1.6898e-01,
         -2.2098e-01, -1.2092e-01, -1.3189e-02,  2.6864e-02,  2.7784e-01,
         -6.3231e-02,  6.4781e-02,  3.4362e-01, -2.2732e-02, -3.5275e-01,
          9.8142e-03, -2.4852e-01, -2.7906e-02, -5.1745e-02, -9.5929e-02,
          1.7100e-02,  4.3980e-02, -4.3783e-02,  3.7871e-01,  1.2682e-01,
         -3.2154e-01, -2.1401e-01,  6.4307e-02,  7.8588e-02,  2.3951e-01,
          1.3997e-02, -7.8034e-02, -2.2260e-03,  1.9591e-02, -2.4945e-01,
          8.2607e-02, -3.2764e-02,  1.7143e-02,  2.0525e-01, -5.7016e-02,
          1.8229e-01, -2.7006e-02,  1.1357e-01,  9.7994e-02,  5.1746e-02,
          7.9860e-02,  3.8248e-02, -2.3227e-01, -1.3862e-01,  6.6153e-02,
          2.6249e-01, -5.7370e-02, -1.3890e-01, -4.0898e-01, -2.8864e-01,
         -2.3905e-02,  2.0195e-01,  1.1847e-01, -2.4378e-01, -1.7138e-01,
         -1.7932e-01,  2.7026e-01, -1.0431e-01,  8.6107e-02, -1.0131e-01,
         -8.2868e-02,  9.2653e-02, -3.3071e-02, -3.0312e-01, -2.7937e-01,
         -4.0222e-01, -3.8210e-01,  1.1642e-01, -3.3853e-02,  7.7303e-02,
          1.2355e-01, -2.2809e-01,  4.3379e-02,  1.0617e-02, -1.7532e-01,
         -1.5698e-01,  1.7562e-01, -6.4621e-02, -2.2386e-01, -5.4332e-02,
         -1.4223e-01,  1.1155e-01,  2.3546e-01,  3.8853e-02,  1.1227e-01,
         -2.9981e-02, -3.5384e-02, -1.3603e-01,  3.1001e-01, -1.4615e-01,
          2.5058e-02, -9.1489e-02, -2.1435e-01,  3.0697e-02, -2.5929e-02,
          3.2241e-01,  3.7920e-01,  2.0833e-01,  2.8212e-01, -1.0324e-01,
         -1.6585e-01, -1.8386e-01,  2.0189e-02,  2.7767e-01, -1.3822e-01,
         -1.3711e-01,  9.5865e-03,  1.1056e-02, -1.0575e-01, -5.5155e-02,
          1.1809e-01, -1.2300e-01,  2.4179e-01, -2.8298e-02,  7.4502e-02,
         -3.0307e-01,  2.4275e-02,  7.3545e-02, -6.9410e-02,  1.2794e-01,
          7.5693e-02,  1.5065e-01, -9.7550e-02,  3.4761e-01,  2.7095e-01,
         -4.5170e-02, -1.2539e-01,  3.8548e-03,  2.8808e-01, -9.7298e-02,
          3.0336e-02, -3.0973e-01,  7.9866e-02,  2.3738e-01,  1.2550e-01,
          7.3314e-03, -1.5217e-01,  2.0428e-01, -3.5740e-02,  1.4864e-01,
         -3.2093e-01, -5.4270e-02,  1.2042e-02, -1.4123e-01, -1.0701e-01,
         -3.3997e-01, -3.5408e-02, -1.8080e-01,  1.0898e-01,  1.1176e-01,
          1.4616e-01,  5.3214e-02, -8.9582e-02, -2.4846e-01, -2.1512e-01,
         -1.8585e-01, -7.4543e-02,  1.8495e-01,  1.3112e-01, -2.9189e-01,
         -2.3716e-01,  1.4231e-01,  1.8316e-01,  1.1146e-01,  1.0917e-01,
         -7.8276e-02, -3.5784e-01, -2.0579e-01, -1.9995e-01, -2.0056e-02,
          1.9377e-01,  2.1209e-01,  1.1557e-01,  1.5575e-01,  3.7109e-02,
         -1.9799e-01,  1.2692e-01, -1.2687e-01,  3.2986e-01, -4.0038e-01,
         -5.4040e-02,  1.9603e-01, -1.0432e-01, -4.4281e-02, -9.4947e-02,
          1.9225e-01,  2.4538e-01,  1.6693e-01,  1.7623e-02,  2.7655e-01,
         -2.7252e-01, -3.5374e-02,  2.1649e-01,  1.1542e-01, -3.2575e-01,
         -9.5898e-02, -1.7769e-01, -1.7478e-01,  1.2413e-02, -9.4918e-02,
         -1.7591e-01, -7.7178e-02,  8.4715e-02,  1.1410e-01,  2.3401e-01,
          4.6640e-02,  1.6343e-02,  8.4249e-02, -9.6963e-02,  4.9761e-03,
         -1.2689e-01, -8.9776e-02,  1.4109e-01, -2.3152e-01,  3.0876e-01,
         -2.1192e-01,  2.8687e-01, -1.6530e-01,  1.0156e-01,  1.3087e-01,
         -1.3041e-01,  2.9489e-01, -1.7078e-01,  2.2375e-01,  1.1248e-01,
          7.6252e-02, -2.7074e-02,  7.4888e-02, -2.2466e-01,  5.8946e-02,
         -1.5730e-01,  1.9001e-01,  1.6656e-01,  2.3558e-01,  8.8893e-02,
          4.6385e-02, -4.6853e-03, -3.9195e-02, -6.8834e-02,  6.7098e-02,
         -2.5421e-01, -1.3098e-01,  3.3280e-01, -1.1145e-01,  3.0578e-01,
          1.4134e-01, -7.8914e-02,  1.6024e-01, -2.5977e-01,  4.2665e-02,
          1.7734e-01, -1.5616e-01, -3.0725e-01, -1.6099e-01, -9.4044e-02,
          5.9506e-02,  2.2976e-01, -1.7763e-01, -2.6950e-01,  3.8000e-01,
          1.7562e-01,  3.7771e-02,  1.0278e-01,  3.2222e-01, -2.1560e-02,
         -1.5225e-01, -2.3036e-01, -1.3021e-01, -3.0180e-01,  2.4546e-01,
          6.7754e-02, -3.0570e-01, -2.9963e-01,  1.5654e-02, -1.8943e-01,
         -2.3144e-01,  2.8879e-01,  7.4722e-02, -1.5568e-01, -2.0820e-01,
          4.9041e-02,  3.9942e-01,  4.3492e-02,  8.4227e-02,  1.3005e-01,
         -9.3410e-02,  2.1089e-01,  8.5407e-02, -2.3646e-01,  1.1235e-01,
          2.1036e-01, -1.5971e-01,  7.9207e-02,  4.4568e-03, -2.2224e-01,
          3.1588e-02, -2.5247e-01, -1.8240e-01, -8.3811e-02, -3.5396e-01,
         -9.2816e-02,  3.5115e-02,  2.7964e-01, -9.2329e-02, -3.5224e-01,
         -2.4336e-01,  1.5316e-01, -1.8951e-01, -1.5665e-01, -2.2028e-01,
         -2.0556e-01, -1.5417e-01,  4.3107e-02, -1.4996e-01, -1.7838e-01,
         -1.0692e-01, -1.7765e-01, -8.7658e-03,  1.6875e-01, -2.1353e-01,
          1.7508e-01, -8.9377e-02, -1.9662e-01,  2.5831e-02, -5.6539e-02,
          5.4678e-02,  2.4562e-01,  9.2557e-02, -5.1224e-02, -2.1296e-02,
         -2.0423e-01,  4.4212e-03, -1.5627e-01,  1.5368e-01, -9.0215e-02,
          4.5537e-02,  2.7957e-01, -1.7029e-01,  2.9199e-01, -9.2008e-02,
         -1.1972e-01, -1.1973e-01, -3.9062e-01, -6.3714e-03, -1.8681e-02,
          4.3476e-02,  1.0941e-01,  1.5459e-02, -2.8024e-02, -2.9083e-01,
         -2.1282e-01,  1.7896e-01, -1.1782e-01, -2.9580e-01,  9.5791e-02,
         -1.6213e-01, -1.0290e-03,  9.6139e-02, -6.0271e-02,  7.4006e-02,
          1.4452e-01, -6.5647e-02,  2.0084e-01, -1.4069e-02, -1.2897e-01,
          2.6349e-01, -2.3983e-01,  1.7705e-02,  1.7037e-01,  3.0223e-02,
         -2.9845e-01,  1.0508e-01, -1.6406e-01, -3.5936e-02, -8.8760e-02,
         -2.6679e-01,  3.3564e-01,  3.7482e-04,  5.0903e-02, -7.3588e-02,
         -1.6273e-01, -5.2870e-02, -2.3405e-01, -7.3421e-02,  6.7705e-03,
         -1.6738e-01, -3.5341e-02,  1.6683e-01, -1.5272e-01,  3.9206e-01,
          2.2943e-01, -1.5948e-01, -2.8566e-02, -1.0889e-02, -2.4224e-01,
          6.9751e-02,  9.7223e-03, -2.8483e-01, -6.3500e-02,  1.8432e-01,
          4.8258e-02,  3.6758e-01, -1.7792e-01,  1.7039e-02,  3.1149e-01,
          9.3328e-02, -2.3158e-01, -3.5395e-01, -1.0560e-01,  2.2844e-01,
          8.0793e-02,  1.4810e-01, -3.0206e-01, -3.4022e-01,  1.7620e-01,
          4.3809e-02,  9.0087e-02,  1.5914e-01, -6.8204e-02,  2.5665e-01,
         -1.8012e-01,  3.7484e-02,  4.8088e-02,  2.3510e-02, -1.1832e-01,
          1.0292e-01,  1.4461e-01,  9.2944e-02, -1.4286e-03,  1.8287e-01,
          1.7623e-01,  6.0158e-02,  1.7079e-01,  1.9798e-01, -1.8351e-01,
         -4.1477e-01,  2.4582e-01,  1.0249e-01, -3.3297e-02, -2.0311e-01,
         -2.5807e-01, -2.9441e-02,  2.5160e-01, -2.8196e-01, -2.2125e-01,
         -1.3952e-01, -5.6166e-02,  2.8915e-02, -1.9787e-01, -1.6558e-01,
         -1.7816e-01, -3.1428e-01, -1.7320e-01,  1.2468e-02, -3.4252e-01,
         -2.5784e-01, -7.1416e-02, -9.2996e-02,  1.6461e-01, -3.2702e-01,
          1.4028e-01,  2.0673e-01, -5.7455e-02, -2.6848e-01,  1.1147e-01,
         -4.8179e-02,  9.7699e-02, -2.2503e-01, -1.3832e-01,  1.9701e-02,
         -8.6869e-02, -4.8751e-02,  6.2277e-02, -3.3196e-01,  2.0513e-01,
          4.2906e-02, -3.1485e-01, -1.9783e-01, -1.8252e-01,  8.9203e-03,
          2.3651e-01, -1.7068e-01,  9.3493e-02,  1.2389e-01, -2.1146e-01,
          8.2649e-03,  1.5572e-01, -2.2668e-01,  6.7394e-02,  1.9482e-02,
          7.9862e-02, -7.1562e-02,  8.8041e-02,  2.9078e-01, -2.9057e-01,
          1.9924e-01,  7.9529e-02,  2.3030e-01, -1.8758e-01, -2.1552e-01,
          1.6985e-01, -1.6580e-01, -2.1389e-01, -2.4635e-01, -1.9783e-01,
          7.4881e-02, -9.2235e-02,  5.3853e-02, -2.7720e-01, -4.0823e-02,
          2.2342e-01,  1.6657e-01, -1.2662e-01, -1.9808e-02,  5.6801e-02,
          2.1904e-02,  6.3948e-02, -2.0601e-01, -2.2994e-01, -1.4389e-01,
          5.3174e-02,  6.4131e-02,  4.4927e-02, -3.7137e-01, -1.5741e-01,
         -2.5703e-01,  1.2718e-01,  2.0193e-02,  3.8737e-02, -7.7474e-02,
          1.9992e-02, -1.8949e-01, -1.0489e-01, -6.3354e-02, -2.5072e-01,
         -2.2850e-01, -1.9215e-01, -2.7560e-01, -5.5963e-02, -1.2828e-01,
          1.2989e-01,  8.0240e-02,  1.1768e-01,  3.6903e-01,  3.2748e-01,
          2.8244e-01,  7.6465e-02, -1.3270e-01,  1.6062e-01,  1.6020e-02,
          1.8847e-01, -1.0488e-01, -2.7101e-01,  2.8572e-01,  2.2558e-01,
         -1.0301e-01,  1.4395e-01,  1.6343e-01, -2.3641e-01, -1.2907e-01,
         -2.7267e-02,  2.6302e-01, -1.8550e-01,  2.4225e-01,  1.8058e-01,
         -2.0332e-01, -1.1652e-01,  2.2879e-01, -6.5583e-02,  3.0162e-02,
          5.5261e-02,  3.3168e-02,  1.4287e-03,  2.4233e-01, -2.5623e-01,
          1.4555e-01, -9.7221e-02, -1.4414e-01,  2.5351e-01, -1.6486e-01,
          8.8751e-03, -1.3138e-01, -5.0337e-02, -2.3413e-01, -1.2613e-01,
          1.0165e-01,  1.7950e-01, -4.9932e-02,  1.9990e-01, -2.5763e-01,
         -3.1997e-01, -1.8444e-01, -2.4811e-01, -6.7918e-02,  2.8393e-01,
          3.9342e-02, -3.8641e-01, -2.4957e-01, -4.2098e-02,  1.2375e-01,
         -2.3292e-01, -1.2942e-01, -9.9817e-02,  2.1444e-02, -2.8142e-02,
         -6.4317e-02, -2.3902e-01, -3.0746e-01, -8.9892e-02, -1.2646e-01,
         -3.2790e-01,  2.3242e-01,  9.5534e-02, -1.0982e-01,  2.6479e-01,
          1.5996e-01, -3.1966e-01, -1.9431e-01,  7.6029e-02, -2.0056e-01,
         -4.4122e-02, -1.9894e-01, -3.2973e-01,  1.0175e-01, -3.6016e-02,
         -1.1808e-01, -1.6042e-01, -2.7750e-01,  4.7654e-02, -6.1258e-02,
         -4.5565e-01, -1.4322e-01, -1.6719e-01,  1.1150e-01, -6.1786e-02,
         -1.3581e-01, -1.2923e-01, -8.6849e-02, -2.0869e-01, -9.5694e-02,
         -3.2178e-01,  4.5601e-02,  9.0810e-02, -3.4218e-01, -1.2418e-01,
         -2.1381e-01, -1.2038e-01, -4.3754e-02,  6.3526e-02,  9.4710e-02,
          1.8775e-02, -2.5421e-01,  9.3967e-03,  2.0880e-01,  1.6493e-01,
         -2.8366e-01, -1.0564e-01,  2.7419e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train - epoch: 0/5 iter: 0/15 loss: 1.0610 Acc: 43.7500% F1: 0.315 Time: 0.96s (0.00s)
Fold 1 train - epoch: 0/5 iter: 1/15 loss: 0.9567 Acc: 53.1250% F1: 0.231 Time: 0.93s (1.22s)
Fold 1 train - epoch: 0/5 iter: 2/15 loss: 1.1307 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 3/15 loss: 0.9962 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.09s)
Fold 1 train - epoch: 0/5 iter: 4/15 loss: 1.0099 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 5/15 loss: 0.9172 Acc: 62.5000% F1: 0.358 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 6/15 loss: 0.9187 Acc: 37.5000% F1: 0.208 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 7/15 loss: 0.8589 Acc: 43.7500% F1: 0.282 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 8/15 loss: 1.1237 Acc: 46.8750% F1: 0.313 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 9/15 loss: 0.9943 Acc: 50.0000% F1: 0.343 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 10/15 loss: 0.9694 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 11/15 loss: 0.9457 Acc: 53.1250% F1: 0.306 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 12/15 loss: 1.0265 Acc: 46.8750% F1: 0.217 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 13/15 loss: 0.9566 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 1 train - epoch: 0/5 iter: 14/15 loss: 0.5562 Acc: 100.0000% F1: 1.000 Time: 0.09s (0.02s)
*****************************************************
* Fold 1 Epoch 0 train Avg acc: 50.0000% F1: 0.2953 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 1 train-dev - epoch: 0/5 iter: 0/2 loss: 0.6352 Acc: 84.3750% F1: 0.458 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6847 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 1 train - epoch: 1/5 iter: 0/15 loss: 0.8585 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.00s)
Fold 1 train - epoch: 1/5 iter: 1/15 loss: 0.9528 Acc: 56.2500% F1: 0.240 Time: 0.93s (1.20s)
Fold 1 train - epoch: 1/5 iter: 2/15 loss: 0.9883 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 3/15 loss: 0.8830 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 1 train - epoch: 1/5 iter: 4/15 loss: 1.0335 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 5/15 loss: 0.9372 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 6/15 loss: 0.7768 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 7/15 loss: 0.8960 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 8/15 loss: 1.0150 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 9/15 loss: 0.9813 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 10/15 loss: 0.9613 Acc: 53.1250% F1: 0.335 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 11/15 loss: 0.9358 Acc: 46.8750% F1: 0.251 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 12/15 loss: 1.0178 Acc: 46.8750% F1: 0.320 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 13/15 loss: 0.9117 Acc: 50.0000% F1: 0.292 Time: 0.93s (0.02s)
Fold 1 train - epoch: 1/5 iter: 14/15 loss: 0.6468 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 1 train Avg acc: 53.7778% F1: 0.2738 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 1/5 iter: 0/2 loss: 0.8074 Acc: 53.1250% F1: 0.347 Time: 0.35s (0.00s)
Fold 1 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3366 Acc: 22.2222% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 1 train-dev Avg acc: 42.0000% F1: 0.2683 *
*********************************************************
Performing epoch 2 of 5
Fold 1 train - epoch: 2/5 iter: 0/15 loss: 0.8622 Acc: 56.2500% F1: 0.350 Time: 0.95s (0.00s)
Fold 1 train - epoch: 2/5 iter: 1/15 loss: 0.8606 Acc: 62.5000% F1: 0.389 Time: 0.93s (1.20s)
Fold 1 train - epoch: 2/5 iter: 2/15 loss: 0.8886 Acc: 56.2500% F1: 0.321 Time: 0.92s (0.02s)
Fold 1 train - epoch: 2/5 iter: 3/15 loss: 0.8436 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 1 train - epoch: 2/5 iter: 4/15 loss: 0.9401 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 5/15 loss: 0.8558 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 6/15 loss: 0.7843 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 7/15 loss: 0.9105 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 8/15 loss: 0.9647 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 9/15 loss: 0.9392 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 10/15 loss: 0.9642 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 1 train - epoch: 2/5 iter: 11/15 loss: 0.8007 Acc: 62.5000% F1: 0.256 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 12/15 loss: 0.9533 Acc: 59.3750% F1: 0.352 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 13/15 loss: 0.8509 Acc: 62.5000% F1: 0.351 Time: 0.93s (0.02s)
Fold 1 train - epoch: 2/5 iter: 14/15 loss: 0.3530 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 2 train Avg acc: 56.8889% F1: 0.2920 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 2/5 iter: 0/2 loss: 0.8732 Acc: 50.0000% F1: 0.333 Time: 0.33s (0.00s)
Fold 1 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3293 Acc: 22.2222% F1: 0.133 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 2 train-dev Avg acc: 40.0000% F1: 0.2612 *
*********************************************************
Performing epoch 3 of 5
Fold 1 train - epoch: 3/5 iter: 0/15 loss: 0.8462 Acc: 59.3750% F1: 0.341 Time: 0.95s (0.00s)
Fold 1 train - epoch: 3/5 iter: 1/15 loss: 0.8284 Acc: 65.6250% F1: 0.556 Time: 0.92s (1.22s)
Fold 1 train - epoch: 3/5 iter: 2/15 loss: 0.8285 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.03s)
Fold 1 train - epoch: 3/5 iter: 3/15 loss: 0.7342 Acc: 75.0000% F1: 0.509 Time: 0.93s (0.06s)
Fold 1 train - epoch: 3/5 iter: 4/15 loss: 0.7754 Acc: 71.8750% F1: 0.607 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 5/15 loss: 0.8436 Acc: 56.2500% F1: 0.327 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 6/15 loss: 0.7251 Acc: 68.7500% F1: 0.336 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 7/15 loss: 0.8067 Acc: 53.1250% F1: 0.317 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 8/15 loss: 0.9194 Acc: 56.2500% F1: 0.337 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 9/15 loss: 0.9254 Acc: 62.5000% F1: 0.437 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 10/15 loss: 0.9409 Acc: 53.1250% F1: 0.356 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 11/15 loss: 0.6684 Acc: 81.2500% F1: 0.777 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 12/15 loss: 0.9194 Acc: 65.6250% F1: 0.551 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 13/15 loss: 0.8638 Acc: 59.3750% F1: 0.307 Time: 0.93s (0.02s)
Fold 1 train - epoch: 3/5 iter: 14/15 loss: 0.1128 Acc: 100.0000% F1: 1.000 Time: 0.07s (0.02s)
*****************************************************
* Fold 1 Epoch 3 train Avg acc: 63.5556% F1: 0.4607 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 3/5 iter: 0/2 loss: 0.9530 Acc: 50.0000% F1: 0.227 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 3/5 iter: 1/2 loss: 1.5409 Acc: 11.1111% F1: 0.074 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 3 train-dev Avg acc: 36.0000% F1: 0.2399 *
*********************************************************
Performing epoch 4 of 5
Fold 1 train - epoch: 4/5 iter: 0/15 loss: 0.7226 Acc: 65.6250% F1: 0.604 Time: 0.94s (0.00s)
Fold 1 train - epoch: 4/5 iter: 1/15 loss: 0.7554 Acc: 78.1250% F1: 0.723 Time: 0.93s (1.28s)
Fold 1 train - epoch: 4/5 iter: 2/15 loss: 0.7250 Acc: 71.8750% F1: 0.664 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 3/15 loss: 0.6315 Acc: 78.1250% F1: 0.774 Time: 0.93s (0.03s)
Fold 1 train - epoch: 4/5 iter: 4/15 loss: 0.6682 Acc: 71.8750% F1: 0.674 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 5/15 loss: 0.6840 Acc: 65.6250% F1: 0.524 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 6/15 loss: 0.6279 Acc: 71.8750% F1: 0.448 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 7/15 loss: 0.7174 Acc: 62.5000% F1: 0.414 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 8/15 loss: 0.8523 Acc: 65.6250% F1: 0.517 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 9/15 loss: 0.7848 Acc: 75.0000% F1: 0.534 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 10/15 loss: 0.7992 Acc: 71.8750% F1: 0.517 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 11/15 loss: 0.5753 Acc: 68.7500% F1: 0.686 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 12/15 loss: 0.8384 Acc: 68.7500% F1: 0.583 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 13/15 loss: 0.6529 Acc: 65.6250% F1: 0.495 Time: 0.93s (0.02s)
Fold 1 train - epoch: 4/5 iter: 14/15 loss: 0.0364 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 1 Epoch 4 train Avg acc: 70.2222% F1: 0.6118 *
*****************************************************
Saving model...
Fold 1 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0582 Acc: 50.0000% F1: 0.283 Time: 0.34s (0.00s)
Fold 1 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7169 Acc: 22.2222% F1: 0.186 Time: 0.18s (0.00s)
*********************************************************
* Fold 1 Epoch 4 train-dev Avg acc: 40.0000% F1: 0.3310 *
*********************************************************
Creating 1 distributed models for fold 2...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[ 1.3939e-03, -6.8716e-02, -2.5557e-01,  2.8171e-02, -8.7808e-03,
         -2.5491e-01,  2.4635e-01, -1.7451e-01, -2.3248e-02,  2.3335e-01,
          2.6176e-01,  1.3909e-01, -1.2589e-01,  1.6319e-01,  2.4349e-02,
         -1.3283e-01,  4.8036e-02,  4.0980e-01, -2.1196e-01, -2.3831e-01,
         -9.6281e-03, -5.5775e-02,  1.1464e-02, -2.9264e-01,  9.9885e-02,
         -2.0102e-01, -1.9214e-01, -3.0318e-01, -4.2442e-02, -5.4449e-02,
         -1.2967e-04, -7.1719e-02,  2.0007e-01,  9.2549e-03,  5.3880e-02,
         -6.0826e-02,  8.9025e-02,  2.9168e-01,  2.3036e-01, -1.1622e-01,
         -6.1775e-02,  1.8710e-01, -1.6608e-01, -2.2137e-01,  1.6387e-01,
         -1.2285e-01,  3.0026e-01,  2.8564e-01, -3.5229e-01, -1.0878e-01,
          4.0280e-01,  1.5114e-01,  1.1360e-02, -2.0095e-02, -1.3847e-01,
          7.2091e-04, -1.1731e-01, -2.7096e-01,  2.7509e-02,  1.0600e-01,
          4.5664e-02,  1.3611e-01, -1.6287e-01, -2.6011e-01, -8.1596e-02,
          1.9391e-01, -6.8843e-02,  6.6795e-02,  3.0044e-01,  1.4518e-04,
          1.3836e-01, -2.1662e-01, -1.4751e-01, -4.5692e-03,  7.7596e-03,
          1.5091e-01, -1.9701e-01,  3.9364e-02,  2.4326e-02, -1.4290e-01,
         -2.3443e-01, -2.8338e-01,  2.4571e-01, -3.4772e-01, -2.1874e-01,
          4.0258e-02,  1.0768e-01,  2.4987e-01,  2.3627e-01, -1.1554e-01,
          2.1028e-01,  1.6711e-01, -1.5543e-01, -1.8313e-02, -3.3042e-01,
         -2.3139e-01,  1.1625e-01, -3.2922e-01, -1.7406e-01, -2.3016e-01,
         -1.3942e-01, -4.9668e-02, -2.7628e-01,  1.0165e-02,  8.2532e-02,
          6.4771e-02,  1.9546e-01, -3.2410e-01,  1.8964e-01, -1.6898e-01,
         -2.2098e-01, -1.2092e-01, -1.3189e-02,  2.6864e-02,  2.7784e-01,
         -6.3231e-02,  6.4781e-02,  3.4362e-01, -2.2732e-02, -3.5275e-01,
          9.8142e-03, -2.4852e-01, -2.7906e-02, -5.1745e-02, -9.5929e-02,
          1.7100e-02,  4.3980e-02, -4.3783e-02,  3.7871e-01,  1.2682e-01,
         -3.2154e-01, -2.1401e-01,  6.4307e-02,  7.8588e-02,  2.3951e-01,
          1.3997e-02, -7.8034e-02, -2.2260e-03,  1.9591e-02, -2.4945e-01,
          8.2607e-02, -3.2764e-02,  1.7143e-02,  2.0525e-01, -5.7016e-02,
          1.8229e-01, -2.7006e-02,  1.1357e-01,  9.7994e-02,  5.1746e-02,
          7.9860e-02,  3.8248e-02, -2.3227e-01, -1.3862e-01,  6.6153e-02,
          2.6249e-01, -5.7370e-02, -1.3890e-01, -4.0898e-01, -2.8864e-01,
         -2.3905e-02,  2.0195e-01,  1.1847e-01, -2.4378e-01, -1.7138e-01,
         -1.7932e-01,  2.7026e-01, -1.0431e-01,  8.6107e-02, -1.0131e-01,
         -8.2868e-02,  9.2653e-02, -3.3071e-02, -3.0312e-01, -2.7937e-01,
         -4.0222e-01, -3.8210e-01,  1.1642e-01, -3.3853e-02,  7.7303e-02,
          1.2355e-01, -2.2809e-01,  4.3379e-02,  1.0617e-02, -1.7532e-01,
         -1.5698e-01,  1.7562e-01, -6.4621e-02, -2.2386e-01, -5.4332e-02,
         -1.4223e-01,  1.1155e-01,  2.3546e-01,  3.8853e-02,  1.1227e-01,
         -2.9981e-02, -3.5384e-02, -1.3603e-01,  3.1001e-01, -1.4615e-01,
          2.5058e-02, -9.1489e-02, -2.1435e-01,  3.0697e-02, -2.5929e-02,
          3.2241e-01,  3.7920e-01,  2.0833e-01,  2.8212e-01, -1.0324e-01,
         -1.6585e-01, -1.8386e-01,  2.0189e-02,  2.7767e-01, -1.3822e-01,
         -1.3711e-01,  9.5865e-03,  1.1056e-02, -1.0575e-01, -5.5155e-02,
          1.1809e-01, -1.2300e-01,  2.4179e-01, -2.8298e-02,  7.4502e-02,
         -3.0307e-01,  2.4275e-02,  7.3545e-02, -6.9410e-02,  1.2794e-01,
          7.5693e-02,  1.5065e-01, -9.7550e-02,  3.4761e-01,  2.7095e-01,
         -4.5170e-02, -1.2539e-01,  3.8548e-03,  2.8808e-01, -9.7298e-02,
          3.0336e-02, -3.0973e-01,  7.9866e-02,  2.3738e-01,  1.2550e-01,
          7.3314e-03, -1.5217e-01,  2.0428e-01, -3.5740e-02,  1.4864e-01,
         -3.2093e-01, -5.4270e-02,  1.2042e-02, -1.4123e-01, -1.0701e-01,
         -3.3997e-01, -3.5408e-02, -1.8080e-01,  1.0898e-01,  1.1176e-01,
          1.4616e-01,  5.3214e-02, -8.9582e-02, -2.4846e-01, -2.1512e-01,
         -1.8585e-01, -7.4543e-02,  1.8495e-01,  1.3112e-01, -2.9189e-01,
         -2.3716e-01,  1.4231e-01,  1.8316e-01,  1.1146e-01,  1.0917e-01,
         -7.8276e-02, -3.5784e-01, -2.0579e-01, -1.9995e-01, -2.0056e-02,
          1.9377e-01,  2.1209e-01,  1.1557e-01,  1.5575e-01,  3.7109e-02,
         -1.9799e-01,  1.2692e-01, -1.2687e-01,  3.2986e-01, -4.0038e-01,
         -5.4040e-02,  1.9603e-01, -1.0432e-01, -4.4281e-02, -9.4947e-02,
          1.9225e-01,  2.4538e-01,  1.6693e-01,  1.7623e-02,  2.7655e-01,
         -2.7252e-01, -3.5374e-02,  2.1649e-01,  1.1542e-01, -3.2575e-01,
         -9.5898e-02, -1.7769e-01, -1.7478e-01,  1.2413e-02, -9.4918e-02,
         -1.7591e-01, -7.7178e-02,  8.4715e-02,  1.1410e-01,  2.3401e-01,
          4.6640e-02,  1.6343e-02,  8.4249e-02, -9.6963e-02,  4.9761e-03,
         -1.2689e-01, -8.9776e-02,  1.4109e-01, -2.3152e-01,  3.0876e-01,
         -2.1192e-01,  2.8687e-01, -1.6530e-01,  1.0156e-01,  1.3087e-01,
         -1.3041e-01,  2.9489e-01, -1.7078e-01,  2.2375e-01,  1.1248e-01,
          7.6252e-02, -2.7074e-02,  7.4888e-02, -2.2466e-01,  5.8946e-02,
         -1.5730e-01,  1.9001e-01,  1.6656e-01,  2.3558e-01,  8.8893e-02,
          4.6385e-02, -4.6853e-03, -3.9195e-02, -6.8834e-02,  6.7098e-02,
         -2.5421e-01, -1.3098e-01,  3.3280e-01, -1.1145e-01,  3.0578e-01,
          1.4134e-01, -7.8914e-02,  1.6024e-01, -2.5977e-01,  4.2665e-02,
          1.7734e-01, -1.5616e-01, -3.0725e-01, -1.6099e-01, -9.4044e-02,
          5.9506e-02,  2.2976e-01, -1.7763e-01, -2.6950e-01,  3.8000e-01,
          1.7562e-01,  3.7771e-02,  1.0278e-01,  3.2222e-01, -2.1560e-02,
         -1.5225e-01, -2.3036e-01, -1.3021e-01, -3.0180e-01,  2.4546e-01,
          6.7754e-02, -3.0570e-01, -2.9963e-01,  1.5654e-02, -1.8943e-01,
         -2.3144e-01,  2.8879e-01,  7.4722e-02, -1.5568e-01, -2.0820e-01,
          4.9041e-02,  3.9942e-01,  4.3492e-02,  8.4227e-02,  1.3005e-01,
         -9.3410e-02,  2.1089e-01,  8.5407e-02, -2.3646e-01,  1.1235e-01,
          2.1036e-01, -1.5971e-01,  7.9207e-02,  4.4568e-03, -2.2224e-01,
          3.1588e-02, -2.5247e-01, -1.8240e-01, -8.3811e-02, -3.5396e-01,
         -9.2816e-02,  3.5115e-02,  2.7964e-01, -9.2329e-02, -3.5224e-01,
         -2.4336e-01,  1.5316e-01, -1.8951e-01, -1.5665e-01, -2.2028e-01,
         -2.0556e-01, -1.5417e-01,  4.3107e-02, -1.4996e-01, -1.7838e-01,
         -1.0692e-01, -1.7765e-01, -8.7658e-03,  1.6875e-01, -2.1353e-01,
          1.7508e-01, -8.9377e-02, -1.9662e-01,  2.5831e-02, -5.6539e-02,
          5.4678e-02,  2.4562e-01,  9.2557e-02, -5.1224e-02, -2.1296e-02,
         -2.0423e-01,  4.4212e-03, -1.5627e-01,  1.5368e-01, -9.0215e-02,
          4.5537e-02,  2.7957e-01, -1.7029e-01,  2.9199e-01, -9.2008e-02,
         -1.1972e-01, -1.1973e-01, -3.9062e-01, -6.3714e-03, -1.8681e-02,
          4.3476e-02,  1.0941e-01,  1.5459e-02, -2.8024e-02, -2.9083e-01,
         -2.1282e-01,  1.7896e-01, -1.1782e-01, -2.9580e-01,  9.5791e-02,
         -1.6213e-01, -1.0290e-03,  9.6139e-02, -6.0271e-02,  7.4006e-02,
          1.4452e-01, -6.5647e-02,  2.0084e-01, -1.4069e-02, -1.2897e-01,
          2.6349e-01, -2.3983e-01,  1.7705e-02,  1.7037e-01,  3.0223e-02,
         -2.9845e-01,  1.0508e-01, -1.6406e-01, -3.5936e-02, -8.8760e-02,
         -2.6679e-01,  3.3564e-01,  3.7482e-04,  5.0903e-02, -7.3588e-02,
         -1.6273e-01, -5.2870e-02, -2.3405e-01, -7.3421e-02,  6.7705e-03,
         -1.6738e-01, -3.5341e-02,  1.6683e-01, -1.5272e-01,  3.9206e-01,
          2.2943e-01, -1.5948e-01, -2.8566e-02, -1.0889e-02, -2.4224e-01,
          6.9751e-02,  9.7223e-03, -2.8483e-01, -6.3500e-02,  1.8432e-01,
          4.8258e-02,  3.6758e-01, -1.7792e-01,  1.7039e-02,  3.1149e-01,
          9.3328e-02, -2.3158e-01, -3.5395e-01, -1.0560e-01,  2.2844e-01,
          8.0793e-02,  1.4810e-01, -3.0206e-01, -3.4022e-01,  1.7620e-01,
          4.3809e-02,  9.0087e-02,  1.5914e-01, -6.8204e-02,  2.5665e-01,
         -1.8012e-01,  3.7484e-02,  4.8088e-02,  2.3510e-02, -1.1832e-01,
          1.0292e-01,  1.4461e-01,  9.2944e-02, -1.4286e-03,  1.8287e-01,
          1.7623e-01,  6.0158e-02,  1.7079e-01,  1.9798e-01, -1.8351e-01,
         -4.1477e-01,  2.4582e-01,  1.0249e-01, -3.3297e-02, -2.0311e-01,
         -2.5807e-01, -2.9441e-02,  2.5160e-01, -2.8196e-01, -2.2125e-01,
         -1.3952e-01, -5.6166e-02,  2.8915e-02, -1.9787e-01, -1.6558e-01,
         -1.7816e-01, -3.1428e-01, -1.7320e-01,  1.2468e-02, -3.4252e-01,
         -2.5784e-01, -7.1416e-02, -9.2996e-02,  1.6461e-01, -3.2702e-01,
          1.4028e-01,  2.0673e-01, -5.7455e-02, -2.6848e-01,  1.1147e-01,
         -4.8179e-02,  9.7699e-02, -2.2503e-01, -1.3832e-01,  1.9701e-02,
         -8.6869e-02, -4.8751e-02,  6.2277e-02, -3.3196e-01,  2.0513e-01,
          4.2906e-02, -3.1485e-01, -1.9783e-01, -1.8252e-01,  8.9203e-03,
          2.3651e-01, -1.7068e-01,  9.3493e-02,  1.2389e-01, -2.1146e-01,
          8.2649e-03,  1.5572e-01, -2.2668e-01,  6.7394e-02,  1.9482e-02,
          7.9862e-02, -7.1562e-02,  8.8041e-02,  2.9078e-01, -2.9057e-01,
          1.9924e-01,  7.9529e-02,  2.3030e-01, -1.8758e-01, -2.1552e-01,
          1.6985e-01, -1.6580e-01, -2.1389e-01, -2.4635e-01, -1.9783e-01,
          7.4881e-02, -9.2235e-02,  5.3853e-02, -2.7720e-01, -4.0823e-02,
          2.2342e-01,  1.6657e-01, -1.2662e-01, -1.9808e-02,  5.6801e-02,
          2.1904e-02,  6.3948e-02, -2.0601e-01, -2.2994e-01, -1.4389e-01,
          5.3174e-02,  6.4131e-02,  4.4927e-02, -3.7137e-01, -1.5741e-01,
         -2.5703e-01,  1.2718e-01,  2.0193e-02,  3.8737e-02, -7.7474e-02,
          1.9992e-02, -1.8949e-01, -1.0489e-01, -6.3354e-02, -2.5072e-01,
         -2.2850e-01, -1.9215e-01, -2.7560e-01, -5.5963e-02, -1.2828e-01,
          1.2989e-01,  8.0240e-02,  1.1768e-01,  3.6903e-01,  3.2748e-01,
          2.8244e-01,  7.6465e-02, -1.3270e-01,  1.6062e-01,  1.6020e-02,
          1.8847e-01, -1.0488e-01, -2.7101e-01,  2.8572e-01,  2.2558e-01,
         -1.0301e-01,  1.4395e-01,  1.6343e-01, -2.3641e-01, -1.2907e-01,
         -2.7267e-02,  2.6302e-01, -1.8550e-01,  2.4225e-01,  1.8058e-01,
         -2.0332e-01, -1.1652e-01,  2.2879e-01, -6.5583e-02,  3.0162e-02,
          5.5261e-02,  3.3168e-02,  1.4287e-03,  2.4233e-01, -2.5623e-01,
          1.4555e-01, -9.7221e-02, -1.4414e-01,  2.5351e-01, -1.6486e-01,
          8.8751e-03, -1.3138e-01, -5.0337e-02, -2.3413e-01, -1.2613e-01,
          1.0165e-01,  1.7950e-01, -4.9932e-02,  1.9990e-01, -2.5763e-01,
         -3.1997e-01, -1.8444e-01, -2.4811e-01, -6.7918e-02,  2.8393e-01,
          3.9342e-02, -3.8641e-01, -2.4957e-01, -4.2098e-02,  1.2375e-01,
         -2.3292e-01, -1.2942e-01, -9.9817e-02,  2.1444e-02, -2.8142e-02,
         -6.4317e-02, -2.3902e-01, -3.0746e-01, -8.9892e-02, -1.2646e-01,
         -3.2790e-01,  2.3242e-01,  9.5534e-02, -1.0982e-01,  2.6479e-01,
          1.5996e-01, -3.1966e-01, -1.9431e-01,  7.6029e-02, -2.0056e-01,
         -4.4122e-02, -1.9894e-01, -3.2973e-01,  1.0175e-01, -3.6016e-02,
         -1.1808e-01, -1.6042e-01, -2.7750e-01,  4.7654e-02, -6.1258e-02,
         -4.5565e-01, -1.4322e-01, -1.6719e-01,  1.1150e-01, -6.1786e-02,
         -1.3581e-01, -1.2923e-01, -8.6849e-02, -2.0869e-01, -9.5694e-02,
         -3.2178e-01,  4.5601e-02,  9.0810e-02, -3.4218e-01, -1.2418e-01,
         -2.1381e-01, -1.2038e-01, -4.3754e-02,  6.3526e-02,  9.4710e-02,
          1.8775e-02, -2.5421e-01,  9.3967e-03,  2.0880e-01,  1.6493e-01,
         -2.8366e-01, -1.0564e-01,  2.7419e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train - epoch: 0/5 iter: 0/15 loss: 1.0925 Acc: 40.6250% F1: 0.363 Time: 0.96s (0.00s)
Fold 2 train - epoch: 0/5 iter: 1/15 loss: 0.9213 Acc: 56.2500% F1: 0.292 Time: 0.92s (0.03s)
Fold 2 train - epoch: 0/5 iter: 2/15 loss: 1.1079 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/5 iter: 3/15 loss: 0.9484 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/5 iter: 4/15 loss: 1.0295 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 2 train - epoch: 0/5 iter: 5/15 loss: 0.9361 Acc: 59.3750% F1: 0.344 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/5 iter: 6/15 loss: 0.9068 Acc: 56.2500% F1: 0.338 Time: 0.93s (0.39s)
Fold 2 train - epoch: 0/5 iter: 7/15 loss: 0.8828 Acc: 53.1250% F1: 0.352 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/5 iter: 8/15 loss: 1.1885 Acc: 28.1250% F1: 0.183 Time: 0.93s (0.70s)
Fold 2 train - epoch: 0/5 iter: 9/15 loss: 0.9883 Acc: 59.3750% F1: 0.410 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/5 iter: 10/15 loss: 0.9692 Acc: 46.8750% F1: 0.315 Time: 0.93s (0.18s)
Fold 2 train - epoch: 0/5 iter: 11/15 loss: 0.9275 Acc: 50.0000% F1: 0.291 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/5 iter: 12/15 loss: 1.0323 Acc: 59.3750% F1: 0.352 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/5 iter: 13/15 loss: 0.8963 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 2 train - epoch: 0/5 iter: 14/15 loss: 0.5163 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 0 train Avg acc: 52.0000% F1: 0.3243 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 2 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5555 Acc: 87.5000% F1: 0.467 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6747 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 2 train - epoch: 1/5 iter: 0/15 loss: 0.8726 Acc: 59.3750% F1: 0.378 Time: 0.95s (0.00s)
Fold 2 train - epoch: 1/5 iter: 1/15 loss: 0.9394 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 2/15 loss: 1.0378 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/5 iter: 3/15 loss: 0.8439 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 4/15 loss: 0.9681 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 5/15 loss: 0.9058 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.03s)
Fold 2 train - epoch: 1/5 iter: 6/15 loss: 0.7987 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.49s)
Fold 2 train - epoch: 1/5 iter: 7/15 loss: 0.9056 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 8/15 loss: 1.1051 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.75s)
Fold 2 train - epoch: 1/5 iter: 9/15 loss: 0.9965 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 10/15 loss: 0.9303 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.21s)
Fold 2 train - epoch: 1/5 iter: 11/15 loss: 0.8980 Acc: 56.2500% F1: 0.344 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 12/15 loss: 1.0315 Acc: 59.3750% F1: 0.413 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 13/15 loss: 0.9291 Acc: 46.8750% F1: 0.278 Time: 0.93s (0.02s)
Fold 2 train - epoch: 1/5 iter: 14/15 loss: 0.5746 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 1 train Avg acc: 55.7778% F1: 0.3084 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7701 Acc: 62.5000% F1: 0.451 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 1/5 iter: 1/2 loss: 1.2704 Acc: 22.2222% F1: 0.140 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.3053 *
*********************************************************
Performing epoch 2 of 5
Fold 2 train - epoch: 2/5 iter: 0/15 loss: 0.8519 Acc: 59.3750% F1: 0.367 Time: 0.94s (0.00s)
Fold 2 train - epoch: 2/5 iter: 1/15 loss: 0.8924 Acc: 56.2500% F1: 0.326 Time: 0.93s (0.12s)
Fold 2 train - epoch: 2/5 iter: 2/15 loss: 0.9134 Acc: 53.1250% F1: 0.275 Time: 0.93s (0.03s)
Fold 2 train - epoch: 2/5 iter: 3/15 loss: 0.7992 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 4/15 loss: 0.9115 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 5/15 loss: 0.9006 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 6/15 loss: 0.7678 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.34s)
Fold 2 train - epoch: 2/5 iter: 7/15 loss: 0.9375 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 8/15 loss: 1.0897 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.76s)
Fold 2 train - epoch: 2/5 iter: 9/15 loss: 1.0597 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 10/15 loss: 0.9355 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.13s)
Fold 2 train - epoch: 2/5 iter: 11/15 loss: 0.8634 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 12/15 loss: 0.9476 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 13/15 loss: 0.8571 Acc: 62.5000% F1: 0.314 Time: 0.93s (0.02s)
Fold 2 train - epoch: 2/5 iter: 14/15 loss: 0.3642 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 2 train Avg acc: 55.7778% F1: 0.2691 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7233 Acc: 71.8750% F1: 0.506 Time: 0.34s (0.00s)
Fold 2 train-dev - epoch: 2/5 iter: 1/2 loss: 1.3878 Acc: 16.6667% F1: 0.111 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 2 train-dev Avg acc: 52.0000% F1: 0.3142 *
*********************************************************
Performing epoch 3 of 5
Fold 2 train - epoch: 3/5 iter: 0/15 loss: 0.7984 Acc: 75.0000% F1: 0.638 Time: 0.94s (0.00s)
Fold 2 train - epoch: 3/5 iter: 1/15 loss: 0.7809 Acc: 68.7500% F1: 0.564 Time: 0.93s (0.11s)
Fold 2 train - epoch: 3/5 iter: 2/15 loss: 0.8702 Acc: 53.1250% F1: 0.306 Time: 0.93s (0.03s)
Fold 2 train - epoch: 3/5 iter: 3/15 loss: 0.7862 Acc: 68.7500% F1: 0.428 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/5 iter: 4/15 loss: 0.8011 Acc: 59.3750% F1: 0.370 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/5 iter: 5/15 loss: 0.8335 Acc: 62.5000% F1: 0.389 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/5 iter: 6/15 loss: 0.7305 Acc: 68.7500% F1: 0.336 Time: 0.93s (0.27s)
Fold 2 train - epoch: 3/5 iter: 7/15 loss: 0.8312 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/5 iter: 8/15 loss: 1.0850 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.74s)
Fold 2 train - epoch: 3/5 iter: 9/15 loss: 0.9958 Acc: 59.3750% F1: 0.386 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/5 iter: 10/15 loss: 0.8482 Acc: 65.6250% F1: 0.475 Time: 0.93s (0.11s)
Fold 2 train - epoch: 3/5 iter: 11/15 loss: 0.7709 Acc: 68.7500% F1: 0.457 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/5 iter: 12/15 loss: 0.9257 Acc: 56.2500% F1: 0.332 Time: 0.93s (0.02s)
Fold 2 train - epoch: 3/5 iter: 13/15 loss: 0.8083 Acc: 59.3750% F1: 0.301 Time: 0.94s (0.02s)
Fold 2 train - epoch: 3/5 iter: 14/15 loss: 0.2119 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 3 train Avg acc: 61.7778% F1: 0.4071 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7097 Acc: 78.1250% F1: 0.616 Time: 0.33s (0.00s)
Fold 2 train-dev - epoch: 3/5 iter: 1/2 loss: 1.5635 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 2 Epoch 3 train-dev Avg acc: 54.0000% F1: 0.3241 *
*********************************************************
Performing epoch 4 of 5
Fold 2 train - epoch: 4/5 iter: 0/15 loss: 0.7412 Acc: 62.5000% F1: 0.499 Time: 0.95s (0.00s)
Fold 2 train - epoch: 4/5 iter: 1/15 loss: 0.6851 Acc: 71.8750% F1: 0.664 Time: 0.93s (0.07s)
Fold 2 train - epoch: 4/5 iter: 2/15 loss: 0.8315 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.04s)
Fold 2 train - epoch: 4/5 iter: 3/15 loss: 0.6644 Acc: 71.8750% F1: 0.463 Time: 0.93s (0.04s)
Fold 2 train - epoch: 4/5 iter: 4/15 loss: 0.7220 Acc: 65.6250% F1: 0.559 Time: 0.93s (0.04s)
Fold 2 train - epoch: 4/5 iter: 5/15 loss: 0.6374 Acc: 71.8750% F1: 0.595 Time: 0.93s (0.04s)
Fold 2 train - epoch: 4/5 iter: 6/15 loss: 0.7036 Acc: 78.1250% F1: 0.491 Time: 0.93s (0.34s)
Fold 2 train - epoch: 4/5 iter: 7/15 loss: 0.7334 Acc: 59.3750% F1: 0.379 Time: 0.94s (0.03s)
Fold 2 train - epoch: 4/5 iter: 8/15 loss: 0.9826 Acc: 53.1250% F1: 0.358 Time: 0.93s (0.77s)
Fold 2 train - epoch: 4/5 iter: 9/15 loss: 0.9067 Acc: 53.1250% F1: 0.378 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/5 iter: 10/15 loss: 0.8366 Acc: 59.3750% F1: 0.437 Time: 0.93s (0.15s)
Fold 2 train - epoch: 4/5 iter: 11/15 loss: 0.6534 Acc: 65.6250% F1: 0.601 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/5 iter: 12/15 loss: 0.7811 Acc: 65.6250% F1: 0.537 Time: 0.94s (0.02s)
Fold 2 train - epoch: 4/5 iter: 13/15 loss: 0.7660 Acc: 59.3750% F1: 0.351 Time: 0.93s (0.02s)
Fold 2 train - epoch: 4/5 iter: 14/15 loss: 0.0368 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 2 Epoch 4 train Avg acc: 63.7778% F1: 0.5012 *
*****************************************************
Saving model...
Fold 2 train-dev - epoch: 4/5 iter: 0/2 loss: 0.6490 Acc: 78.1250% F1: 0.374 Time: 0.35s (0.00s)
Fold 2 train-dev - epoch: 4/5 iter: 1/2 loss: 1.8908 Acc: 11.1111% F1: 0.078 Time: 0.19s (0.00s)
*********************************************************
* Fold 2 Epoch 4 train-dev Avg acc: 54.0000% F1: 0.3153 *
*********************************************************
Creating 1 distributed models for fold 3...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[ 1.3939e-03, -6.8716e-02, -2.5557e-01,  2.8171e-02, -8.7808e-03,
         -2.5491e-01,  2.4635e-01, -1.7451e-01, -2.3248e-02,  2.3335e-01,
          2.6176e-01,  1.3909e-01, -1.2589e-01,  1.6319e-01,  2.4349e-02,
         -1.3283e-01,  4.8036e-02,  4.0980e-01, -2.1196e-01, -2.3831e-01,
         -9.6281e-03, -5.5775e-02,  1.1464e-02, -2.9264e-01,  9.9885e-02,
         -2.0102e-01, -1.9214e-01, -3.0318e-01, -4.2442e-02, -5.4449e-02,
         -1.2967e-04, -7.1719e-02,  2.0007e-01,  9.2549e-03,  5.3880e-02,
         -6.0826e-02,  8.9025e-02,  2.9168e-01,  2.3036e-01, -1.1622e-01,
         -6.1775e-02,  1.8710e-01, -1.6608e-01, -2.2137e-01,  1.6387e-01,
         -1.2285e-01,  3.0026e-01,  2.8564e-01, -3.5229e-01, -1.0878e-01,
          4.0280e-01,  1.5114e-01,  1.1360e-02, -2.0095e-02, -1.3847e-01,
          7.2091e-04, -1.1731e-01, -2.7096e-01,  2.7509e-02,  1.0600e-01,
          4.5664e-02,  1.3611e-01, -1.6287e-01, -2.6011e-01, -8.1596e-02,
          1.9391e-01, -6.8843e-02,  6.6795e-02,  3.0044e-01,  1.4518e-04,
          1.3836e-01, -2.1662e-01, -1.4751e-01, -4.5692e-03,  7.7596e-03,
          1.5091e-01, -1.9701e-01,  3.9364e-02,  2.4326e-02, -1.4290e-01,
         -2.3443e-01, -2.8338e-01,  2.4571e-01, -3.4772e-01, -2.1874e-01,
          4.0258e-02,  1.0768e-01,  2.4987e-01,  2.3627e-01, -1.1554e-01,
          2.1028e-01,  1.6711e-01, -1.5543e-01, -1.8313e-02, -3.3042e-01,
         -2.3139e-01,  1.1625e-01, -3.2922e-01, -1.7406e-01, -2.3016e-01,
         -1.3942e-01, -4.9668e-02, -2.7628e-01,  1.0165e-02,  8.2532e-02,
          6.4771e-02,  1.9546e-01, -3.2410e-01,  1.8964e-01, -1.6898e-01,
         -2.2098e-01, -1.2092e-01, -1.3189e-02,  2.6864e-02,  2.7784e-01,
         -6.3231e-02,  6.4781e-02,  3.4362e-01, -2.2732e-02, -3.5275e-01,
          9.8142e-03, -2.4852e-01, -2.7906e-02, -5.1745e-02, -9.5929e-02,
          1.7100e-02,  4.3980e-02, -4.3783e-02,  3.7871e-01,  1.2682e-01,
         -3.2154e-01, -2.1401e-01,  6.4307e-02,  7.8588e-02,  2.3951e-01,
          1.3997e-02, -7.8034e-02, -2.2260e-03,  1.9591e-02, -2.4945e-01,
          8.2607e-02, -3.2764e-02,  1.7143e-02,  2.0525e-01, -5.7016e-02,
          1.8229e-01, -2.7006e-02,  1.1357e-01,  9.7994e-02,  5.1746e-02,
          7.9860e-02,  3.8248e-02, -2.3227e-01, -1.3862e-01,  6.6153e-02,
          2.6249e-01, -5.7370e-02, -1.3890e-01, -4.0898e-01, -2.8864e-01,
         -2.3905e-02,  2.0195e-01,  1.1847e-01, -2.4378e-01, -1.7138e-01,
         -1.7932e-01,  2.7026e-01, -1.0431e-01,  8.6107e-02, -1.0131e-01,
         -8.2868e-02,  9.2653e-02, -3.3071e-02, -3.0312e-01, -2.7937e-01,
         -4.0222e-01, -3.8210e-01,  1.1642e-01, -3.3853e-02,  7.7303e-02,
          1.2355e-01, -2.2809e-01,  4.3379e-02,  1.0617e-02, -1.7532e-01,
         -1.5698e-01,  1.7562e-01, -6.4621e-02, -2.2386e-01, -5.4332e-02,
         -1.4223e-01,  1.1155e-01,  2.3546e-01,  3.8853e-02,  1.1227e-01,
         -2.9981e-02, -3.5384e-02, -1.3603e-01,  3.1001e-01, -1.4615e-01,
          2.5058e-02, -9.1489e-02, -2.1435e-01,  3.0697e-02, -2.5929e-02,
          3.2241e-01,  3.7920e-01,  2.0833e-01,  2.8212e-01, -1.0324e-01,
         -1.6585e-01, -1.8386e-01,  2.0189e-02,  2.7767e-01, -1.3822e-01,
         -1.3711e-01,  9.5865e-03,  1.1056e-02, -1.0575e-01, -5.5155e-02,
          1.1809e-01, -1.2300e-01,  2.4179e-01, -2.8298e-02,  7.4502e-02,
         -3.0307e-01,  2.4275e-02,  7.3545e-02, -6.9410e-02,  1.2794e-01,
          7.5693e-02,  1.5065e-01, -9.7550e-02,  3.4761e-01,  2.7095e-01,
         -4.5170e-02, -1.2539e-01,  3.8548e-03,  2.8808e-01, -9.7298e-02,
          3.0336e-02, -3.0973e-01,  7.9866e-02,  2.3738e-01,  1.2550e-01,
          7.3314e-03, -1.5217e-01,  2.0428e-01, -3.5740e-02,  1.4864e-01,
         -3.2093e-01, -5.4270e-02,  1.2042e-02, -1.4123e-01, -1.0701e-01,
         -3.3997e-01, -3.5408e-02, -1.8080e-01,  1.0898e-01,  1.1176e-01,
          1.4616e-01,  5.3214e-02, -8.9582e-02, -2.4846e-01, -2.1512e-01,
         -1.8585e-01, -7.4543e-02,  1.8495e-01,  1.3112e-01, -2.9189e-01,
         -2.3716e-01,  1.4231e-01,  1.8316e-01,  1.1146e-01,  1.0917e-01,
         -7.8276e-02, -3.5784e-01, -2.0579e-01, -1.9995e-01, -2.0056e-02,
          1.9377e-01,  2.1209e-01,  1.1557e-01,  1.5575e-01,  3.7109e-02,
         -1.9799e-01,  1.2692e-01, -1.2687e-01,  3.2986e-01, -4.0038e-01,
         -5.4040e-02,  1.9603e-01, -1.0432e-01, -4.4281e-02, -9.4947e-02,
          1.9225e-01,  2.4538e-01,  1.6693e-01,  1.7623e-02,  2.7655e-01,
         -2.7252e-01, -3.5374e-02,  2.1649e-01,  1.1542e-01, -3.2575e-01,
         -9.5898e-02, -1.7769e-01, -1.7478e-01,  1.2413e-02, -9.4918e-02,
         -1.7591e-01, -7.7178e-02,  8.4715e-02,  1.1410e-01,  2.3401e-01,
          4.6640e-02,  1.6343e-02,  8.4249e-02, -9.6963e-02,  4.9761e-03,
         -1.2689e-01, -8.9776e-02,  1.4109e-01, -2.3152e-01,  3.0876e-01,
         -2.1192e-01,  2.8687e-01, -1.6530e-01,  1.0156e-01,  1.3087e-01,
         -1.3041e-01,  2.9489e-01, -1.7078e-01,  2.2375e-01,  1.1248e-01,
          7.6252e-02, -2.7074e-02,  7.4888e-02, -2.2466e-01,  5.8946e-02,
         -1.5730e-01,  1.9001e-01,  1.6656e-01,  2.3558e-01,  8.8893e-02,
          4.6385e-02, -4.6853e-03, -3.9195e-02, -6.8834e-02,  6.7098e-02,
         -2.5421e-01, -1.3098e-01,  3.3280e-01, -1.1145e-01,  3.0578e-01,
          1.4134e-01, -7.8914e-02,  1.6024e-01, -2.5977e-01,  4.2665e-02,
          1.7734e-01, -1.5616e-01, -3.0725e-01, -1.6099e-01, -9.4044e-02,
          5.9506e-02,  2.2976e-01, -1.7763e-01, -2.6950e-01,  3.8000e-01,
          1.7562e-01,  3.7771e-02,  1.0278e-01,  3.2222e-01, -2.1560e-02,
         -1.5225e-01, -2.3036e-01, -1.3021e-01, -3.0180e-01,  2.4546e-01,
          6.7754e-02, -3.0570e-01, -2.9963e-01,  1.5654e-02, -1.8943e-01,
         -2.3144e-01,  2.8879e-01,  7.4722e-02, -1.5568e-01, -2.0820e-01,
          4.9041e-02,  3.9942e-01,  4.3492e-02,  8.4227e-02,  1.3005e-01,
         -9.3410e-02,  2.1089e-01,  8.5407e-02, -2.3646e-01,  1.1235e-01,
          2.1036e-01, -1.5971e-01,  7.9207e-02,  4.4568e-03, -2.2224e-01,
          3.1588e-02, -2.5247e-01, -1.8240e-01, -8.3811e-02, -3.5396e-01,
         -9.2816e-02,  3.5115e-02,  2.7964e-01, -9.2329e-02, -3.5224e-01,
         -2.4336e-01,  1.5316e-01, -1.8951e-01, -1.5665e-01, -2.2028e-01,
         -2.0556e-01, -1.5417e-01,  4.3107e-02, -1.4996e-01, -1.7838e-01,
         -1.0692e-01, -1.7765e-01, -8.7658e-03,  1.6875e-01, -2.1353e-01,
          1.7508e-01, -8.9377e-02, -1.9662e-01,  2.5831e-02, -5.6539e-02,
          5.4678e-02,  2.4562e-01,  9.2557e-02, -5.1224e-02, -2.1296e-02,
         -2.0423e-01,  4.4212e-03, -1.5627e-01,  1.5368e-01, -9.0215e-02,
          4.5537e-02,  2.7957e-01, -1.7029e-01,  2.9199e-01, -9.2008e-02,
         -1.1972e-01, -1.1973e-01, -3.9062e-01, -6.3714e-03, -1.8681e-02,
          4.3476e-02,  1.0941e-01,  1.5459e-02, -2.8024e-02, -2.9083e-01,
         -2.1282e-01,  1.7896e-01, -1.1782e-01, -2.9580e-01,  9.5791e-02,
         -1.6213e-01, -1.0290e-03,  9.6139e-02, -6.0271e-02,  7.4006e-02,
          1.4452e-01, -6.5647e-02,  2.0084e-01, -1.4069e-02, -1.2897e-01,
          2.6349e-01, -2.3983e-01,  1.7705e-02,  1.7037e-01,  3.0223e-02,
         -2.9845e-01,  1.0508e-01, -1.6406e-01, -3.5936e-02, -8.8760e-02,
         -2.6679e-01,  3.3564e-01,  3.7482e-04,  5.0903e-02, -7.3588e-02,
         -1.6273e-01, -5.2870e-02, -2.3405e-01, -7.3421e-02,  6.7705e-03,
         -1.6738e-01, -3.5341e-02,  1.6683e-01, -1.5272e-01,  3.9206e-01,
          2.2943e-01, -1.5948e-01, -2.8566e-02, -1.0889e-02, -2.4224e-01,
          6.9751e-02,  9.7223e-03, -2.8483e-01, -6.3500e-02,  1.8432e-01,
          4.8258e-02,  3.6758e-01, -1.7792e-01,  1.7039e-02,  3.1149e-01,
          9.3328e-02, -2.3158e-01, -3.5395e-01, -1.0560e-01,  2.2844e-01,
          8.0793e-02,  1.4810e-01, -3.0206e-01, -3.4022e-01,  1.7620e-01,
          4.3809e-02,  9.0087e-02,  1.5914e-01, -6.8204e-02,  2.5665e-01,
         -1.8012e-01,  3.7484e-02,  4.8088e-02,  2.3510e-02, -1.1832e-01,
          1.0292e-01,  1.4461e-01,  9.2944e-02, -1.4286e-03,  1.8287e-01,
          1.7623e-01,  6.0158e-02,  1.7079e-01,  1.9798e-01, -1.8351e-01,
         -4.1477e-01,  2.4582e-01,  1.0249e-01, -3.3297e-02, -2.0311e-01,
         -2.5807e-01, -2.9441e-02,  2.5160e-01, -2.8196e-01, -2.2125e-01,
         -1.3952e-01, -5.6166e-02,  2.8915e-02, -1.9787e-01, -1.6558e-01,
         -1.7816e-01, -3.1428e-01, -1.7320e-01,  1.2468e-02, -3.4252e-01,
         -2.5784e-01, -7.1416e-02, -9.2996e-02,  1.6461e-01, -3.2702e-01,
          1.4028e-01,  2.0673e-01, -5.7455e-02, -2.6848e-01,  1.1147e-01,
         -4.8179e-02,  9.7699e-02, -2.2503e-01, -1.3832e-01,  1.9701e-02,
         -8.6869e-02, -4.8751e-02,  6.2277e-02, -3.3196e-01,  2.0513e-01,
          4.2906e-02, -3.1485e-01, -1.9783e-01, -1.8252e-01,  8.9203e-03,
          2.3651e-01, -1.7068e-01,  9.3493e-02,  1.2389e-01, -2.1146e-01,
          8.2649e-03,  1.5572e-01, -2.2668e-01,  6.7394e-02,  1.9482e-02,
          7.9862e-02, -7.1562e-02,  8.8041e-02,  2.9078e-01, -2.9057e-01,
          1.9924e-01,  7.9529e-02,  2.3030e-01, -1.8758e-01, -2.1552e-01,
          1.6985e-01, -1.6580e-01, -2.1389e-01, -2.4635e-01, -1.9783e-01,
          7.4881e-02, -9.2235e-02,  5.3853e-02, -2.7720e-01, -4.0823e-02,
          2.2342e-01,  1.6657e-01, -1.2662e-01, -1.9808e-02,  5.6801e-02,
          2.1904e-02,  6.3948e-02, -2.0601e-01, -2.2994e-01, -1.4389e-01,
          5.3174e-02,  6.4131e-02,  4.4927e-02, -3.7137e-01, -1.5741e-01,
         -2.5703e-01,  1.2718e-01,  2.0193e-02,  3.8737e-02, -7.7474e-02,
          1.9992e-02, -1.8949e-01, -1.0489e-01, -6.3354e-02, -2.5072e-01,
         -2.2850e-01, -1.9215e-01, -2.7560e-01, -5.5963e-02, -1.2828e-01,
          1.2989e-01,  8.0240e-02,  1.1768e-01,  3.6903e-01,  3.2748e-01,
          2.8244e-01,  7.6465e-02, -1.3270e-01,  1.6062e-01,  1.6020e-02,
          1.8847e-01, -1.0488e-01, -2.7101e-01,  2.8572e-01,  2.2558e-01,
         -1.0301e-01,  1.4395e-01,  1.6343e-01, -2.3641e-01, -1.2907e-01,
         -2.7267e-02,  2.6302e-01, -1.8550e-01,  2.4225e-01,  1.8058e-01,
         -2.0332e-01, -1.1652e-01,  2.2879e-01, -6.5583e-02,  3.0162e-02,
          5.5261e-02,  3.3168e-02,  1.4287e-03,  2.4233e-01, -2.5623e-01,
          1.4555e-01, -9.7221e-02, -1.4414e-01,  2.5351e-01, -1.6486e-01,
          8.8751e-03, -1.3138e-01, -5.0337e-02, -2.3413e-01, -1.2613e-01,
          1.0165e-01,  1.7950e-01, -4.9932e-02,  1.9990e-01, -2.5763e-01,
         -3.1997e-01, -1.8444e-01, -2.4811e-01, -6.7918e-02,  2.8393e-01,
          3.9342e-02, -3.8641e-01, -2.4957e-01, -4.2098e-02,  1.2375e-01,
         -2.3292e-01, -1.2942e-01, -9.9817e-02,  2.1444e-02, -2.8142e-02,
         -6.4317e-02, -2.3902e-01, -3.0746e-01, -8.9892e-02, -1.2646e-01,
         -3.2790e-01,  2.3242e-01,  9.5534e-02, -1.0982e-01,  2.6479e-01,
          1.5996e-01, -3.1966e-01, -1.9431e-01,  7.6029e-02, -2.0056e-01,
         -4.4122e-02, -1.9894e-01, -3.2973e-01,  1.0175e-01, -3.6016e-02,
         -1.1808e-01, -1.6042e-01, -2.7750e-01,  4.7654e-02, -6.1258e-02,
         -4.5565e-01, -1.4322e-01, -1.6719e-01,  1.1150e-01, -6.1786e-02,
         -1.3581e-01, -1.2923e-01, -8.6849e-02, -2.0869e-01, -9.5694e-02,
         -3.2178e-01,  4.5601e-02,  9.0810e-02, -3.4218e-01, -1.2418e-01,
         -2.1381e-01, -1.2038e-01, -4.3754e-02,  6.3526e-02,  9.4710e-02,
          1.8775e-02, -2.5421e-01,  9.3967e-03,  2.0880e-01,  1.6493e-01,
         -2.8366e-01, -1.0564e-01,  2.7419e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train - epoch: 0/5 iter: 0/15 loss: 1.1452 Acc: 34.3750% F1: 0.248 Time: 1.31s (0.00s)
Fold 3 train - epoch: 0/5 iter: 1/15 loss: 0.9334 Acc: 53.1250% F1: 0.278 Time: 0.92s (0.03s)
Fold 3 train - epoch: 0/5 iter: 2/15 loss: 1.0686 Acc: 50.0000% F1: 0.222 Time: 0.92s (0.02s)
Fold 3 train - epoch: 0/5 iter: 3/15 loss: 0.9687 Acc: 59.3750% F1: 0.248 Time: 0.92s (0.02s)
Fold 3 train - epoch: 0/5 iter: 4/15 loss: 1.0256 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 5/15 loss: 0.9058 Acc: 65.6250% F1: 0.375 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 6/15 loss: 0.9002 Acc: 53.1250% F1: 0.301 Time: 0.93s (0.07s)
Fold 3 train - epoch: 0/5 iter: 7/15 loss: 0.8922 Acc: 53.1250% F1: 0.317 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 8/15 loss: 1.1899 Acc: 34.3750% F1: 0.229 Time: 0.93s (0.40s)
Fold 3 train - epoch: 0/5 iter: 9/15 loss: 1.0329 Acc: 43.7500% F1: 0.297 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 10/15 loss: 0.9474 Acc: 59.3750% F1: 0.416 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 11/15 loss: 0.9080 Acc: 56.2500% F1: 0.360 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 12/15 loss: 0.9717 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 13/15 loss: 0.9129 Acc: 62.5000% F1: 0.314 Time: 0.93s (0.02s)
Fold 3 train - epoch: 0/5 iter: 14/15 loss: 0.7154 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 0 train Avg acc: 53.1111% F1: 0.3209 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 3 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5791 Acc: 84.3750% F1: 0.458 Time: 0.33s (0.00s)
Fold 3 train-dev - epoch: 0/5 iter: 1/2 loss: 1.7283 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 0 train-dev Avg acc: 54.0000% F1: 0.2338 *
*********************************************************
Performing epoch 1 of 5
Fold 3 train - epoch: 1/5 iter: 0/15 loss: 0.8699 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 3 train - epoch: 1/5 iter: 1/15 loss: 0.8960 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.27s)
Fold 3 train - epoch: 1/5 iter: 2/15 loss: 0.9533 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 3/15 loss: 0.8457 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 4/15 loss: 1.0383 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 5/15 loss: 0.8869 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 6/15 loss: 0.7401 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.10s)
Fold 3 train - epoch: 1/5 iter: 7/15 loss: 0.9309 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 8/15 loss: 1.1457 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.39s)
Fold 3 train - epoch: 1/5 iter: 9/15 loss: 1.0213 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.02s)
Fold 3 train - epoch: 1/5 iter: 10/15 loss: 1.0121 Acc: 43.7500% F1: 0.203 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 11/15 loss: 0.8547 Acc: 50.0000% F1: 0.227 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 12/15 loss: 0.9117 Acc: 59.3750% F1: 0.344 Time: 0.93s (0.02s)
Fold 3 train - epoch: 1/5 iter: 13/15 loss: 0.9253 Acc: 56.2500% F1: 0.321 Time: 0.94s (0.02s)
Fold 3 train - epoch: 1/5 iter: 14/15 loss: 0.6547 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 1 train Avg acc: 54.2222% F1: 0.2501 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7727 Acc: 71.8750% F1: 0.566 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3610 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.3303 *
*********************************************************
Performing epoch 2 of 5
Fold 3 train - epoch: 2/5 iter: 0/15 loss: 0.8684 Acc: 50.0000% F1: 0.269 Time: 0.94s (0.00s)
Fold 3 train - epoch: 2/5 iter: 1/15 loss: 0.8427 Acc: 62.5000% F1: 0.409 Time: 0.93s (0.25s)
Fold 3 train - epoch: 2/5 iter: 2/15 loss: 0.9264 Acc: 50.0000% F1: 0.291 Time: 0.92s (0.02s)
Fold 3 train - epoch: 2/5 iter: 3/15 loss: 0.8307 Acc: 59.3750% F1: 0.253 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 4/15 loss: 0.9130 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 5/15 loss: 0.8142 Acc: 62.5000% F1: 0.320 Time: 0.93s (0.03s)
Fold 3 train - epoch: 2/5 iter: 6/15 loss: 0.7597 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.19s)
Fold 3 train - epoch: 2/5 iter: 7/15 loss: 0.9072 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 8/15 loss: 1.1134 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.40s)
Fold 3 train - epoch: 2/5 iter: 9/15 loss: 1.0114 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 10/15 loss: 0.9298 Acc: 53.1250% F1: 0.311 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 11/15 loss: 0.7930 Acc: 65.6250% F1: 0.327 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 12/15 loss: 0.9039 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 13/15 loss: 0.8865 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 3 train - epoch: 2/5 iter: 14/15 loss: 0.3699 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 2 train Avg acc: 56.2222% F1: 0.2835 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 2/5 iter: 0/2 loss: 0.6959 Acc: 75.0000% F1: 0.590 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 2/5 iter: 1/2 loss: 1.5417 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 2 train-dev Avg acc: 54.0000% F1: 0.3413 *
*********************************************************
Performing epoch 3 of 5
Fold 3 train - epoch: 3/5 iter: 0/15 loss: 0.7985 Acc: 53.1250% F1: 0.236 Time: 0.94s (0.00s)
Fold 3 train - epoch: 3/5 iter: 1/15 loss: 0.7756 Acc: 75.0000% F1: 0.628 Time: 0.93s (0.35s)
Fold 3 train - epoch: 3/5 iter: 2/15 loss: 0.8911 Acc: 50.0000% F1: 0.262 Time: 0.92s (0.02s)
Fold 3 train - epoch: 3/5 iter: 3/15 loss: 0.7501 Acc: 75.0000% F1: 0.478 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 4/15 loss: 0.8317 Acc: 59.3750% F1: 0.491 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 5/15 loss: 0.7484 Acc: 71.8750% F1: 0.570 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 6/15 loss: 0.7261 Acc: 75.0000% F1: 0.447 Time: 0.93s (0.11s)
Fold 3 train - epoch: 3/5 iter: 7/15 loss: 0.8820 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 8/15 loss: 1.0254 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.43s)
Fold 3 train - epoch: 3/5 iter: 9/15 loss: 0.9246 Acc: 56.2500% F1: 0.368 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 10/15 loss: 0.9393 Acc: 50.0000% F1: 0.338 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 11/15 loss: 0.7190 Acc: 78.1250% F1: 0.748 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 12/15 loss: 0.7793 Acc: 75.0000% F1: 0.523 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 13/15 loss: 0.8180 Acc: 59.3750% F1: 0.360 Time: 0.93s (0.02s)
Fold 3 train - epoch: 3/5 iter: 14/15 loss: 0.2476 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 3 train Avg acc: 62.8889% F1: 0.4442 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7640 Acc: 65.6250% F1: 0.521 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6245 Acc: 22.2222% F1: 0.157 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 3 train-dev Avg acc: 50.0000% F1: 0.3293 *
*********************************************************
Performing epoch 4 of 5
Fold 3 train - epoch: 4/5 iter: 0/15 loss: 0.7099 Acc: 68.7500% F1: 0.607 Time: 0.94s (0.00s)
Fold 3 train - epoch: 4/5 iter: 1/15 loss: 0.6514 Acc: 75.0000% F1: 0.621 Time: 0.93s (0.30s)
Fold 3 train - epoch: 4/5 iter: 2/15 loss: 0.7754 Acc: 56.2500% F1: 0.350 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 3/15 loss: 0.6282 Acc: 75.0000% F1: 0.478 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 4/15 loss: 0.6682 Acc: 71.8750% F1: 0.611 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 5/15 loss: 0.6084 Acc: 75.0000% F1: 0.615 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 6/15 loss: 0.6584 Acc: 75.0000% F1: 0.468 Time: 0.93s (0.05s)
Fold 3 train - epoch: 4/5 iter: 7/15 loss: 0.8243 Acc: 53.1250% F1: 0.333 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 8/15 loss: 0.9193 Acc: 56.2500% F1: 0.470 Time: 0.93s (0.39s)
Fold 3 train - epoch: 4/5 iter: 9/15 loss: 0.7275 Acc: 71.8750% F1: 0.517 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 10/15 loss: 0.7400 Acc: 65.6250% F1: 0.629 Time: 0.93s (0.03s)
Fold 3 train - epoch: 4/5 iter: 11/15 loss: 0.5906 Acc: 78.1250% F1: 0.806 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 12/15 loss: 0.6783 Acc: 68.7500% F1: 0.583 Time: 0.94s (0.02s)
Fold 3 train - epoch: 4/5 iter: 13/15 loss: 0.7936 Acc: 68.7500% F1: 0.630 Time: 0.93s (0.02s)
Fold 3 train - epoch: 4/5 iter: 14/15 loss: 0.1035 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 3 Epoch 4 train Avg acc: 68.6667% F1: 0.5900 *
*****************************************************
Saving model...
Fold 3 train-dev - epoch: 4/5 iter: 0/2 loss: 1.0138 Acc: 53.1250% F1: 0.311 Time: 0.34s (0.00s)
Fold 3 train-dev - epoch: 4/5 iter: 1/2 loss: 1.9557 Acc: 22.2222% F1: 0.148 Time: 0.18s (0.00s)
*********************************************************
* Fold 3 Epoch 4 train-dev Avg acc: 42.0000% F1: 0.3030 *
*********************************************************
Creating 1 distributed models for fold 4...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[ 1.3939e-03, -6.8716e-02, -2.5557e-01,  2.8171e-02, -8.7808e-03,
         -2.5491e-01,  2.4635e-01, -1.7451e-01, -2.3248e-02,  2.3335e-01,
          2.6176e-01,  1.3909e-01, -1.2589e-01,  1.6319e-01,  2.4349e-02,
         -1.3283e-01,  4.8036e-02,  4.0980e-01, -2.1196e-01, -2.3831e-01,
         -9.6281e-03, -5.5775e-02,  1.1464e-02, -2.9264e-01,  9.9885e-02,
         -2.0102e-01, -1.9214e-01, -3.0318e-01, -4.2442e-02, -5.4449e-02,
         -1.2967e-04, -7.1719e-02,  2.0007e-01,  9.2549e-03,  5.3880e-02,
         -6.0826e-02,  8.9025e-02,  2.9168e-01,  2.3036e-01, -1.1622e-01,
         -6.1775e-02,  1.8710e-01, -1.6608e-01, -2.2137e-01,  1.6387e-01,
         -1.2285e-01,  3.0026e-01,  2.8564e-01, -3.5229e-01, -1.0878e-01,
          4.0280e-01,  1.5114e-01,  1.1360e-02, -2.0095e-02, -1.3847e-01,
          7.2091e-04, -1.1731e-01, -2.7096e-01,  2.7509e-02,  1.0600e-01,
          4.5664e-02,  1.3611e-01, -1.6287e-01, -2.6011e-01, -8.1596e-02,
          1.9391e-01, -6.8843e-02,  6.6795e-02,  3.0044e-01,  1.4518e-04,
          1.3836e-01, -2.1662e-01, -1.4751e-01, -4.5692e-03,  7.7596e-03,
          1.5091e-01, -1.9701e-01,  3.9364e-02,  2.4326e-02, -1.4290e-01,
         -2.3443e-01, -2.8338e-01,  2.4571e-01, -3.4772e-01, -2.1874e-01,
          4.0258e-02,  1.0768e-01,  2.4987e-01,  2.3627e-01, -1.1554e-01,
          2.1028e-01,  1.6711e-01, -1.5543e-01, -1.8313e-02, -3.3042e-01,
         -2.3139e-01,  1.1625e-01, -3.2922e-01, -1.7406e-01, -2.3016e-01,
         -1.3942e-01, -4.9668e-02, -2.7628e-01,  1.0165e-02,  8.2532e-02,
          6.4771e-02,  1.9546e-01, -3.2410e-01,  1.8964e-01, -1.6898e-01,
         -2.2098e-01, -1.2092e-01, -1.3189e-02,  2.6864e-02,  2.7784e-01,
         -6.3231e-02,  6.4781e-02,  3.4362e-01, -2.2732e-02, -3.5275e-01,
          9.8142e-03, -2.4852e-01, -2.7906e-02, -5.1745e-02, -9.5929e-02,
          1.7100e-02,  4.3980e-02, -4.3783e-02,  3.7871e-01,  1.2682e-01,
         -3.2154e-01, -2.1401e-01,  6.4307e-02,  7.8588e-02,  2.3951e-01,
          1.3997e-02, -7.8034e-02, -2.2260e-03,  1.9591e-02, -2.4945e-01,
          8.2607e-02, -3.2764e-02,  1.7143e-02,  2.0525e-01, -5.7016e-02,
          1.8229e-01, -2.7006e-02,  1.1357e-01,  9.7994e-02,  5.1746e-02,
          7.9860e-02,  3.8248e-02, -2.3227e-01, -1.3862e-01,  6.6153e-02,
          2.6249e-01, -5.7370e-02, -1.3890e-01, -4.0898e-01, -2.8864e-01,
         -2.3905e-02,  2.0195e-01,  1.1847e-01, -2.4378e-01, -1.7138e-01,
         -1.7932e-01,  2.7026e-01, -1.0431e-01,  8.6107e-02, -1.0131e-01,
         -8.2868e-02,  9.2653e-02, -3.3071e-02, -3.0312e-01, -2.7937e-01,
         -4.0222e-01, -3.8210e-01,  1.1642e-01, -3.3853e-02,  7.7303e-02,
          1.2355e-01, -2.2809e-01,  4.3379e-02,  1.0617e-02, -1.7532e-01,
         -1.5698e-01,  1.7562e-01, -6.4621e-02, -2.2386e-01, -5.4332e-02,
         -1.4223e-01,  1.1155e-01,  2.3546e-01,  3.8853e-02,  1.1227e-01,
         -2.9981e-02, -3.5384e-02, -1.3603e-01,  3.1001e-01, -1.4615e-01,
          2.5058e-02, -9.1489e-02, -2.1435e-01,  3.0697e-02, -2.5929e-02,
          3.2241e-01,  3.7920e-01,  2.0833e-01,  2.8212e-01, -1.0324e-01,
         -1.6585e-01, -1.8386e-01,  2.0189e-02,  2.7767e-01, -1.3822e-01,
         -1.3711e-01,  9.5865e-03,  1.1056e-02, -1.0575e-01, -5.5155e-02,
          1.1809e-01, -1.2300e-01,  2.4179e-01, -2.8298e-02,  7.4502e-02,
         -3.0307e-01,  2.4275e-02,  7.3545e-02, -6.9410e-02,  1.2794e-01,
          7.5693e-02,  1.5065e-01, -9.7550e-02,  3.4761e-01,  2.7095e-01,
         -4.5170e-02, -1.2539e-01,  3.8548e-03,  2.8808e-01, -9.7298e-02,
          3.0336e-02, -3.0973e-01,  7.9866e-02,  2.3738e-01,  1.2550e-01,
          7.3314e-03, -1.5217e-01,  2.0428e-01, -3.5740e-02,  1.4864e-01,
         -3.2093e-01, -5.4270e-02,  1.2042e-02, -1.4123e-01, -1.0701e-01,
         -3.3997e-01, -3.5408e-02, -1.8080e-01,  1.0898e-01,  1.1176e-01,
          1.4616e-01,  5.3214e-02, -8.9582e-02, -2.4846e-01, -2.1512e-01,
         -1.8585e-01, -7.4543e-02,  1.8495e-01,  1.3112e-01, -2.9189e-01,
         -2.3716e-01,  1.4231e-01,  1.8316e-01,  1.1146e-01,  1.0917e-01,
         -7.8276e-02, -3.5784e-01, -2.0579e-01, -1.9995e-01, -2.0056e-02,
          1.9377e-01,  2.1209e-01,  1.1557e-01,  1.5575e-01,  3.7109e-02,
         -1.9799e-01,  1.2692e-01, -1.2687e-01,  3.2986e-01, -4.0038e-01,
         -5.4040e-02,  1.9603e-01, -1.0432e-01, -4.4281e-02, -9.4947e-02,
          1.9225e-01,  2.4538e-01,  1.6693e-01,  1.7623e-02,  2.7655e-01,
         -2.7252e-01, -3.5374e-02,  2.1649e-01,  1.1542e-01, -3.2575e-01,
         -9.5898e-02, -1.7769e-01, -1.7478e-01,  1.2413e-02, -9.4918e-02,
         -1.7591e-01, -7.7178e-02,  8.4715e-02,  1.1410e-01,  2.3401e-01,
          4.6640e-02,  1.6343e-02,  8.4249e-02, -9.6963e-02,  4.9761e-03,
         -1.2689e-01, -8.9776e-02,  1.4109e-01, -2.3152e-01,  3.0876e-01,
         -2.1192e-01,  2.8687e-01, -1.6530e-01,  1.0156e-01,  1.3087e-01,
         -1.3041e-01,  2.9489e-01, -1.7078e-01,  2.2375e-01,  1.1248e-01,
          7.6252e-02, -2.7074e-02,  7.4888e-02, -2.2466e-01,  5.8946e-02,
         -1.5730e-01,  1.9001e-01,  1.6656e-01,  2.3558e-01,  8.8893e-02,
          4.6385e-02, -4.6853e-03, -3.9195e-02, -6.8834e-02,  6.7098e-02,
         -2.5421e-01, -1.3098e-01,  3.3280e-01, -1.1145e-01,  3.0578e-01,
          1.4134e-01, -7.8914e-02,  1.6024e-01, -2.5977e-01,  4.2665e-02,
          1.7734e-01, -1.5616e-01, -3.0725e-01, -1.6099e-01, -9.4044e-02,
          5.9506e-02,  2.2976e-01, -1.7763e-01, -2.6950e-01,  3.8000e-01,
          1.7562e-01,  3.7771e-02,  1.0278e-01,  3.2222e-01, -2.1560e-02,
         -1.5225e-01, -2.3036e-01, -1.3021e-01, -3.0180e-01,  2.4546e-01,
          6.7754e-02, -3.0570e-01, -2.9963e-01,  1.5654e-02, -1.8943e-01,
         -2.3144e-01,  2.8879e-01,  7.4722e-02, -1.5568e-01, -2.0820e-01,
          4.9041e-02,  3.9942e-01,  4.3492e-02,  8.4227e-02,  1.3005e-01,
         -9.3410e-02,  2.1089e-01,  8.5407e-02, -2.3646e-01,  1.1235e-01,
          2.1036e-01, -1.5971e-01,  7.9207e-02,  4.4568e-03, -2.2224e-01,
          3.1588e-02, -2.5247e-01, -1.8240e-01, -8.3811e-02, -3.5396e-01,
         -9.2816e-02,  3.5115e-02,  2.7964e-01, -9.2329e-02, -3.5224e-01,
         -2.4336e-01,  1.5316e-01, -1.8951e-01, -1.5665e-01, -2.2028e-01,
         -2.0556e-01, -1.5417e-01,  4.3107e-02, -1.4996e-01, -1.7838e-01,
         -1.0692e-01, -1.7765e-01, -8.7658e-03,  1.6875e-01, -2.1353e-01,
          1.7508e-01, -8.9377e-02, -1.9662e-01,  2.5831e-02, -5.6539e-02,
          5.4678e-02,  2.4562e-01,  9.2557e-02, -5.1224e-02, -2.1296e-02,
         -2.0423e-01,  4.4212e-03, -1.5627e-01,  1.5368e-01, -9.0215e-02,
          4.5537e-02,  2.7957e-01, -1.7029e-01,  2.9199e-01, -9.2008e-02,
         -1.1972e-01, -1.1973e-01, -3.9062e-01, -6.3714e-03, -1.8681e-02,
          4.3476e-02,  1.0941e-01,  1.5459e-02, -2.8024e-02, -2.9083e-01,
         -2.1282e-01,  1.7896e-01, -1.1782e-01, -2.9580e-01,  9.5791e-02,
         -1.6213e-01, -1.0290e-03,  9.6139e-02, -6.0271e-02,  7.4006e-02,
          1.4452e-01, -6.5647e-02,  2.0084e-01, -1.4069e-02, -1.2897e-01,
          2.6349e-01, -2.3983e-01,  1.7705e-02,  1.7037e-01,  3.0223e-02,
         -2.9845e-01,  1.0508e-01, -1.6406e-01, -3.5936e-02, -8.8760e-02,
         -2.6679e-01,  3.3564e-01,  3.7482e-04,  5.0903e-02, -7.3588e-02,
         -1.6273e-01, -5.2870e-02, -2.3405e-01, -7.3421e-02,  6.7705e-03,
         -1.6738e-01, -3.5341e-02,  1.6683e-01, -1.5272e-01,  3.9206e-01,
          2.2943e-01, -1.5948e-01, -2.8566e-02, -1.0889e-02, -2.4224e-01,
          6.9751e-02,  9.7223e-03, -2.8483e-01, -6.3500e-02,  1.8432e-01,
          4.8258e-02,  3.6758e-01, -1.7792e-01,  1.7039e-02,  3.1149e-01,
          9.3328e-02, -2.3158e-01, -3.5395e-01, -1.0560e-01,  2.2844e-01,
          8.0793e-02,  1.4810e-01, -3.0206e-01, -3.4022e-01,  1.7620e-01,
          4.3809e-02,  9.0087e-02,  1.5914e-01, -6.8204e-02,  2.5665e-01,
         -1.8012e-01,  3.7484e-02,  4.8088e-02,  2.3510e-02, -1.1832e-01,
          1.0292e-01,  1.4461e-01,  9.2944e-02, -1.4286e-03,  1.8287e-01,
          1.7623e-01,  6.0158e-02,  1.7079e-01,  1.9798e-01, -1.8351e-01,
         -4.1477e-01,  2.4582e-01,  1.0249e-01, -3.3297e-02, -2.0311e-01,
         -2.5807e-01, -2.9441e-02,  2.5160e-01, -2.8196e-01, -2.2125e-01,
         -1.3952e-01, -5.6166e-02,  2.8915e-02, -1.9787e-01, -1.6558e-01,
         -1.7816e-01, -3.1428e-01, -1.7320e-01,  1.2468e-02, -3.4252e-01,
         -2.5784e-01, -7.1416e-02, -9.2996e-02,  1.6461e-01, -3.2702e-01,
          1.4028e-01,  2.0673e-01, -5.7455e-02, -2.6848e-01,  1.1147e-01,
         -4.8179e-02,  9.7699e-02, -2.2503e-01, -1.3832e-01,  1.9701e-02,
         -8.6869e-02, -4.8751e-02,  6.2277e-02, -3.3196e-01,  2.0513e-01,
          4.2906e-02, -3.1485e-01, -1.9783e-01, -1.8252e-01,  8.9203e-03,
          2.3651e-01, -1.7068e-01,  9.3493e-02,  1.2389e-01, -2.1146e-01,
          8.2649e-03,  1.5572e-01, -2.2668e-01,  6.7394e-02,  1.9482e-02,
          7.9862e-02, -7.1562e-02,  8.8041e-02,  2.9078e-01, -2.9057e-01,
          1.9924e-01,  7.9529e-02,  2.3030e-01, -1.8758e-01, -2.1552e-01,
          1.6985e-01, -1.6580e-01, -2.1389e-01, -2.4635e-01, -1.9783e-01,
          7.4881e-02, -9.2235e-02,  5.3853e-02, -2.7720e-01, -4.0823e-02,
          2.2342e-01,  1.6657e-01, -1.2662e-01, -1.9808e-02,  5.6801e-02,
          2.1904e-02,  6.3948e-02, -2.0601e-01, -2.2994e-01, -1.4389e-01,
          5.3174e-02,  6.4131e-02,  4.4927e-02, -3.7137e-01, -1.5741e-01,
         -2.5703e-01,  1.2718e-01,  2.0193e-02,  3.8737e-02, -7.7474e-02,
          1.9992e-02, -1.8949e-01, -1.0489e-01, -6.3354e-02, -2.5072e-01,
         -2.2850e-01, -1.9215e-01, -2.7560e-01, -5.5963e-02, -1.2828e-01,
          1.2989e-01,  8.0240e-02,  1.1768e-01,  3.6903e-01,  3.2748e-01,
          2.8244e-01,  7.6465e-02, -1.3270e-01,  1.6062e-01,  1.6020e-02,
          1.8847e-01, -1.0488e-01, -2.7101e-01,  2.8572e-01,  2.2558e-01,
         -1.0301e-01,  1.4395e-01,  1.6343e-01, -2.3641e-01, -1.2907e-01,
         -2.7267e-02,  2.6302e-01, -1.8550e-01,  2.4225e-01,  1.8058e-01,
         -2.0332e-01, -1.1652e-01,  2.2879e-01, -6.5583e-02,  3.0162e-02,
          5.5261e-02,  3.3168e-02,  1.4287e-03,  2.4233e-01, -2.5623e-01,
          1.4555e-01, -9.7221e-02, -1.4414e-01,  2.5351e-01, -1.6486e-01,
          8.8751e-03, -1.3138e-01, -5.0337e-02, -2.3413e-01, -1.2613e-01,
          1.0165e-01,  1.7950e-01, -4.9932e-02,  1.9990e-01, -2.5763e-01,
         -3.1997e-01, -1.8444e-01, -2.4811e-01, -6.7918e-02,  2.8393e-01,
          3.9342e-02, -3.8641e-01, -2.4957e-01, -4.2098e-02,  1.2375e-01,
         -2.3292e-01, -1.2942e-01, -9.9817e-02,  2.1444e-02, -2.8142e-02,
         -6.4317e-02, -2.3902e-01, -3.0746e-01, -8.9892e-02, -1.2646e-01,
         -3.2790e-01,  2.3242e-01,  9.5534e-02, -1.0982e-01,  2.6479e-01,
          1.5996e-01, -3.1966e-01, -1.9431e-01,  7.6029e-02, -2.0056e-01,
         -4.4122e-02, -1.9894e-01, -3.2973e-01,  1.0175e-01, -3.6016e-02,
         -1.1808e-01, -1.6042e-01, -2.7750e-01,  4.7654e-02, -6.1258e-02,
         -4.5565e-01, -1.4322e-01, -1.6719e-01,  1.1150e-01, -6.1786e-02,
         -1.3581e-01, -1.2923e-01, -8.6849e-02, -2.0869e-01, -9.5694e-02,
         -3.2178e-01,  4.5601e-02,  9.0810e-02, -3.4218e-01, -1.2418e-01,
         -2.1381e-01, -1.2038e-01, -4.3754e-02,  6.3526e-02,  9.4710e-02,
          1.8775e-02, -2.5421e-01,  9.3967e-03,  2.0880e-01,  1.6493e-01,
         -2.8366e-01, -1.0564e-01,  2.7419e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train - epoch: 0/5 iter: 0/15 loss: 1.1093 Acc: 34.3750% F1: 0.258 Time: 1.22s (0.00s)
Fold 4 train - epoch: 0/5 iter: 1/15 loss: 1.0099 Acc: 53.1250% F1: 0.231 Time: 0.92s (0.03s)
Fold 4 train - epoch: 0/5 iter: 2/15 loss: 1.0565 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 3/15 loss: 0.9242 Acc: 59.3750% F1: 0.253 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 4/15 loss: 1.0315 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 5/15 loss: 0.9242 Acc: 59.3750% F1: 0.342 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 6/15 loss: 0.9349 Acc: 53.1250% F1: 0.301 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 7/15 loss: 0.8659 Acc: 50.0000% F1: 0.326 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 8/15 loss: 1.2395 Acc: 37.5000% F1: 0.221 Time: 0.93s (0.10s)
Fold 4 train - epoch: 0/5 iter: 9/15 loss: 1.0627 Acc: 37.5000% F1: 0.214 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 10/15 loss: 0.9965 Acc: 50.0000% F1: 0.301 Time: 0.93s (0.02s)
Fold 4 train - epoch: 0/5 iter: 11/15 loss: 0.9209 Acc: 46.8750% F1: 0.251 Time: 0.94s (0.02s)
Fold 4 train - epoch: 0/5 iter: 12/15 loss: 0.9909 Acc: 56.2500% F1: 0.292 Time: 0.94s (0.03s)
Fold 4 train - epoch: 0/5 iter: 13/15 loss: 0.9128 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.03s)
Fold 4 train - epoch: 0/5 iter: 14/15 loss: 0.5866 Acc: 100.0000% F1: 1.000 Time: 0.13s (0.04s)
*****************************************************
* Fold 4 Epoch 0 train Avg acc: 49.7778% F1: 0.2802 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 4 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5554 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 4 train-dev - epoch: 0/5 iter: 1/2 loss: 1.7541 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 4 train - epoch: 1/5 iter: 0/15 loss: 0.8732 Acc: 56.2500% F1: 0.240 Time: 0.94s (0.00s)
Fold 4 train - epoch: 1/5 iter: 1/15 loss: 0.8959 Acc: 56.2500% F1: 0.240 Time: 0.92s (0.02s)
Fold 4 train - epoch: 1/5 iter: 2/15 loss: 0.9519 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 3/15 loss: 0.8236 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 4/15 loss: 1.0093 Acc: 50.0000% F1: 0.222 Time: 0.92s (0.02s)
Fold 4 train - epoch: 1/5 iter: 5/15 loss: 0.8697 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 6/15 loss: 0.8022 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.45s)
Fold 4 train - epoch: 1/5 iter: 7/15 loss: 0.8728 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 4 train - epoch: 1/5 iter: 8/15 loss: 1.1147 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.22s)
Fold 4 train - epoch: 1/5 iter: 9/15 loss: 1.0081 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 10/15 loss: 0.9721 Acc: 50.0000% F1: 0.265 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 11/15 loss: 0.8469 Acc: 65.6250% F1: 0.327 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 12/15 loss: 0.9813 Acc: 56.2500% F1: 0.326 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 13/15 loss: 0.9103 Acc: 53.1250% F1: 0.328 Time: 0.93s (0.02s)
Fold 4 train - epoch: 1/5 iter: 14/15 loss: 0.7032 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 1 train Avg acc: 55.3333% F1: 0.2642 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7580 Acc: 78.1250% F1: 0.547 Time: 0.35s (0.00s)
Fold 4 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3519 Acc: 5.5556% F1: 0.051 Time: 0.19s (0.00s)
*********************************************************
* Fold 4 Epoch 1 train-dev Avg acc: 52.0000% F1: 0.2828 *
*********************************************************
Performing epoch 2 of 5
Fold 4 train - epoch: 2/5 iter: 0/15 loss: 0.8614 Acc: 62.5000% F1: 0.358 Time: 0.94s (0.00s)
Fold 4 train - epoch: 2/5 iter: 1/15 loss: 0.8244 Acc: 65.6250% F1: 0.409 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 2/15 loss: 0.9052 Acc: 53.1250% F1: 0.275 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 3/15 loss: 0.8081 Acc: 62.5000% F1: 0.309 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 4/15 loss: 0.9411 Acc: 50.0000% F1: 0.222 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 5/15 loss: 0.8321 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 6/15 loss: 0.7600 Acc: 68.7500% F1: 0.272 Time: 0.93s (0.27s)
Fold 4 train - epoch: 2/5 iter: 7/15 loss: 0.8952 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 8/15 loss: 1.0783 Acc: 53.1250% F1: 0.231 Time: 0.93s (0.11s)
Fold 4 train - epoch: 2/5 iter: 9/15 loss: 1.0120 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 10/15 loss: 0.9985 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 11/15 loss: 0.7983 Acc: 62.5000% F1: 0.314 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 12/15 loss: 0.9425 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 13/15 loss: 0.9066 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 4 train - epoch: 2/5 iter: 14/15 loss: 0.3867 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 2 train Avg acc: 56.6667% F1: 0.2722 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7113 Acc: 65.6250% F1: 0.396 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 2/5 iter: 1/2 loss: 1.5126 Acc: 5.5556% F1: 0.051 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 2 train-dev Avg acc: 44.0000% F1: 0.2278 *
*********************************************************
Performing epoch 3 of 5
Fold 4 train - epoch: 3/5 iter: 0/15 loss: 0.7856 Acc: 59.3750% F1: 0.306 Time: 0.96s (0.00s)
Fold 4 train - epoch: 3/5 iter: 1/15 loss: 0.8235 Acc: 71.8750% F1: 0.489 Time: 0.92s (0.02s)
Fold 4 train - epoch: 3/5 iter: 2/15 loss: 0.8577 Acc: 56.2500% F1: 0.321 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/5 iter: 3/15 loss: 0.7845 Acc: 62.5000% F1: 0.310 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/5 iter: 4/15 loss: 0.8578 Acc: 59.3750% F1: 0.370 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/5 iter: 5/15 loss: 0.7793 Acc: 68.7500% F1: 0.518 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/5 iter: 6/15 loss: 0.7807 Acc: 71.8750% F1: 0.351 Time: 0.93s (0.36s)
Fold 4 train - epoch: 3/5 iter: 7/15 loss: 0.7898 Acc: 56.2500% F1: 0.333 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/5 iter: 8/15 loss: 1.0609 Acc: 50.0000% F1: 0.283 Time: 0.93s (0.18s)
Fold 4 train - epoch: 3/5 iter: 9/15 loss: 0.9371 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/5 iter: 10/15 loss: 0.8773 Acc: 62.5000% F1: 0.423 Time: 0.93s (0.03s)
Fold 4 train - epoch: 3/5 iter: 11/15 loss: 0.7543 Acc: 62.5000% F1: 0.394 Time: 0.94s (0.02s)
Fold 4 train - epoch: 3/5 iter: 12/15 loss: 0.8498 Acc: 62.5000% F1: 0.482 Time: 0.94s (0.02s)
Fold 4 train - epoch: 3/5 iter: 13/15 loss: 0.8329 Acc: 59.3750% F1: 0.335 Time: 0.93s (0.02s)
Fold 4 train - epoch: 3/5 iter: 14/15 loss: 0.2045 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 3 train Avg acc: 60.8889% F1: 0.3817 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 3/5 iter: 0/2 loss: 0.7589 Acc: 65.6250% F1: 0.396 Time: 0.35s (0.00s)
Fold 4 train-dev - epoch: 3/5 iter: 1/2 loss: 1.6050 Acc: 11.1111% F1: 0.078 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.2597 *
*********************************************************
Performing epoch 4 of 5
Fold 4 train - epoch: 4/5 iter: 0/15 loss: 0.6885 Acc: 65.6250% F1: 0.512 Time: 0.94s (0.00s)
Fold 4 train - epoch: 4/5 iter: 1/15 loss: 0.7508 Acc: 75.0000% F1: 0.632 Time: 0.92s (0.03s)
Fold 4 train - epoch: 4/5 iter: 2/15 loss: 0.6921 Acc: 71.8750% F1: 0.637 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 3/15 loss: 0.6563 Acc: 68.7500% F1: 0.424 Time: 0.93s (0.04s)
Fold 4 train - epoch: 4/5 iter: 4/15 loss: 0.7035 Acc: 68.7500% F1: 0.583 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 5/15 loss: 0.6480 Acc: 78.1250% F1: 0.653 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 6/15 loss: 0.6996 Acc: 75.0000% F1: 0.461 Time: 0.93s (0.26s)
Fold 4 train - epoch: 4/5 iter: 7/15 loss: 0.6644 Acc: 59.3750% F1: 0.378 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 8/15 loss: 1.0554 Acc: 53.1250% F1: 0.400 Time: 0.93s (0.12s)
Fold 4 train - epoch: 4/5 iter: 9/15 loss: 0.8247 Acc: 68.7500% F1: 0.484 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 10/15 loss: 0.7888 Acc: 59.3750% F1: 0.585 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 11/15 loss: 0.6199 Acc: 68.7500% F1: 0.622 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 12/15 loss: 0.7584 Acc: 62.5000% F1: 0.482 Time: 0.93s (0.02s)
Fold 4 train - epoch: 4/5 iter: 13/15 loss: 0.7935 Acc: 62.5000% F1: 0.508 Time: 0.94s (0.02s)
Fold 4 train - epoch: 4/5 iter: 14/15 loss: 0.0673 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 4 Epoch 4 train Avg acc: 67.1111% F1: 0.5551 *
*****************************************************
Saving model...
Fold 4 train-dev - epoch: 4/5 iter: 0/2 loss: 0.8925 Acc: 59.3750% F1: 0.248 Time: 0.33s (0.00s)
Fold 4 train-dev - epoch: 4/5 iter: 1/2 loss: 1.7875 Acc: 22.2222% F1: 0.231 Time: 0.18s (0.00s)
*********************************************************
* Fold 4 Epoch 4 train-dev Avg acc: 46.0000% F1: 0.3853 *
*********************************************************
Creating 1 distributed models for fold 5...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[ 1.3939e-03, -6.8716e-02, -2.5557e-01,  2.8171e-02, -8.7808e-03,
         -2.5491e-01,  2.4635e-01, -1.7451e-01, -2.3248e-02,  2.3335e-01,
          2.6176e-01,  1.3909e-01, -1.2589e-01,  1.6319e-01,  2.4349e-02,
         -1.3283e-01,  4.8036e-02,  4.0980e-01, -2.1196e-01, -2.3831e-01,
         -9.6281e-03, -5.5775e-02,  1.1464e-02, -2.9264e-01,  9.9885e-02,
         -2.0102e-01, -1.9214e-01, -3.0318e-01, -4.2442e-02, -5.4449e-02,
         -1.2967e-04, -7.1719e-02,  2.0007e-01,  9.2549e-03,  5.3880e-02,
         -6.0826e-02,  8.9025e-02,  2.9168e-01,  2.3036e-01, -1.1622e-01,
         -6.1775e-02,  1.8710e-01, -1.6608e-01, -2.2137e-01,  1.6387e-01,
         -1.2285e-01,  3.0026e-01,  2.8564e-01, -3.5229e-01, -1.0878e-01,
          4.0280e-01,  1.5114e-01,  1.1360e-02, -2.0095e-02, -1.3847e-01,
          7.2091e-04, -1.1731e-01, -2.7096e-01,  2.7509e-02,  1.0600e-01,
          4.5664e-02,  1.3611e-01, -1.6287e-01, -2.6011e-01, -8.1596e-02,
          1.9391e-01, -6.8843e-02,  6.6795e-02,  3.0044e-01,  1.4518e-04,
          1.3836e-01, -2.1662e-01, -1.4751e-01, -4.5692e-03,  7.7596e-03,
          1.5091e-01, -1.9701e-01,  3.9364e-02,  2.4326e-02, -1.4290e-01,
         -2.3443e-01, -2.8338e-01,  2.4571e-01, -3.4772e-01, -2.1874e-01,
          4.0258e-02,  1.0768e-01,  2.4987e-01,  2.3627e-01, -1.1554e-01,
          2.1028e-01,  1.6711e-01, -1.5543e-01, -1.8313e-02, -3.3042e-01,
         -2.3139e-01,  1.1625e-01, -3.2922e-01, -1.7406e-01, -2.3016e-01,
         -1.3942e-01, -4.9668e-02, -2.7628e-01,  1.0165e-02,  8.2532e-02,
          6.4771e-02,  1.9546e-01, -3.2410e-01,  1.8964e-01, -1.6898e-01,
         -2.2098e-01, -1.2092e-01, -1.3189e-02,  2.6864e-02,  2.7784e-01,
         -6.3231e-02,  6.4781e-02,  3.4362e-01, -2.2732e-02, -3.5275e-01,
          9.8142e-03, -2.4852e-01, -2.7906e-02, -5.1745e-02, -9.5929e-02,
          1.7100e-02,  4.3980e-02, -4.3783e-02,  3.7871e-01,  1.2682e-01,
         -3.2154e-01, -2.1401e-01,  6.4307e-02,  7.8588e-02,  2.3951e-01,
          1.3997e-02, -7.8034e-02, -2.2260e-03,  1.9591e-02, -2.4945e-01,
          8.2607e-02, -3.2764e-02,  1.7143e-02,  2.0525e-01, -5.7016e-02,
          1.8229e-01, -2.7006e-02,  1.1357e-01,  9.7994e-02,  5.1746e-02,
          7.9860e-02,  3.8248e-02, -2.3227e-01, -1.3862e-01,  6.6153e-02,
          2.6249e-01, -5.7370e-02, -1.3890e-01, -4.0898e-01, -2.8864e-01,
         -2.3905e-02,  2.0195e-01,  1.1847e-01, -2.4378e-01, -1.7138e-01,
         -1.7932e-01,  2.7026e-01, -1.0431e-01,  8.6107e-02, -1.0131e-01,
         -8.2868e-02,  9.2653e-02, -3.3071e-02, -3.0312e-01, -2.7937e-01,
         -4.0222e-01, -3.8210e-01,  1.1642e-01, -3.3853e-02,  7.7303e-02,
          1.2355e-01, -2.2809e-01,  4.3379e-02,  1.0617e-02, -1.7532e-01,
         -1.5698e-01,  1.7562e-01, -6.4621e-02, -2.2386e-01, -5.4332e-02,
         -1.4223e-01,  1.1155e-01,  2.3546e-01,  3.8853e-02,  1.1227e-01,
         -2.9981e-02, -3.5384e-02, -1.3603e-01,  3.1001e-01, -1.4615e-01,
          2.5058e-02, -9.1489e-02, -2.1435e-01,  3.0697e-02, -2.5929e-02,
          3.2241e-01,  3.7920e-01,  2.0833e-01,  2.8212e-01, -1.0324e-01,
         -1.6585e-01, -1.8386e-01,  2.0189e-02,  2.7767e-01, -1.3822e-01,
         -1.3711e-01,  9.5865e-03,  1.1056e-02, -1.0575e-01, -5.5155e-02,
          1.1809e-01, -1.2300e-01,  2.4179e-01, -2.8298e-02,  7.4502e-02,
         -3.0307e-01,  2.4275e-02,  7.3545e-02, -6.9410e-02,  1.2794e-01,
          7.5693e-02,  1.5065e-01, -9.7550e-02,  3.4761e-01,  2.7095e-01,
         -4.5170e-02, -1.2539e-01,  3.8548e-03,  2.8808e-01, -9.7298e-02,
          3.0336e-02, -3.0973e-01,  7.9866e-02,  2.3738e-01,  1.2550e-01,
          7.3314e-03, -1.5217e-01,  2.0428e-01, -3.5740e-02,  1.4864e-01,
         -3.2093e-01, -5.4270e-02,  1.2042e-02, -1.4123e-01, -1.0701e-01,
         -3.3997e-01, -3.5408e-02, -1.8080e-01,  1.0898e-01,  1.1176e-01,
          1.4616e-01,  5.3214e-02, -8.9582e-02, -2.4846e-01, -2.1512e-01,
         -1.8585e-01, -7.4543e-02,  1.8495e-01,  1.3112e-01, -2.9189e-01,
         -2.3716e-01,  1.4231e-01,  1.8316e-01,  1.1146e-01,  1.0917e-01,
         -7.8276e-02, -3.5784e-01, -2.0579e-01, -1.9995e-01, -2.0056e-02,
          1.9377e-01,  2.1209e-01,  1.1557e-01,  1.5575e-01,  3.7109e-02,
         -1.9799e-01,  1.2692e-01, -1.2687e-01,  3.2986e-01, -4.0038e-01,
         -5.4040e-02,  1.9603e-01, -1.0432e-01, -4.4281e-02, -9.4947e-02,
          1.9225e-01,  2.4538e-01,  1.6693e-01,  1.7623e-02,  2.7655e-01,
         -2.7252e-01, -3.5374e-02,  2.1649e-01,  1.1542e-01, -3.2575e-01,
         -9.5898e-02, -1.7769e-01, -1.7478e-01,  1.2413e-02, -9.4918e-02,
         -1.7591e-01, -7.7178e-02,  8.4715e-02,  1.1410e-01,  2.3401e-01,
          4.6640e-02,  1.6343e-02,  8.4249e-02, -9.6963e-02,  4.9761e-03,
         -1.2689e-01, -8.9776e-02,  1.4109e-01, -2.3152e-01,  3.0876e-01,
         -2.1192e-01,  2.8687e-01, -1.6530e-01,  1.0156e-01,  1.3087e-01,
         -1.3041e-01,  2.9489e-01, -1.7078e-01,  2.2375e-01,  1.1248e-01,
          7.6252e-02, -2.7074e-02,  7.4888e-02, -2.2466e-01,  5.8946e-02,
         -1.5730e-01,  1.9001e-01,  1.6656e-01,  2.3558e-01,  8.8893e-02,
          4.6385e-02, -4.6853e-03, -3.9195e-02, -6.8834e-02,  6.7098e-02,
         -2.5421e-01, -1.3098e-01,  3.3280e-01, -1.1145e-01,  3.0578e-01,
          1.4134e-01, -7.8914e-02,  1.6024e-01, -2.5977e-01,  4.2665e-02,
          1.7734e-01, -1.5616e-01, -3.0725e-01, -1.6099e-01, -9.4044e-02,
          5.9506e-02,  2.2976e-01, -1.7763e-01, -2.6950e-01,  3.8000e-01,
          1.7562e-01,  3.7771e-02,  1.0278e-01,  3.2222e-01, -2.1560e-02,
         -1.5225e-01, -2.3036e-01, -1.3021e-01, -3.0180e-01,  2.4546e-01,
          6.7754e-02, -3.0570e-01, -2.9963e-01,  1.5654e-02, -1.8943e-01,
         -2.3144e-01,  2.8879e-01,  7.4722e-02, -1.5568e-01, -2.0820e-01,
          4.9041e-02,  3.9942e-01,  4.3492e-02,  8.4227e-02,  1.3005e-01,
         -9.3410e-02,  2.1089e-01,  8.5407e-02, -2.3646e-01,  1.1235e-01,
          2.1036e-01, -1.5971e-01,  7.9207e-02,  4.4568e-03, -2.2224e-01,
          3.1588e-02, -2.5247e-01, -1.8240e-01, -8.3811e-02, -3.5396e-01,
         -9.2816e-02,  3.5115e-02,  2.7964e-01, -9.2329e-02, -3.5224e-01,
         -2.4336e-01,  1.5316e-01, -1.8951e-01, -1.5665e-01, -2.2028e-01,
         -2.0556e-01, -1.5417e-01,  4.3107e-02, -1.4996e-01, -1.7838e-01,
         -1.0692e-01, -1.7765e-01, -8.7658e-03,  1.6875e-01, -2.1353e-01,
          1.7508e-01, -8.9377e-02, -1.9662e-01,  2.5831e-02, -5.6539e-02,
          5.4678e-02,  2.4562e-01,  9.2557e-02, -5.1224e-02, -2.1296e-02,
         -2.0423e-01,  4.4212e-03, -1.5627e-01,  1.5368e-01, -9.0215e-02,
          4.5537e-02,  2.7957e-01, -1.7029e-01,  2.9199e-01, -9.2008e-02,
         -1.1972e-01, -1.1973e-01, -3.9062e-01, -6.3714e-03, -1.8681e-02,
          4.3476e-02,  1.0941e-01,  1.5459e-02, -2.8024e-02, -2.9083e-01,
         -2.1282e-01,  1.7896e-01, -1.1782e-01, -2.9580e-01,  9.5791e-02,
         -1.6213e-01, -1.0290e-03,  9.6139e-02, -6.0271e-02,  7.4006e-02,
          1.4452e-01, -6.5647e-02,  2.0084e-01, -1.4069e-02, -1.2897e-01,
          2.6349e-01, -2.3983e-01,  1.7705e-02,  1.7037e-01,  3.0223e-02,
         -2.9845e-01,  1.0508e-01, -1.6406e-01, -3.5936e-02, -8.8760e-02,
         -2.6679e-01,  3.3564e-01,  3.7482e-04,  5.0903e-02, -7.3588e-02,
         -1.6273e-01, -5.2870e-02, -2.3405e-01, -7.3421e-02,  6.7705e-03,
         -1.6738e-01, -3.5341e-02,  1.6683e-01, -1.5272e-01,  3.9206e-01,
          2.2943e-01, -1.5948e-01, -2.8566e-02, -1.0889e-02, -2.4224e-01,
          6.9751e-02,  9.7223e-03, -2.8483e-01, -6.3500e-02,  1.8432e-01,
          4.8258e-02,  3.6758e-01, -1.7792e-01,  1.7039e-02,  3.1149e-01,
          9.3328e-02, -2.3158e-01, -3.5395e-01, -1.0560e-01,  2.2844e-01,
          8.0793e-02,  1.4810e-01, -3.0206e-01, -3.4022e-01,  1.7620e-01,
          4.3809e-02,  9.0087e-02,  1.5914e-01, -6.8204e-02,  2.5665e-01,
         -1.8012e-01,  3.7484e-02,  4.8088e-02,  2.3510e-02, -1.1832e-01,
          1.0292e-01,  1.4461e-01,  9.2944e-02, -1.4286e-03,  1.8287e-01,
          1.7623e-01,  6.0158e-02,  1.7079e-01,  1.9798e-01, -1.8351e-01,
         -4.1477e-01,  2.4582e-01,  1.0249e-01, -3.3297e-02, -2.0311e-01,
         -2.5807e-01, -2.9441e-02,  2.5160e-01, -2.8196e-01, -2.2125e-01,
         -1.3952e-01, -5.6166e-02,  2.8915e-02, -1.9787e-01, -1.6558e-01,
         -1.7816e-01, -3.1428e-01, -1.7320e-01,  1.2468e-02, -3.4252e-01,
         -2.5784e-01, -7.1416e-02, -9.2996e-02,  1.6461e-01, -3.2702e-01,
          1.4028e-01,  2.0673e-01, -5.7455e-02, -2.6848e-01,  1.1147e-01,
         -4.8179e-02,  9.7699e-02, -2.2503e-01, -1.3832e-01,  1.9701e-02,
         -8.6869e-02, -4.8751e-02,  6.2277e-02, -3.3196e-01,  2.0513e-01,
          4.2906e-02, -3.1485e-01, -1.9783e-01, -1.8252e-01,  8.9203e-03,
          2.3651e-01, -1.7068e-01,  9.3493e-02,  1.2389e-01, -2.1146e-01,
          8.2649e-03,  1.5572e-01, -2.2668e-01,  6.7394e-02,  1.9482e-02,
          7.9862e-02, -7.1562e-02,  8.8041e-02,  2.9078e-01, -2.9057e-01,
          1.9924e-01,  7.9529e-02,  2.3030e-01, -1.8758e-01, -2.1552e-01,
          1.6985e-01, -1.6580e-01, -2.1389e-01, -2.4635e-01, -1.9783e-01,
          7.4881e-02, -9.2235e-02,  5.3853e-02, -2.7720e-01, -4.0823e-02,
          2.2342e-01,  1.6657e-01, -1.2662e-01, -1.9808e-02,  5.6801e-02,
          2.1904e-02,  6.3948e-02, -2.0601e-01, -2.2994e-01, -1.4389e-01,
          5.3174e-02,  6.4131e-02,  4.4927e-02, -3.7137e-01, -1.5741e-01,
         -2.5703e-01,  1.2718e-01,  2.0193e-02,  3.8737e-02, -7.7474e-02,
          1.9992e-02, -1.8949e-01, -1.0489e-01, -6.3354e-02, -2.5072e-01,
         -2.2850e-01, -1.9215e-01, -2.7560e-01, -5.5963e-02, -1.2828e-01,
          1.2989e-01,  8.0240e-02,  1.1768e-01,  3.6903e-01,  3.2748e-01,
          2.8244e-01,  7.6465e-02, -1.3270e-01,  1.6062e-01,  1.6020e-02,
          1.8847e-01, -1.0488e-01, -2.7101e-01,  2.8572e-01,  2.2558e-01,
         -1.0301e-01,  1.4395e-01,  1.6343e-01, -2.3641e-01, -1.2907e-01,
         -2.7267e-02,  2.6302e-01, -1.8550e-01,  2.4225e-01,  1.8058e-01,
         -2.0332e-01, -1.1652e-01,  2.2879e-01, -6.5583e-02,  3.0162e-02,
          5.5261e-02,  3.3168e-02,  1.4287e-03,  2.4233e-01, -2.5623e-01,
          1.4555e-01, -9.7221e-02, -1.4414e-01,  2.5351e-01, -1.6486e-01,
          8.8751e-03, -1.3138e-01, -5.0337e-02, -2.3413e-01, -1.2613e-01,
          1.0165e-01,  1.7950e-01, -4.9932e-02,  1.9990e-01, -2.5763e-01,
         -3.1997e-01, -1.8444e-01, -2.4811e-01, -6.7918e-02,  2.8393e-01,
          3.9342e-02, -3.8641e-01, -2.4957e-01, -4.2098e-02,  1.2375e-01,
         -2.3292e-01, -1.2942e-01, -9.9817e-02,  2.1444e-02, -2.8142e-02,
         -6.4317e-02, -2.3902e-01, -3.0746e-01, -8.9892e-02, -1.2646e-01,
         -3.2790e-01,  2.3242e-01,  9.5534e-02, -1.0982e-01,  2.6479e-01,
          1.5996e-01, -3.1966e-01, -1.9431e-01,  7.6029e-02, -2.0056e-01,
         -4.4122e-02, -1.9894e-01, -3.2973e-01,  1.0175e-01, -3.6016e-02,
         -1.1808e-01, -1.6042e-01, -2.7750e-01,  4.7654e-02, -6.1258e-02,
         -4.5565e-01, -1.4322e-01, -1.6719e-01,  1.1150e-01, -6.1786e-02,
         -1.3581e-01, -1.2923e-01, -8.6849e-02, -2.0869e-01, -9.5694e-02,
         -3.2178e-01,  4.5601e-02,  9.0810e-02, -3.4218e-01, -1.2418e-01,
         -2.1381e-01, -1.2038e-01, -4.3754e-02,  6.3526e-02,  9.4710e-02,
          1.8775e-02, -2.5421e-01,  9.3967e-03,  2.0880e-01,  1.6493e-01,
         -2.8366e-01, -1.0564e-01,  2.7419e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train - epoch: 0/5 iter: 0/15 loss: 1.1264 Acc: 34.3750% F1: 0.248 Time: 0.97s (0.00s)
Fold 5 train - epoch: 0/5 iter: 1/15 loss: 0.9183 Acc: 59.3750% F1: 0.306 Time: 0.93s (0.03s)
Fold 5 train - epoch: 0/5 iter: 2/15 loss: 1.0527 Acc: 50.0000% F1: 0.222 Time: 0.95s (6.80s)
Fold 5 train - epoch: 0/5 iter: 3/15 loss: 0.9460 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/5 iter: 4/15 loss: 1.0260 Acc: 50.0000% F1: 0.222 Time: 0.93s (6.32s)
Fold 5 train - epoch: 0/5 iter: 5/15 loss: 0.9197 Acc: 56.2500% F1: 0.245 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/5 iter: 6/15 loss: 0.8772 Acc: 53.1250% F1: 0.301 Time: 0.93s (6.34s)
Fold 5 train - epoch: 0/5 iter: 7/15 loss: 0.8991 Acc: 43.7500% F1: 0.254 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/5 iter: 8/15 loss: 1.2613 Acc: 28.1250% F1: 0.154 Time: 0.93s (6.67s)
Fold 5 train - epoch: 0/5 iter: 9/15 loss: 1.0239 Acc: 50.0000% F1: 0.317 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/5 iter: 10/15 loss: 0.9921 Acc: 40.6250% F1: 0.266 Time: 0.93s (6.51s)
Fold 5 train - epoch: 0/5 iter: 11/15 loss: 0.9288 Acc: 53.1250% F1: 0.308 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/5 iter: 12/15 loss: 1.0169 Acc: 59.3750% F1: 0.306 Time: 0.95s (7.05s)
Fold 5 train - epoch: 0/5 iter: 13/15 loss: 0.9084 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 5 train - epoch: 0/5 iter: 14/15 loss: 0.6042 Acc: 100.0000% F1: 1.000 Time: 0.10s (0.02s)
*****************************************************
* Fold 5 Epoch 0 train Avg acc: 49.7778% F1: 0.2773 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 5 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5865 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 0/5 iter: 1/2 loss: 1.6811 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 5 train - epoch: 1/5 iter: 0/15 loss: 0.8751 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 5 train - epoch: 1/5 iter: 1/15 loss: 0.9270 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/5 iter: 2/15 loss: 0.9591 Acc: 50.0000% F1: 0.222 Time: 0.93s (6.31s)
Fold 5 train - epoch: 1/5 iter: 3/15 loss: 0.8271 Acc: 59.3750% F1: 0.248 Time: 0.92s (0.02s)
Fold 5 train - epoch: 1/5 iter: 4/15 loss: 1.0091 Acc: 50.0000% F1: 0.222 Time: 0.92s (5.93s)
Fold 5 train - epoch: 1/5 iter: 5/15 loss: 0.8483 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/5 iter: 6/15 loss: 0.7705 Acc: 68.7500% F1: 0.272 Time: 0.93s (5.97s)
Fold 5 train - epoch: 1/5 iter: 7/15 loss: 0.8675 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/5 iter: 8/15 loss: 1.1292 Acc: 53.1250% F1: 0.231 Time: 0.95s (6.29s)
Fold 5 train - epoch: 1/5 iter: 9/15 loss: 1.0297 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.03s)
Fold 5 train - epoch: 1/5 iter: 10/15 loss: 0.9648 Acc: 56.2500% F1: 0.352 Time: 0.93s (5.92s)
Fold 5 train - epoch: 1/5 iter: 11/15 loss: 0.8856 Acc: 62.5000% F1: 0.377 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/5 iter: 12/15 loss: 0.9700 Acc: 56.2500% F1: 0.326 Time: 0.92s (6.53s)
Fold 5 train - epoch: 1/5 iter: 13/15 loss: 0.9257 Acc: 50.0000% F1: 0.291 Time: 0.93s (0.02s)
Fold 5 train - epoch: 1/5 iter: 14/15 loss: 0.6957 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 1 train Avg acc: 55.3333% F1: 0.2737 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7809 Acc: 71.8750% F1: 0.418 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3578 Acc: 22.2222% F1: 0.157 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 1 train-dev Avg acc: 54.0000% F1: 0.3281 *
*********************************************************
Performing epoch 2 of 5
Fold 5 train - epoch: 2/5 iter: 0/15 loss: 0.8372 Acc: 56.2500% F1: 0.326 Time: 0.94s (0.00s)
Fold 5 train - epoch: 2/5 iter: 1/15 loss: 0.8689 Acc: 68.7500% F1: 0.451 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/5 iter: 2/15 loss: 0.8564 Acc: 56.2500% F1: 0.321 Time: 0.95s (6.61s)
Fold 5 train - epoch: 2/5 iter: 3/15 loss: 0.7893 Acc: 59.3750% F1: 0.253 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/5 iter: 4/15 loss: 0.9132 Acc: 53.1250% F1: 0.278 Time: 0.93s (5.85s)
Fold 5 train - epoch: 2/5 iter: 5/15 loss: 0.8333 Acc: 62.5000% F1: 0.320 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/5 iter: 6/15 loss: 0.7605 Acc: 68.7500% F1: 0.272 Time: 0.93s (5.89s)
Fold 5 train - epoch: 2/5 iter: 7/15 loss: 0.8780 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/5 iter: 8/15 loss: 1.0916 Acc: 53.1250% F1: 0.231 Time: 0.93s (6.17s)
Fold 5 train - epoch: 2/5 iter: 9/15 loss: 1.0521 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/5 iter: 10/15 loss: 0.9696 Acc: 50.0000% F1: 0.265 Time: 0.93s (5.85s)
Fold 5 train - epoch: 2/5 iter: 11/15 loss: 0.8194 Acc: 68.7500% F1: 0.383 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/5 iter: 12/15 loss: 0.8829 Acc: 59.3750% F1: 0.306 Time: 0.93s (6.46s)
Fold 5 train - epoch: 2/5 iter: 13/15 loss: 0.8668 Acc: 56.2500% F1: 0.245 Time: 0.93s (0.02s)
Fold 5 train - epoch: 2/5 iter: 14/15 loss: 0.3924 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 2 train Avg acc: 57.7778% F1: 0.2959 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7804 Acc: 62.5000% F1: 0.385 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4660 Acc: 16.6667% F1: 0.118 Time: 0.19s (0.00s)
*********************************************************
* Fold 5 Epoch 2 train-dev Avg acc: 46.0000% F1: 0.2741 *
*********************************************************
Performing epoch 3 of 5
Fold 5 train - epoch: 3/5 iter: 0/15 loss: 0.8210 Acc: 53.1250% F1: 0.311 Time: 0.95s (0.00s)
Fold 5 train - epoch: 3/5 iter: 1/15 loss: 0.7794 Acc: 68.7500% F1: 0.476 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/5 iter: 2/15 loss: 0.8632 Acc: 56.2500% F1: 0.345 Time: 0.95s (6.32s)
Fold 5 train - epoch: 3/5 iter: 3/15 loss: 0.7503 Acc: 71.8750% F1: 0.442 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/5 iter: 4/15 loss: 0.8123 Acc: 68.7500% F1: 0.643 Time: 0.92s (5.94s)
Fold 5 train - epoch: 3/5 iter: 5/15 loss: 0.7763 Acc: 65.6250% F1: 0.500 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/5 iter: 6/15 loss: 0.7763 Acc: 78.1250% F1: 0.469 Time: 0.93s (6.04s)
Fold 5 train - epoch: 3/5 iter: 7/15 loss: 0.8159 Acc: 59.3750% F1: 0.379 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/5 iter: 8/15 loss: 1.0141 Acc: 56.2500% F1: 0.345 Time: 0.93s (6.28s)
Fold 5 train - epoch: 3/5 iter: 9/15 loss: 0.9964 Acc: 50.0000% F1: 0.295 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/5 iter: 10/15 loss: 0.8675 Acc: 53.1250% F1: 0.352 Time: 0.93s (5.97s)
Fold 5 train - epoch: 3/5 iter: 11/15 loss: 0.7547 Acc: 65.6250% F1: 0.392 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/5 iter: 12/15 loss: 0.9167 Acc: 59.3750% F1: 0.306 Time: 0.93s (6.60s)
Fold 5 train - epoch: 3/5 iter: 13/15 loss: 0.8865 Acc: 56.2500% F1: 0.287 Time: 0.93s (0.02s)
Fold 5 train - epoch: 3/5 iter: 14/15 loss: 0.1375 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 3 train Avg acc: 61.7778% F1: 0.4137 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8593 Acc: 59.3750% F1: 0.373 Time: 0.34s (0.00s)
Fold 5 train-dev - epoch: 3/5 iter: 1/2 loss: 1.5274 Acc: 16.6667% F1: 0.118 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 3 train-dev Avg acc: 44.0000% F1: 0.2677 *
*********************************************************
Performing epoch 4 of 5
Fold 5 train - epoch: 4/5 iter: 0/15 loss: 0.7062 Acc: 75.0000% F1: 0.683 Time: 0.95s (0.00s)
Fold 5 train - epoch: 4/5 iter: 1/15 loss: 0.6971 Acc: 68.7500% F1: 0.668 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/5 iter: 2/15 loss: 0.8678 Acc: 59.3750% F1: 0.386 Time: 0.94s (6.11s)
Fold 5 train - epoch: 4/5 iter: 3/15 loss: 0.6300 Acc: 75.0000% F1: 0.490 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/5 iter: 4/15 loss: 0.6671 Acc: 81.2500% F1: 0.773 Time: 0.93s (5.75s)
Fold 5 train - epoch: 4/5 iter: 5/15 loss: 0.6505 Acc: 71.8750% F1: 0.664 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/5 iter: 6/15 loss: 0.5568 Acc: 81.2500% F1: 0.530 Time: 0.93s (5.87s)
Fold 5 train - epoch: 4/5 iter: 7/15 loss: 0.6889 Acc: 68.7500% F1: 0.453 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/5 iter: 8/15 loss: 0.9657 Acc: 56.2500% F1: 0.345 Time: 0.92s (6.10s)
Fold 5 train - epoch: 4/5 iter: 9/15 loss: 0.8348 Acc: 62.5000% F1: 0.430 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/5 iter: 10/15 loss: 0.6960 Acc: 65.6250% F1: 0.567 Time: 0.93s (5.81s)
Fold 5 train - epoch: 4/5 iter: 11/15 loss: 0.5803 Acc: 68.7500% F1: 0.585 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/5 iter: 12/15 loss: 0.7027 Acc: 75.0000% F1: 0.632 Time: 0.93s (6.38s)
Fold 5 train - epoch: 4/5 iter: 13/15 loss: 0.7531 Acc: 68.7500% F1: 0.587 Time: 0.93s (0.02s)
Fold 5 train - epoch: 4/5 iter: 14/15 loss: 0.0576 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 5 Epoch 4 train Avg acc: 70.0000% F1: 0.5886 *
*****************************************************
Saving model...
Fold 5 train-dev - epoch: 4/5 iter: 0/2 loss: 1.3198 Acc: 37.5000% F1: 0.225 Time: 0.33s (0.00s)
Fold 5 train-dev - epoch: 4/5 iter: 1/2 loss: 1.5369 Acc: 33.3333% F1: 0.259 Time: 0.18s (0.00s)
*********************************************************
* Fold 5 Epoch 4 train-dev Avg acc: 36.0000% F1: 0.3212 *
*********************************************************
Creating 1 distributed models for fold 6...
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at boychaboy/SNLI_roberta-base were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at boychaboy/SNLI_roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Initializing PubmedQARunner on device 0...
	torch.cuda.device_count(): 1
PubmedQARunner on device 0 creating datasets...
dtes[0]: tensor([[ 1.3939e-03, -6.8716e-02, -2.5557e-01,  2.8171e-02, -8.7808e-03,
         -2.5491e-01,  2.4635e-01, -1.7451e-01, -2.3248e-02,  2.3335e-01,
          2.6176e-01,  1.3909e-01, -1.2589e-01,  1.6319e-01,  2.4349e-02,
         -1.3283e-01,  4.8036e-02,  4.0980e-01, -2.1196e-01, -2.3831e-01,
         -9.6281e-03, -5.5775e-02,  1.1464e-02, -2.9264e-01,  9.9885e-02,
         -2.0102e-01, -1.9214e-01, -3.0318e-01, -4.2442e-02, -5.4449e-02,
         -1.2967e-04, -7.1719e-02,  2.0007e-01,  9.2549e-03,  5.3880e-02,
         -6.0826e-02,  8.9025e-02,  2.9168e-01,  2.3036e-01, -1.1622e-01,
         -6.1775e-02,  1.8710e-01, -1.6608e-01, -2.2137e-01,  1.6387e-01,
         -1.2285e-01,  3.0026e-01,  2.8564e-01, -3.5229e-01, -1.0878e-01,
          4.0280e-01,  1.5114e-01,  1.1360e-02, -2.0095e-02, -1.3847e-01,
          7.2091e-04, -1.1731e-01, -2.7096e-01,  2.7509e-02,  1.0600e-01,
          4.5664e-02,  1.3611e-01, -1.6287e-01, -2.6011e-01, -8.1596e-02,
          1.9391e-01, -6.8843e-02,  6.6795e-02,  3.0044e-01,  1.4518e-04,
          1.3836e-01, -2.1662e-01, -1.4751e-01, -4.5692e-03,  7.7596e-03,
          1.5091e-01, -1.9701e-01,  3.9364e-02,  2.4326e-02, -1.4290e-01,
         -2.3443e-01, -2.8338e-01,  2.4571e-01, -3.4772e-01, -2.1874e-01,
          4.0258e-02,  1.0768e-01,  2.4987e-01,  2.3627e-01, -1.1554e-01,
          2.1028e-01,  1.6711e-01, -1.5543e-01, -1.8313e-02, -3.3042e-01,
         -2.3139e-01,  1.1625e-01, -3.2922e-01, -1.7406e-01, -2.3016e-01,
         -1.3942e-01, -4.9668e-02, -2.7628e-01,  1.0165e-02,  8.2532e-02,
          6.4771e-02,  1.9546e-01, -3.2410e-01,  1.8964e-01, -1.6898e-01,
         -2.2098e-01, -1.2092e-01, -1.3189e-02,  2.6864e-02,  2.7784e-01,
         -6.3231e-02,  6.4781e-02,  3.4362e-01, -2.2732e-02, -3.5275e-01,
          9.8142e-03, -2.4852e-01, -2.7906e-02, -5.1745e-02, -9.5929e-02,
          1.7100e-02,  4.3980e-02, -4.3783e-02,  3.7871e-01,  1.2682e-01,
         -3.2154e-01, -2.1401e-01,  6.4307e-02,  7.8588e-02,  2.3951e-01,
          1.3997e-02, -7.8034e-02, -2.2260e-03,  1.9591e-02, -2.4945e-01,
          8.2607e-02, -3.2764e-02,  1.7143e-02,  2.0525e-01, -5.7016e-02,
          1.8229e-01, -2.7006e-02,  1.1357e-01,  9.7994e-02,  5.1746e-02,
          7.9860e-02,  3.8248e-02, -2.3227e-01, -1.3862e-01,  6.6153e-02,
          2.6249e-01, -5.7370e-02, -1.3890e-01, -4.0898e-01, -2.8864e-01,
         -2.3905e-02,  2.0195e-01,  1.1847e-01, -2.4378e-01, -1.7138e-01,
         -1.7932e-01,  2.7026e-01, -1.0431e-01,  8.6107e-02, -1.0131e-01,
         -8.2868e-02,  9.2653e-02, -3.3071e-02, -3.0312e-01, -2.7937e-01,
         -4.0222e-01, -3.8210e-01,  1.1642e-01, -3.3853e-02,  7.7303e-02,
          1.2355e-01, -2.2809e-01,  4.3379e-02,  1.0617e-02, -1.7532e-01,
         -1.5698e-01,  1.7562e-01, -6.4621e-02, -2.2386e-01, -5.4332e-02,
         -1.4223e-01,  1.1155e-01,  2.3546e-01,  3.8853e-02,  1.1227e-01,
         -2.9981e-02, -3.5384e-02, -1.3603e-01,  3.1001e-01, -1.4615e-01,
          2.5058e-02, -9.1489e-02, -2.1435e-01,  3.0697e-02, -2.5929e-02,
          3.2241e-01,  3.7920e-01,  2.0833e-01,  2.8212e-01, -1.0324e-01,
         -1.6585e-01, -1.8386e-01,  2.0189e-02,  2.7767e-01, -1.3822e-01,
         -1.3711e-01,  9.5865e-03,  1.1056e-02, -1.0575e-01, -5.5155e-02,
          1.1809e-01, -1.2300e-01,  2.4179e-01, -2.8298e-02,  7.4502e-02,
         -3.0307e-01,  2.4275e-02,  7.3545e-02, -6.9410e-02,  1.2794e-01,
          7.5693e-02,  1.5065e-01, -9.7550e-02,  3.4761e-01,  2.7095e-01,
         -4.5170e-02, -1.2539e-01,  3.8548e-03,  2.8808e-01, -9.7298e-02,
          3.0336e-02, -3.0973e-01,  7.9866e-02,  2.3738e-01,  1.2550e-01,
          7.3314e-03, -1.5217e-01,  2.0428e-01, -3.5740e-02,  1.4864e-01,
         -3.2093e-01, -5.4270e-02,  1.2042e-02, -1.4123e-01, -1.0701e-01,
         -3.3997e-01, -3.5408e-02, -1.8080e-01,  1.0898e-01,  1.1176e-01,
          1.4616e-01,  5.3214e-02, -8.9582e-02, -2.4846e-01, -2.1512e-01,
         -1.8585e-01, -7.4543e-02,  1.8495e-01,  1.3112e-01, -2.9189e-01,
         -2.3716e-01,  1.4231e-01,  1.8316e-01,  1.1146e-01,  1.0917e-01,
         -7.8276e-02, -3.5784e-01, -2.0579e-01, -1.9995e-01, -2.0056e-02,
          1.9377e-01,  2.1209e-01,  1.1557e-01,  1.5575e-01,  3.7109e-02,
         -1.9799e-01,  1.2692e-01, -1.2687e-01,  3.2986e-01, -4.0038e-01,
         -5.4040e-02,  1.9603e-01, -1.0432e-01, -4.4281e-02, -9.4947e-02,
          1.9225e-01,  2.4538e-01,  1.6693e-01,  1.7623e-02,  2.7655e-01,
         -2.7252e-01, -3.5374e-02,  2.1649e-01,  1.1542e-01, -3.2575e-01,
         -9.5898e-02, -1.7769e-01, -1.7478e-01,  1.2413e-02, -9.4918e-02,
         -1.7591e-01, -7.7178e-02,  8.4715e-02,  1.1410e-01,  2.3401e-01,
          4.6640e-02,  1.6343e-02,  8.4249e-02, -9.6963e-02,  4.9761e-03,
         -1.2689e-01, -8.9776e-02,  1.4109e-01, -2.3152e-01,  3.0876e-01,
         -2.1192e-01,  2.8687e-01, -1.6530e-01,  1.0156e-01,  1.3087e-01,
         -1.3041e-01,  2.9489e-01, -1.7078e-01,  2.2375e-01,  1.1248e-01,
          7.6252e-02, -2.7074e-02,  7.4888e-02, -2.2466e-01,  5.8946e-02,
         -1.5730e-01,  1.9001e-01,  1.6656e-01,  2.3558e-01,  8.8893e-02,
          4.6385e-02, -4.6853e-03, -3.9195e-02, -6.8834e-02,  6.7098e-02,
         -2.5421e-01, -1.3098e-01,  3.3280e-01, -1.1145e-01,  3.0578e-01,
          1.4134e-01, -7.8914e-02,  1.6024e-01, -2.5977e-01,  4.2665e-02,
          1.7734e-01, -1.5616e-01, -3.0725e-01, -1.6099e-01, -9.4044e-02,
          5.9506e-02,  2.2976e-01, -1.7763e-01, -2.6950e-01,  3.8000e-01,
          1.7562e-01,  3.7771e-02,  1.0278e-01,  3.2222e-01, -2.1560e-02,
         -1.5225e-01, -2.3036e-01, -1.3021e-01, -3.0180e-01,  2.4546e-01,
          6.7754e-02, -3.0570e-01, -2.9963e-01,  1.5654e-02, -1.8943e-01,
         -2.3144e-01,  2.8879e-01,  7.4722e-02, -1.5568e-01, -2.0820e-01,
          4.9041e-02,  3.9942e-01,  4.3492e-02,  8.4227e-02,  1.3005e-01,
         -9.3410e-02,  2.1089e-01,  8.5407e-02, -2.3646e-01,  1.1235e-01,
          2.1036e-01, -1.5971e-01,  7.9207e-02,  4.4568e-03, -2.2224e-01,
          3.1588e-02, -2.5247e-01, -1.8240e-01, -8.3811e-02, -3.5396e-01,
         -9.2816e-02,  3.5115e-02,  2.7964e-01, -9.2329e-02, -3.5224e-01,
         -2.4336e-01,  1.5316e-01, -1.8951e-01, -1.5665e-01, -2.2028e-01,
         -2.0556e-01, -1.5417e-01,  4.3107e-02, -1.4996e-01, -1.7838e-01,
         -1.0692e-01, -1.7765e-01, -8.7658e-03,  1.6875e-01, -2.1353e-01,
          1.7508e-01, -8.9377e-02, -1.9662e-01,  2.5831e-02, -5.6539e-02,
          5.4678e-02,  2.4562e-01,  9.2557e-02, -5.1224e-02, -2.1296e-02,
         -2.0423e-01,  4.4212e-03, -1.5627e-01,  1.5368e-01, -9.0215e-02,
          4.5537e-02,  2.7957e-01, -1.7029e-01,  2.9199e-01, -9.2008e-02,
         -1.1972e-01, -1.1973e-01, -3.9062e-01, -6.3714e-03, -1.8681e-02,
          4.3476e-02,  1.0941e-01,  1.5459e-02, -2.8024e-02, -2.9083e-01,
         -2.1282e-01,  1.7896e-01, -1.1782e-01, -2.9580e-01,  9.5791e-02,
         -1.6213e-01, -1.0290e-03,  9.6139e-02, -6.0271e-02,  7.4006e-02,
          1.4452e-01, -6.5647e-02,  2.0084e-01, -1.4069e-02, -1.2897e-01,
          2.6349e-01, -2.3983e-01,  1.7705e-02,  1.7037e-01,  3.0223e-02,
         -2.9845e-01,  1.0508e-01, -1.6406e-01, -3.5936e-02, -8.8760e-02,
         -2.6679e-01,  3.3564e-01,  3.7482e-04,  5.0903e-02, -7.3588e-02,
         -1.6273e-01, -5.2870e-02, -2.3405e-01, -7.3421e-02,  6.7705e-03,
         -1.6738e-01, -3.5341e-02,  1.6683e-01, -1.5272e-01,  3.9206e-01,
          2.2943e-01, -1.5948e-01, -2.8566e-02, -1.0889e-02, -2.4224e-01,
          6.9751e-02,  9.7223e-03, -2.8483e-01, -6.3500e-02,  1.8432e-01,
          4.8258e-02,  3.6758e-01, -1.7792e-01,  1.7039e-02,  3.1149e-01,
          9.3328e-02, -2.3158e-01, -3.5395e-01, -1.0560e-01,  2.2844e-01,
          8.0793e-02,  1.4810e-01, -3.0206e-01, -3.4022e-01,  1.7620e-01,
          4.3809e-02,  9.0087e-02,  1.5914e-01, -6.8204e-02,  2.5665e-01,
         -1.8012e-01,  3.7484e-02,  4.8088e-02,  2.3510e-02, -1.1832e-01,
          1.0292e-01,  1.4461e-01,  9.2944e-02, -1.4286e-03,  1.8287e-01,
          1.7623e-01,  6.0158e-02,  1.7079e-01,  1.9798e-01, -1.8351e-01,
         -4.1477e-01,  2.4582e-01,  1.0249e-01, -3.3297e-02, -2.0311e-01,
         -2.5807e-01, -2.9441e-02,  2.5160e-01, -2.8196e-01, -2.2125e-01,
         -1.3952e-01, -5.6166e-02,  2.8915e-02, -1.9787e-01, -1.6558e-01,
         -1.7816e-01, -3.1428e-01, -1.7320e-01,  1.2468e-02, -3.4252e-01,
         -2.5784e-01, -7.1416e-02, -9.2996e-02,  1.6461e-01, -3.2702e-01,
          1.4028e-01,  2.0673e-01, -5.7455e-02, -2.6848e-01,  1.1147e-01,
         -4.8179e-02,  9.7699e-02, -2.2503e-01, -1.3832e-01,  1.9701e-02,
         -8.6869e-02, -4.8751e-02,  6.2277e-02, -3.3196e-01,  2.0513e-01,
          4.2906e-02, -3.1485e-01, -1.9783e-01, -1.8252e-01,  8.9203e-03,
          2.3651e-01, -1.7068e-01,  9.3493e-02,  1.2389e-01, -2.1146e-01,
          8.2649e-03,  1.5572e-01, -2.2668e-01,  6.7394e-02,  1.9482e-02,
          7.9862e-02, -7.1562e-02,  8.8041e-02,  2.9078e-01, -2.9057e-01,
          1.9924e-01,  7.9529e-02,  2.3030e-01, -1.8758e-01, -2.1552e-01,
          1.6985e-01, -1.6580e-01, -2.1389e-01, -2.4635e-01, -1.9783e-01,
          7.4881e-02, -9.2235e-02,  5.3853e-02, -2.7720e-01, -4.0823e-02,
          2.2342e-01,  1.6657e-01, -1.2662e-01, -1.9808e-02,  5.6801e-02,
          2.1904e-02,  6.3948e-02, -2.0601e-01, -2.2994e-01, -1.4389e-01,
          5.3174e-02,  6.4131e-02,  4.4927e-02, -3.7137e-01, -1.5741e-01,
         -2.5703e-01,  1.2718e-01,  2.0193e-02,  3.8737e-02, -7.7474e-02,
          1.9992e-02, -1.8949e-01, -1.0489e-01, -6.3354e-02, -2.5072e-01,
         -2.2850e-01, -1.9215e-01, -2.7560e-01, -5.5963e-02, -1.2828e-01,
          1.2989e-01,  8.0240e-02,  1.1768e-01,  3.6903e-01,  3.2748e-01,
          2.8244e-01,  7.6465e-02, -1.3270e-01,  1.6062e-01,  1.6020e-02,
          1.8847e-01, -1.0488e-01, -2.7101e-01,  2.8572e-01,  2.2558e-01,
         -1.0301e-01,  1.4395e-01,  1.6343e-01, -2.3641e-01, -1.2907e-01,
         -2.7267e-02,  2.6302e-01, -1.8550e-01,  2.4225e-01,  1.8058e-01,
         -2.0332e-01, -1.1652e-01,  2.2879e-01, -6.5583e-02,  3.0162e-02,
          5.5261e-02,  3.3168e-02,  1.4287e-03,  2.4233e-01, -2.5623e-01,
          1.4555e-01, -9.7221e-02, -1.4414e-01,  2.5351e-01, -1.6486e-01,
          8.8751e-03, -1.3138e-01, -5.0337e-02, -2.3413e-01, -1.2613e-01,
          1.0165e-01,  1.7950e-01, -4.9932e-02,  1.9990e-01, -2.5763e-01,
         -3.1997e-01, -1.8444e-01, -2.4811e-01, -6.7918e-02,  2.8393e-01,
          3.9342e-02, -3.8641e-01, -2.4957e-01, -4.2098e-02,  1.2375e-01,
         -2.3292e-01, -1.2942e-01, -9.9817e-02,  2.1444e-02, -2.8142e-02,
         -6.4317e-02, -2.3902e-01, -3.0746e-01, -8.9892e-02, -1.2646e-01,
         -3.2790e-01,  2.3242e-01,  9.5534e-02, -1.0982e-01,  2.6479e-01,
          1.5996e-01, -3.1966e-01, -1.9431e-01,  7.6029e-02, -2.0056e-01,
         -4.4122e-02, -1.9894e-01, -3.2973e-01,  1.0175e-01, -3.6016e-02,
         -1.1808e-01, -1.6042e-01, -2.7750e-01,  4.7654e-02, -6.1258e-02,
         -4.5565e-01, -1.4322e-01, -1.6719e-01,  1.1150e-01, -6.1786e-02,
         -1.3581e-01, -1.2923e-01, -8.6849e-02, -2.0869e-01, -9.5694e-02,
         -3.2178e-01,  4.5601e-02,  9.0810e-02, -3.4218e-01, -1.2418e-01,
         -2.1381e-01, -1.2038e-01, -4.3754e-02,  6.3526e-02,  9.4710e-02,
          1.8775e-02, -2.5421e-01,  9.3967e-03,  2.0880e-01,  1.6493e-01,
         -2.8366e-01, -1.0564e-01,  2.7419e-01]])
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
InputMaker reading metamap...
InputMaker reading DTE lookup table...
InputMaker creating model and tokenizer...
	model name: boychaboy/SNLI_roberta-base
PubmedQARunner on device 0 creating model...
n parms: 201
len(optimizer_grouped_parameters[0]): 201
len(optimizer_grouped_parameters[1]): 0
Performing epoch 0 of 5
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train - epoch: 0/5 iter: 0/15 loss: 1.0655 Acc: 43.7500% F1: 0.407 Time: 0.96s (0.00s)
Fold 6 train - epoch: 0/5 iter: 1/15 loss: 0.9660 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.03s)
Fold 6 train - epoch: 0/5 iter: 2/15 loss: 1.0522 Acc: 50.0000% F1: 0.222 Time: 0.94s (5.87s)
Fold 6 train - epoch: 0/5 iter: 3/15 loss: 0.9728 Acc: 59.3750% F1: 0.253 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/5 iter: 4/15 loss: 1.0551 Acc: 50.0000% F1: 0.222 Time: 0.92s (5.89s)
Fold 6 train - epoch: 0/5 iter: 5/15 loss: 0.8871 Acc: 62.5000% F1: 0.361 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/5 iter: 6/15 loss: 0.8768 Acc: 56.2500% F1: 0.287 Time: 0.93s (6.17s)
Fold 6 train - epoch: 0/5 iter: 7/15 loss: 0.8383 Acc: 53.1250% F1: 0.296 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/5 iter: 8/15 loss: 1.2470 Acc: 50.0000% F1: 0.278 Time: 0.93s (6.28s)
Fold 6 train - epoch: 0/5 iter: 9/15 loss: 1.0746 Acc: 34.3750% F1: 0.200 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/5 iter: 10/15 loss: 1.0657 Acc: 31.2500% F1: 0.202 Time: 0.93s (6.19s)
Fold 6 train - epoch: 0/5 iter: 11/15 loss: 0.9332 Acc: 62.5000% F1: 0.377 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/5 iter: 12/15 loss: 0.9800 Acc: 65.6250% F1: 0.404 Time: 0.93s (6.66s)
Fold 6 train - epoch: 0/5 iter: 13/15 loss: 0.9652 Acc: 53.1250% F1: 0.236 Time: 0.93s (0.02s)
Fold 6 train - epoch: 0/5 iter: 14/15 loss: 0.6596 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 0 train Avg acc: 52.2222% F1: 0.3086 *
*****************************************************
Saving model...
input_ids: torch.Size([32, 512])
attention_mask: torch.Size([32, 512])
token_type_ids: torch.Size([32, 512])
label: torch.Size([32])
item_id: torch.Size([32])
Fold 6 train-dev - epoch: 0/5 iter: 0/2 loss: 0.5885 Acc: 87.5000% F1: 0.467 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 0/5 iter: 1/2 loss: 1.5182 Acc: 0.0000% F1: 0.000 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 0 train-dev Avg acc: 56.0000% F1: 0.2393 *
*********************************************************
Performing epoch 1 of 5
Fold 6 train - epoch: 1/5 iter: 0/15 loss: 0.8797 Acc: 56.2500% F1: 0.240 Time: 0.95s (0.00s)
Fold 6 train - epoch: 1/5 iter: 1/15 loss: 0.8683 Acc: 56.2500% F1: 0.240 Time: 0.93s (0.04s)
Fold 6 train - epoch: 1/5 iter: 2/15 loss: 0.9277 Acc: 50.0000% F1: 0.222 Time: 0.94s (5.78s)
Fold 6 train - epoch: 1/5 iter: 3/15 loss: 0.8518 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.04s)
Fold 6 train - epoch: 1/5 iter: 4/15 loss: 1.0337 Acc: 50.0000% F1: 0.222 Time: 0.93s (5.66s)
Fold 6 train - epoch: 1/5 iter: 5/15 loss: 0.9008 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.04s)
Fold 6 train - epoch: 1/5 iter: 6/15 loss: 0.7929 Acc: 68.7500% F1: 0.272 Time: 0.93s (5.94s)
Fold 6 train - epoch: 1/5 iter: 7/15 loss: 0.8998 Acc: 46.8750% F1: 0.213 Time: 0.94s (0.04s)
Fold 6 train - epoch: 1/5 iter: 8/15 loss: 1.1558 Acc: 53.1250% F1: 0.231 Time: 0.93s (6.07s)
Fold 6 train - epoch: 1/5 iter: 9/15 loss: 1.0311 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.04s)
Fold 6 train - epoch: 1/5 iter: 10/15 loss: 0.9473 Acc: 43.7500% F1: 0.203 Time: 0.93s (5.98s)
Fold 6 train - epoch: 1/5 iter: 11/15 loss: 0.8793 Acc: 56.2500% F1: 0.320 Time: 0.93s (0.03s)
Fold 6 train - epoch: 1/5 iter: 12/15 loss: 0.9491 Acc: 56.2500% F1: 0.292 Time: 0.93s (6.37s)
Fold 6 train - epoch: 1/5 iter: 13/15 loss: 0.9296 Acc: 46.8750% F1: 0.217 Time: 0.93s (0.02s)
Fold 6 train - epoch: 1/5 iter: 14/15 loss: 0.5807 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 1 train Avg acc: 53.7778% F1: 0.2452 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 1/5 iter: 0/2 loss: 0.7506 Acc: 68.7500% F1: 0.487 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 1/5 iter: 1/2 loss: 1.3201 Acc: 11.1111% F1: 0.095 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 1 train-dev Avg acc: 48.0000% F1: 0.2828 *
*********************************************************
Performing epoch 2 of 5
Fold 6 train - epoch: 2/5 iter: 0/15 loss: 0.8522 Acc: 62.5000% F1: 0.383 Time: 0.95s (0.00s)
Fold 6 train - epoch: 2/5 iter: 1/15 loss: 0.8935 Acc: 56.2500% F1: 0.292 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/5 iter: 2/15 loss: 0.8896 Acc: 50.0000% F1: 0.262 Time: 0.95s (6.07s)
Fold 6 train - epoch: 2/5 iter: 3/15 loss: 0.8289 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.03s)
Fold 6 train - epoch: 2/5 iter: 4/15 loss: 0.9340 Acc: 50.0000% F1: 0.222 Time: 0.93s (6.00s)
Fold 6 train - epoch: 2/5 iter: 5/15 loss: 0.8303 Acc: 59.3750% F1: 0.248 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/5 iter: 6/15 loss: 0.7703 Acc: 71.8750% F1: 0.351 Time: 0.93s (6.23s)
Fold 6 train - epoch: 2/5 iter: 7/15 loss: 0.8772 Acc: 50.0000% F1: 0.257 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/5 iter: 8/15 loss: 1.1831 Acc: 53.1250% F1: 0.231 Time: 0.93s (6.43s)
Fold 6 train - epoch: 2/5 iter: 9/15 loss: 1.0874 Acc: 46.8750% F1: 0.213 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/5 iter: 10/15 loss: 0.9680 Acc: 46.8750% F1: 0.213 Time: 0.93s (6.27s)
Fold 6 train - epoch: 2/5 iter: 11/15 loss: 0.7970 Acc: 65.6250% F1: 0.327 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/5 iter: 12/15 loss: 0.9273 Acc: 59.3750% F1: 0.306 Time: 0.93s (6.72s)
Fold 6 train - epoch: 2/5 iter: 13/15 loss: 0.8865 Acc: 59.3750% F1: 0.300 Time: 0.93s (0.02s)
Fold 6 train - epoch: 2/5 iter: 14/15 loss: 0.4157 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 2 train Avg acc: 56.6667% F1: 0.2791 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 2/5 iter: 0/2 loss: 0.7122 Acc: 75.0000% F1: 0.526 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 2/5 iter: 1/2 loss: 1.4210 Acc: 5.5556% F1: 0.051 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 2 train-dev Avg acc: 50.0000% F1: 0.2739 *
*********************************************************
Performing epoch 3 of 5
Fold 6 train - epoch: 3/5 iter: 0/15 loss: 0.7908 Acc: 59.3750% F1: 0.344 Time: 0.94s (0.00s)
Fold 6 train - epoch: 3/5 iter: 1/15 loss: 0.8187 Acc: 71.8750% F1: 0.584 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/5 iter: 2/15 loss: 0.8718 Acc: 53.1250% F1: 0.275 Time: 0.94s (6.13s)
Fold 6 train - epoch: 3/5 iter: 3/15 loss: 0.7878 Acc: 62.5000% F1: 0.310 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/5 iter: 4/15 loss: 0.8596 Acc: 56.2500% F1: 0.352 Time: 0.93s (5.93s)
Fold 6 train - epoch: 3/5 iter: 5/15 loss: 0.8151 Acc: 62.5000% F1: 0.387 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/5 iter: 6/15 loss: 0.7230 Acc: 78.1250% F1: 0.469 Time: 0.93s (6.27s)
Fold 6 train - epoch: 3/5 iter: 7/15 loss: 0.8049 Acc: 46.8750% F1: 0.244 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/5 iter: 8/15 loss: 1.0702 Acc: 50.0000% F1: 0.222 Time: 0.93s (6.30s)
Fold 6 train - epoch: 3/5 iter: 9/15 loss: 1.0159 Acc: 46.8750% F1: 0.252 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/5 iter: 10/15 loss: 0.9257 Acc: 46.8750% F1: 0.287 Time: 0.93s (6.19s)
Fold 6 train - epoch: 3/5 iter: 11/15 loss: 0.7456 Acc: 65.6250% F1: 0.412 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/5 iter: 12/15 loss: 0.8938 Acc: 50.0000% F1: 0.227 Time: 0.93s (6.62s)
Fold 6 train - epoch: 3/5 iter: 13/15 loss: 0.8708 Acc: 62.5000% F1: 0.353 Time: 0.93s (0.02s)
Fold 6 train - epoch: 3/5 iter: 14/15 loss: 0.1665 Acc: 100.0000% F1: 1.000 Time: 0.08s (0.02s)
*****************************************************
* Fold 6 Epoch 3 train Avg acc: 58.2222% F1: 0.3486 *
*****************************************************
Saving model...
Fold 6 train-dev - epoch: 3/5 iter: 0/2 loss: 0.8172 Acc: 62.5000% F1: 0.339 Time: 0.34s (0.00s)
Fold 6 train-dev - epoch: 3/5 iter: 1/2 loss: 1.3597 Acc: 16.6667% F1: 0.125 Time: 0.18s (0.00s)
*********************************************************
* Fold 6 Epoch 3 train-dev Avg acc: 46.0000% F1: 0.3009 *
*********************************************************
Performing epoch 4 of 5
